cpu-bind=MASK - ixb, task  0  0 [1158836]: mask 0x3ffff3fff00000000000000000003ffff3fff00000000000000000 set
INFO 07-07 14:54:44 [__init__.py:248] Automatically detected platform cuda.
Using GaMSTaskAdapter for model: DarioVajda/GaMS-DPO-Translator
Number of examples: 1000
Using Hugging Face model ID: DarioVajda/GaMS-DPO-Translator
INFO 07-07 14:55:02 [config.py:752] This model supports multiple tasks: {'embed', 'score', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.
INFO 07-07 14:55:02 [config.py:2064] Chunked prefill is enabled with max_num_batched_tokens=16384.
/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
INFO 07-07 14:55:05 [core.py:61] Initializing a V1 LLM engine (v0.8.5.dev558+g9f64e9341.d20250509) with config: model='DarioVajda/GaMS-DPO-Translator', speculative_config=None, tokenizer='DarioVajda/GaMS-DPO-Translator', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=DarioVajda/GaMS-DPO-Translator, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
2025-07-07 14:55:05,303 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend
WARNING 07-07 14:55:05 [utils.py:2587] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x78c71691d790>
ERROR 07-07 14:55:06 [core.py:412] EngineCore failed to start.
ERROR 07-07 14:55:06 [core.py:412] Traceback (most recent call last):
ERROR 07-07 14:55:06 [core.py:412]   File "/blackwell_install_dir/vllm/vllm/v1/engine/core.py", line 403, in run_engine_core
ERROR 07-07 14:55:06 [core.py:412]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 07-07 14:55:06 [core.py:412]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-07 14:55:06 [core.py:412]   File "/blackwell_install_dir/vllm/vllm/v1/engine/core.py", line 345, in __init__
ERROR 07-07 14:55:06 [core.py:412]     super().__init__(vllm_config, executor_class, log_stats,
ERROR 07-07 14:55:06 [core.py:412]   File "/blackwell_install_dir/vllm/vllm/v1/engine/core.py", line 67, in __init__
ERROR 07-07 14:55:06 [core.py:412]     self.model_executor = executor_class(vllm_config)
ERROR 07-07 14:55:06 [core.py:412]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-07 14:55:06 [core.py:412]   File "/blackwell_install_dir/vllm/vllm/executor/executor_base.py", line 52, in __init__
ERROR 07-07 14:55:06 [core.py:412]     self._init_executor()
ERROR 07-07 14:55:06 [core.py:412]   File "/blackwell_install_dir/vllm/vllm/executor/uniproc_executor.py", line 46, in _init_executor
ERROR 07-07 14:55:06 [core.py:412]     self.collective_rpc("init_device")
ERROR 07-07 14:55:06 [core.py:412]   File "/blackwell_install_dir/vllm/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
ERROR 07-07 14:55:06 [core.py:412]     answer = run_method(self.driver_worker, method, args, kwargs)
ERROR 07-07 14:55:06 [core.py:412]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-07 14:55:06 [core.py:412]   File "/blackwell_install_dir/vllm/vllm/utils.py", line 2521, in run_method
ERROR 07-07 14:55:06 [core.py:412]     return func(*args, **kwargs)
ERROR 07-07 14:55:06 [core.py:412]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 07-07 14:55:06 [core.py:412]   File "/blackwell_install_dir/vllm/vllm/worker/worker_base.py", line 604, in init_device
ERROR 07-07 14:55:06 [core.py:412]     self.worker.init_device()  # type: ignore
ERROR 07-07 14:55:06 [core.py:412]     ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-07 14:55:06 [core.py:412]   File "/blackwell_install_dir/vllm/vllm/v1/worker/gpu_worker.py", line 131, in init_device
ERROR 07-07 14:55:06 [core.py:412]     self.init_gpu_memory = torch.cuda.mem_get_info()[0]
ERROR 07-07 14:55:06 [core.py:412]                            ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-07 14:55:06 [core.py:412]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 738, in mem_get_info
ERROR 07-07 14:55:06 [core.py:412]     return torch.cuda.cudart().cudaMemGetInfo(device)
ERROR 07-07 14:55:06 [core.py:412]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 07-07 14:55:06 [core.py:412] RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
ERROR 07-07 14:55:06 [core.py:412] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
ERROR 07-07 14:55:06 [core.py:412] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
ERROR 07-07 14:55:06 [core.py:412] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
ERROR 07-07 14:55:06 [core.py:412] 
Process EngineCore_0:
Traceback (most recent call last):
  File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/blackwell_install_dir/vllm/vllm/v1/engine/core.py", line 416, in run_engine_core
    raise e
  File "/blackwell_install_dir/vllm/vllm/v1/engine/core.py", line 403, in run_engine_core
    engine_core = EngineCoreProc(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/blackwell_install_dir/vllm/vllm/v1/engine/core.py", line 345, in __init__
    super().__init__(vllm_config, executor_class, log_stats,
  File "/blackwell_install_dir/vllm/vllm/v1/engine/core.py", line 67, in __init__
    self.model_executor = executor_class(vllm_config)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/blackwell_install_dir/vllm/vllm/executor/executor_base.py", line 52, in __init__
    self._init_executor()
  File "/blackwell_install_dir/vllm/vllm/executor/uniproc_executor.py", line 46, in _init_executor
    self.collective_rpc("init_device")
  File "/blackwell_install_dir/vllm/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
    answer = run_method(self.driver_worker, method, args, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/blackwell_install_dir/vllm/vllm/utils.py", line 2521, in run_method
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/blackwell_install_dir/vllm/vllm/worker/worker_base.py", line 604, in init_device
    self.worker.init_device()  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/blackwell_install_dir/vllm/vllm/v1/worker/gpu_worker.py", line 131, in init_device
    self.init_gpu_memory = torch.cuda.mem_get_info()[0]
                           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py", line 738, in mem_get_info
    return torch.cuda.cudart().cudaMemGetInfo(device)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: CUDA-capable device(s) is/are busy or unavailable
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/ceph/hpc/data/s24o01-42-users/translation_optimization/get_translations/translate_ccnews.py", line 153, in <module>
    correct_examples(args.model_path, args.input_path, args.output_path, args.gpu_memory_util, args.tp_size, args.id)
  File "/ceph/hpc/data/s24o01-42-users/translation_optimization/get_translations/translate_ccnews.py", line 50, in correct_examples
    model = LLM(
            ^^^^
  File "/blackwell_install_dir/vllm/vllm/utils.py", line 1171, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/blackwell_install_dir/vllm/vllm/entrypoints/llm.py", line 245, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/blackwell_install_dir/vllm/vllm/engine/llm_engine.py", line 508, in from_engine_args
    return engine_cls.from_vllm_config(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/blackwell_install_dir/vllm/vllm/v1/engine/llm_engine.py", line 112, in from_vllm_config
    return cls(vllm_config=vllm_config,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/blackwell_install_dir/vllm/vllm/v1/engine/llm_engine.py", line 92, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/blackwell_install_dir/vllm/vllm/v1/engine/core_client.py", line 74, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/blackwell_install_dir/vllm/vllm/v1/engine/core_client.py", line 502, in __init__
    super().__init__(
  File "/blackwell_install_dir/vllm/vllm/v1/engine/core_client.py", line 400, in __init__
    self._wait_for_engine_startup()
  File "/blackwell_install_dir/vllm/vllm/v1/engine/core_client.py", line 432, in _wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above.
