cpu-bind=MASK - gn11, task  0  0 [311642]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 0 ---
Total Nodes: 2
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn11
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 2     --machine_rank 0     --main_process_ip gn11     --main_process_port 29500     --num_processes 8     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62083146     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train.py"     --rank=64 --learning_rate=3e-7 --total_epochs=3 --beta=0.2
-------------------------------------------
[2025-05-31 01:12:47,313] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0531 01:12:48.682000 311693 torch/distributed/run.py:792] 
W0531 01:12:48.682000 311693 torch/distributed/run.py:792] *****************************************
W0531 01:12:48.682000 311693 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0531 01:12:48.682000 311693 torch/distributed/run.py:792] *****************************************
[2025-05-31 01:12:53,738] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 01:12:53,750] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 01:12:53,794] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 01:12:53,798] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
World size: 8
Setting gradient accumulation steps to: 2
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
[2025-05-31 01:12:56,311] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-31 01:12:56,318] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Steps per epoch: 4282
Eval steps: 2141
[2025-05-31 01:12:56,321] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-31 01:12:56,321] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-31 01:12:56,323] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
[2025-05-31 01:12:56,883] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
hpZeRO group size: 4
[2025-05-31 01:12:57,131] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-31 01:12:57,133] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-31 01:12:57,351] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-31 01:13:11,444] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 465, num_elems = 10.16B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.47s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.47s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.47s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.44s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
Using LoRA and set up the model
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5415.46 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1113/8564 [00:00<00:01, 5508.20 examples/s]Extracting prompt in train dataset:  20%|█▉        | 1670/8564 [00:00<00:01, 5513.69 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2239/8564 [00:00<00:01, 5578.67 examples/s]Extracting prompt in train dataset:  33%|███▎      | 2807/8564 [00:00<00:01, 5599.70 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3620/8564 [00:00<00:00, 5510.86 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4190/8564 [00:00<00:00, 5550.76 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 4757/8564 [00:00<00:00, 5585.42 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 5330/8564 [00:00<00:00, 5626.90 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 5910/8564 [00:01<00:00, 5672.13 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 6490/8564 [00:01<00:00, 5705.01 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 7070/8564 [00:01<00:00, 5720.90 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 7650/8564 [00:01<00:00, 5735.92 examples/s]Extracting prompt in train dataset:  99%|█████████▊| 8438/8564 [00:01<00:00, 5546.56 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5539.65 examples/s]
Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   3%|▎         | 280/8564 [00:00<00:02, 2770.32 examples/s]Applying chat template to train dataset:   7%|▋         | 586/8564 [00:00<00:02, 2938.60 examples/s]Applying chat template to train dataset:  10%|█         | 890/8564 [00:00<00:02, 2981.59 examples/s]Applying chat template to train dataset:  14%|█▍        | 1194/8564 [00:00<00:02, 3002.36 examples/s]Applying chat template to train dataset:  17%|█▋        | 1497/8564 [00:00<00:02, 3006.74 examples/s]Applying chat template to train dataset:  21%|██        | 1805/8564 [00:00<00:02, 3026.75 examples/s]Applying chat template to train dataset:  25%|██▍       | 2115/8564 [00:00<00:02, 3044.14 examples/s]Applying chat template to train dataset:  28%|██▊       | 2425/8564 [00:00<00:02, 3056.41 examples/s]Applying chat template to train dataset:  32%|███▏      | 2735/8564 [00:00<00:01, 3063.93 examples/s]Applying chat template to train dataset:  36%|███▌      | 3046/8564 [00:01<00:01, 3070.97 examples/s]Applying chat template to train dataset:  41%|████      | 3490/8564 [00:01<00:01, 3020.00 examples/s]Applying chat template to train dataset:  44%|████▍     | 3799/8564 [00:01<00:01, 3037.90 examples/s]Applying chat template to train dataset:  48%|████▊     | 4106/8564 [00:01<00:01, 3045.20 examples/s]Applying chat template to train dataset:  52%|█████▏    | 4415/8564 [00:01<00:01, 3052.23 examples/s]Applying chat template to train dataset:  55%|█████▌    | 4721/8564 [00:01<00:01, 3054.28 examples/s]Applying chat template to train dataset:  59%|█████▉    | 5034/8564 [00:01<00:01, 3074.99 examples/s]Applying chat template to train dataset:  63%|██████▎   | 5413/8564 [00:01<00:01, 2858.96 examples/s]Applying chat template to train dataset:  67%|██████▋   | 5729/8564 [00:01<00:00, 2937.34 examples/s]Applying chat template to train dataset:  71%|███████   | 6043/8564 [00:02<00:00, 2991.82 examples/s]Applying chat template to train dataset:  74%|███████▍  | 6359/8564 [00:02<00:00, 3037.98 examples/s]Applying chat template to train dataset:  78%|███████▊  | 6672/8564 [00:02<00:00, 3062.94 examples/s]Applying chat template to train dataset:  82%|████████▏ | 6989/8564 [00:02<00:00, 3089.14 examples/s]Applying chat template to train dataset:  85%|████████▌ | 7303/8564 [00:02<00:00, 3101.01 examples/s]Applying chat template to train dataset:  89%|████████▉ | 7619/8564 [00:02<00:00, 3116.64 examples/s]Applying chat template to train dataset:  94%|█████████▍| 8064/8564 [00:02<00:00, 3008.45 examples/s]Applying chat template to train dataset:  98%|█████████▊| 8380/8564 [00:02<00:00, 3042.51 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3024.58 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 41/8564 [00:00<00:21, 403.92 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:25, 337.31 examples/s]Tokenizing train dataset:   2%|▏         | 135/8564 [00:00<00:26, 318.01 examples/s]Tokenizing train dataset:   2%|▏         | 179/8564 [00:00<00:27, 303.97 examples/s]Tokenizing train dataset:   2%|▏         | 212/8564 [00:00<00:27, 308.09 examples/s]Tokenizing train dataset:   3%|▎         | 247/8564 [00:00<00:26, 318.35 examples/s]Tokenizing train dataset:   3%|▎         | 282/8564 [00:00<00:25, 326.04 examples/s]Tokenizing train dataset:   4%|▍         | 332/8564 [00:01<00:25, 322.11 examples/s]Tokenizing train dataset:   4%|▍         | 381/8564 [00:01<00:25, 319.94 examples/s]Tokenizing train dataset:   5%|▌         | 430/8564 [00:01<00:25, 319.91 examples/s]Tokenizing train dataset:   6%|▌         | 476/8564 [00:01<00:26, 308.13 examples/s]Tokenizing train dataset:   6%|▌         | 524/8564 [00:01<00:26, 305.41 examples/s]Tokenizing train dataset:   7%|▋         | 557/8564 [00:01<00:25, 309.80 examples/s]Tokenizing train dataset:   7%|▋         | 602/8564 [00:01<00:26, 303.62 examples/s]Tokenizing train dataset:   7%|▋         | 640/8564 [00:02<00:24, 317.69 examples/s]Tokenizing train dataset:   8%|▊         | 686/8564 [00:02<00:25, 312.06 examples/s]Tokenizing train dataset:   9%|▊         | 735/8564 [00:02<00:25, 313.14 examples/s]Tokenizing train dataset:   9%|▉         | 780/8564 [00:02<00:25, 304.02 examples/s]Tokenizing train dataset:  10%|▉         | 824/8564 [00:02<00:26, 295.95 examples/s]Tokenizing train dataset:  10%|▉         | 854/8564 [00:02<00:26, 294.39 examples/s]Tokenizing train dataset:  10%|█         | 890/8564 [00:02<00:24, 307.54 examples/s]Tokenizing train dataset:  11%|█         | 922/8564 [00:02<00:25, 303.55 examples/s]Tokenizing train dataset:  11%|█         | 953/8564 [00:03<00:25, 304.22 examples/s]Tokenizing train dataset:  12%|█▏        | 997/8564 [00:03<00:25, 294.64 examples/s]Tokenizing train dataset:  12%|█▏        | 1027/8564 [00:03<00:26, 289.76 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 291.44 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 305.09 examples/s]Tokenizing train dataset:  13%|█▎        | 1149/8564 [00:03<00:26, 284.73 examples/s]Tokenizing train dataset:  14%|█▍        | 1197/8564 [00:03<00:25, 292.57 examples/s]Tokenizing train dataset:  14%|█▍        | 1229/8564 [00:04<00:24, 295.08 examples/s]Tokenizing train dataset:  15%|█▍        | 1265/8564 [00:04<00:23, 306.67 examples/s]Tokenizing train dataset:  15%|█▌        | 1298/8564 [00:04<00:23, 306.64 examples/s]Tokenizing train dataset:  16%|█▌        | 1330/8564 [00:04<00:23, 308.29 examples/s]Tokenizing train dataset:  16%|█▌        | 1373/8564 [00:04<00:24, 292.88 examples/s]Tokenizing train dataset:  16%|█▋        | 1407/8564 [00:04<00:23, 300.01 examples/s]Tokenizing train dataset:  17%|█▋        | 1453/8564 [00:04<00:23, 296.77 examples/s]Tokenizing train dataset:  17%|█▋        | 1496/8564 [00:04<00:24, 291.64 examples/s]Tokenizing train dataset:  18%|█▊        | 1530/8564 [00:05<00:23, 295.05 examples/s]Tokenizing train dataset:  18%|█▊        | 1561/8564 [00:05<00:23, 295.85 examples/s]Tokenizing train dataset:  19%|█▉        | 1608/8564 [00:05<00:23, 298.78 examples/s]Tokenizing train dataset:  19%|█▉        | 1641/8564 [00:05<00:22, 304.95 examples/s]Tokenizing train dataset:  20%|█▉        | 1676/8564 [00:05<00:22, 311.84 examples/s]Tokenizing train dataset:  20%|██        | 1718/8564 [00:05<00:20, 334.11 examples/s]Tokenizing train dataset:  21%|██        | 1769/8564 [00:05<00:20, 328.70 examples/s]Tokenizing train dataset:  21%|██        | 1815/8564 [00:05<00:21, 318.01 examples/s]Tokenizing train dataset:  22%|██▏       | 1848/8564 [00:06<00:21, 316.69 examples/s]Tokenizing train dataset:  22%|██▏       | 1890/8564 [00:06<00:22, 300.44 examples/s]Tokenizing train dataset:  23%|██▎       | 1935/8564 [00:06<00:19, 333.01 examples/s]Tokenizing train dataset:  23%|██▎       | 1970/8564 [00:06<00:19, 334.73 examples/s]Tokenizing train dataset:  23%|██▎       | 2010/8564 [00:06<00:19, 343.95 examples/s]Tokenizing train dataset:  24%|██▍       | 2050/8564 [00:06<00:18, 354.50 examples/s]Tokenizing train dataset:  24%|██▍       | 2088/8564 [00:06<00:18, 357.31 examples/s]Tokenizing train dataset:  25%|██▍       | 2128/8564 [00:06<00:17, 365.72 examples/s]Tokenizing train dataset:  25%|██▌       | 2180/8564 [00:06<00:17, 357.85 examples/s]Tokenizing train dataset:  26%|██▌       | 2217/8564 [00:07<00:17, 358.58 examples/s]Tokenizing train dataset:  26%|██▋       | 2256/8564 [00:07<00:17, 365.21 examples/s]Tokenizing train dataset:  27%|██▋       | 2299/8564 [00:07<00:16, 379.14 examples/s]Tokenizing train dataset:  27%|██▋       | 2348/8564 [00:07<00:17, 355.01 examples/s]Tokenizing train dataset:  28%|██▊       | 2385/8564 [00:07<00:17, 357.98 examples/s]Tokenizing train dataset:  28%|██▊       | 2426/8564 [00:07<00:16, 369.36 examples/s]Tokenizing train dataset:  29%|██▉       | 2465/8564 [00:07<00:16, 372.10 examples/s]Tokenizing train dataset:  29%|██▉       | 2514/8564 [00:07<00:17, 351.02 examples/s]Tokenizing train dataset:  30%|██▉       | 2558/8564 [00:07<00:16, 371.57 examples/s]Tokenizing train dataset:  30%|███       | 2596/8564 [00:08<00:16, 370.30 examples/s]Tokenizing train dataset:  31%|███       | 2644/8564 [00:08<00:17, 344.60 examples/s]Tokenizing train dataset:  31%|███▏      | 2694/8564 [00:08<00:17, 335.04 examples/s]Tokenizing train dataset:  32%|███▏      | 2733/8564 [00:08<00:16, 344.30 examples/s]Tokenizing train dataset:  33%|███▎      | 2787/8564 [00:08<00:16, 345.22 examples/s]Tokenizing train dataset:  33%|███▎      | 2828/8564 [00:08<00:16, 355.08 examples/s]Tokenizing train dataset:  34%|███▎      | 2877/8564 [00:08<00:16, 339.87 examples/s]Tokenizing train dataset:  34%|███▍      | 2923/8564 [00:09<00:15, 365.04 examples/s]Tokenizing train dataset:  35%|███▍      | 2964/8564 [00:09<00:14, 374.75 examples/s]Tokenizing train dataset:  35%|███▌      | 3007/8564 [00:09<00:14, 383.02 examples/s]Tokenizing train dataset:  36%|███▌      | 3058/8564 [00:09<00:15, 364.89 examples/s]Tokenizing train dataset:  36%|███▌      | 3097/8564 [00:09<00:14, 369.90 examples/s]Tokenizing train dataset:  37%|███▋      | 3150/8564 [00:09<00:14, 361.66 examples/s]Tokenizing train dataset:  37%|███▋      | 3205/8564 [00:09<00:14, 359.87 examples/s]Tokenizing train dataset:  38%|███▊      | 3247/8564 [00:09<00:14, 369.95 examples/s]Tokenizing train dataset:  39%|███▊      | 3300/8564 [00:10<00:14, 357.44 examples/s]Tokenizing train dataset:  39%|███▉      | 3355/8564 [00:10<00:14, 353.98 examples/s]Tokenizing train dataset:  40%|███▉      | 3396/8564 [00:10<00:14, 365.44 examples/s]Tokenizing train dataset:  40%|████      | 3437/8564 [00:10<00:13, 371.94 examples/s]Tokenizing train dataset:  41%|████      | 3491/8564 [00:10<00:13, 362.57 examples/s]Tokenizing train dataset:  41%|████▏     | 3533/8564 [00:10<00:13, 373.44 examples/s]Tokenizing train dataset:  42%|████▏     | 3590/8564 [00:10<00:13, 369.10 examples/s]Tokenizing train dataset:  42%|████▏     | 3631/8564 [00:10<00:13, 371.06 examples/s]Tokenizing train dataset:  43%|████▎     | 3680/8564 [00:11<00:13, 349.45 examples/s]Tokenizing train dataset:  43%|████▎     | 3724/8564 [00:11<00:13, 367.28 examples/s]Tokenizing train dataset:  44%|████▍     | 3780/8564 [00:11<00:13, 365.78 examples/s]Tokenizing train dataset:  45%|████▍     | 3834/8564 [00:11<00:13, 359.94 examples/s]Tokenizing train dataset:  45%|████▌     | 3890/8564 [00:11<00:13, 358.53 examples/s]Tokenizing train dataset:  46%|████▌     | 3940/8564 [00:11<00:13, 348.47 examples/s]Tokenizing train dataset:  46%|████▋     | 3978/8564 [00:11<00:13, 349.98 examples/s]Tokenizing train dataset:  47%|████▋     | 4014/8564 [00:12<00:12, 351.34 examples/s]Tokenizing train dataset:  47%|████▋     | 4054/8564 [00:12<00:12, 359.81 examples/s]Tokenizing train dataset:  48%|████▊     | 4110/8564 [00:12<00:12, 355.78 examples/s]Tokenizing train dataset:  49%|████▊     | 4162/8564 [00:12<00:12, 348.91 examples/s]Tokenizing train dataset:  49%|████▉     | 4203/8564 [00:12<00:12, 359.21 examples/s]Tokenizing train dataset:  50%|████▉     | 4257/8564 [00:12<00:12, 354.34 examples/s]Tokenizing train dataset:  50%|█████     | 4293/8564 [00:12<00:12, 353.10 examples/s]Tokenizing train dataset:  51%|█████     | 4330/8564 [00:12<00:11, 353.55 examples/s]Tokenizing train dataset:  51%|█████     | 4386/8564 [00:13<00:11, 356.65 examples/s]Tokenizing train dataset:  52%|█████▏    | 4425/8564 [00:13<00:11, 359.38 examples/s]Tokenizing train dataset:  52%|█████▏    | 4482/8564 [00:13<00:11, 359.53 examples/s]Tokenizing train dataset:  53%|█████▎    | 4536/8564 [00:13<00:11, 354.66 examples/s]Tokenizing train dataset:  53%|█████▎    | 4572/8564 [00:13<00:11, 352.29 examples/s]Tokenizing train dataset:  54%|█████▍    | 4609/8564 [00:13<00:11, 352.58 examples/s]Tokenizing train dataset:  54%|█████▍    | 4657/8564 [00:13<00:11, 340.82 examples/s]Tokenizing train dataset:  55%|█████▍    | 4702/8564 [00:14<00:11, 324.91 examples/s]Tokenizing train dataset:  55%|█████▌    | 4738/8564 [00:14<00:11, 329.46 examples/s]Tokenizing train dataset:  56%|█████▌    | 4782/8564 [00:14<00:12, 314.36 examples/s]Tokenizing train dataset:  56%|█████▋    | 4836/8564 [00:14<00:10, 367.91 examples/s]Tokenizing train dataset:  57%|█████▋    | 4894/8564 [00:14<00:08, 418.76 examples/s]Tokenizing train dataset:  58%|█████▊    | 4959/8564 [00:14<00:07, 476.41 examples/s]Tokenizing train dataset:  59%|█████▊    | 5017/8564 [00:14<00:07, 501.85 examples/s]Tokenizing train dataset:  59%|█████▉    | 5086/8564 [00:14<00:06, 549.25 examples/s]Tokenizing train dataset:  60%|██████    | 5152/8564 [00:14<00:05, 577.67 examples/s]Tokenizing train dataset:  61%|██████    | 5223/8564 [00:14<00:05, 614.29 examples/s]Tokenizing train dataset:  62%|██████▏   | 5298/8564 [00:15<00:05, 651.08 examples/s]Tokenizing train dataset:  63%|██████▎   | 5383/8564 [00:15<00:05, 615.32 examples/s]Tokenizing train dataset:  64%|██████▎   | 5451/8564 [00:15<00:04, 626.27 examples/s]Tokenizing train dataset:  65%|██████▍   | 5537/8564 [00:15<00:05, 602.86 examples/s]Tokenizing train dataset:  65%|██████▌   | 5601/8564 [00:15<00:04, 607.08 examples/s]Tokenizing train dataset:  66%|██████▌   | 5669/8564 [00:15<00:04, 623.23 examples/s]Tokenizing train dataset:  67%|██████▋   | 5772/8564 [00:15<00:04, 642.31 examples/s]Tokenizing train dataset:  68%|██████▊   | 5842/8564 [00:15<00:04, 647.65 examples/s]Tokenizing train dataset:  69%|██████▉   | 5928/8564 [00:16<00:04, 620.43 examples/s]Tokenizing train dataset:  70%|███████   | 6014/8564 [00:16<00:04, 595.97 examples/s]Tokenizing train dataset:  71%|███████   | 6099/8564 [00:16<00:04, 583.45 examples/s]Tokenizing train dataset:  72%|███████▏  | 6173/8564 [00:16<00:03, 619.44 examples/s]Tokenizing train dataset:  73%|███████▎  | 6241/8564 [00:16<00:03, 631.89 examples/s]Tokenizing train dataset:  74%|███████▎  | 6307/8564 [00:16<00:03, 635.88 examples/s]Tokenizing train dataset:  74%|███████▍  | 6379/8564 [00:16<00:03, 656.23 examples/s]Tokenizing train dataset:  76%|███████▌  | 6470/8564 [00:16<00:03, 630.10 examples/s]Tokenizing train dataset:  76%|███████▋  | 6535/8564 [00:17<00:03, 625.00 examples/s]Tokenizing train dataset:  77%|███████▋  | 6613/8564 [00:17<00:03, 576.01 examples/s]Tokenizing train dataset:  78%|███████▊  | 6679/8564 [00:17<00:03, 595.51 examples/s]Tokenizing train dataset:  79%|███████▉  | 6749/8564 [00:17<00:02, 618.78 examples/s]Tokenizing train dataset:  80%|███████▉  | 6835/8564 [00:17<00:02, 599.69 examples/s]Tokenizing train dataset:  81%|████████  | 6899/8564 [00:17<00:02, 608.97 examples/s]Tokenizing train dataset:  81%|████████▏ | 6964/8564 [00:17<00:02, 614.49 examples/s]Tokenizing train dataset:  82%|████████▏ | 7052/8564 [00:17<00:02, 599.53 examples/s]Tokenizing train dataset:  83%|████████▎ | 7120/8564 [00:18<00:02, 613.84 examples/s]Tokenizing train dataset:  84%|████████▍ | 7187/8564 [00:18<00:02, 625.09 examples/s]Tokenizing train dataset:  85%|████████▌ | 7280/8564 [00:18<00:02, 616.08 examples/s]Tokenizing train dataset:  86%|████████▌ | 7376/8564 [00:18<00:01, 615.87 examples/s]Tokenizing train dataset:  87%|████████▋ | 7442/8564 [00:18<00:01, 625.31 examples/s]Tokenizing train dataset:  88%|████████▊ | 7538/8564 [00:18<00:01, 627.72 examples/s]Tokenizing train dataset:  89%|████████▉ | 7609/8564 [00:18<00:01, 642.09 examples/s]Tokenizing train dataset:  90%|████████▉ | 7695/8564 [00:18<00:01, 611.03 examples/s]Tokenizing train dataset:  91%|█████████ | 7780/8564 [00:19<00:01, 595.27 examples/s]Tokenizing train dataset:  92%|█████████▏| 7870/8564 [00:19<00:01, 592.71 examples/s]Tokenizing train dataset:  93%|█████████▎| 7933/8564 [00:19<00:01, 595.79 examples/s]Tokenizing train dataset:  93%|█████████▎| 7999/8564 [00:19<00:00, 610.86 examples/s]Tokenizing train dataset:  94%|█████████▍| 8062/8564 [00:19<00:00, 611.72 examples/s]Tokenizing train dataset:  95%|█████████▌| 8144/8564 [00:19<00:00, 582.76 examples/s]Tokenizing train dataset:  96%|█████████▌| 8209/8564 [00:19<00:00, 591.50 examples/s]Tokenizing train dataset:  97%|█████████▋| 8286/8564 [00:19<00:00, 635.92 examples/s]Tokenizing train dataset:  98%|█████████▊| 8373/8564 [00:20<00:00, 610.41 examples/s]Tokenizing train dataset:  99%|█████████▉| 8464/8564 [00:20<00:00, 604.60 examples/s]Tokenizing train dataset: 100%|█████████▉| 8530/8564 [00:20<00:00, 612.56 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 418.98 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11226.88 examples/s]
Extracting prompt in train dataset:   6%|▌         | 531/8564 [00:00<00:01, 5267.86 examples/s]Extracting prompt in train dataset:   6%|▋         | 540/8564 [00:00<00:01, 5312.99 examples/s]Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5403.27 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1080/8564 [00:00<00:01, 5378.88 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1090/8564 [00:00<00:01, 5409.11 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1112/8564 [00:00<00:01, 5512.71 examples/s]Extracting prompt in train dataset:  20%|█▉        | 1670/8564 [00:00<00:01, 5535.54 examples/s]Extracting prompt in train dataset:  19%|█▉        | 1630/8564 [00:00<00:01, 5401.58 examples/s]Extracting prompt in train dataset:  19%|█▉        | 1640/8564 [00:00<00:01, 5434.50 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2240/8564 [00:00<00:01, 5587.25 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2200/8564 [00:00<00:01, 5481.56 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2190/8564 [00:00<00:01, 5455.54 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  33%|███▎      | 2810/8564 [00:00<00:01, 5618.00 examples/s]Extracting prompt in train dataset:  32%|███▏      | 2760/8564 [00:00<00:01, 5509.20 examples/s]Extracting prompt in train dataset:  32%|███▏      | 2750/8564 [00:00<00:01, 5485.01 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13670.73 examples/s]
Extracting prompt in train dataset:  42%|████▏     | 3560/8564 [00:00<00:00, 5423.26 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3639/8564 [00:00<00:00, 5553.67 examples/s]Extracting prompt in train dataset:  41%|████▏     | 3550/8564 [00:00<00:00, 5404.52 examples/s]Extracting prompt in train dataset:  48%|████▊     | 4120/8564 [00:00<00:00, 5460.85 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4204/8564 [00:00<00:00, 5567.70 examples/s]Extracting prompt in train dataset:  48%|████▊     | 4110/8564 [00:00<00:00, 5439.39 examples/s]Extracting prompt in train dataset:  55%|█████▍    | 4680/8564 [00:00<00:00, 5488.80 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 4777/8564 [00:00<00:00, 5598.39 examples/s]Extracting prompt in train dataset:  55%|█████▍    | 4670/8564 [00:00<00:00, 5464.80 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  63%|██████▎   | 5355/8564 [00:00<00:00, 5650.52 examples/s]Extracting prompt in train dataset:  61%|██████▏   | 5250/8564 [00:00<00:00, 5535.13 examples/s]Extracting prompt in train dataset:  61%|██████    | 5237/8564 [00:00<00:00, 5518.83 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 325.00 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 5930/8564 [00:01<00:00, 5678.03 examples/s]Extracting prompt in train dataset:  68%|██████▊   | 5820/8564 [00:01<00:00, 5575.31 examples/s]Extracting prompt in train dataset:  68%|██████▊   | 5807/8564 [00:01<00:00, 5558.96 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 6510/8564 [00:01<00:00, 5708.88 examples/s]Extracting prompt in train dataset:  75%|███████▍  | 6390/8564 [00:01<00:00, 5603.88 examples/s]Extracting prompt in train dataset:  74%|███████▍  | 6379/8564 [00:01<00:00, 5590.78 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 295.09 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 7090/8564 [00:01<00:00, 5727.69 examples/s]Extracting prompt in train dataset:  81%|████████▏ | 6960/8564 [00:01<00:00, 5620.81 examples/s]Extracting prompt in train dataset:  81%|████████  | 6947/8564 [00:01<00:00, 5603.93 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:03, 276.60 examples/s]Extracting prompt in train dataset:  90%|████████▉ | 7670/8564 [00:01<00:00, 5744.65 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 7530/8564 [00:01<00:00, 5633.66 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 7517/8564 [00:01<00:00, 5617.51 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 267.14 examples/s]Extracting prompt in train dataset:  99%|█████████▉| 8460/8564 [00:01<00:00, 5541.85 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 8300/8564 [00:01<00:00, 5394.92 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 8300/8564 [00:01<00:00, 5373.76 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5572.22 examples/s]
Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5458.22 examples/s]
Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5437.48 examples/s]
Tokenizing eval dataset:  21%|██        | 196/953 [00:00<00:02, 256.77 examples/s]Tokenizing eval dataset:  25%|██▍       | 235/953 [00:00<00:02, 287.12 examples/s]Tokenizing eval dataset:  32%|███▏      | 303/953 [00:00<00:01, 386.42 examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing eval dataset:  38%|███▊      | 364/953 [00:01<00:01, 445.80 examples/s]Applying chat template to train dataset:   3%|▎         | 287/8564 [00:00<00:02, 2848.12 examples/s]Applying chat template to train dataset:   3%|▎         | 277/8564 [00:00<00:03, 2745.81 examples/s]Applying chat template to train dataset:   3%|▎         | 284/8564 [00:00<00:02, 2796.59 examples/s]Tokenizing eval dataset:  45%|████▌     | 433/953 [00:01<00:01, 510.66 examples/s]Applying chat template to train dataset:   7%|▋         | 601/8564 [00:00<00:02, 3013.52 examples/s]Applying chat template to train dataset:   7%|▋         | 580/8564 [00:00<00:02, 2900.84 examples/s]Applying chat template to train dataset:   7%|▋         | 597/8564 [00:00<00:02, 2975.88 examples/s]Tokenizing eval dataset:  53%|█████▎    | 501/953 [00:01<00:00, 556.07 examples/s]Applying chat template to train dataset:  11%|█         | 915/8564 [00:00<00:02, 3066.65 examples/s]Applying chat template to train dataset:  10%|█         | 883/8564 [00:00<00:02, 2952.18 examples/s]Applying chat template to train dataset:  11%|█         | 907/8564 [00:00<00:02, 3028.09 examples/s]Tokenizing eval dataset:  59%|█████▉    | 566/953 [00:01<00:00, 576.33 examples/s]Applying chat template to train dataset:  14%|█▍        | 1230/8564 [00:00<00:02, 3089.86 examples/s]Applying chat template to train dataset:  14%|█▍        | 1188/8564 [00:00<00:02, 2984.85 examples/s]Applying chat template to train dataset:  14%|█▍        | 1218/8564 [00:00<00:02, 3055.21 examples/s]Tokenizing eval dataset:  67%|██████▋   | 636/953 [00:01<00:00, 609.71 examples/s]Applying chat template to train dataset:  18%|█▊        | 1540/8564 [00:00<00:02, 3089.84 examples/s]Applying chat template to train dataset:  17%|█▋        | 1488/8564 [00:00<00:02, 2985.33 examples/s]Applying chat template to train dataset:  18%|█▊        | 1525/8564 [00:00<00:02, 3057.47 examples/s]Applying chat template to train dataset:  22%|██▏       | 1855/8564 [00:00<00:02, 3108.13 examples/s]Applying chat template to train dataset:  21%|██        | 1793/8564 [00:00<00:02, 2999.15 examples/s]Tokenizing eval dataset:  76%|███████▌  | 725/953 [00:01<00:00, 598.04 examples/s]Applying chat template to train dataset:  21%|██▏       | 1837/8564 [00:00<00:02, 3073.95 examples/s]Applying chat template to train dataset:  25%|██▌       | 2171/8564 [00:00<00:02, 3122.03 examples/s]Applying chat template to train dataset:  25%|██▍       | 2100/8564 [00:00<00:02, 3013.93 examples/s]Applying chat template to train dataset:  25%|██▌       | 2150/8564 [00:00<00:02, 3087.23 examples/s]Tokenizing eval dataset:  84%|████████▍ | 803/953 [00:01<00:00, 562.02 examples/s]Applying chat template to train dataset:  29%|██▉       | 2489/8564 [00:00<00:01, 3135.82 examples/s]Applying chat template to train dataset:  28%|██▊       | 2407/8564 [00:00<00:02, 3028.26 examples/s]Applying chat template to train dataset:  29%|██▉       | 2464/8564 [00:00<00:01, 3101.50 examples/s]Applying chat template to train dataset:  33%|███▎      | 2805/8564 [00:00<00:01, 3140.11 examples/s]Applying chat template to train dataset:  32%|███▏      | 2713/8564 [00:00<00:01, 3034.08 examples/s]Applying chat template to train dataset:  32%|███▏      | 2777/8564 [00:00<00:01, 3108.66 examples/s]Tokenizing eval dataset:  92%|█████████▏| 881/953 [00:01<00:00, 541.07 examples/s]Applying chat template to train dataset:  35%|███▌      | 3020/8564 [00:01<00:01, 3039.91 examples/s]Applying chat template to train dataset:  38%|███▊      | 3260/8564 [00:01<00:01, 3093.50 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 528.82 examples/s]Applying chat template to train dataset:  38%|███▊      | 3228/8564 [00:01<00:01, 3063.60 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 461.83 examples/s]
Applying chat template to train dataset:  42%|████▏     | 3576/8564 [00:01<00:01, 3108.26 examples/s]Applying chat template to train dataset:  40%|████      | 3459/8564 [00:01<00:01, 2993.34 examples/s]Applying chat template to train dataset:  41%|████▏     | 3540/8564 [00:01<00:01, 3073.91 examples/s]Applying chat template to train dataset:  45%|████▌     | 3890/8564 [00:01<00:01, 3116.30 examples/s]Applying chat template to train dataset:  44%|████▍     | 3763/8564 [00:01<00:01, 3003.85 examples/s]Applying chat template to train dataset:  45%|████▍     | 3852/8564 [00:01<00:01, 3084.16 examples/s]Applying chat template to train dataset:  49%|████▉     | 4206/8564 [00:01<00:01, 3126.54 examples/s]Applying chat template to train dataset:  48%|████▊     | 4068/8564 [00:01<00:01, 3014.75 examples/s]Applying chat template to train dataset:  49%|████▊     | 4163/8564 [00:01<00:01, 3090.60 examples/s]Applying chat template to train dataset:  53%|█████▎    | 4520/8564 [00:01<00:01, 3128.21 examples/s]Applying chat template to train dataset:  51%|█████     | 4373/8564 [00:01<00:01, 3019.16 examples/s]Applying chat template to train dataset:  52%|█████▏    | 4477/8564 [00:01<00:01, 3099.04 examples/s]Applying chat template to train dataset:  56%|█████▋    | 4835/8564 [00:01<00:01, 3132.65 examples/s]Applying chat template to train dataset:  55%|█████▍    | 4678/8564 [00:01<00:01, 3025.12 examples/s]Applying chat template to train dataset:  56%|█████▌    | 4788/8564 [00:01<00:01, 3099.82 examples/s]Applying chat template to train dataset:  60%|██████    | 5157/8564 [00:01<00:01, 3155.99 examples/s]Applying chat template to train dataset:  58%|█████▊    | 4986/8564 [00:01<00:01, 3036.47 examples/s]Applying chat template to train dataset:  60%|█████▉    | 5106/8564 [00:01<00:01, 3120.25 examples/s]Applying chat template to train dataset:  64%|██████▍   | 5479/8564 [00:01<00:00, 3171.70 examples/s]Applying chat template to train dataset:  62%|██████▏   | 5347/8564 [00:01<00:01, 2788.50 examples/s]Applying chat template to train dataset:  64%|██████▍   | 5495/8564 [00:01<00:01, 2912.23 examples/s]Applying chat template to train dataset:  68%|██████▊   | 5802/8564 [00:01<00:00, 3181.59 examples/s]Applying chat template to train dataset:  66%|██████▌   | 5659/8564 [00:01<00:01, 2873.67 examples/s]Applying chat template to train dataset:  68%|██████▊   | 5814/8564 [00:01<00:00, 2985.52 examples/s]Applying chat template to train dataset:  72%|███████▏  | 6124/8564 [00:01<00:00, 3189.03 examples/s]Applying chat template to train dataset:  70%|██████▉   | 5970/8564 [00:02<00:00, 2936.52 examples/s]Applying chat template to train dataset:  72%|███████▏  | 6132/8564 [00:02<00:00, 3037.65 examples/s]Applying chat template to train dataset:  75%|███████▌  | 6447/8564 [00:02<00:00, 3198.22 examples/s]Applying chat template to train dataset:  73%|███████▎  | 6282/8564 [00:02<00:00, 2984.87 examples/s]Applying chat template to train dataset:  75%|███████▌  | 6452/8564 [00:02<00:00, 3081.21 examples/s]Applying chat template to train dataset:  79%|███████▉  | 6768/8564 [00:02<00:00, 3200.42 examples/s]Applying chat template to train dataset:  77%|███████▋  | 6593/8564 [00:02<00:00, 3018.08 examples/s]Applying chat template to train dataset:  79%|███████▉  | 6771/8564 [00:02<00:00, 3109.00 examples/s]Applying chat template to train dataset:  83%|████████▎ | 7090/8564 [00:02<00:00, 3198.45 examples/s]Applying chat template to train dataset:  81%|████████  | 6904/8564 [00:02<00:00, 3041.54 examples/s]Applying chat template to train dataset:  83%|████████▎ | 7090/8564 [00:02<00:00, 3128.30 examples/s]Applying chat template to train dataset:  87%|████████▋ | 7411/8564 [00:02<00:00, 3201.12 examples/s]Applying chat template to train dataset:  84%|████████▍ | 7217/8564 [00:02<00:00, 3061.37 examples/s]Applying chat template to train dataset:  87%|████████▋ | 7410/8564 [00:02<00:00, 3143.35 examples/s]Applying chat template to train dataset:  90%|█████████ | 7733/8564 [00:02<00:00, 3205.67 examples/s]Applying chat template to train dataset:  88%|████████▊ | 7528/8564 [00:02<00:00, 3073.34 examples/s]Applying chat template to train dataset:  90%|█████████ | 7730/8564 [00:02<00:00, 3153.66 examples/s]Applying chat template to train dataset:  95%|█████████▌| 8172/8564 [00:02<00:00, 3089.09 examples/s]Applying chat template to train dataset:  93%|█████████▎| 7947/8564 [00:02<00:00, 2957.17 examples/s]Applying chat template to train dataset:  95%|█████████▌| 8160/8564 [00:02<00:00, 3036.03 examples/s]Applying chat template to train dataset:  99%|█████████▉| 8496/8564 [00:02<00:00, 3126.33 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3128.69 examples/s]
Applying chat template to train dataset:  96%|█████████▋| 8259/8564 [00:02<00:00, 2998.20 examples/s]Applying chat template to train dataset:  99%|█████████▉| 8480/8564 [00:02<00:00, 3077.03 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3064.37 examples/s]
Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 2986.92 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 41/8564 [00:00<00:21, 405.52 examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 43/8564 [00:00<00:20, 407.15 examples/s]Tokenizing train dataset:   1%|          | 43/8564 [00:00<00:21, 405.41 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:25, 336.34 examples/s]Tokenizing train dataset:   1%|          | 92/8564 [00:00<00:25, 338.68 examples/s]Tokenizing train dataset:   1%|          | 92/8564 [00:00<00:25, 336.92 examples/s]Tokenizing train dataset:   2%|▏         | 135/8564 [00:00<00:26, 316.90 examples/s]Tokenizing train dataset:   2%|▏         | 140/8564 [00:00<00:25, 324.33 examples/s]Tokenizing train dataset:   2%|▏         | 140/8564 [00:00<00:26, 322.55 examples/s]Tokenizing train dataset:   2%|▏         | 179/8564 [00:00<00:27, 303.16 examples/s]Tokenizing train dataset:   2%|▏         | 186/8564 [00:00<00:26, 312.37 examples/s]Tokenizing train dataset:   2%|▏         | 212/8564 [00:00<00:27, 307.26 examples/s]Tokenizing train dataset:   2%|▏         | 186/8564 [00:00<00:26, 310.70 examples/s]Tokenizing train dataset:   3%|▎         | 221/8564 [00:00<00:26, 319.22 examples/s]Tokenizing train dataset:   3%|▎         | 247/8564 [00:00<00:26, 317.64 examples/s]Tokenizing train dataset:   3%|▎         | 220/8564 [00:00<00:26, 315.54 examples/s]Tokenizing train dataset:   3%|▎         | 257/8564 [00:00<00:25, 323.56 examples/s]Tokenizing train dataset:   3%|▎         | 282/8564 [00:00<00:25, 325.09 examples/s]Tokenizing train dataset:   3%|▎         | 254/8564 [00:00<00:25, 320.08 examples/s]Tokenizing train dataset:   3%|▎         | 295/8564 [00:00<00:24, 335.53 examples/s]Tokenizing train dataset:   4%|▎         | 315/8564 [00:00<00:25, 321.34 examples/s]Tokenizing train dataset:   3%|▎         | 291/8564 [00:00<00:24, 332.36 examples/s]Tokenizing train dataset:   4%|▍         | 344/8564 [00:01<00:25, 323.49 examples/s]Tokenizing train dataset:   4%|▍         | 325/8564 [00:00<00:24, 329.64 examples/s]Tokenizing train dataset:   4%|▍         | 362/8564 [00:01<00:26, 315.42 examples/s]Tokenizing train dataset:   4%|▍         | 378/8564 [00:01<00:25, 323.53 examples/s]Tokenizing train dataset:   5%|▍         | 397/8564 [00:01<00:25, 317.56 examples/s]Tokenizing train dataset:   4%|▍         | 372/8564 [00:01<00:25, 318.86 examples/s]Tokenizing train dataset:   5%|▌         | 430/8564 [00:01<00:25, 318.60 examples/s]Tokenizing train dataset:   5%|▌         | 429/8564 [00:01<00:25, 324.74 examples/s]Tokenizing train dataset:   5%|▍         | 422/8564 [00:01<00:25, 319.00 examples/s]Tokenizing train dataset:   6%|▌         | 475/8564 [00:01<00:26, 306.87 examples/s]Tokenizing train dataset:   6%|▌         | 475/8564 [00:01<00:25, 313.17 examples/s]Tokenizing train dataset:   5%|▌         | 469/8564 [00:01<00:25, 314.62 examples/s]Tokenizing train dataset:   6%|▌         | 519/8564 [00:01<00:26, 300.23 examples/s]Tokenizing train dataset:   6%|▌         | 520/8564 [00:01<00:26, 305.65 examples/s]Tokenizing train dataset:   6%|▌         | 512/8564 [00:01<00:26, 304.35 examples/s]Tokenizing train dataset:   6%|▋         | 553/8564 [00:01<00:26, 307.34 examples/s]Tokenizing train dataset:   6%|▋         | 555/8564 [00:01<00:25, 312.96 examples/s]Tokenizing train dataset:   6%|▋         | 549/8564 [00:01<00:25, 317.48 examples/s]Tokenizing train dataset:   7%|▋         | 585/8564 [00:01<00:26, 306.76 examples/s]Tokenizing train dataset:   7%|▋         | 600/8564 [00:01<00:26, 305.46 examples/s]Tokenizing train dataset:   7%|▋         | 596/8564 [00:01<00:25, 308.48 examples/s]Tokenizing train dataset:   7%|▋         | 637/8564 [00:02<00:25, 314.15 examples/s]Tokenizing train dataset:   7%|▋         | 638/8564 [00:01<00:24, 320.79 examples/s]Tokenizing train dataset:   7%|▋         | 634/8564 [00:01<00:24, 322.91 examples/s]Tokenizing train dataset:   8%|▊         | 683/8564 [00:02<00:25, 309.85 examples/s]Tokenizing train dataset:   8%|▊         | 686/8564 [00:02<00:24, 316.49 examples/s]Tokenizing train dataset:   8%|▊         | 681/8564 [00:02<00:25, 314.27 examples/s]Tokenizing train dataset:   9%|▊         | 730/8564 [00:02<00:25, 308.38 examples/s]Tokenizing train dataset:   9%|▊         | 736/8564 [00:02<00:24, 317.81 examples/s]Tokenizing train dataset:   9%|▊         | 729/8564 [00:02<00:25, 312.62 examples/s]Tokenizing train dataset:   9%|▉         | 762/8564 [00:02<00:25, 309.61 examples/s]Tokenizing train dataset:   9%|▉         | 781/8564 [00:02<00:25, 310.24 examples/s]Tokenizing train dataset:   9%|▉         | 762/8564 [00:02<00:24, 314.36 examples/s]Tokenizing train dataset:   9%|▉         | 802/8564 [00:02<00:26, 294.82 examples/s]Tokenizing train dataset:  10%|▉         | 824/8564 [00:02<00:25, 299.59 examples/s]Tokenizing train dataset:   9%|▉         | 806/8564 [00:02<00:25, 300.56 examples/s]Tokenizing train dataset:  10%|▉         | 847/8564 [00:02<00:26, 292.89 examples/s]Tokenizing train dataset:  10%|█         | 870/8564 [00:02<00:25, 298.44 examples/s]Tokenizing train dataset:  10%|▉         | 850/8564 [00:02<00:26, 294.69 examples/s]Tokenizing train dataset:  10%|█         | 879/8564 [00:02<00:26, 293.58 examples/s]Tokenizing train dataset:  11%|█         | 910/8564 [00:02<00:24, 316.59 examples/s]Tokenizing train dataset:  10%|█         | 885/8564 [00:02<00:25, 302.74 examples/s]Tokenizing train dataset:  11%|█         | 916/8564 [00:02<00:24, 310.85 examples/s]Tokenizing train dataset:  11%|█         | 920/8564 [00:02<00:24, 308.01 examples/s]Tokenizing train dataset:  11%|█         | 954/8564 [00:03<00:24, 306.02 examples/s]Tokenizing train dataset:  11%|█         | 962/8564 [00:03<00:25, 301.63 examples/s]Tokenizing train dataset:  11%|█         | 952/8564 [00:03<00:24, 306.51 examples/s]Tokenizing train dataset:  12%|█▏        | 998/8564 [00:03<00:25, 298.47 examples/s]Tokenizing train dataset:  11%|█▏        | 983/8564 [00:03<00:25, 302.16 examples/s]Tokenizing train dataset:  12%|█▏        | 1007/8564 [00:03<00:25, 293.66 examples/s]Tokenizing train dataset:  12%|█▏        | 1040/8564 [00:03<00:25, 289.92 examples/s]Tokenizing train dataset:  12%|█▏        | 1027/8564 [00:03<00:25, 292.72 examples/s]Tokenizing train dataset:  12%|█▏        | 1046/8564 [00:03<00:26, 282.86 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 296.74 examples/s]Tokenizing train dataset:  13%|█▎        | 1081/8564 [00:03<00:25, 294.81 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 294.27 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 309.43 examples/s]Tokenizing train dataset:  13%|█▎        | 1116/8564 [00:03<00:24, 305.66 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 307.11 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:03<00:25, 296.43 examples/s]Tokenizing train dataset:  13%|█▎        | 1156/8564 [00:03<00:25, 288.68 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:03<00:24, 296.65 examples/s]Tokenizing train dataset:  13%|█▎        | 1152/8564 [00:03<00:28, 261.68 examples/s]Tokenizing train dataset:  14%|█▍        | 1190/8564 [00:03<00:25, 293.87 examples/s]Tokenizing train dataset:  14%|█▍        | 1218/8564 [00:03<00:24, 304.83 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:03<00:27, 270.28 examples/s]Tokenizing train dataset:  14%|█▍        | 1222/8564 [00:04<00:24, 295.36 examples/s]Tokenizing train dataset:  15%|█▍        | 1252/8564 [00:04<00:23, 313.03 examples/s]Tokenizing train dataset:  14%|█▍        | 1218/8564 [00:03<00:25, 284.42 examples/s]Tokenizing train dataset:  15%|█▍        | 1258/8564 [00:04<00:23, 310.36 examples/s]Tokenizing train dataset:  15%|█▌        | 1286/8564 [00:04<00:23, 310.92 examples/s]Tokenizing train dataset:  15%|█▍        | 1252/8564 [00:04<00:24, 297.17 examples/s]Tokenizing train dataset:  15%|█▌        | 1320/8564 [00:04<00:22, 317.89 examples/s]Tokenizing train dataset:  15%|█▌        | 1309/8564 [00:04<00:23, 313.84 examples/s]Tokenizing train dataset:  15%|█▌        | 1286/8564 [00:04<00:24, 299.06 examples/s]Tokenizing train dataset:  16%|█▌        | 1353/8564 [00:04<00:22, 314.33 examples/s]Tokenizing train dataset:  15%|█▌        | 1320/8564 [00:04<00:23, 308.96 examples/s]Tokenizing train dataset:  16%|█▌        | 1354/8564 [00:04<00:23, 305.64 examples/s]Tokenizing train dataset:  16%|█▌        | 1353/8564 [00:04<00:23, 308.12 examples/s]Tokenizing train dataset:  16%|█▋        | 1397/8564 [00:04<00:23, 303.96 examples/s]Tokenizing train dataset:  16%|█▋        | 1399/8564 [00:04<00:23, 300.25 examples/s]Tokenizing train dataset:  17%|█▋        | 1428/8564 [00:04<00:23, 303.13 examples/s]Tokenizing train dataset:  16%|█▋        | 1397/8564 [00:04<00:23, 299.92 examples/s]Tokenizing train dataset:  17%|█▋        | 1442/8564 [00:04<00:24, 293.28 examples/s]Tokenizing train dataset:  17%|█▋        | 1428/8564 [00:04<00:23, 300.02 examples/s]Tokenizing train dataset:  17%|█▋        | 1473/8564 [00:04<00:23, 298.80 examples/s]Tokenizing train dataset:  17%|█▋        | 1474/8564 [00:04<00:24, 293.04 examples/s]Tokenizing train dataset:  17%|█▋        | 1473/8564 [00:04<00:23, 296.41 examples/s]Tokenizing train dataset:  18%|█▊        | 1515/8564 [00:04<00:24, 290.46 examples/s]Tokenizing train dataset:  18%|█▊        | 1505/8564 [00:04<00:24, 289.42 examples/s]Tokenizing train dataset:  18%|█▊        | 1550/8564 [00:05<00:23, 299.05 examples/s]Tokenizing train dataset:  18%|█▊        | 1538/8564 [00:05<00:23, 295.88 examples/s]Tokenizing train dataset:  18%|█▊        | 1515/8564 [00:04<00:24, 288.67 examples/s]Tokenizing train dataset:  18%|█▊        | 1581/8564 [00:05<00:23, 298.32 examples/s]Tokenizing train dataset:  18%|█▊        | 1568/8564 [00:05<00:23, 294.88 examples/s]Tokenizing train dataset:  18%|█▊        | 1550/8564 [00:05<00:23, 297.37 examples/s]Tokenizing train dataset:  19%|█▉        | 1614/8564 [00:05<00:22, 304.10 examples/s]Tokenizing train dataset:  19%|█▊        | 1598/8564 [00:05<00:23, 294.14 examples/s]Tokenizing train dataset:  18%|█▊        | 1581/8564 [00:05<00:23, 296.78 examples/s]Tokenizing train dataset:  19%|█▉        | 1650/8564 [00:05<00:21, 317.19 examples/s]Tokenizing train dataset:  19%|█▉        | 1630/8564 [00:05<00:23, 301.00 examples/s]Tokenizing train dataset:  19%|█▉        | 1614/8564 [00:05<00:22, 302.45 examples/s]Tokenizing train dataset:  20%|█▉        | 1686/8564 [00:05<00:21, 323.04 examples/s]Tokenizing train dataset:  19%|█▉        | 1667/8564 [00:05<00:22, 311.11 examples/s]Tokenizing train dataset:  19%|█▉        | 1650/8564 [00:05<00:21, 315.14 examples/s]Tokenizing train dataset:  20%|██        | 1725/8564 [00:05<00:20, 334.62 examples/s]Tokenizing train dataset:  20%|█▉        | 1710/8564 [00:05<00:20, 336.13 examples/s]Tokenizing train dataset:  20%|█▉        | 1686/8564 [00:05<00:21, 320.99 examples/s]Tokenizing train dataset:  21%|██        | 1760/8564 [00:05<00:20, 338.27 examples/s]Tokenizing train dataset:  20%|██        | 1725/8564 [00:05<00:20, 332.62 examples/s]Tokenizing train dataset:  21%|██        | 1758/8564 [00:05<00:20, 329.61 examples/s]Tokenizing train dataset:  21%|██        | 1760/8564 [00:05<00:20, 336.55 examples/s]Tokenizing train dataset:  21%|██        | 1808/8564 [00:05<00:20, 324.32 examples/s]Tokenizing train dataset:  21%|██        | 1804/8564 [00:05<00:21, 318.60 examples/s]Tokenizing train dataset:  22%|██▏       | 1842/8564 [00:05<00:20, 325.15 examples/s]Tokenizing train dataset:  21%|██        | 1808/8564 [00:05<00:20, 322.90 examples/s]Tokenizing train dataset:  21%|██▏       | 1837/8564 [00:05<00:21, 319.77 examples/s]Tokenizing train dataset:  22%|██▏       | 1842/8564 [00:05<00:20, 323.78 examples/s]Tokenizing train dataset:  22%|██▏       | 1887/8564 [00:06<00:21, 311.57 examples/s]Tokenizing train dataset:  22%|██▏       | 1880/8564 [00:06<00:21, 304.01 examples/s]Tokenizing train dataset:  23%|██▎       | 1930/8564 [00:06<00:19, 340.58 examples/s]Tokenizing train dataset:  22%|██▏       | 1887/8564 [00:06<00:21, 310.48 examples/s]Tokenizing train dataset:  22%|██▏       | 1920/8564 [00:06<00:20, 325.74 examples/s]Tokenizing train dataset:  23%|██▎       | 1966/8564 [00:06<00:19, 342.44 examples/s]Tokenizing train dataset:  23%|██▎       | 1930/8564 [00:06<00:19, 339.55 examples/s]Tokenizing train dataset:  23%|██▎       | 1957/8564 [00:06<00:19, 334.88 examples/s]Tokenizing train dataset:  23%|██▎       | 2003/8564 [00:06<00:18, 346.19 examples/s]Tokenizing train dataset:  23%|██▎       | 1966/8564 [00:06<00:19, 341.40 examples/s]Tokenizing train dataset:  23%|██▎       | 1994/8564 [00:06<00:19, 340.77 examples/s]Tokenizing train dataset:  24%|██▍       | 2043/8564 [00:06<00:18, 358.57 examples/s]Tokenizing train dataset:  23%|██▎       | 2003/8564 [00:06<00:18, 345.34 examples/s]Tokenizing train dataset:  24%|██▍       | 2035/8564 [00:06<00:18, 353.42 examples/s]Tokenizing train dataset:  24%|██▍       | 2080/8564 [00:06<00:18, 359.72 examples/s]Tokenizing train dataset:  24%|██▍       | 2043/8564 [00:06<00:18, 357.62 examples/s]Tokenizing train dataset:  24%|██▍       | 2071/8564 [00:06<00:18, 352.97 examples/s]Tokenizing train dataset:  25%|██▍       | 2120/8564 [00:06<00:17, 364.80 examples/s]Tokenizing train dataset:  24%|██▍       | 2080/8564 [00:06<00:18, 358.71 examples/s]Tokenizing train dataset:  25%|██▍       | 2110/8564 [00:06<00:17, 359.73 examples/s]Tokenizing train dataset:  25%|██▌       | 2157/8564 [00:06<00:17, 365.47 examples/s]Tokenizing train dataset:  25%|██▍       | 2119/8564 [00:06<00:17, 365.25 examples/s]Tokenizing train dataset:  25%|██▌       | 2149/8564 [00:06<00:17, 364.73 examples/s]Tokenizing train dataset:  26%|██▌       | 2194/8564 [00:06<00:17, 365.31 examples/s]Tokenizing train dataset:  25%|██▌       | 2156/8564 [00:06<00:17, 365.06 examples/s]Tokenizing train dataset:  26%|██▌       | 2186/8564 [00:06<00:17, 361.21 examples/s]Tokenizing train dataset:  26%|██▌       | 2232/8564 [00:06<00:17, 366.68 examples/s]Tokenizing train dataset:  26%|██▌       | 2193/8564 [00:06<00:17, 363.36 examples/s]Tokenizing train dataset:  26%|██▌       | 2223/8564 [00:07<00:17, 361.39 examples/s]Tokenizing train dataset:  27%|██▋       | 2272/8564 [00:07<00:16, 374.14 examples/s]Tokenizing train dataset:  26%|██▌       | 2230/8564 [00:07<00:17, 363.05 examples/s]Tokenizing train dataset:  26%|██▋       | 2260/8564 [00:07<00:17, 359.15 examples/s]Tokenizing train dataset:  27%|██▋       | 2270/8564 [00:07<00:16, 371.54 examples/s]Tokenizing train dataset:  27%|██▋       | 2328/8564 [00:07<00:16, 370.25 examples/s]Tokenizing train dataset:  27%|██▋       | 2303/8564 [00:07<00:16, 377.44 examples/s]Tokenizing train dataset:  27%|██▋       | 2309/8564 [00:07<00:16, 371.60 examples/s]Tokenizing train dataset:  28%|██▊       | 2380/8564 [00:07<00:17, 358.24 examples/s]Tokenizing train dataset:  27%|██▋       | 2351/8564 [00:07<00:17, 350.00 examples/s]Tokenizing train dataset:  28%|██▊       | 2359/8564 [00:07<00:17, 352.97 examples/s]Tokenizing train dataset:  28%|██▊       | 2421/8564 [00:07<00:16, 368.70 examples/s]Tokenizing train dataset:  28%|██▊       | 2391/8564 [00:07<00:17, 361.48 examples/s]Tokenizing train dataset:  28%|██▊       | 2403/8564 [00:07<00:16, 373.55 examples/s]Tokenizing train dataset:  29%|██▊       | 2461/8564 [00:07<00:16, 376.35 examples/s]Tokenizing train dataset:  28%|██▊       | 2430/8564 [00:07<00:16, 366.66 examples/s]Tokenizing train dataset:  29%|██▉       | 2470/8564 [00:07<00:16, 369.77 examples/s]Tokenizing train dataset:  29%|██▊       | 2461/8564 [00:07<00:16, 376.20 examples/s]Tokenizing train dataset:  29%|██▉       | 2511/8564 [00:07<00:16, 356.51 examples/s]Tokenizing train dataset:  30%|██▉       | 2555/8564 [00:07<00:16, 373.10 examples/s]Tokenizing train dataset:  29%|██▉       | 2521/8564 [00:07<00:16, 356.14 examples/s]Tokenizing train dataset:  29%|██▉       | 2511/8564 [00:07<00:16, 357.82 examples/s]Tokenizing train dataset:  30%|███       | 2594/8564 [00:07<00:15, 373.68 examples/s]Tokenizing train dataset:  30%|██▉       | 2563/8564 [00:08<00:16, 371.45 examples/s]Tokenizing train dataset:  30%|██▉       | 2555/8564 [00:07<00:16, 373.46 examples/s]Tokenizing train dataset:  30%|███       | 2593/8564 [00:08<00:16, 372.06 examples/s]Tokenizing train dataset:  31%|███       | 2642/8564 [00:08<00:16, 350.32 examples/s]Tokenizing train dataset:  31%|███       | 2618/8564 [00:08<00:16, 361.59 examples/s]Tokenizing train dataset:  31%|███       | 2642/8564 [00:08<00:16, 349.86 examples/s]Tokenizing train dataset:  31%|███▏      | 2693/8564 [00:08<00:17, 340.86 examples/s]Tokenizing train dataset:  31%|███       | 2665/8564 [00:08<00:17, 340.90 examples/s]Tokenizing train dataset:  32%|███▏      | 2731/8564 [00:08<00:16, 348.49 examples/s]Tokenizing train dataset:  31%|███▏      | 2693/8564 [00:08<00:17, 340.64 examples/s]Tokenizing train dataset:  32%|███▏      | 2719/8564 [00:08<00:17, 343.34 examples/s]Tokenizing train dataset:  32%|███▏      | 2731/8564 [00:08<00:16, 348.26 examples/s]Tokenizing train dataset:  33%|███▎      | 2785/8564 [00:08<00:16, 348.18 examples/s]Tokenizing train dataset:  32%|███▏      | 2769/8564 [00:08<00:17, 336.81 examples/s]Tokenizing train dataset:  33%|███▎      | 2825/8564 [00:08<00:15, 359.88 examples/s]Tokenizing train dataset:  33%|███▎      | 2785/8564 [00:08<00:16, 347.94 examples/s]Tokenizing train dataset:  33%|███▎      | 2809/8564 [00:08<00:16, 347.56 examples/s]Tokenizing train dataset:  33%|███▎      | 2825/8564 [00:08<00:15, 359.00 examples/s]Tokenizing train dataset:  34%|███▎      | 2874/8564 [00:08<00:16, 345.32 examples/s]Tokenizing train dataset:  33%|███▎      | 2845/8564 [00:08<00:16, 348.47 examples/s]Tokenizing train dataset:  34%|███▍      | 2910/8564 [00:08<00:16, 347.44 examples/s]Tokenizing train dataset:  33%|███▎      | 2868/8564 [00:08<00:17, 329.75 examples/s]Tokenizing train dataset:  34%|███▍      | 2900/8564 [00:08<00:16, 348.56 examples/s]Tokenizing train dataset:  34%|███▍      | 2953/8564 [00:08<00:15, 362.42 examples/s]Tokenizing train dataset:  34%|███▍      | 2911/8564 [00:08<00:16, 351.78 examples/s]Tokenizing train dataset:  34%|███▍      | 2944/8564 [00:09<00:15, 367.86 examples/s]Tokenizing train dataset:  35%|███▍      | 2997/8564 [00:09<00:14, 380.71 examples/s]Tokenizing train dataset:  34%|███▍      | 2953/8564 [00:09<00:15, 364.56 examples/s]Tokenizing train dataset:  35%|███▍      | 2989/8564 [00:09<00:14, 384.06 examples/s]Tokenizing train dataset:  36%|███▌      | 3051/8564 [00:09<00:14, 368.52 examples/s]Tokenizing train dataset:  35%|███▍      | 2997/8564 [00:09<00:14, 381.68 examples/s]Tokenizing train dataset:  36%|███▌      | 3042/8564 [00:09<00:14, 369.80 examples/s]Tokenizing train dataset:  36%|███▌      | 3050/8564 [00:09<00:15, 367.08 examples/s]Tokenizing train dataset:  36%|███▋      | 3109/8564 [00:09<00:14, 369.22 examples/s]Tokenizing train dataset:  36%|███▌      | 3097/8564 [00:09<00:14, 364.50 examples/s]Tokenizing train dataset:  36%|███▋      | 3106/8564 [00:09<00:14, 364.82 examples/s]Tokenizing train dataset:  37%|███▋      | 3165/8564 [00:09<00:14, 363.53 examples/s]Tokenizing train dataset:  37%|███▋      | 3150/8564 [00:09<00:15, 358.49 examples/s]Tokenizing train dataset:  37%|███▋      | 3162/8564 [00:09<00:14, 364.00 examples/s]Tokenizing train dataset:  38%|███▊      | 3220/8564 [00:09<00:14, 362.91 examples/s]Tokenizing train dataset:  37%|███▋      | 3205/8564 [00:09<00:15, 356.99 examples/s]Tokenizing train dataset:  38%|███▊      | 3261/8564 [00:09<00:14, 372.95 examples/s]Tokenizing train dataset:  38%|███▊      | 3217/8564 [00:09<00:14, 360.58 examples/s]Tokenizing train dataset:  38%|███▊      | 3245/8564 [00:09<00:14, 366.16 examples/s]Tokenizing train dataset:  38%|███▊      | 3259/8564 [00:09<00:14, 372.30 examples/s]Tokenizing train dataset:  39%|███▊      | 3314/8564 [00:09<00:14, 359.59 examples/s]Tokenizing train dataset:  38%|███▊      | 3297/8564 [00:10<00:14, 357.19 examples/s]Tokenizing train dataset:  39%|███▊      | 3311/8564 [00:10<00:14, 359.26 examples/s]Tokenizing train dataset:  39%|███▉      | 3370/8564 [00:10<00:14, 355.59 examples/s]Tokenizing train dataset:  39%|███▉      | 3350/8564 [00:10<00:14, 350.15 examples/s]Tokenizing train dataset:  40%|███▉      | 3414/8564 [00:10<00:13, 371.03 examples/s]Tokenizing train dataset:  39%|███▉      | 3365/8564 [00:10<00:14, 355.13 examples/s]Tokenizing train dataset:  40%|███▉      | 3391/8564 [00:10<00:14, 362.17 examples/s]Tokenizing train dataset:  40%|████      | 3452/8564 [00:10<00:13, 370.29 examples/s]Tokenizing train dataset:  40%|███▉      | 3409/8564 [00:10<00:13, 372.25 examples/s]Tokenizing train dataset:  40%|████      | 3430/8564 [00:10<00:14, 366.37 examples/s]Tokenizing train dataset:  41%|████      | 3513/8564 [00:10<00:13, 379.32 examples/s]Tokenizing train dataset:  41%|████      | 3470/8564 [00:10<00:13, 363.92 examples/s]Tokenizing train dataset:  40%|████      | 3465/8564 [00:10<00:13, 371.18 examples/s]Tokenizing train dataset:  41%|████      | 3512/8564 [00:10<00:13, 373.32 examples/s]Tokenizing train dataset:  41%|████      | 3504/8564 [00:10<00:13, 372.82 examples/s]Tokenizing train dataset:  42%|████▏     | 3564/8564 [00:10<00:13, 365.94 examples/s]Tokenizing train dataset:  41%|████▏     | 3542/8564 [00:10<00:13, 372.82 examples/s]Tokenizing train dataset:  42%|████▏     | 3607/8564 [00:10<00:13, 378.60 examples/s]Tokenizing train dataset:  42%|████▏     | 3564/8564 [00:10<00:13, 358.73 examples/s]Tokenizing train dataset:  42%|████▏     | 3606/8564 [00:10<00:13, 372.02 examples/s]Tokenizing train dataset:  42%|████▏     | 3599/8564 [00:10<00:13, 372.49 examples/s]Tokenizing train dataset:  43%|████▎     | 3661/8564 [00:10<00:13, 366.41 examples/s]Tokenizing train dataset:  42%|████▏     | 3637/8564 [00:10<00:13, 369.77 examples/s]Tokenizing train dataset:  43%|████▎     | 3658/8564 [00:11<00:13, 360.72 examples/s]Tokenizing train dataset:  43%|████▎     | 3720/8564 [00:11<00:13, 367.99 examples/s]Tokenizing train dataset:  43%|████▎     | 3689/8564 [00:11<00:13, 357.29 examples/s]Tokenizing train dataset:  43%|████▎     | 3710/8564 [00:11<00:13, 353.67 examples/s]Tokenizing train dataset:  44%|████▍     | 3776/8564 [00:11<00:12, 369.02 examples/s]Tokenizing train dataset:  44%|████▎     | 3729/8564 [00:11<00:13, 363.86 examples/s]Tokenizing train dataset:  44%|████▍     | 3750/8564 [00:11<00:13, 362.22 examples/s]Tokenizing train dataset:  44%|████▍     | 3768/8564 [00:11<00:13, 367.89 examples/s]Tokenizing train dataset:  45%|████▍     | 3831/8564 [00:11<00:12, 364.34 examples/s]Tokenizing train dataset:  44%|████▍     | 3788/8564 [00:11<00:13, 364.21 examples/s]Tokenizing train dataset:  45%|████▌     | 3868/8564 [00:11<00:12, 364.82 examples/s]Tokenizing train dataset:  45%|████▍     | 3823/8564 [00:11<00:13, 363.20 examples/s]Tokenizing train dataset:  45%|████▍     | 3841/8564 [00:11<00:13, 353.85 examples/s]Tokenizing train dataset:  46%|████▌     | 3919/8564 [00:11<00:13, 351.80 examples/s]Tokenizing train dataset:  45%|████▌     | 3878/8564 [00:11<00:13, 356.01 examples/s]Tokenizing train dataset:  45%|████▌     | 3878/8564 [00:11<00:12, 361.14 examples/s]Tokenizing train dataset:  46%|████▌     | 3959/8564 [00:11<00:12, 358.20 examples/s]Tokenizing train dataset:  46%|████▌     | 3930/8564 [00:11<00:13, 345.57 examples/s]Tokenizing train dataset:  46%|████▌     | 3930/8564 [00:11<00:13, 351.83 examples/s]Tokenizing train dataset:  47%|████▋     | 4013/8564 [00:11<00:12, 355.87 examples/s]Tokenizing train dataset:  46%|████▋     | 3967/8564 [00:11<00:13, 349.11 examples/s]Tokenizing train dataset:  46%|████▋     | 3967/8564 [00:11<00:12, 355.24 examples/s]Tokenizing train dataset:  47%|████▋     | 4053/8564 [00:11<00:12, 364.48 examples/s]Tokenizing train dataset:  47%|████▋     | 4004/8564 [00:12<00:13, 347.48 examples/s]Tokenizing train dataset:  47%|████▋     | 4004/8564 [00:11<00:12, 354.14 examples/s]Tokenizing train dataset:  47%|████▋     | 4044/8564 [00:12<00:12, 359.38 examples/s]Tokenizing train dataset:  47%|████▋     | 4045/8564 [00:12<00:12, 365.96 examples/s]Tokenizing train dataset:  48%|████▊     | 4106/8564 [00:12<00:12, 359.14 examples/s]Tokenizing train dataset:  48%|████▊     | 4095/8564 [00:12<00:12, 349.74 examples/s]Tokenizing train dataset:  48%|████▊     | 4098/8564 [00:12<00:12, 358.07 examples/s]Tokenizing train dataset:  49%|████▊     | 4160/8564 [00:12<00:12, 353.87 examples/s]Tokenizing train dataset:  48%|████▊     | 4132/8564 [00:12<00:12, 351.31 examples/s]Tokenizing train dataset:  49%|████▉     | 4202/8564 [00:12<00:11, 363.75 examples/s]Tokenizing train dataset:  48%|████▊     | 4151/8564 [00:12<00:12, 353.72 examples/s]Tokenizing train dataset:  49%|████▊     | 4169/8564 [00:12<00:12, 349.36 examples/s]Tokenizing train dataset:  49%|████▉     | 4239/8564 [00:12<00:12, 359.54 examples/s]Tokenizing train dataset:  49%|████▉     | 4191/8564 [00:12<00:12, 359.88 examples/s]Tokenizing train dataset:  49%|████▉     | 4210/8564 [00:12<00:12, 357.46 examples/s]Tokenizing train dataset:  50%|████▉     | 4277/8564 [00:12<00:11, 360.08 examples/s]Tokenizing train dataset:  49%|████▉     | 4229/8564 [00:12<00:11, 362.39 examples/s]Tokenizing train dataset:  50%|█████     | 4314/8564 [00:12<00:11, 360.28 examples/s]Tokenizing train dataset:  50%|████▉     | 4264/8564 [00:12<00:12, 352.02 examples/s]Tokenizing train dataset:  50%|█████     | 4284/8564 [00:12<00:11, 361.25 examples/s]Tokenizing train dataset:  50%|█████     | 4301/8564 [00:12<00:12, 352.40 examples/s]Tokenizing train dataset:  51%|█████     | 4369/8564 [00:12<00:11, 357.34 examples/s]Tokenizing train dataset:  51%|█████     | 4337/8564 [00:12<00:11, 355.59 examples/s]Tokenizing train dataset:  51%|█████     | 4337/8564 [00:12<00:12, 351.42 examples/s]Tokenizing train dataset:  51%|█████▏    | 4407/8564 [00:12<00:11, 361.32 examples/s]Tokenizing train dataset:  51%|█████     | 4375/8564 [00:12<00:11, 357.66 examples/s]Tokenizing train dataset:  51%|█████     | 4373/8564 [00:13<00:11, 352.79 examples/s]Tokenizing train dataset:  52%|█████▏    | 4446/8564 [00:13<00:11, 363.65 examples/s]Tokenizing train dataset:  52%|█████▏    | 4413/8564 [00:13<00:11, 361.42 examples/s]Tokenizing train dataset:  52%|█████▏    | 4411/8564 [00:13<00:11, 356.95 examples/s]Tokenizing train dataset:  52%|█████▏    | 4484/8564 [00:13<00:11, 364.48 examples/s]Tokenizing train dataset:  52%|█████▏    | 4450/8564 [00:13<00:11, 362.52 examples/s]Tokenizing train dataset:  52%|█████▏    | 4450/8564 [00:13<00:11, 358.16 examples/s]Tokenizing train dataset:  53%|█████▎    | 4537/8564 [00:13<00:11, 356.26 examples/s]Tokenizing train dataset:  52%|█████▏    | 4490/8564 [00:13<00:11, 364.39 examples/s]Tokenizing train dataset:  52%|█████▏    | 4489/8564 [00:13<00:11, 362.30 examples/s]Tokenizing train dataset:  53%|█████▎    | 4573/8564 [00:13<00:11, 355.78 examples/s]Tokenizing train dataset:  53%|█████▎    | 4544/8564 [00:13<00:11, 358.20 examples/s]Tokenizing train dataset:  53%|█████▎    | 4540/8564 [00:13<00:11, 353.14 examples/s]Tokenizing train dataset:  54%|█████▍    | 4610/8564 [00:13<00:11, 354.92 examples/s]Tokenizing train dataset:  54%|█████▎    | 4593/8564 [00:13<00:11, 347.70 examples/s]Tokenizing train dataset:  54%|█████▎    | 4600/8564 [00:13<00:11, 353.27 examples/s]Tokenizing train dataset:  54%|█████▍    | 4661/8564 [00:13<00:11, 343.60 examples/s]Tokenizing train dataset:  54%|█████▍    | 4630/8564 [00:13<00:11, 351.04 examples/s]Tokenizing train dataset:  54%|█████▍    | 4636/8564 [00:13<00:11, 347.33 examples/s]Tokenizing train dataset:  55%|█████▍    | 4709/8564 [00:13<00:11, 331.95 examples/s]Tokenizing train dataset:  55%|█████▍    | 4676/8564 [00:13<00:11, 333.37 examples/s]Tokenizing train dataset:  55%|█████▍    | 4684/8564 [00:13<00:11, 337.47 examples/s]Tokenizing train dataset:  55%|█████▌    | 4744/8564 [00:13<00:11, 332.54 examples/s]Tokenizing train dataset:  55%|█████▌    | 4721/8564 [00:14<00:11, 321.59 examples/s]Tokenizing train dataset:  55%|█████▌    | 4735/8564 [00:14<00:11, 332.11 examples/s]Tokenizing train dataset:  56%|█████▌    | 4793/8564 [00:14<00:11, 325.52 examples/s]Tokenizing train dataset:  56%|█████▌    | 4754/8564 [00:14<00:11, 319.90 examples/s]Tokenizing train dataset:  57%|█████▋    | 4858/8564 [00:14<00:09, 398.94 examples/s]Tokenizing train dataset:  56%|█████▌    | 4781/8564 [00:14<00:11, 319.41 examples/s]Tokenizing train dataset:  56%|█████▌    | 4788/8564 [00:14<00:11, 320.39 examples/s]Tokenizing train dataset:  57%|█████▋    | 4913/8564 [00:14<00:08, 436.32 examples/s]Tokenizing train dataset:  56%|█████▋    | 4834/8564 [00:14<00:10, 365.82 examples/s]Tokenizing train dataset:  57%|█████▋    | 4847/8564 [00:14<00:09, 389.14 examples/s]Tokenizing train dataset:  58%|█████▊    | 4976/8564 [00:14<00:07, 486.64 examples/s]Tokenizing train dataset:  57%|█████▋    | 4894/8564 [00:14<00:08, 419.20 examples/s]Tokenizing train dataset:  57%|█████▋    | 4901/8564 [00:14<00:08, 429.10 examples/s]Tokenizing train dataset:  59%|█████▉    | 5033/8564 [00:14<00:06, 508.00 examples/s]Tokenizing train dataset:  58%|█████▊    | 4959/8564 [00:14<00:07, 475.52 examples/s]Tokenizing train dataset:  58%|█████▊    | 4963/8564 [00:14<00:07, 477.17 examples/s]Tokenizing train dataset:  60%|█████▉    | 5106/8564 [00:14<00:06, 564.18 examples/s]Tokenizing train dataset:  59%|█████▊    | 5017/8564 [00:14<00:07, 500.93 examples/s]Tokenizing train dataset:  59%|█████▊    | 5025/8564 [00:14<00:06, 510.24 examples/s]Tokenizing train dataset:  60%|██████    | 5177/8564 [00:14<00:05, 599.47 examples/s]Tokenizing train dataset:  59%|█████▉    | 5086/8564 [00:14<00:06, 548.16 examples/s]Tokenizing train dataset:  59%|█████▉    | 5091/8564 [00:14<00:06, 549.50 examples/s]Tokenizing train dataset:  61%|██████▏   | 5247/8564 [00:14<00:05, 627.11 examples/s]Tokenizing train dataset:  60%|██████    | 5153/8564 [00:14<00:05, 578.68 examples/s]Tokenizing train dataset:  60%|██████    | 5160/8564 [00:14<00:05, 587.59 examples/s]Tokenizing train dataset:  62%|██████▏   | 5319/8564 [00:14<00:04, 651.44 examples/s]Tokenizing train dataset:  61%|██████    | 5224/8564 [00:14<00:05, 615.27 examples/s]Tokenizing train dataset:  61%|██████    | 5230/8564 [00:15<00:05, 613.85 examples/s]Tokenizing train dataset:  62%|██████▏   | 5299/8564 [00:15<00:05, 651.96 examples/s]Tokenizing train dataset:  63%|██████▎   | 5402/8564 [00:15<00:05, 605.54 examples/s]Tokenizing train dataset:  62%|██████▏   | 5305/8564 [00:15<00:05, 651.40 examples/s]Tokenizing train dataset:  64%|██████▍   | 5470/8564 [00:15<00:04, 619.41 examples/s]Tokenizing train dataset:  63%|██████▎   | 5387/8564 [00:15<00:05, 619.32 examples/s]Tokenizing train dataset:  63%|██████▎   | 5390/8564 [00:15<00:05, 613.79 examples/s]Tokenizing train dataset:  64%|██████▎   | 5453/8564 [00:15<00:05, 621.99 examples/s]Tokenizing train dataset:  65%|██████▍   | 5560/8564 [00:15<00:04, 601.93 examples/s]Tokenizing train dataset:  64%|██████▎   | 5454/8564 [00:15<00:05, 617.56 examples/s]Tokenizing train dataset:  66%|██████▌   | 5631/8564 [00:15<00:04, 624.25 examples/s]Tokenizing train dataset:  65%|██████▍   | 5540/8564 [00:15<00:05, 603.33 examples/s]Tokenizing train dataset:  65%|██████▍   | 5541/8564 [00:15<00:05, 602.45 examples/s]Tokenizing train dataset:  67%|██████▋   | 5698/8564 [00:15<00:04, 633.35 examples/s]Tokenizing train dataset:  65%|██████▌   | 5605/8564 [00:15<00:04, 611.59 examples/s]Tokenizing train dataset:  65%|██████▌   | 5604/8564 [00:15<00:04, 607.54 examples/s]Tokenizing train dataset:  67%|██████▋   | 5770/8564 [00:15<00:04, 650.60 examples/s]Tokenizing train dataset:  66%|██████▌   | 5673/8564 [00:15<00:04, 626.83 examples/s]Tokenizing train dataset:  66%|██████▌   | 5671/8564 [00:15<00:04, 621.44 examples/s]Tokenizing train dataset:  68%|██████▊   | 5840/8564 [00:15<00:04, 660.81 examples/s]Tokenizing train dataset:  67%|██████▋   | 5735/8564 [00:15<00:04, 620.00 examples/s]Tokenizing train dataset:  67%|██████▋   | 5778/8564 [00:15<00:04, 647.62 examples/s]Tokenizing train dataset:  69%|██████▉   | 5928/8564 [00:15<00:04, 626.00 examples/s]Tokenizing train dataset:  68%|██████▊   | 5813/8564 [00:15<00:04, 662.92 examples/s]Tokenizing train dataset:  68%|██████▊   | 5846/8564 [00:15<00:04, 649.47 examples/s]Tokenizing train dataset:  70%|███████   | 6015/8564 [00:16<00:04, 602.59 examples/s]Tokenizing train dataset:  69%|██████▉   | 5893/8564 [00:16<00:04, 611.97 examples/s]Tokenizing train dataset:  69%|██████▉   | 5937/8564 [00:16<00:04, 624.77 examples/s]Tokenizing train dataset:  70%|██████▉   | 5961/8564 [00:16<00:04, 627.55 examples/s]Tokenizing train dataset:  71%|███████   | 6100/8564 [00:16<00:04, 586.89 examples/s]Tokenizing train dataset:  70%|███████   | 6021/8564 [00:16<00:04, 593.47 examples/s]Tokenizing train dataset:  72%|███████▏  | 6175/8564 [00:16<00:03, 624.08 examples/s]Tokenizing train dataset:  71%|███████   | 6041/8564 [00:16<00:04, 582.35 examples/s]Tokenizing train dataset:  71%|███████▏  | 6111/8564 [00:16<00:04, 592.30 examples/s]Tokenizing train dataset:  73%|███████▎  | 6244/8564 [00:16<00:03, 638.25 examples/s]Tokenizing train dataset:  72%|███████▏  | 6132/8564 [00:16<00:04, 585.88 examples/s]Tokenizing train dataset:  72%|███████▏  | 6183/8564 [00:16<00:03, 618.48 examples/s]Tokenizing train dataset:  74%|███████▎  | 6311/8564 [00:16<00:03, 640.06 examples/s]Tokenizing train dataset:  72%|███████▏  | 6208/8564 [00:16<00:03, 625.00 examples/s]Tokenizing train dataset:  73%|███████▎  | 6258/8564 [00:16<00:03, 646.89 examples/s]Tokenizing train dataset:  75%|███████▍  | 6384/8564 [00:16<00:03, 660.57 examples/s]Tokenizing train dataset:  73%|███████▎  | 6277/8564 [00:16<00:03, 637.75 examples/s]Tokenizing train dataset:  74%|███████▍  | 6360/8564 [00:16<00:03, 650.73 examples/s]Tokenizing train dataset:  76%|███████▌  | 6477/8564 [00:16<00:03, 635.88 examples/s]Tokenizing train dataset:  74%|███████▍  | 6379/8564 [00:16<00:03, 647.18 examples/s]Tokenizing train dataset:  75%|███████▌  | 6457/8564 [00:16<00:03, 641.14 examples/s]Tokenizing train dataset:  77%|███████▋  | 6567/8564 [00:16<00:03, 620.90 examples/s]Tokenizing train dataset:  76%|███████▌  | 6470/8564 [00:17<00:03, 625.07 examples/s]Tokenizing train dataset:  76%|███████▋  | 6548/8564 [00:17<00:03, 625.52 examples/s]Tokenizing train dataset:  78%|███████▊  | 6646/8564 [00:17<00:03, 584.72 examples/s]Tokenizing train dataset:  77%|███████▋  | 6561/8564 [00:17<00:03, 615.25 examples/s]Tokenizing train dataset:  78%|███████▊  | 6718/8564 [00:17<00:03, 613.16 examples/s]Tokenizing train dataset:  77%|███████▋  | 6628/8564 [00:17<00:03, 590.69 examples/s]Tokenizing train dataset:  79%|███████▉  | 6783/8564 [00:17<00:02, 621.72 examples/s]Tokenizing train dataset:  78%|███████▊  | 6640/8564 [00:17<00:03, 580.92 examples/s]Tokenizing train dataset:  78%|███████▊  | 6695/8564 [00:17<00:03, 605.53 examples/s]Tokenizing train dataset:  78%|███████▊  | 6710/8564 [00:17<00:03, 605.64 examples/s]Tokenizing train dataset:  79%|███████▉  | 6761/8564 [00:17<00:02, 617.26 examples/s]Tokenizing train dataset:  80%|████████  | 6871/8564 [00:17<00:02, 604.25 examples/s]Tokenizing train dataset:  79%|███████▉  | 6774/8564 [00:17<00:02, 612.01 examples/s]Tokenizing train dataset:  81%|████████  | 6943/8564 [00:17<00:02, 632.75 examples/s]Tokenizing train dataset:  80%|███████▉  | 6847/8564 [00:17<00:02, 597.99 examples/s]Tokenizing train dataset:  80%|████████  | 6857/8564 [00:17<00:02, 588.43 examples/s]Tokenizing train dataset:  81%|████████  | 6916/8564 [00:17<00:02, 617.65 examples/s]Tokenizing train dataset:  82%|████████▏ | 7020/8564 [00:17<00:02, 584.16 examples/s]Tokenizing train dataset:  81%|████████  | 6928/8564 [00:17<00:02, 615.62 examples/s]Tokenizing train dataset:  83%|████████▎ | 7091/8564 [00:17<00:02, 613.15 examples/s]Tokenizing train dataset:  82%|████████▏ | 6982/8564 [00:17<00:02, 617.39 examples/s]Tokenizing train dataset:  82%|████████▏ | 7014/8564 [00:17<00:02, 593.02 examples/s]Tokenizing train dataset:  83%|████████▎ | 7069/8564 [00:17<00:02, 602.62 examples/s]Tokenizing train dataset:  84%|████████▍ | 7192/8564 [00:17<00:02, 630.78 examples/s]Tokenizing train dataset:  83%|████████▎ | 7077/8564 [00:18<00:02, 601.48 examples/s]Tokenizing train dataset:  83%|████████▎ | 7137/8564 [00:17<00:02, 618.89 examples/s]Tokenizing train dataset:  85%|████████▌ | 7282/8564 [00:18<00:02, 616.74 examples/s]Tokenizing train dataset:  83%|████████▎ | 7143/8564 [00:18<00:02, 612.37 examples/s]Tokenizing train dataset:  84%|████████▍ | 7206/8564 [00:18<00:02, 636.83 examples/s]Tokenizing train dataset:  84%|████████▍ | 7212/8564 [00:18<00:02, 630.17 examples/s]Tokenizing train dataset:  86%|████████▌ | 7379/8564 [00:18<00:01, 622.14 examples/s]Tokenizing train dataset:  85%|████████▌ | 7292/8564 [00:18<00:02, 608.31 examples/s]Tokenizing train dataset:  87%|████████▋ | 7443/8564 [00:18<00:01, 624.13 examples/s]Tokenizing train dataset:  85%|████████▌ | 7296/8564 [00:18<00:02, 602.26 examples/s]Tokenizing train dataset:  86%|████████▌ | 7360/8564 [00:18<00:01, 620.47 examples/s]Tokenizing train dataset:  88%|████████▊ | 7506/8564 [00:18<00:01, 621.51 examples/s]Tokenizing train dataset:  86%|████████▌ | 7361/8564 [00:18<00:01, 612.13 examples/s]Tokenizing train dataset:  87%|████████▋ | 7457/8564 [00:18<00:01, 627.35 examples/s]Tokenizing train dataset:  89%|████████▊ | 7581/8564 [00:18<00:01, 650.44 examples/s]Tokenizing train dataset:  87%|████████▋ | 7460/8564 [00:18<00:01, 622.80 examples/s]Tokenizing train dataset:  88%|████████▊ | 7555/8564 [00:18<00:01, 631.99 examples/s]Tokenizing train dataset:  90%|████████▉ | 7665/8564 [00:18<00:01, 615.77 examples/s]Tokenizing train dataset:  88%|████████▊ | 7554/8564 [00:18<00:01, 622.88 examples/s]Tokenizing train dataset:  90%|█████████ | 7728/8564 [00:18<00:01, 617.75 examples/s]Tokenizing train dataset:  89%|████████▉ | 7627/8564 [00:18<00:01, 648.77 examples/s]Tokenizing train dataset:  89%|████████▉ | 7625/8564 [00:18<00:01, 643.44 examples/s]Tokenizing train dataset:  90%|█████████ | 7710/8564 [00:18<00:01, 614.22 examples/s]Tokenizing train dataset:  91%|█████████ | 7809/8564 [00:18<00:01, 585.27 examples/s]Tokenizing train dataset:  90%|████████▉ | 7706/8564 [00:19<00:01, 607.31 examples/s]Tokenizing train dataset:  92%|█████████▏| 7874/8564 [00:19<00:01, 515.11 examples/s]Tokenizing train dataset:  91%|█████████ | 7795/8564 [00:19<00:01, 515.85 examples/s]Tokenizing train dataset:  91%|█████████ | 7790/8564 [00:19<00:01, 582.68 examples/s]Tokenizing train dataset:  93%|█████████▎| 7937/8564 [00:19<00:01, 540.20 examples/s]Tokenizing train dataset:  92%|█████████▏| 7853/8564 [00:19<00:01, 591.60 examples/s]Tokenizing train dataset:  92%|█████████▏| 7860/8564 [00:19<00:01, 538.89 examples/s]Tokenizing train dataset:  93%|█████████▎| 8004/8564 [00:19<00:00, 570.18 examples/s]Tokenizing train dataset:  93%|█████████▎| 7924/8564 [00:19<00:01, 561.74 examples/s]Tokenizing train dataset:  92%|█████████▏| 7915/8564 [00:19<00:01, 590.21 examples/s]Tokenizing train dataset:  94%|█████████▍| 8064/8564 [00:19<00:00, 571.38 examples/s]Tokenizing train dataset:  93%|█████████▎| 7986/8564 [00:19<00:01, 573.99 examples/s]Tokenizing train dataset:  93%|█████████▎| 7976/8564 [00:19<00:00, 593.13 examples/s]Tokenizing train dataset:  95%|█████████▌| 8151/8564 [00:19<00:00, 568.53 examples/s]Tokenizing train dataset:  94%|█████████▍| 8049/8564 [00:19<00:00, 585.26 examples/s]Tokenizing train dataset:  94%|█████████▍| 8041/8564 [00:19<00:00, 607.60 examples/s]Tokenizing train dataset:  96%|█████████▌| 8213/8564 [00:19<00:00, 576.24 examples/s]Tokenizing train dataset:  95%|█████████▍| 8132/8564 [00:19<00:00, 570.10 examples/s]Tokenizing train dataset:  95%|█████████▍| 8121/8564 [00:19<00:00, 574.89 examples/s]Tokenizing train dataset:  97%|█████████▋| 8297/8564 [00:19<00:00, 633.41 examples/s]Tokenizing train dataset:  96%|█████████▌| 8188/8564 [00:19<00:00, 598.07 examples/s]Tokenizing train dataset:  96%|█████████▌| 8200/8564 [00:19<00:00, 593.23 examples/s]Tokenizing train dataset:  96%|█████████▋| 8250/8564 [00:20<00:00, 602.41 examples/s]Tokenizing train dataset:  97%|█████████▋| 8269/8564 [00:19<00:00, 616.04 examples/s]Tokenizing train dataset:  98%|█████████▊| 8384/8564 [00:19<00:00, 611.79 examples/s]Tokenizing train dataset:  97%|█████████▋| 8326/8564 [00:20<00:00, 639.59 examples/s]Tokenizing train dataset:  97%|█████████▋| 8339/8564 [00:20<00:00, 630.87 examples/s]Tokenizing train dataset:  99%|█████████▉| 8478/8564 [00:20<00:00, 610.80 examples/s]Tokenizing train dataset:  98%|█████████▊| 8407/8564 [00:20<00:00, 601.65 examples/s]Tokenizing train dataset:  98%|█████████▊| 8427/8564 [00:20<00:00, 612.28 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 607.75 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 422.30 examples/s]
Tokenizing train dataset:  99%|█████████▉| 8490/8564 [00:20<00:00, 605.84 examples/s]Tokenizing train dataset:  99%|█████████▉| 8498/8564 [00:20<00:00, 602.08 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 608.23 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 419.98 examples/s]
Tokenizing train dataset: 100%|█████████▉| 8560/8564 [00:20<00:00, 600.41 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 417.17 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11150.64 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 10888.69 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 10833.68 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 12952.72 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13259.46 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13177.37 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 329.39 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 328.49 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 322.39 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 299.13 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 298.25 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 291.18 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:02, 280.03 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:02, 279.53 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 278.33 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 270.67 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 270.42 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 269.14 examples/s]Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:02, 262.67 examples/s]Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:02, 262.29 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 254.89 examples/s]Tokenizing eval dataset:  25%|██▍       | 238/953 [00:00<00:02, 294.25 examples/s]Tokenizing eval dataset:  25%|██▍       | 238/953 [00:00<00:02, 293.74 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:02, 278.87 examples/s]Tokenizing eval dataset:  32%|███▏      | 305/953 [00:00<00:01, 391.88 examples/s]Tokenizing eval dataset:  32%|███▏      | 305/953 [00:00<00:01, 391.24 examples/s]Tokenizing eval dataset:  31%|███▏      | 298/953 [00:00<00:01, 380.06 examples/s]Tokenizing eval dataset:  39%|███▊      | 369/953 [00:01<00:01, 455.67 examples/s]Tokenizing eval dataset:  39%|███▊      | 368/953 [00:01<00:01, 455.98 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 442.17 examples/s]Tokenizing eval dataset:  46%|████▌     | 439/953 [00:01<00:00, 522.13 examples/s]Tokenizing eval dataset:  46%|████▌     | 437/953 [00:01<00:00, 520.82 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 500.91 examples/s]Tokenizing eval dataset:  53%|█████▎    | 508/953 [00:01<00:00, 563.33 examples/s]Tokenizing eval dataset:  53%|█████▎    | 505/953 [00:01<00:00, 562.06 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 544.47 examples/s]Tokenizing eval dataset:  60%|██████    | 574/953 [00:01<00:00, 588.83 examples/s]Tokenizing eval dataset:  60%|█████▉    | 570/953 [00:01<00:00, 584.24 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 577.21 examples/s]Tokenizing eval dataset:  67%|██████▋   | 643/953 [00:01<00:00, 616.35 examples/s]Tokenizing eval dataset:  67%|██████▋   | 640/953 [00:01<00:00, 613.34 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 599.17 examples/s]Tokenizing eval dataset:  77%|███████▋  | 733/953 [00:01<00:00, 600.58 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 603.69 examples/s]Tokenizing eval dataset:  77%|███████▋  | 730/953 [00:01<00:00, 602.46 examples/s]Tokenizing eval dataset:  85%|████████▍ | 810/953 [00:01<00:00, 562.61 examples/s]Tokenizing eval dataset:  85%|████████▍ | 807/953 [00:01<00:00, 566.27 examples/s]Tokenizing eval dataset:  82%|████████▏ | 777/953 [00:01<00:00, 578.50 examples/s]Tokenizing eval dataset:  93%|█████████▎| 883/953 [00:01<00:00, 544.59 examples/s]Tokenizing eval dataset:  93%|█████████▎| 889/953 [00:01<00:00, 546.27 examples/s]Tokenizing eval dataset:  89%|████████▉ | 851/953 [00:01<00:00, 539.75 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 535.24 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 467.66 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 533.57 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 466.81 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  97%|█████████▋| 929/953 [00:02<00:00, 525.98 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 458.83 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Parameter Offload: Total persistent parameters: 605696 in 169 params
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
wandb: Currently logged in as: vajdadario (slolama) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
wandb: Tracking run with wandb version 0.19.7
wandb: Run data is saved locally in /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/wandb/run-20250531_011443-t8je0b9h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DPO_r-64_lr-3e-07_e-3_b-0.2
wandb: ⭐️ View project at https://wandb.ai/slolama/GaMS-9B-Translation-DPO
wandb: 🚀 View run at https://wandb.ai/slolama/GaMS-9B-Translation-DPO/runs/t8je0b9h
  0%|          | 0/1605 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Traceback (most recent call last):
[rank3]: Traceback (most recent call last):
[rank3]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 195, in <module>
[rank3]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank3]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 174, in main
[rank3]:     dpo_trainer.train()
[rank3]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
[rank3]:     self.accelerator.backward(loss, **kwargs)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2321, in backward
[rank3]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/deepspeed.py", line 266, in backward
[rank3]:     self.engine.backward(loss, **kwargs)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 2126, in backward
[rank3]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank3]:     ret_val = func(*args, **kwargs)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py", line 2284, in backward
[rank3]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank3]:     scaled_loss.backward(retain_graph=retain_graph)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 626, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 1129, in unpack_hook
[rank3]:     frame.check_recomputed_tensors_match(gid)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 903, in check_recomputed_tensors_match
[rank3]:     raise CheckpointError(
[rank3]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank3]: tensor at position 6:
[rank3]: saved metadata: {'shape': torch.Size([4096, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: tensor at position 12:
[rank3]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: tensor at position 18:
[rank3]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: tensor at position 34:
[rank3]: saved metadata: {'shape': torch.Size([3584, 4096]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: tensor at position 50:
[rank3]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: tensor at position 57:
[rank3]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: tensor at position 65:
[rank3]: saved metadata: {'shape': torch.Size([3584, 14336]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}

[rank1]: Traceback (most recent call last):
[rank1]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 195, in <module>
[rank1]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank1]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 174, in main
[rank1]:     dpo_trainer.train()
[rank1]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
[rank1]:     self.accelerator.backward(loss, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2321, in backward
[rank1]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/deepspeed.py", line 266, in backward
[rank1]:     self.engine.backward(loss, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 2126, in backward
[rank1]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank1]:     ret_val = func(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py", line 2284, in backward
[rank1]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank1]:     scaled_loss.backward(retain_graph=retain_graph)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 626, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 1129, in unpack_hook
[rank1]:     frame.check_recomputed_tensors_match(gid)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 903, in check_recomputed_tensors_match
[rank1]:     raise CheckpointError(
[rank1]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank1]: tensor at position 6:
[rank1]: saved metadata: {'shape': torch.Size([4096, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: tensor at position 12:
[rank1]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: tensor at position 18:
[rank1]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: tensor at position 34:
[rank1]: saved metadata: {'shape': torch.Size([3584, 4096]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: tensor at position 50:
[rank1]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: tensor at position 57:
[rank1]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: tensor at position 65:
[rank1]: saved metadata: {'shape': torch.Size([3584, 14336]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}

[rank2]: Traceback (most recent call last):
[rank2]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 195, in <module>
[rank2]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank2]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 174, in main
[rank2]:     dpo_trainer.train()
[rank2]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
[rank2]:     self.accelerator.backward(loss, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2321, in backward
[rank2]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/deepspeed.py", line 266, in backward
[rank2]:     self.engine.backward(loss, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 2126, in backward
[rank2]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank2]:     ret_val = func(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py", line 2284, in backward
[rank2]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank2]:     scaled_loss.backward(retain_graph=retain_graph)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 626, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 1129, in unpack_hook
[rank2]:     frame.check_recomputed_tensors_match(gid)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 903, in check_recomputed_tensors_match
[rank2]:     raise CheckpointError(
[rank2]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank2]: tensor at position 6:
[rank2]: saved metadata: {'shape': torch.Size([4096, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: tensor at position 12:
[rank2]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: tensor at position 18:
[rank2]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: tensor at position 34:
[rank2]: saved metadata: {'shape': torch.Size([3584, 4096]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: tensor at position 50:
[rank2]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: tensor at position 57:
[rank2]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: tensor at position 65:
[rank2]: saved metadata: {'shape': torch.Size([3584, 14336]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}

  File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 195, in <module>
    main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
  File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 174, in main
    dpo_trainer.train()
  File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2321, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/deepspeed.py", line 266, in backward
    self.engine.backward(loss, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 2126, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py", line 2284, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 626, in backward
    torch.autograd.backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 1129, in unpack_hook
    frame.check_recomputed_tensors_match(gid)
  File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 903, in check_recomputed_tensors_match
    raise CheckpointError(
torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
tensor at position 6:
saved metadata: {'shape': torch.Size([4096, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
tensor at position 12:
saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
tensor at position 18:
saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
tensor at position 34:
saved metadata: {'shape': torch.Size([3584, 4096]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
tensor at position 50:
saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
tensor at position 57:
saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
tensor at position 65:
saved metadata: {'shape': torch.Size([3584, 14336]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}

[rank0]: Traceback (most recent call last):
[rank0]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 195, in <module>
[rank0]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank0]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 174, in main
[rank0]:     dpo_trainer.train()
[rank0]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2321, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/deepspeed.py", line 266, in backward
[rank0]:     self.engine.backward(loss, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 2126, in backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py", line 2284, in backward
[rank0]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank0]:     scaled_loss.backward(retain_graph=retain_graph)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 626, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 1129, in unpack_hook
[rank0]:     frame.check_recomputed_tensors_match(gid)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 903, in check_recomputed_tensors_match
[rank0]:     raise CheckpointError(
[rank0]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank0]: tensor at position 6:
[rank0]: saved metadata: {'shape': torch.Size([4096, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: tensor at position 12:
[rank0]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: tensor at position 18:
[rank0]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: tensor at position 34:
[rank0]: saved metadata: {'shape': torch.Size([3584, 4096]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: tensor at position 50:
[rank0]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: tensor at position 57:
[rank0]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: tensor at position 65:
[rank0]: saved metadata: {'shape': torch.Size([3584, 14336]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}

[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mDPO_r-64_lr-3e-07_e-3_b-0.2[0m at: [34mhttps://wandb.ai/slolama/GaMS-9B-Translation-DPO/runs/t8je0b9h[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250531_011443-t8je0b9h/logs[0m
W0531 01:14:55.264000 311693 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 311869 closing signal SIGTERM
W0531 01:14:55.264000 311693 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 311871 closing signal SIGTERM
W0531 01:14:55.266000 311693 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 311872 closing signal SIGTERM
E0531 01:14:55.832000 311693 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 311870) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1182, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 861, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-31_01:14:55
  host      : pm5-nod30.vega.pri
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 311870)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
