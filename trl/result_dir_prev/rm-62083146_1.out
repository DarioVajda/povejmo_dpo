cpu-bind=MASK - gn15, task  1  0 [529751]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 1 ---
Total Nodes: 2
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn11
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 2     --machine_rank 1     --main_process_ip gn11     --main_process_port 29500     --num_processes 8     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62083146     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train.py"     --rank=64 --learning_rate=3e-7 --total_epochs=3 --beta=0.2
-------------------------------------------
[2025-05-31 01:12:47,035] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0531 01:12:48.409000 529801 torch/distributed/run.py:792] 
W0531 01:12:48.409000 529801 torch/distributed/run.py:792] *****************************************
W0531 01:12:48.409000 529801 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0531 01:12:48.409000 529801 torch/distributed/run.py:792] *****************************************
[2025-05-31 01:12:53,720] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 01:12:53,766] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 01:12:53,780] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 01:12:53,788] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
World size: 8
Setting gradient accumulation steps to: 2
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
[2025-05-31 01:12:56,256] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Steps per epoch: 4282
Eval steps: 2141
[2025-05-31 01:12:56,265] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-31 01:12:56,273] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-31 01:12:56,274] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
[2025-05-31 01:12:56,975] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-31 01:12:57,053] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-31 01:12:57,064] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-31 01:12:57,251] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.46s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.46s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.46s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.46s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
Using LoRA and set up the model
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5409.30 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1115/8564 [00:00<00:01, 5532.14 examples/s]Extracting prompt in train dataset:  20%|█▉        | 1680/8564 [00:00<00:01, 5537.83 examples/s]Extracting prompt in train dataset:  26%|██▋       | 2250/8564 [00:00<00:01, 5592.70 examples/s]Extracting prompt in train dataset:  33%|███▎      | 2820/8564 [00:00<00:01, 5618.63 examples/s]Extracting prompt in train dataset:  43%|████▎     | 3650/8564 [00:00<00:00, 5553.49 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4220/8564 [00:00<00:00, 5589.95 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 4790/8564 [00:00<00:00, 5615.63 examples/s]Extracting prompt in train dataset:  63%|██████▎   | 5375/8564 [00:00<00:00, 5682.38 examples/s]Extracting prompt in train dataset:  73%|███████▎  | 6220/8564 [00:01<00:00, 5648.77 examples/s]Extracting prompt in train dataset:  82%|████████▏ | 7060/8564 [00:01<00:00, 5626.26 examples/s]Extracting prompt in train dataset:  91%|█████████▏| 7830/8564 [00:01<00:00, 5452.11 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 8420/8564 [00:01<00:00, 5546.54 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5524.60 examples/s]
Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   3%|▎         | 280/8564 [00:00<00:02, 2778.59 examples/s]Applying chat template to train dataset:   7%|▋         | 588/8564 [00:00<00:02, 2951.64 examples/s]Applying chat template to train dataset:  10%|█         | 894/8564 [00:00<00:02, 2996.32 examples/s]Applying chat template to train dataset:  14%|█▍        | 1202/8564 [00:00<00:02, 3023.92 examples/s]Applying chat template to train dataset:  19%|█▉        | 1657/8564 [00:00<00:02, 3025.24 examples/s]Applying chat template to train dataset:  23%|██▎       | 1966/8564 [00:00<00:02, 3042.44 examples/s]Applying chat template to train dataset:  27%|██▋       | 2276/8564 [00:00<00:02, 3058.53 examples/s]Applying chat template to train dataset:  30%|███       | 2586/8564 [00:00<00:01, 3067.67 examples/s]Applying chat template to train dataset:  34%|███▍      | 2896/8564 [00:00<00:01, 3073.69 examples/s]Applying chat template to train dataset:  37%|███▋      | 3204/8564 [00:01<00:01, 3023.60 examples/s]Applying chat template to train dataset:  41%|████      | 3513/8564 [00:01<00:01, 3041.81 examples/s]Applying chat template to train dataset:  45%|████▍     | 3822/8564 [00:01<00:01, 3053.66 examples/s]Applying chat template to train dataset:  48%|████▊     | 4131/8564 [00:01<00:01, 3062.55 examples/s]Applying chat template to train dataset:  52%|█████▏    | 4440/8564 [00:01<00:01, 3069.06 examples/s]Applying chat template to train dataset:  57%|█████▋    | 4902/8564 [00:01<00:01, 3071.35 examples/s]Applying chat template to train dataset:  61%|██████    | 5220/8564 [00:01<00:01, 3093.31 examples/s]Applying chat template to train dataset:  65%|██████▍   | 5537/8564 [00:01<00:00, 3112.32 examples/s]Applying chat template to train dataset:  68%|██████▊   | 5853/8564 [00:01<00:00, 3123.27 examples/s]Applying chat template to train dataset:  72%|███████▏  | 6170/8564 [00:02<00:00, 3130.44 examples/s]Applying chat template to train dataset:  76%|███████▌  | 6487/8564 [00:02<00:00, 3135.97 examples/s]Applying chat template to train dataset:  81%|████████▏ | 6959/8564 [00:02<00:00, 3137.62 examples/s]Applying chat template to train dataset:  85%|████████▍ | 7274/8564 [00:02<00:00, 3138.87 examples/s]Applying chat template to train dataset:  89%|████████▊ | 7590/8564 [00:02<00:00, 3140.95 examples/s]Applying chat template to train dataset:  94%|█████████▎| 8020/8564 [00:02<00:00, 3037.69 examples/s]Applying chat template to train dataset:  97%|█████████▋| 8337/8564 [00:02<00:00, 3070.62 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3067.42 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 42/8564 [00:00<00:20, 408.90 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:24, 342.03 examples/s]Tokenizing train dataset:   2%|▏         | 139/8564 [00:00<00:26, 322.18 examples/s]Tokenizing train dataset:   2%|▏         | 183/8564 [00:00<00:27, 307.48 examples/s]Tokenizing train dataset:   3%|▎         | 218/8564 [00:00<00:26, 315.12 examples/s]Tokenizing train dataset:   3%|▎         | 252/8564 [00:00<00:26, 319.53 examples/s]Tokenizing train dataset:   3%|▎         | 290/8564 [00:00<00:24, 332.81 examples/s]Tokenizing train dataset:   4%|▍         | 324/8564 [00:00<00:24, 332.05 examples/s]Tokenizing train dataset:   4%|▍         | 370/8564 [00:01<00:25, 318.43 examples/s]Tokenizing train dataset:   5%|▍         | 420/8564 [00:01<00:25, 316.94 examples/s]Tokenizing train dataset:   5%|▌         | 469/8564 [00:01<00:25, 314.97 examples/s]Tokenizing train dataset:   6%|▌         | 512/8564 [00:01<00:26, 304.76 examples/s]Tokenizing train dataset:   6%|▋         | 549/8564 [00:01<00:25, 318.13 examples/s]Tokenizing train dataset:   7%|▋         | 596/8564 [00:01<00:25, 309.35 examples/s]Tokenizing train dataset:   7%|▋         | 634/8564 [00:01<00:24, 323.87 examples/s]Tokenizing train dataset:   8%|▊         | 681/8564 [00:02<00:24, 315.43 examples/s]Tokenizing train dataset:   9%|▊         | 729/8564 [00:02<00:25, 313.33 examples/s]Tokenizing train dataset:   9%|▉         | 762/8564 [00:02<00:24, 314.82 examples/s]Tokenizing train dataset:   9%|▉         | 806/8564 [00:02<00:25, 300.75 examples/s]Tokenizing train dataset:  10%|▉         | 850/8564 [00:02<00:26, 294.82 examples/s]Tokenizing train dataset:  10%|█         | 885/8564 [00:02<00:25, 302.82 examples/s]Tokenizing train dataset:  11%|█         | 920/8564 [00:02<00:24, 307.88 examples/s]Tokenizing train dataset:  11%|█▏        | 968/8564 [00:03<00:24, 310.67 examples/s]Tokenizing train dataset:  12%|█▏        | 1009/8564 [00:03<00:25, 296.78 examples/s]Tokenizing train dataset:  12%|█▏        | 1053/8564 [00:03<00:25, 291.75 examples/s]Tokenizing train dataset:  13%|█▎        | 1087/8564 [00:03<00:24, 300.56 examples/s]Tokenizing train dataset:  13%|█▎        | 1120/8564 [00:03<00:24, 303.07 examples/s]Tokenizing train dataset:  14%|█▎        | 1161/8564 [00:03<00:25, 290.14 examples/s]Tokenizing train dataset:  14%|█▍        | 1197/8564 [00:03<00:24, 304.64 examples/s]Tokenizing train dataset:  14%|█▍        | 1229/8564 [00:03<00:24, 304.68 examples/s]Tokenizing train dataset:  15%|█▍        | 1265/8564 [00:04<00:23, 314.84 examples/s]Tokenizing train dataset:  15%|█▌        | 1298/8564 [00:04<00:23, 313.63 examples/s]Tokenizing train dataset:  16%|█▌        | 1330/8564 [00:04<00:23, 313.82 examples/s]Tokenizing train dataset:  16%|█▌        | 1373/8564 [00:04<00:24, 296.62 examples/s]Tokenizing train dataset:  16%|█▋        | 1407/8564 [00:04<00:23, 303.40 examples/s]Tokenizing train dataset:  17%|█▋        | 1453/8564 [00:04<00:23, 299.94 examples/s]Tokenizing train dataset:  18%|█▊        | 1499/8564 [00:04<00:23, 297.39 examples/s]Tokenizing train dataset:  18%|█▊        | 1530/8564 [00:04<00:23, 297.30 examples/s]Tokenizing train dataset:  18%|█▊        | 1561/8564 [00:05<00:23, 298.26 examples/s]Tokenizing train dataset:  19%|█▊        | 1591/8564 [00:05<00:23, 294.15 examples/s]Tokenizing train dataset:  19%|█▉        | 1627/8564 [00:05<00:22, 304.88 examples/s]Tokenizing train dataset:  19%|█▉        | 1661/8564 [00:05<00:22, 312.83 examples/s]Tokenizing train dataset:  20%|█▉        | 1703/8564 [00:05<00:20, 340.33 examples/s]Tokenizing train dataset:  20%|██        | 1753/8564 [00:05<00:20, 329.31 examples/s]Tokenizing train dataset:  21%|██        | 1800/8564 [00:05<00:20, 322.90 examples/s]Tokenizing train dataset:  21%|██▏       | 1835/8564 [00:05<00:20, 326.54 examples/s]Tokenizing train dataset:  22%|██▏       | 1880/8564 [00:06<00:21, 311.37 examples/s]Tokenizing train dataset:  22%|██▏       | 1921/8564 [00:06<00:19, 333.45 examples/s]Tokenizing train dataset:  23%|██▎       | 1960/8564 [00:06<00:19, 341.34 examples/s]Tokenizing train dataset:  23%|██▎       | 1999/8564 [00:06<00:18, 350.23 examples/s]Tokenizing train dataset:  24%|██▍       | 2039/8564 [00:06<00:18, 359.65 examples/s]Tokenizing train dataset:  24%|██▍       | 2077/8564 [00:06<00:17, 361.73 examples/s]Tokenizing train dataset:  25%|██▍       | 2116/8564 [00:06<00:17, 366.20 examples/s]Tokenizing train dataset:  25%|██▌       | 2154/8564 [00:06<00:17, 366.47 examples/s]Tokenizing train dataset:  26%|██▌       | 2192/8564 [00:06<00:17, 365.87 examples/s]Tokenizing train dataset:  26%|██▌       | 2230/8564 [00:06<00:17, 365.84 examples/s]Tokenizing train dataset:  27%|██▋       | 2270/8564 [00:07<00:16, 373.49 examples/s]Tokenizing train dataset:  27%|██▋       | 2309/8564 [00:07<00:16, 373.07 examples/s]Tokenizing train dataset:  28%|██▊       | 2359/8564 [00:07<00:17, 354.37 examples/s]Tokenizing train dataset:  28%|██▊       | 2403/8564 [00:07<00:16, 374.67 examples/s]Tokenizing train dataset:  29%|██▊       | 2462/8564 [00:07<00:16, 376.83 examples/s]Tokenizing train dataset:  29%|██▉       | 2513/8564 [00:07<00:16, 358.60 examples/s]Tokenizing train dataset:  30%|██▉       | 2558/8564 [00:07<00:15, 375.84 examples/s]Tokenizing train dataset:  31%|███       | 2614/8564 [00:08<00:16, 371.71 examples/s]Tokenizing train dataset:  31%|███       | 2661/8564 [00:08<00:16, 347.86 examples/s]Tokenizing train dataset:  32%|███▏      | 2716/8564 [00:08<00:16, 349.16 examples/s]Tokenizing train dataset:  32%|███▏      | 2768/8564 [00:08<00:16, 343.85 examples/s]Tokenizing train dataset:  33%|███▎      | 2808/8564 [00:08<00:16, 354.51 examples/s]Tokenizing train dataset:  33%|███▎      | 2860/8564 [00:08<00:16, 347.97 examples/s]Tokenizing train dataset:  34%|███▍      | 2900/8564 [00:08<00:15, 356.44 examples/s]Tokenizing train dataset:  34%|███▍      | 2944/8564 [00:08<00:14, 375.96 examples/s]Tokenizing train dataset:  35%|███▍      | 2989/8564 [00:09<00:14, 391.76 examples/s]Tokenizing train dataset:  36%|███▌      | 3043/8564 [00:09<00:14, 376.55 examples/s]Tokenizing train dataset:  36%|███▌      | 3098/8564 [00:09<00:14, 370.73 examples/s]Tokenizing train dataset:  37%|███▋      | 3153/8564 [00:09<00:14, 366.70 examples/s]Tokenizing train dataset:  37%|███▋      | 3208/8564 [00:09<00:14, 363.68 examples/s]Tokenizing train dataset:  38%|███▊      | 3250/8564 [00:09<00:14, 372.91 examples/s]Tokenizing train dataset:  39%|███▊      | 3302/8564 [00:09<00:14, 361.11 examples/s]Tokenizing train dataset:  39%|███▉      | 3357/8564 [00:10<00:14, 360.93 examples/s]Tokenizing train dataset:  40%|███▉      | 3397/8564 [00:10<00:14, 368.85 examples/s]Tokenizing train dataset:  40%|████      | 3438/8564 [00:10<00:13, 375.54 examples/s]Tokenizing train dataset:  41%|████      | 3493/8564 [00:10<00:13, 367.80 examples/s]Tokenizing train dataset:  41%|████▏     | 3536/8564 [00:10<00:13, 378.00 examples/s]Tokenizing train dataset:  42%|████▏     | 3592/8564 [00:10<00:13, 374.23 examples/s]Tokenizing train dataset:  42%|████▏     | 3631/8564 [00:10<00:13, 375.03 examples/s]Tokenizing train dataset:  43%|████▎     | 3680/8564 [00:10<00:13, 353.22 examples/s]Tokenizing train dataset:  43%|████▎     | 3724/8564 [00:11<00:13, 371.43 examples/s]Tokenizing train dataset:  44%|████▍     | 3780/8564 [00:11<00:12, 369.79 examples/s]Tokenizing train dataset:  45%|████▍     | 3834/8564 [00:11<00:12, 364.04 examples/s]Tokenizing train dataset:  45%|████▌     | 3890/8564 [00:11<00:12, 362.85 examples/s]Tokenizing train dataset:  46%|████▌     | 3941/8564 [00:11<00:13, 352.84 examples/s]Tokenizing train dataset:  46%|████▋     | 3978/8564 [00:11<00:12, 354.81 examples/s]Tokenizing train dataset:  47%|████▋     | 4015/8564 [00:11<00:12, 357.35 examples/s]Tokenizing train dataset:  47%|████▋     | 4056/8564 [00:11<00:12, 365.81 examples/s]Tokenizing train dataset:  48%|████▊     | 4110/8564 [00:12<00:12, 360.22 examples/s]Tokenizing train dataset:  49%|████▊     | 4162/8564 [00:12<00:12, 353.02 examples/s]Tokenizing train dataset:  49%|████▉     | 4203/8564 [00:12<00:12, 363.25 examples/s]Tokenizing train dataset:  50%|████▉     | 4257/8564 [00:12<00:12, 357.81 examples/s]Tokenizing train dataset:  50%|█████     | 4313/8564 [00:12<00:11, 359.36 examples/s]Tokenizing train dataset:  51%|█████     | 4365/8564 [00:12<00:11, 353.18 examples/s]Tokenizing train dataset:  51%|█████▏    | 4405/8564 [00:12<00:11, 360.16 examples/s]Tokenizing train dataset:  52%|█████▏    | 4444/8564 [00:13<00:11, 363.31 examples/s]Tokenizing train dataset:  52%|█████▏    | 4482/8564 [00:13<00:11, 362.88 examples/s]Tokenizing train dataset:  53%|█████▎    | 4536/8564 [00:13<00:11, 357.27 examples/s]Tokenizing train dataset:  53%|█████▎    | 4572/8564 [00:13<00:11, 354.90 examples/s]Tokenizing train dataset:  54%|█████▍    | 4609/8564 [00:13<00:11, 355.41 examples/s]Tokenizing train dataset:  54%|█████▍    | 4660/8564 [00:13<00:11, 342.90 examples/s]Tokenizing train dataset:  55%|█████▍    | 4708/8564 [00:13<00:11, 330.56 examples/s]Tokenizing train dataset:  55%|█████▌    | 4742/8564 [00:13<00:11, 330.40 examples/s]Tokenizing train dataset:  56%|█████▌    | 4792/8564 [00:14<00:11, 324.74 examples/s]Tokenizing train dataset:  57%|█████▋    | 4853/8564 [00:14<00:09, 390.92 examples/s]Tokenizing train dataset:  57%|█████▋    | 4912/8564 [00:14<00:08, 434.48 examples/s]Tokenizing train dataset:  58%|█████▊    | 4974/8564 [00:14<00:07, 481.91 examples/s]Tokenizing train dataset:  59%|█████▉    | 5033/8564 [00:14<00:06, 507.70 examples/s]Tokenizing train dataset:  60%|█████▉    | 5106/8564 [00:14<00:06, 563.12 examples/s]Tokenizing train dataset:  60%|██████    | 5177/8564 [00:14<00:05, 599.04 examples/s]Tokenizing train dataset:  61%|██████▏   | 5248/8564 [00:14<00:05, 628.08 examples/s]Tokenizing train dataset:  62%|██████▏   | 5319/8564 [00:14<00:04, 650.86 examples/s]Tokenizing train dataset:  63%|██████▎   | 5402/8564 [00:15<00:05, 605.20 examples/s]Tokenizing train dataset:  64%|██████▍   | 5470/8564 [00:15<00:04, 619.19 examples/s]Tokenizing train dataset:  65%|██████▍   | 5560/8564 [00:15<00:04, 601.91 examples/s]Tokenizing train dataset:  66%|██████▌   | 5631/8564 [00:15<00:04, 624.85 examples/s]Tokenizing train dataset:  67%|██████▋   | 5698/8564 [00:15<00:04, 634.23 examples/s]Tokenizing train dataset:  67%|██████▋   | 5770/8564 [00:15<00:04, 651.43 examples/s]Tokenizing train dataset:  68%|██████▊   | 5840/8564 [00:15<00:04, 661.49 examples/s]Tokenizing train dataset:  69%|██████▉   | 5928/8564 [00:15<00:04, 626.28 examples/s]Tokenizing train dataset:  70%|███████   | 6015/8564 [00:16<00:04, 604.71 examples/s]Tokenizing train dataset:  71%|███████   | 6100/8564 [00:16<00:04, 588.13 examples/s]Tokenizing train dataset:  72%|███████▏  | 6175/8564 [00:16<00:03, 625.15 examples/s]Tokenizing train dataset:  73%|███████▎  | 6244/8564 [00:16<00:03, 638.83 examples/s]Tokenizing train dataset:  74%|███████▎  | 6311/8564 [00:16<00:03, 637.54 examples/s]Tokenizing train dataset:  74%|███████▍  | 6380/8564 [00:16<00:03, 648.33 examples/s]Tokenizing train dataset:  76%|███████▌  | 6471/8564 [00:16<00:03, 627.97 examples/s]Tokenizing train dataset:  76%|███████▋  | 6535/8564 [00:16<00:03, 625.00 examples/s]Tokenizing train dataset:  77%|███████▋  | 6613/8564 [00:17<00:03, 577.29 examples/s]Tokenizing train dataset:  78%|███████▊  | 6679/8564 [00:17<00:03, 596.00 examples/s]Tokenizing train dataset:  79%|███████▉  | 6749/8564 [00:17<00:02, 620.13 examples/s]Tokenizing train dataset:  80%|███████▉  | 6835/8564 [00:17<00:02, 601.55 examples/s]Tokenizing train dataset:  81%|████████  | 6900/8564 [00:17<00:02, 610.54 examples/s]Tokenizing train dataset:  81%|████████▏ | 6965/8564 [00:17<00:02, 614.98 examples/s]Tokenizing train dataset:  82%|████████▏ | 7058/8564 [00:17<00:02, 606.36 examples/s]Tokenizing train dataset:  83%|████████▎ | 7124/8564 [00:17<00:02, 616.53 examples/s]Tokenizing train dataset:  84%|████████▍ | 7192/8564 [00:17<00:02, 632.32 examples/s]Tokenizing train dataset:  85%|████████▌ | 7282/8564 [00:18<00:02, 616.27 examples/s]Tokenizing train dataset:  86%|████████▌ | 7379/8564 [00:18<00:01, 622.86 examples/s]Tokenizing train dataset:  87%|████████▋ | 7443/8564 [00:18<00:01, 625.04 examples/s]Tokenizing train dataset:  88%|████████▊ | 7506/8564 [00:18<00:01, 622.59 examples/s]Tokenizing train dataset:  89%|████████▊ | 7581/8564 [00:18<00:01, 653.24 examples/s]Tokenizing train dataset:  90%|████████▉ | 7665/8564 [00:18<00:01, 617.15 examples/s]Tokenizing train dataset:  90%|█████████ | 7728/8564 [00:18<00:01, 619.09 examples/s]Tokenizing train dataset:  91%|█████████ | 7810/8564 [00:18<00:01, 585.42 examples/s]Tokenizing train dataset:  92%|█████████▏| 7874/8564 [00:19<00:01, 598.34 examples/s]Tokenizing train dataset:  93%|█████████▎| 7937/8564 [00:19<00:01, 604.76 examples/s]Tokenizing train dataset:  93%|█████████▎| 8004/8564 [00:19<00:00, 619.71 examples/s]Tokenizing train dataset:  94%|█████████▍| 8091/8564 [00:19<00:00, 598.63 examples/s]Tokenizing train dataset:  96%|█████████▌| 8184/8564 [00:19<00:00, 599.48 examples/s]Tokenizing train dataset:  96%|█████████▋| 8251/8564 [00:19<00:00, 608.69 examples/s]Tokenizing train dataset:  97%|█████████▋| 8328/8564 [00:19<00:00, 645.31 examples/s]Tokenizing train dataset:  98%|█████████▊| 8413/8564 [00:19<00:00, 609.95 examples/s]Tokenizing train dataset:  99%|█████████▉| 8478/8564 [00:20<00:00, 616.34 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 611.31 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 423.53 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11223.28 examples/s]
Extracting prompt in train dataset:   6%|▋         | 551/8564 [00:00<00:01, 5455.88 examples/s]Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5396.09 examples/s]Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5375.41 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1119/8564 [00:00<00:01, 5578.34 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1109/8564 [00:00<00:01, 5499.24 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1113/8564 [00:00<00:01, 5514.60 examples/s]Extracting prompt in train dataset:  20%|█▉        | 1680/8564 [00:00<00:01, 5567.21 examples/s]Extracting prompt in train dataset:  19%|█▉        | 1669/8564 [00:00<00:01, 5534.65 examples/s]Extracting prompt in train dataset:  19%|█▉        | 1660/8564 [00:00<00:01, 5482.31 examples/s]Extracting prompt in train dataset:  26%|██▋       | 2253/8564 [00:00<00:01, 5629.95 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2230/8564 [00:00<00:01, 5551.18 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2227/8564 [00:00<00:01, 5552.76 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13711.77 examples/s]
Extracting prompt in train dataset:  33%|███▎      | 2830/8564 [00:00<00:01, 5663.78 examples/s]Extracting prompt in train dataset:  33%|███▎      | 2800/8564 [00:00<00:01, 5587.39 examples/s]Extracting prompt in train dataset:  33%|███▎      | 2795/8564 [00:00<00:01, 5582.09 examples/s]Extracting prompt in train dataset:  43%|████▎     | 3670/8564 [00:00<00:00, 5611.06 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3620/8564 [00:00<00:00, 5517.48 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3619/8564 [00:00<00:00, 5518.76 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4189/8564 [00:00<00:00, 5566.70 examples/s]Extracting prompt in train dataset:  50%|████▉     | 4250/8564 [00:00<00:00, 5646.44 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4183/8564 [00:00<00:00, 5537.30 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  56%|█████▌    | 4755/8564 [00:00<00:00, 5581.26 examples/s]Extracting prompt in train dataset:  55%|█████▌    | 4749/8564 [00:00<00:00, 5572.65 examples/s]Extracting prompt in train dataset:  56%|█████▋    | 4830/8564 [00:00<00:00, 5672.31 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 328.78 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 5330/8564 [00:00<00:00, 5623.95 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 5320/8564 [00:00<00:00, 5605.23 examples/s]Extracting prompt in train dataset:  63%|██████▎   | 5420/8564 [00:00<00:00, 5724.39 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 5910/8564 [00:01<00:00, 5665.50 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 5900/8564 [00:01<00:00, 5649.44 examples/s]Extracting prompt in train dataset:  70%|███████   | 6010/8564 [00:01<00:00, 5762.47 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 297.79 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 6490/8564 [00:01<00:00, 5697.15 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 6480/8564 [00:01<00:00, 5681.15 examples/s]Extracting prompt in train dataset:  77%|███████▋  | 6600/8564 [00:01<00:00, 5790.76 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 7070/8564 [00:01<00:00, 5716.04 examples/s]Extracting prompt in train dataset:  82%|████████▏ | 7060/8564 [00:01<00:00, 5700.05 examples/s]Extracting prompt in train dataset:  84%|████████▍ | 7190/8564 [00:01<00:00, 5806.47 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:02, 278.79 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 7650/8564 [00:01<00:00, 5733.02 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 7640/8564 [00:01<00:00, 5714.04 examples/s]Extracting prompt in train dataset:  94%|█████████▎| 8023/8564 [00:01<00:00, 5633.10 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 269.32 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 8430/8564 [00:01<00:00, 5514.99 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 8420/8564 [00:01<00:00, 5497.34 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5644.40 examples/s]
Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5554.60 examples/s]
Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5538.37 examples/s]
Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:02, 260.88 examples/s]Tokenizing eval dataset:  25%|██▍       | 238/953 [00:00<00:02, 291.64 examples/s]Tokenizing eval dataset:  32%|███▏      | 304/953 [00:00<00:01, 387.01 examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing eval dataset:  39%|███▊      | 367/953 [00:01<00:01, 451.64 examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   3%|▎         | 290/8564 [00:00<00:02, 2858.01 examples/s]Tokenizing eval dataset:  46%|████▌     | 437/953 [00:01<00:00, 517.30 examples/s]Applying chat template to train dataset:   3%|▎         | 287/8564 [00:00<00:02, 2842.92 examples/s]Applying chat template to train dataset:   3%|▎         | 289/8564 [00:00<00:02, 2862.73 examples/s]Applying chat template to train dataset:   7%|▋         | 608/8564 [00:00<00:02, 3045.36 examples/s]Tokenizing eval dataset:  53%|█████▎    | 504/953 [00:01<00:00, 560.05 examples/s]Applying chat template to train dataset:   7%|▋         | 601/8564 [00:00<00:02, 3011.63 examples/s]Applying chat template to train dataset:   7%|▋         | 606/8564 [00:00<00:02, 3035.33 examples/s]Applying chat template to train dataset:  11%|█         | 924/8564 [00:00<00:02, 3096.88 examples/s]Tokenizing eval dataset:  60%|█████▉    | 569/953 [00:01<00:00, 583.23 examples/s]Applying chat template to train dataset:  11%|█         | 916/8564 [00:00<00:02, 3071.71 examples/s]Applying chat template to train dataset:  11%|█         | 922/8564 [00:00<00:02, 3085.80 examples/s]Applying chat template to train dataset:  15%|█▍        | 1243/8564 [00:00<00:02, 3127.87 examples/s]Tokenizing eval dataset:  67%|██████▋   | 640/953 [00:01<00:00, 609.52 examples/s]Applying chat template to train dataset:  14%|█▍        | 1231/8564 [00:00<00:02, 3098.78 examples/s]Applying chat template to train dataset:  14%|█▍        | 1240/8564 [00:00<00:02, 3116.99 examples/s]Applying chat template to train dataset:  18%|█▊        | 1556/8564 [00:00<00:02, 3124.64 examples/s]Tokenizing eval dataset:  77%|███████▋  | 730/953 [00:01<00:00, 599.53 examples/s]Applying chat template to train dataset:  20%|█▉        | 1698/8564 [00:00<00:02, 3099.07 examples/s]Applying chat template to train dataset:  20%|█▉        | 1710/8564 [00:00<00:02, 3115.57 examples/s]Applying chat template to train dataset:  22%|██▏       | 1872/8564 [00:00<00:02, 3132.15 examples/s]Applying chat template to train dataset:  24%|██▎       | 2014/8564 [00:00<00:02, 3115.79 examples/s]Applying chat template to train dataset:  24%|██▎       | 2030/8564 [00:00<00:02, 3134.72 examples/s]Applying chat template to train dataset:  26%|██▌       | 2193/8564 [00:00<00:02, 3154.98 examples/s]Tokenizing eval dataset:  85%|████████▍ | 807/953 [00:01<00:00, 564.20 examples/s]Applying chat template to train dataset:  27%|██▋       | 2332/8564 [00:00<00:01, 3130.81 examples/s]Applying chat template to train dataset:  27%|██▋       | 2350/8564 [00:00<00:01, 3151.69 examples/s]Applying chat template to train dataset:  29%|██▉       | 2513/8564 [00:00<00:01, 3168.43 examples/s]Applying chat template to train dataset:  31%|███       | 2650/8564 [00:00<00:01, 3141.66 examples/s]Tokenizing eval dataset:  93%|█████████▎| 883/953 [00:01<00:00, 542.83 examples/s]Applying chat template to train dataset:  31%|███       | 2670/8564 [00:00<00:01, 3162.42 examples/s]Applying chat template to train dataset:  33%|███▎      | 2833/8564 [00:00<00:01, 3176.58 examples/s]Applying chat template to train dataset:  35%|███▍      | 2970/8564 [00:00<00:01, 3151.75 examples/s]Applying chat template to train dataset:  35%|███▍      | 2991/8564 [00:00<00:01, 3172.08 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 531.61 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 464.66 examples/s]
Applying chat template to train dataset:  38%|███▊      | 3296/8564 [00:01<00:01, 3138.69 examples/s]Applying chat template to train dataset:  40%|████      | 3426/8564 [00:01<00:01, 3102.73 examples/s]Applying chat template to train dataset:  40%|████      | 3450/8564 [00:01<00:01, 3120.37 examples/s]Applying chat template to train dataset:  42%|████▏     | 3615/8564 [00:01<00:01, 3151.78 examples/s]Applying chat template to train dataset:  44%|████▎     | 3742/8564 [00:01<00:01, 3116.02 examples/s]Applying chat template to train dataset:  44%|████▍     | 3770/8564 [00:01<00:01, 3135.75 examples/s]Applying chat template to train dataset:  46%|████▌     | 3935/8564 [00:01<00:01, 3161.98 examples/s]Applying chat template to train dataset:  47%|████▋     | 4060/8564 [00:01<00:01, 3126.49 examples/s]Applying chat template to train dataset:  48%|████▊     | 4090/8564 [00:01<00:01, 3147.01 examples/s]Applying chat template to train dataset:  50%|████▉     | 4255/8564 [00:01<00:01, 3168.84 examples/s]Applying chat template to train dataset:  51%|█████     | 4377/8564 [00:01<00:01, 3135.80 examples/s]Applying chat template to train dataset:  51%|█████▏    | 4410/8564 [00:01<00:01, 3153.48 examples/s]Applying chat template to train dataset:  53%|█████▎    | 4575/8564 [00:01<00:01, 3175.62 examples/s]Applying chat template to train dataset:  55%|█████▍    | 4693/8564 [00:01<00:01, 3139.21 examples/s]Applying chat template to train dataset:  55%|█████▌    | 4730/8564 [00:01<00:01, 3156.30 examples/s]Applying chat template to train dataset:  57%|█████▋    | 4895/8564 [00:01<00:01, 3180.97 examples/s]Applying chat template to train dataset:  59%|█████▊    | 5013/8564 [00:01<00:01, 3156.06 examples/s]Applying chat template to train dataset:  59%|█████▉    | 5054/8564 [00:01<00:01, 3177.13 examples/s]Applying chat template to train dataset:  61%|██████    | 5222/8564 [00:01<00:01, 3204.91 examples/s]Applying chat template to train dataset:  62%|██████▏   | 5338/8564 [00:01<00:01, 3181.20 examples/s]Applying chat template to train dataset:  63%|██████▎   | 5380/8564 [00:01<00:00, 3196.69 examples/s]Applying chat template to train dataset:  65%|██████▍   | 5550/8564 [00:01<00:00, 3220.56 examples/s]Applying chat template to train dataset:  66%|██████▌   | 5662/8564 [00:01<00:00, 3193.03 examples/s]Applying chat template to train dataset:  67%|██████▋   | 5707/8564 [00:01<00:00, 3214.25 examples/s]Applying chat template to train dataset:  69%|██████▊   | 5879/8564 [00:01<00:00, 3240.04 examples/s]Applying chat template to train dataset:  70%|██████▉   | 5987/8564 [00:01<00:00, 3207.14 examples/s]Applying chat template to train dataset:  70%|███████   | 6033/8564 [00:01<00:00, 3222.58 examples/s]Applying chat template to train dataset:  72%|███████▏  | 6205/8564 [00:01<00:00, 3244.33 examples/s]Applying chat template to train dataset:  74%|███████▎  | 6310/8564 [00:02<00:00, 3212.66 examples/s]Applying chat template to train dataset:  74%|███████▍  | 6360/8564 [00:02<00:00, 3230.54 examples/s]Applying chat template to train dataset:  76%|███████▋  | 6531/8564 [00:02<00:00, 3248.18 examples/s]Applying chat template to train dataset:  77%|███████▋  | 6634/8564 [00:02<00:00, 3216.90 examples/s]Applying chat template to train dataset:  78%|███████▊  | 6686/8564 [00:02<00:00, 3234.61 examples/s]Applying chat template to train dataset:  80%|████████  | 6858/8564 [00:02<00:00, 3252.08 examples/s]Applying chat template to train dataset:  81%|████████  | 6958/8564 [00:02<00:00, 3222.36 examples/s]Applying chat template to train dataset:  82%|████████▏ | 7011/8564 [00:02<00:00, 3235.31 examples/s]Applying chat template to train dataset:  84%|████████▍ | 7185/8564 [00:02<00:00, 3254.20 examples/s]Applying chat template to train dataset:  85%|████████▌ | 7282/8564 [00:02<00:00, 3222.00 examples/s]Applying chat template to train dataset:  86%|████████▌ | 7337/8564 [00:02<00:00, 3239.98 examples/s]Applying chat template to train dataset:  90%|████████▉ | 7674/8564 [00:02<00:00, 3252.87 examples/s]Applying chat template to train dataset:  89%|████████▉ | 7607/8564 [00:02<00:00, 3227.03 examples/s]Applying chat template to train dataset:  89%|████████▉ | 7663/8564 [00:02<00:00, 3240.69 examples/s]Applying chat template to train dataset:  95%|█████████▍| 8122/8564 [00:02<00:00, 3155.13 examples/s]Applying chat template to train dataset:  94%|█████████▍| 8074/8564 [00:02<00:00, 3099.25 examples/s]Applying chat template to train dataset:  95%|█████████▍| 8103/8564 [00:02<00:00, 3114.34 examples/s]Applying chat template to train dataset:  99%|█████████▊| 8450/8564 [00:02<00:00, 3181.75 examples/s]Applying chat template to train dataset:  98%|█████████▊| 8399/8564 [00:02<00:00, 3137.21 examples/s]Applying chat template to train dataset:  98%|█████████▊| 8430/8564 [00:02<00:00, 3150.46 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3173.28 examples/s]
Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3157.68 examples/s]
Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3140.09 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 42/8564 [00:00<00:20, 406.65 examples/s]Tokenizing train dataset:   0%|          | 42/8564 [00:00<00:20, 410.20 examples/s]Tokenizing train dataset:   0%|          | 42/8564 [00:00<00:20, 411.54 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:24, 342.09 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:24, 342.41 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:24, 343.87 examples/s]Tokenizing train dataset:   1%|▏         | 126/8564 [00:00<00:29, 287.39 examples/s]Tokenizing train dataset:   2%|▏         | 139/8564 [00:00<00:26, 322.45 examples/s]Tokenizing train dataset:   2%|▏         | 139/8564 [00:00<00:26, 323.78 examples/s]Tokenizing train dataset:   2%|▏         | 160/8564 [00:00<00:28, 296.16 examples/s]Tokenizing train dataset:   2%|▏         | 182/8564 [00:00<00:27, 302.26 examples/s]Tokenizing train dataset:   2%|▏         | 182/8564 [00:00<00:27, 304.18 examples/s]Tokenizing train dataset:   2%|▏         | 204/8564 [00:00<00:28, 292.29 examples/s]Tokenizing train dataset:   3%|▎         | 218/8564 [00:00<00:26, 313.00 examples/s]Tokenizing train dataset:   3%|▎         | 218/8564 [00:00<00:26, 314.78 examples/s]Tokenizing train dataset:   3%|▎         | 240/8564 [00:00<00:26, 308.83 examples/s]Tokenizing train dataset:   3%|▎         | 252/8564 [00:00<00:26, 318.12 examples/s]Tokenizing train dataset:   3%|▎         | 252/8564 [00:00<00:25, 319.93 examples/s]Tokenizing train dataset:   3%|▎         | 277/8564 [00:00<00:25, 322.47 examples/s]Tokenizing train dataset:   3%|▎         | 290/8564 [00:00<00:24, 331.84 examples/s]Tokenizing train dataset:   3%|▎         | 290/8564 [00:00<00:24, 333.59 examples/s]Tokenizing train dataset:   4%|▎         | 311/8564 [00:00<00:25, 322.88 examples/s]Tokenizing train dataset:   4%|▍         | 324/8564 [00:00<00:24, 331.13 examples/s]Tokenizing train dataset:   4%|▍         | 324/8564 [00:00<00:24, 333.06 examples/s]Tokenizing train dataset:   4%|▍         | 358/8564 [00:01<00:25, 315.99 examples/s]Tokenizing train dataset:   4%|▍         | 370/8564 [00:01<00:26, 314.30 examples/s]Tokenizing train dataset:   4%|▍         | 370/8564 [00:01<00:25, 316.83 examples/s]Tokenizing train dataset:   5%|▍         | 392/8564 [00:01<00:25, 316.60 examples/s]Tokenizing train dataset:   5%|▍         | 420/8564 [00:01<00:25, 314.47 examples/s]Tokenizing train dataset:   5%|▍         | 420/8564 [00:01<00:25, 316.26 examples/s]Tokenizing train dataset:   5%|▍         | 426/8564 [00:01<00:25, 318.31 examples/s]Tokenizing train dataset:   5%|▌         | 469/8564 [00:01<00:25, 313.49 examples/s]Tokenizing train dataset:   5%|▌         | 469/8564 [00:01<00:25, 315.14 examples/s]Tokenizing train dataset:   6%|▌         | 475/8564 [00:01<00:26, 310.73 examples/s]Tokenizing train dataset:   6%|▌         | 512/8564 [00:01<00:26, 303.71 examples/s]Tokenizing train dataset:   6%|▌         | 516/8564 [00:01<00:26, 308.98 examples/s]Tokenizing train dataset:   6%|▌         | 520/8564 [00:01<00:26, 303.49 examples/s]Tokenizing train dataset:   6%|▋         | 549/8564 [00:01<00:25, 317.06 examples/s]Tokenizing train dataset:   6%|▋         | 550/8564 [00:01<00:25, 314.19 examples/s]Tokenizing train dataset:   6%|▋         | 555/8564 [00:01<00:25, 311.55 examples/s]Tokenizing train dataset:   7%|▋         | 596/8564 [00:01<00:25, 309.80 examples/s]Tokenizing train dataset:   7%|▋         | 596/8564 [00:01<00:25, 308.61 examples/s]Tokenizing train dataset:   7%|▋         | 600/8564 [00:01<00:26, 304.21 examples/s]Tokenizing train dataset:   7%|▋         | 634/8564 [00:01<00:24, 324.47 examples/s]Tokenizing train dataset:   7%|▋         | 634/8564 [00:01<00:24, 323.07 examples/s]Tokenizing train dataset:   7%|▋         | 638/8564 [00:02<00:24, 320.12 examples/s]Tokenizing train dataset:   8%|▊         | 681/8564 [00:02<00:24, 315.62 examples/s]Tokenizing train dataset:   8%|▊         | 681/8564 [00:02<00:25, 314.53 examples/s]Tokenizing train dataset:   8%|▊         | 686/8564 [00:02<00:24, 315.83 examples/s]Tokenizing train dataset:   9%|▊         | 729/8564 [00:02<00:24, 313.92 examples/s]Tokenizing train dataset:   9%|▊         | 729/8564 [00:02<00:25, 313.04 examples/s]Tokenizing train dataset:   9%|▊         | 736/8564 [00:02<00:24, 317.03 examples/s]Tokenizing train dataset:   9%|▉         | 763/8564 [00:02<00:24, 315.39 examples/s]Tokenizing train dataset:   9%|▉         | 763/8564 [00:02<00:24, 314.53 examples/s]Tokenizing train dataset:   9%|▉         | 780/8564 [00:02<00:25, 307.35 examples/s]Tokenizing train dataset:   9%|▉         | 808/8564 [00:02<00:25, 303.83 examples/s]Tokenizing train dataset:   9%|▉         | 808/8564 [00:02<00:25, 302.77 examples/s]Tokenizing train dataset:  10%|▉         | 824/8564 [00:02<00:25, 298.99 examples/s]Tokenizing train dataset:  10%|▉         | 839/8564 [00:02<00:25, 300.96 examples/s]Tokenizing train dataset:  10%|▉         | 839/8564 [00:02<00:25, 299.97 examples/s]Tokenizing train dataset:  10%|█         | 870/8564 [00:02<00:25, 297.83 examples/s]Tokenizing train dataset:  10%|█         | 887/8564 [00:02<00:25, 305.80 examples/s]Tokenizing train dataset:  10%|█         | 887/8564 [00:02<00:25, 305.13 examples/s]Tokenizing train dataset:  11%|█         | 910/8564 [00:02<00:24, 316.04 examples/s]Tokenizing train dataset:  11%|█         | 920/8564 [00:02<00:24, 307.16 examples/s]Tokenizing train dataset:  11%|█         | 920/8564 [00:02<00:24, 306.40 examples/s]Tokenizing train dataset:  11%|█         | 952/8564 [00:03<00:24, 306.04 examples/s]Tokenizing train dataset:  11%|█         | 954/8564 [00:03<00:24, 305.35 examples/s]Tokenizing train dataset:  11%|█         | 952/8564 [00:03<00:24, 305.18 examples/s]Tokenizing train dataset:  11%|█▏        | 983/8564 [00:03<00:25, 302.52 examples/s]Tokenizing train dataset:  11%|█▏        | 983/8564 [00:03<00:25, 301.30 examples/s]Tokenizing train dataset:  12%|█▏        | 998/8564 [00:03<00:25, 297.37 examples/s]Tokenizing train dataset:  12%|█▏        | 1027/8564 [00:03<00:25, 293.72 examples/s]Tokenizing train dataset:  12%|█▏        | 1027/8564 [00:03<00:25, 292.90 examples/s]Tokenizing train dataset:  12%|█▏        | 1040/8564 [00:03<00:26, 288.71 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 295.46 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 295.64 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 294.81 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 308.68 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 308.67 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 308.38 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:03<00:25, 296.17 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:03<00:25, 295.73 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:03<00:25, 296.03 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:03<00:24, 296.63 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:03<00:24, 296.47 examples/s]Tokenizing train dataset:  14%|█▍        | 1200/8564 [00:03<00:24, 303.48 examples/s]Tokenizing train dataset:  14%|█▍        | 1218/8564 [00:03<00:24, 305.25 examples/s]Tokenizing train dataset:  14%|█▍        | 1218/8564 [00:03<00:24, 305.16 examples/s]Tokenizing train dataset:  14%|█▍        | 1232/8564 [00:04<00:24, 305.01 examples/s]Tokenizing train dataset:  15%|█▍        | 1252/8564 [00:04<00:23, 313.53 examples/s]Tokenizing train dataset:  15%|█▍        | 1252/8564 [00:04<00:23, 313.26 examples/s]Tokenizing train dataset:  15%|█▍        | 1269/8564 [00:04<00:22, 317.94 examples/s]Tokenizing train dataset:  15%|█▌        | 1286/8564 [00:04<00:23, 311.01 examples/s]Tokenizing train dataset:  15%|█▌        | 1286/8564 [00:04<00:23, 310.72 examples/s]Tokenizing train dataset:  15%|█▌        | 1315/8564 [00:04<00:23, 312.67 examples/s]Tokenizing train dataset:  15%|█▌        | 1320/8564 [00:04<00:22, 318.24 examples/s]Tokenizing train dataset:  15%|█▌        | 1320/8564 [00:04<00:22, 317.88 examples/s]Tokenizing train dataset:  16%|█▌        | 1348/8564 [00:04<00:22, 314.34 examples/s]Tokenizing train dataset:  16%|█▌        | 1353/8564 [00:04<00:22, 314.80 examples/s]Tokenizing train dataset:  16%|█▌        | 1353/8564 [00:04<00:22, 314.48 examples/s]Tokenizing train dataset:  16%|█▌        | 1390/8564 [00:04<00:24, 296.32 examples/s]Tokenizing train dataset:  16%|█▋        | 1397/8564 [00:04<00:23, 304.86 examples/s]Tokenizing train dataset:  16%|█▋        | 1397/8564 [00:04<00:23, 304.33 examples/s]Tokenizing train dataset:  17%|█▋        | 1422/8564 [00:04<00:23, 301.31 examples/s]Tokenizing train dataset:  17%|█▋        | 1428/8564 [00:04<00:23, 304.43 examples/s]Tokenizing train dataset:  17%|█▋        | 1428/8564 [00:04<00:23, 303.92 examples/s]Tokenizing train dataset:  17%|█▋        | 1453/8564 [00:04<00:23, 299.11 examples/s]Tokenizing train dataset:  17%|█▋        | 1473/8564 [00:04<00:23, 299.53 examples/s]Tokenizing train dataset:  17%|█▋        | 1473/8564 [00:04<00:23, 299.40 examples/s]Tokenizing train dataset:  18%|█▊        | 1499/8564 [00:04<00:23, 297.00 examples/s]Tokenizing train dataset:  18%|█▊        | 1515/8564 [00:04<00:24, 290.81 examples/s]Tokenizing train dataset:  18%|█▊        | 1515/8564 [00:04<00:24, 291.04 examples/s]Tokenizing train dataset:  18%|█▊        | 1530/8564 [00:04<00:23, 297.01 examples/s]Tokenizing train dataset:  18%|█▊        | 1550/8564 [00:05<00:23, 299.49 examples/s]Tokenizing train dataset:  18%|█▊        | 1550/8564 [00:05<00:23, 299.63 examples/s]Tokenizing train dataset:  18%|█▊        | 1561/8564 [00:05<00:23, 298.22 examples/s]Tokenizing train dataset:  18%|█▊        | 1581/8564 [00:05<00:23, 298.94 examples/s]Tokenizing train dataset:  18%|█▊        | 1581/8564 [00:05<00:23, 298.92 examples/s]Tokenizing train dataset:  19%|█▊        | 1591/8564 [00:05<00:23, 294.24 examples/s]Tokenizing train dataset:  19%|█▉        | 1614/8564 [00:05<00:22, 304.59 examples/s]Tokenizing train dataset:  19%|█▉        | 1614/8564 [00:05<00:22, 304.56 examples/s]Tokenizing train dataset:  19%|█▉        | 1627/8564 [00:05<00:22, 305.72 examples/s]Tokenizing train dataset:  19%|█▉        | 1650/8564 [00:05<00:21, 316.95 examples/s]Tokenizing train dataset:  19%|█▉        | 1650/8564 [00:05<00:21, 317.37 examples/s]Tokenizing train dataset:  19%|█▉        | 1661/8564 [00:05<00:21, 314.24 examples/s]Tokenizing train dataset:  20%|█▉        | 1686/8564 [00:05<00:21, 322.88 examples/s]Tokenizing train dataset:  20%|█▉        | 1686/8564 [00:05<00:21, 323.38 examples/s]Tokenizing train dataset:  20%|█▉        | 1703/8564 [00:05<00:20, 341.88 examples/s]Tokenizing train dataset:  20%|██        | 1725/8564 [00:05<00:20, 334.84 examples/s]Tokenizing train dataset:  20%|██        | 1725/8564 [00:05<00:20, 335.43 examples/s]Tokenizing train dataset:  20%|██        | 1754/8564 [00:05<00:20, 332.86 examples/s]Tokenizing train dataset:  21%|██        | 1762/8564 [00:05<00:20, 338.70 examples/s]Tokenizing train dataset:  21%|██        | 1762/8564 [00:05<00:20, 339.09 examples/s]Tokenizing train dataset:  21%|██        | 1802/8564 [00:05<00:20, 324.84 examples/s]Tokenizing train dataset:  21%|██        | 1809/8564 [00:05<00:20, 324.67 examples/s]Tokenizing train dataset:  21%|██        | 1809/8564 [00:05<00:20, 324.45 examples/s]Tokenizing train dataset:  21%|██▏       | 1836/8564 [00:05<00:20, 328.35 examples/s]Tokenizing train dataset:  22%|██▏       | 1843/8564 [00:05<00:20, 324.61 examples/s]Tokenizing train dataset:  22%|██▏       | 1843/8564 [00:05<00:20, 324.10 examples/s]Tokenizing train dataset:  22%|██▏       | 1880/8564 [00:06<00:21, 310.22 examples/s]Tokenizing train dataset:  22%|██▏       | 1888/8564 [00:06<00:21, 311.22 examples/s]Tokenizing train dataset:  22%|██▏       | 1888/8564 [00:06<00:21, 310.34 examples/s]Tokenizing train dataset:  22%|██▏       | 1921/8564 [00:06<00:19, 332.30 examples/s]Tokenizing train dataset:  23%|██▎       | 1933/8564 [00:06<00:19, 342.91 examples/s]Tokenizing train dataset:  23%|██▎       | 1933/8564 [00:06<00:19, 342.01 examples/s]Tokenizing train dataset:  23%|██▎       | 1960/8564 [00:06<00:19, 340.19 examples/s]Tokenizing train dataset:  23%|██▎       | 1969/8564 [00:06<00:19, 342.36 examples/s]Tokenizing train dataset:  23%|██▎       | 1969/8564 [00:06<00:19, 341.63 examples/s]Tokenizing train dataset:  23%|██▎       | 1999/8564 [00:06<00:18, 349.06 examples/s]Tokenizing train dataset:  23%|██▎       | 2008/8564 [00:06<00:18, 351.42 examples/s]Tokenizing train dataset:  23%|██▎       | 2008/8564 [00:06<00:18, 350.99 examples/s]Tokenizing train dataset:  24%|██▍       | 2039/8564 [00:06<00:18, 358.41 examples/s]Tokenizing train dataset:  24%|██▍       | 2048/8564 [00:06<00:18, 361.27 examples/s]Tokenizing train dataset:  24%|██▍       | 2048/8564 [00:06<00:18, 360.64 examples/s]Tokenizing train dataset:  24%|██▍       | 2077/8564 [00:06<00:17, 360.49 examples/s]Tokenizing train dataset:  24%|██▍       | 2086/8564 [00:06<00:17, 365.45 examples/s]Tokenizing train dataset:  24%|██▍       | 2086/8564 [00:06<00:17, 365.06 examples/s]Tokenizing train dataset:  25%|██▍       | 2116/8564 [00:06<00:17, 365.18 examples/s]Tokenizing train dataset:  25%|██▍       | 2126/8564 [00:06<00:17, 373.60 examples/s]Tokenizing train dataset:  25%|██▍       | 2126/8564 [00:06<00:17, 372.42 examples/s]Tokenizing train dataset:  25%|██▌       | 2153/8564 [00:06<00:17, 365.01 examples/s]Tokenizing train dataset:  25%|██▌       | 2180/8564 [00:06<00:17, 362.49 examples/s]Tokenizing train dataset:  25%|██▌       | 2180/8564 [00:06<00:17, 361.27 examples/s]Tokenizing train dataset:  26%|██▌       | 2191/8564 [00:06<00:17, 366.52 examples/s]Tokenizing train dataset:  26%|██▌       | 2217/8564 [00:06<00:17, 363.16 examples/s]Tokenizing train dataset:  26%|██▌       | 2217/8564 [00:06<00:17, 362.10 examples/s]Tokenizing train dataset:  26%|██▌       | 2228/8564 [00:07<00:17, 364.07 examples/s]Tokenizing train dataset:  26%|██▋       | 2257/8564 [00:07<00:17, 367.79 examples/s]Tokenizing train dataset:  26%|██▋       | 2257/8564 [00:07<00:17, 366.96 examples/s]Tokenizing train dataset:  26%|██▋       | 2269/8564 [00:07<00:16, 372.92 examples/s]Tokenizing train dataset:  27%|██▋       | 2300/8564 [00:07<00:16, 382.66 examples/s]Tokenizing train dataset:  27%|██▋       | 2300/8564 [00:07<00:16, 381.94 examples/s]Tokenizing train dataset:  27%|██▋       | 2308/8564 [00:07<00:16, 374.83 examples/s]Tokenizing train dataset:  27%|██▋       | 2349/8564 [00:07<00:17, 359.38 examples/s]Tokenizing train dataset:  27%|██▋       | 2349/8564 [00:07<00:17, 359.18 examples/s]Tokenizing train dataset:  28%|██▊       | 2358/8564 [00:07<00:17, 354.45 examples/s]Tokenizing train dataset:  28%|██▊       | 2388/8564 [00:07<00:16, 365.80 examples/s]Tokenizing train dataset:  28%|██▊       | 2389/8564 [00:07<00:16, 366.43 examples/s]Tokenizing train dataset:  28%|██▊       | 2401/8564 [00:07<00:16, 373.83 examples/s]Tokenizing train dataset:  28%|██▊       | 2428/8564 [00:07<00:16, 372.61 examples/s]Tokenizing train dataset:  28%|██▊       | 2431/8564 [00:07<00:16, 373.04 examples/s]Tokenizing train dataset:  28%|██▊       | 2440/8564 [00:07<00:16, 371.76 examples/s]Tokenizing train dataset:  29%|██▉       | 2468/8564 [00:07<00:16, 376.78 examples/s]Tokenizing train dataset:  29%|██▉       | 2470/8564 [00:07<00:16, 375.26 examples/s]Tokenizing train dataset:  29%|██▉       | 2478/8564 [00:07<00:16, 373.72 examples/s]Tokenizing train dataset:  29%|██▉       | 2520/8564 [00:07<00:16, 362.59 examples/s]Tokenizing train dataset:  29%|██▉       | 2523/8564 [00:07<00:16, 362.82 examples/s]Tokenizing train dataset:  30%|██▉       | 2534/8564 [00:07<00:16, 368.52 examples/s]Tokenizing train dataset:  30%|██▉       | 2563/8564 [00:07<00:15, 377.20 examples/s]Tokenizing train dataset:  30%|██▉       | 2565/8564 [00:07<00:15, 375.47 examples/s]Tokenizing train dataset:  30%|███       | 2577/8564 [00:07<00:15, 378.83 examples/s]Tokenizing train dataset:  30%|███       | 2604/8564 [00:07<00:15, 376.20 examples/s]Tokenizing train dataset:  31%|███       | 2618/8564 [00:08<00:16, 367.53 examples/s]Tokenizing train dataset:  31%|███       | 2623/8564 [00:08<00:16, 350.87 examples/s]Tokenizing train dataset:  31%|███       | 2654/8564 [00:08<00:17, 342.05 examples/s]Tokenizing train dataset:  31%|███       | 2666/8564 [00:08<00:17, 346.03 examples/s]Tokenizing train dataset:  31%|███       | 2673/8564 [00:08<00:17, 342.44 examples/s]Tokenizing train dataset:  32%|███▏      | 2708/8564 [00:08<00:16, 345.92 examples/s]Tokenizing train dataset:  32%|███▏      | 2713/8564 [00:08<00:16, 352.38 examples/s]Tokenizing train dataset:  32%|███▏      | 2720/8564 [00:08<00:16, 347.74 examples/s]Tokenizing train dataset:  32%|███▏      | 2759/8564 [00:08<00:16, 342.20 examples/s]Tokenizing train dataset:  32%|███▏      | 2762/8564 [00:08<00:17, 338.96 examples/s]Tokenizing train dataset:  32%|███▏      | 2772/8564 [00:08<00:16, 340.78 examples/s]Tokenizing train dataset:  33%|███▎      | 2798/8564 [00:08<00:16, 350.32 examples/s]Tokenizing train dataset:  33%|███▎      | 2803/8564 [00:08<00:16, 354.55 examples/s]Tokenizing train dataset:  33%|███▎      | 2814/8564 [00:08<00:16, 357.36 examples/s]Tokenizing train dataset:  33%|███▎      | 2837/8564 [00:08<00:16, 356.49 examples/s]Tokenizing train dataset:  33%|███▎      | 2840/8564 [00:08<00:16, 356.62 examples/s]Tokenizing train dataset:  33%|███▎      | 2866/8564 [00:08<00:16, 350.76 examples/s]Tokenizing train dataset:  34%|███▍      | 2893/8564 [00:08<00:15, 356.07 examples/s]Tokenizing train dataset:  34%|███▍      | 2898/8564 [00:08<00:15, 359.13 examples/s]Tokenizing train dataset:  34%|███▍      | 2908/8564 [00:08<00:15, 366.51 examples/s]Tokenizing train dataset:  34%|███▍      | 2933/8564 [00:08<00:15, 365.02 examples/s]Tokenizing train dataset:  34%|███▍      | 2939/8564 [00:08<00:15, 369.04 examples/s]Tokenizing train dataset:  34%|███▍      | 2948/8564 [00:08<00:15, 372.71 examples/s]Tokenizing train dataset:  35%|███▍      | 2978/8564 [00:09<00:14, 384.33 examples/s]Tokenizing train dataset:  35%|███▍      | 2992/8564 [00:09<00:14, 386.14 examples/s]Tokenizing train dataset:  35%|███▍      | 2987/8564 [00:09<00:14, 390.23 examples/s]Tokenizing train dataset:  35%|███▌      | 3018/8564 [00:09<00:14, 382.70 examples/s]Tokenizing train dataset:  36%|███▌      | 3046/8564 [00:09<00:14, 373.14 examples/s]Tokenizing train dataset:  35%|███▌      | 3040/8564 [00:09<00:14, 372.49 examples/s]Tokenizing train dataset:  36%|███▌      | 3070/8564 [00:09<00:15, 363.73 examples/s]Tokenizing train dataset:  36%|███▌      | 3100/8564 [00:09<00:14, 367.12 examples/s]Tokenizing train dataset:  36%|███▌      | 3097/8564 [00:09<00:14, 371.37 examples/s]Tokenizing train dataset:  36%|███▋      | 3111/8564 [00:09<00:14, 371.98 examples/s]Tokenizing train dataset:  37%|███▋      | 3157/8564 [00:09<00:14, 369.29 examples/s]Tokenizing train dataset:  37%|███▋      | 3151/8564 [00:09<00:14, 365.76 examples/s]Tokenizing train dataset:  37%|███▋      | 3166/8564 [00:09<00:14, 365.67 examples/s]Tokenizing train dataset:  37%|███▋      | 3189/8564 [00:09<00:14, 363.16 examples/s]Tokenizing train dataset:  38%|███▊      | 3212/8564 [00:09<00:14, 365.07 examples/s]Tokenizing train dataset:  38%|███▊      | 3222/8564 [00:09<00:14, 365.26 examples/s]Tokenizing train dataset:  38%|███▊      | 3228/8564 [00:09<00:14, 368.60 examples/s]Tokenizing train dataset:  38%|███▊      | 3254/8564 [00:09<00:14, 374.51 examples/s]Tokenizing train dataset:  38%|███▊      | 3264/8564 [00:09<00:14, 373.90 examples/s]Tokenizing train dataset:  38%|███▊      | 3269/8564 [00:09<00:14, 374.72 examples/s]Tokenizing train dataset:  39%|███▊      | 3307/8564 [00:09<00:14, 364.06 examples/s]Tokenizing train dataset:  39%|███▊      | 3316/8564 [00:09<00:14, 361.87 examples/s]Tokenizing train dataset:  39%|███▉      | 3320/8564 [00:10<00:14, 359.46 examples/s]Tokenizing train dataset:  39%|███▉      | 3362/8564 [00:10<00:14, 359.24 examples/s]Tokenizing train dataset:  39%|███▉      | 3357/8564 [00:10<00:14, 360.44 examples/s]Tokenizing train dataset:  39%|███▉      | 3370/8564 [00:10<00:14, 356.96 examples/s]Tokenizing train dataset:  40%|███▉      | 3406/8564 [00:10<00:13, 374.59 examples/s]Tokenizing train dataset:  40%|███▉      | 3397/8564 [00:10<00:13, 369.40 examples/s]Tokenizing train dataset:  40%|███▉      | 3414/8564 [00:10<00:13, 372.68 examples/s]Tokenizing train dataset:  40%|████      | 3437/8564 [00:10<00:13, 377.45 examples/s]Tokenizing train dataset:  40%|████      | 3452/8564 [00:10<00:13, 371.56 examples/s]Tokenizing train dataset:  40%|████      | 3464/8564 [00:10<00:13, 375.02 examples/s]Tokenizing train dataset:  41%|████      | 3491/8564 [00:10<00:13, 367.07 examples/s]Tokenizing train dataset:  41%|████      | 3513/8564 [00:10<00:13, 380.56 examples/s]Tokenizing train dataset:  41%|████      | 3523/8564 [00:10<00:13, 378.67 examples/s]Tokenizing train dataset:  41%|████▏     | 3533/8564 [00:10<00:13, 378.57 examples/s]Tokenizing train dataset:  42%|████▏     | 3561/8564 [00:10<00:14, 357.26 examples/s]Tokenizing train dataset:  42%|████▏     | 3576/8564 [00:10<00:13, 367.66 examples/s]Tokenizing train dataset:  42%|████▏     | 3592/8564 [00:10<00:13, 374.96 examples/s]Tokenizing train dataset:  42%|████▏     | 3602/8564 [00:10<00:13, 368.41 examples/s]Tokenizing train dataset:  42%|████▏     | 3616/8564 [00:10<00:13, 371.54 examples/s]Tokenizing train dataset:  42%|████▏     | 3631/8564 [00:10<00:13, 375.71 examples/s]Tokenizing train dataset:  43%|████▎     | 3640/8564 [00:10<00:13, 364.69 examples/s]Tokenizing train dataset:  43%|████▎     | 3670/8564 [00:10<00:13, 361.71 examples/s]Tokenizing train dataset:  43%|████▎     | 3680/8564 [00:10<00:13, 353.46 examples/s]Tokenizing train dataset:  43%|████▎     | 3691/8564 [00:10<00:13, 354.21 examples/s]Tokenizing train dataset:  43%|████▎     | 3707/8564 [00:11<00:13, 360.16 examples/s]Tokenizing train dataset:  43%|████▎     | 3724/8564 [00:11<00:13, 372.04 examples/s]Tokenizing train dataset:  44%|████▎     | 3730/8564 [00:11<00:13, 362.32 examples/s]Tokenizing train dataset:  44%|████▍     | 3749/8564 [00:11<00:12, 370.70 examples/s]Tokenizing train dataset:  44%|████▍     | 3769/8564 [00:11<00:13, 368.18 examples/s]Tokenizing train dataset:  44%|████▍     | 3780/8564 [00:11<00:12, 370.45 examples/s]Tokenizing train dataset:  44%|████▍     | 3788/8564 [00:11<00:12, 373.02 examples/s]Tokenizing train dataset:  44%|████▍     | 3807/8564 [00:11<00:12, 368.04 examples/s]Tokenizing train dataset:  45%|████▍     | 3834/8564 [00:11<00:12, 364.70 examples/s]Tokenizing train dataset:  45%|████▍     | 3842/8564 [00:11<00:13, 361.65 examples/s]Tokenizing train dataset:  45%|████▌     | 3861/8564 [00:11<00:13, 359.55 examples/s]Tokenizing train dataset:  45%|████▌     | 3881/8564 [00:11<00:12, 365.64 examples/s]Tokenizing train dataset:  45%|████▌     | 3890/8564 [00:11<00:12, 363.04 examples/s]Tokenizing train dataset:  46%|████▌     | 3898/8564 [00:11<00:13, 358.90 examples/s]Tokenizing train dataset:  46%|████▌     | 3933/8564 [00:11<00:13, 352.81 examples/s]Tokenizing train dataset:  46%|████▌     | 3941/8564 [00:11<00:13, 352.66 examples/s]Tokenizing train dataset:  46%|████▌     | 3951/8564 [00:11<00:13, 351.77 examples/s]Tokenizing train dataset:  46%|████▋     | 3971/8564 [00:11<00:12, 358.65 examples/s]Tokenizing train dataset:  46%|████▋     | 3978/8564 [00:11<00:12, 354.29 examples/s]Tokenizing train dataset:  47%|████▋     | 3987/8564 [00:11<00:12, 353.04 examples/s]Tokenizing train dataset:  47%|████▋     | 4015/8564 [00:11<00:12, 356.67 examples/s]Tokenizing train dataset:  47%|████▋     | 4028/8564 [00:11<00:12, 364.46 examples/s]Tokenizing train dataset:  47%|████▋     | 4028/8564 [00:11<00:12, 366.19 examples/s]Tokenizing train dataset:  47%|████▋     | 4056/8564 [00:12<00:12, 364.96 examples/s]Tokenizing train dataset:  47%|████▋     | 4065/8564 [00:11<00:12, 364.35 examples/s]Tokenizing train dataset:  47%|████▋     | 4065/8564 [00:12<00:12, 365.57 examples/s]Tokenizing train dataset:  48%|████▊     | 4112/8564 [00:12<00:12, 362.42 examples/s]Tokenizing train dataset:  48%|████▊     | 4120/8564 [00:12<00:12, 358.84 examples/s]Tokenizing train dataset:  48%|████▊     | 4120/8564 [00:12<00:12, 359.37 examples/s]Tokenizing train dataset:  49%|████▊     | 4172/8564 [00:12<00:12, 352.54 examples/s]Tokenizing train dataset:  49%|████▊     | 4164/8564 [00:12<00:12, 352.22 examples/s]Tokenizing train dataset:  49%|████▊     | 4172/8564 [00:12<00:12, 352.37 examples/s]Tokenizing train dataset:  49%|████▉     | 4211/8564 [00:12<00:12, 360.56 examples/s]Tokenizing train dataset:  49%|████▉     | 4205/8564 [00:12<00:11, 364.84 examples/s]Tokenizing train dataset:  49%|████▉     | 4212/8564 [00:12<00:12, 360.30 examples/s]Tokenizing train dataset:  50%|████▉     | 4266/8564 [00:12<00:11, 358.67 examples/s]Tokenizing train dataset:  50%|████▉     | 4260/8564 [00:12<00:12, 355.33 examples/s]Tokenizing train dataset:  50%|████▉     | 4266/8564 [00:12<00:11, 358.63 examples/s]Tokenizing train dataset:  50%|█████     | 4304/8564 [00:12<00:11, 360.95 examples/s]Tokenizing train dataset:  50%|█████     | 4299/8564 [00:12<00:11, 360.34 examples/s]Tokenizing train dataset:  50%|█████     | 4304/8564 [00:12<00:11, 360.88 examples/s]Tokenizing train dataset:  51%|█████     | 4359/8564 [00:12<00:11, 358.28 examples/s]Tokenizing train dataset:  51%|█████     | 4351/8564 [00:12<00:11, 352.92 examples/s]Tokenizing train dataset:  51%|█████     | 4359/8564 [00:12<00:11, 358.26 examples/s]Tokenizing train dataset:  51%|█████▏    | 4398/8564 [00:12<00:11, 364.13 examples/s]Tokenizing train dataset:  51%|█████▏    | 4391/8564 [00:12<00:11, 361.87 examples/s]Tokenizing train dataset:  51%|█████▏    | 4398/8564 [00:12<00:11, 364.25 examples/s]Tokenizing train dataset:  52%|█████▏    | 4435/8564 [00:13<00:11, 363.86 examples/s]Tokenizing train dataset:  52%|█████▏    | 4428/8564 [00:13<00:11, 360.80 examples/s]Tokenizing train dataset:  52%|█████▏    | 4435/8564 [00:13<00:11, 364.03 examples/s]Tokenizing train dataset:  52%|█████▏    | 4472/8564 [00:13<00:11, 362.22 examples/s]Tokenizing train dataset:  52%|█████▏    | 4465/8564 [00:13<00:11, 360.64 examples/s]Tokenizing train dataset:  52%|█████▏    | 4472/8564 [00:13<00:11, 362.35 examples/s]Tokenizing train dataset:  53%|█████▎    | 4502/8564 [00:13<00:11, 355.67 examples/s]Tokenizing train dataset:  53%|█████▎    | 4525/8564 [00:13<00:11, 353.98 examples/s]Tokenizing train dataset:  53%|█████▎    | 4525/8564 [00:13<00:11, 354.11 examples/s]Tokenizing train dataset:  53%|█████▎    | 4541/8564 [00:13<00:11, 360.75 examples/s]Tokenizing train dataset:  53%|█████▎    | 4566/8564 [00:13<00:10, 366.18 examples/s]Tokenizing train dataset:  53%|█████▎    | 4566/8564 [00:13<00:10, 366.55 examples/s]Tokenizing train dataset:  54%|█████▎    | 4593/8564 [00:13<00:11, 353.68 examples/s]Tokenizing train dataset:  54%|█████▍    | 4620/8564 [00:13<00:11, 355.39 examples/s]Tokenizing train dataset:  54%|█████▍    | 4620/8564 [00:13<00:11, 355.59 examples/s]Tokenizing train dataset:  54%|█████▍    | 4630/8564 [00:13<00:11, 356.76 examples/s]Tokenizing train dataset:  55%|█████▍    | 4670/8564 [00:13<00:11, 340.98 examples/s]Tokenizing train dataset:  55%|█████▍    | 4670/8564 [00:13<00:11, 341.25 examples/s]Tokenizing train dataset:  55%|█████▍    | 4677/8564 [00:13<00:11, 337.67 examples/s]Tokenizing train dataset:  55%|█████▌    | 4718/8564 [00:13<00:11, 330.42 examples/s]Tokenizing train dataset:  55%|█████▌    | 4718/8564 [00:13<00:11, 330.67 examples/s]Tokenizing train dataset:  55%|█████▌    | 4726/8564 [00:13<00:11, 328.00 examples/s]Tokenizing train dataset:  56%|█████▌    | 4767/8564 [00:14<00:11, 325.68 examples/s]Tokenizing train dataset:  56%|█████▌    | 4767/8564 [00:14<00:11, 325.89 examples/s]Tokenizing train dataset:  56%|█████▌    | 4773/8564 [00:14<00:11, 319.62 examples/s]Tokenizing train dataset:  56%|█████▌    | 4804/8564 [00:14<00:11, 333.04 examples/s]Tokenizing train dataset:  56%|█████▌    | 4804/8564 [00:14<00:11, 333.20 examples/s]Tokenizing train dataset:  56%|█████▋    | 4821/8564 [00:14<00:10, 352.50 examples/s]Tokenizing train dataset:  57%|█████▋    | 4868/8564 [00:14<00:09, 406.52 examples/s]Tokenizing train dataset:  57%|█████▋    | 4868/8564 [00:14<00:09, 406.91 examples/s]Tokenizing train dataset:  57%|█████▋    | 4882/8564 [00:14<00:08, 412.19 examples/s]Tokenizing train dataset:  58%|█████▊    | 4931/8564 [00:14<00:07, 459.50 examples/s]Tokenizing train dataset:  58%|█████▊    | 4932/8564 [00:14<00:07, 459.31 examples/s]Tokenizing train dataset:  58%|█████▊    | 4947/8564 [00:14<00:07, 472.35 examples/s]Tokenizing train dataset:  58%|█████▊    | 4990/8564 [00:14<00:07, 491.16 examples/s]Tokenizing train dataset:  58%|█████▊    | 4991/8564 [00:14<00:07, 492.47 examples/s]Tokenizing train dataset:  58%|█████▊    | 5008/8564 [00:14<00:06, 508.55 examples/s]Tokenizing train dataset:  59%|█████▉    | 5053/8564 [00:14<00:06, 528.77 examples/s]Tokenizing train dataset:  59%|█████▉    | 5055/8564 [00:14<00:06, 528.91 examples/s]Tokenizing train dataset:  59%|█████▉    | 5072/8564 [00:14<00:06, 541.41 examples/s]Tokenizing train dataset:  60%|█████▉    | 5120/8564 [00:14<00:06, 563.70 examples/s]Tokenizing train dataset:  60%|█████▉    | 5123/8564 [00:14<00:06, 567.86 examples/s]Tokenizing train dataset:  60%|██████    | 5140/8564 [00:14<00:05, 572.89 examples/s]Tokenizing train dataset:  61%|██████    | 5193/8564 [00:14<00:05, 609.39 examples/s]Tokenizing train dataset:  61%|██████    | 5195/8564 [00:14<00:05, 609.68 examples/s]Tokenizing train dataset:  61%|██████    | 5214/8564 [00:14<00:05, 617.11 examples/s]Tokenizing train dataset:  62%|██████▏   | 5269/8564 [00:14<00:05, 648.21 examples/s]Tokenizing train dataset:  62%|██████▏   | 5273/8564 [00:14<00:05, 649.11 examples/s]Tokenizing train dataset:  62%|██████▏   | 5288/8564 [00:14<00:05, 650.63 examples/s]Tokenizing train dataset:  63%|██████▎   | 5363/8564 [00:14<00:05, 633.92 examples/s]Tokenizing train dataset:  63%|██████▎   | 5365/8564 [00:15<00:05, 633.05 examples/s]Tokenizing train dataset:  63%|██████▎   | 5375/8564 [00:15<00:05, 620.94 examples/s]Tokenizing train dataset:  64%|██████▎   | 5454/8564 [00:15<00:04, 622.89 examples/s]Tokenizing train dataset:  64%|██████▎   | 5443/8564 [00:15<00:04, 629.48 examples/s]Tokenizing train dataset:  64%|██████▎   | 5459/8564 [00:15<00:04, 627.64 examples/s]Tokenizing train dataset:  65%|██████▍   | 5547/8564 [00:15<00:04, 612.63 examples/s]Tokenizing train dataset:  65%|██████▍   | 5533/8564 [00:15<00:04, 611.55 examples/s]Tokenizing train dataset:  65%|██████▍   | 5547/8564 [00:15<00:04, 610.66 examples/s]Tokenizing train dataset:  66%|██████▌   | 5614/8564 [00:15<00:04, 624.49 examples/s]Tokenizing train dataset:  65%|██████▌   | 5596/8564 [00:15<00:04, 609.94 examples/s]Tokenizing train dataset:  66%|██████▌   | 5614/8564 [00:15<00:04, 623.05 examples/s]Tokenizing train dataset:  66%|██████▋   | 5678/8564 [00:15<00:04, 625.35 examples/s]Tokenizing train dataset:  66%|██████▌   | 5666/8564 [00:15<00:04, 629.36 examples/s]Tokenizing train dataset:  66%|██████▋   | 5678/8564 [00:15<00:04, 624.05 examples/s]Tokenizing train dataset:  67%|██████▋   | 5743/8564 [00:15<00:04, 630.09 examples/s]Tokenizing train dataset:  67%|██████▋   | 5730/8564 [00:15<00:04, 625.34 examples/s]Tokenizing train dataset:  67%|██████▋   | 5743/8564 [00:15<00:04, 629.14 examples/s]Tokenizing train dataset:  68%|██████▊   | 5821/8564 [00:15<00:04, 665.41 examples/s]Tokenizing train dataset:  68%|██████▊   | 5808/8564 [00:15<00:04, 667.50 examples/s]Tokenizing train dataset:  68%|██████▊   | 5822/8564 [00:15<00:04, 671.89 examples/s]Tokenizing train dataset:  69%|██████▉   | 5902/8564 [00:15<00:04, 616.53 examples/s]Tokenizing train dataset:  69%|██████▉   | 5892/8564 [00:15<00:04, 621.65 examples/s]Tokenizing train dataset:  69%|██████▉   | 5905/8564 [00:15<00:04, 621.21 examples/s]Tokenizing train dataset:  70%|██████▉   | 5971/8564 [00:15<00:04, 634.08 examples/s]Tokenizing train dataset:  70%|██████▉   | 5962/8564 [00:16<00:04, 636.63 examples/s]Tokenizing train dataset:  70%|██████▉   | 5974/8564 [00:15<00:04, 637.72 examples/s]Tokenizing train dataset:  71%|███████   | 6055/8564 [00:16<00:04, 602.55 examples/s]Tokenizing train dataset:  71%|███████   | 6043/8564 [00:16<00:04, 592.64 examples/s]Tokenizing train dataset:  71%|███████   | 6055/8564 [00:16<00:04, 597.06 examples/s]Tokenizing train dataset:  72%|███████▏  | 6150/8564 [00:16<00:03, 607.60 examples/s]Tokenizing train dataset:  72%|███████▏  | 6138/8564 [00:16<00:04, 602.36 examples/s]Tokenizing train dataset:  72%|███████▏  | 6150/8564 [00:16<00:03, 603.97 examples/s]Tokenizing train dataset:  73%|███████▎  | 6225/8564 [00:16<00:03, 637.80 examples/s]Tokenizing train dataset:  73%|███████▎  | 6210/8564 [00:16<00:03, 629.99 examples/s]Tokenizing train dataset:  73%|███████▎  | 6225/8564 [00:16<00:03, 635.01 examples/s]Tokenizing train dataset:  73%|███████▎  | 6291/8564 [00:16<00:03, 640.08 examples/s]Tokenizing train dataset:  73%|███████▎  | 6280/8564 [00:16<00:03, 641.05 examples/s]Tokenizing train dataset:  73%|███████▎  | 6291/8564 [00:16<00:03, 638.15 examples/s]Tokenizing train dataset:  74%|███████▍  | 6360/8564 [00:16<00:03, 648.32 examples/s]Tokenizing train dataset:  74%|███████▍  | 6357/8564 [00:16<00:03, 636.22 examples/s]Tokenizing train dataset:  74%|███████▍  | 6380/8564 [00:16<00:03, 642.51 examples/s]Tokenizing train dataset:  75%|███████▌  | 6457/8564 [00:16<00:03, 638.32 examples/s]Tokenizing train dataset:  75%|███████▌  | 6454/8564 [00:16<00:03, 635.68 examples/s]Tokenizing train dataset:  76%|███████▌  | 6471/8564 [00:16<00:03, 626.63 examples/s]Tokenizing train dataset:  76%|███████▋  | 6549/8564 [00:16<00:03, 623.97 examples/s]Tokenizing train dataset:  76%|███████▋  | 6535/8564 [00:16<00:03, 624.49 examples/s]Tokenizing train dataset:  76%|███████▋  | 6544/8564 [00:16<00:03, 617.97 examples/s]Tokenizing train dataset:  77%|███████▋  | 6628/8564 [00:17<00:03, 589.09 examples/s]Tokenizing train dataset:  77%|███████▋  | 6613/8564 [00:17<00:03, 580.07 examples/s]Tokenizing train dataset:  77%|███████▋  | 6622/8564 [00:17<00:03, 582.08 examples/s]Tokenizing train dataset:  78%|███████▊  | 6692/8564 [00:17<00:03, 597.76 examples/s]Tokenizing train dataset:  78%|███████▊  | 6678/8564 [00:17<00:03, 594.01 examples/s]Tokenizing train dataset:  78%|███████▊  | 6687/8564 [00:17<00:03, 593.53 examples/s]Tokenizing train dataset:  79%|███████▉  | 6761/8564 [00:17<00:02, 616.87 examples/s]Tokenizing train dataset:  79%|███████▉  | 6749/8564 [00:17<00:02, 619.88 examples/s]Tokenizing train dataset:  79%|███████▉  | 6756/8564 [00:17<00:02, 614.99 examples/s]Tokenizing train dataset:  80%|███████▉  | 6819/8564 [00:17<00:02, 607.46 examples/s]Tokenizing train dataset:  80%|███████▉  | 6847/8564 [00:17<00:02, 598.06 examples/s]Tokenizing train dataset:  80%|███████▉  | 6837/8564 [00:17<00:02, 598.93 examples/s]Tokenizing train dataset:  80%|████████  | 6881/8564 [00:17<00:02, 609.57 examples/s]Tokenizing train dataset:  81%|████████  | 6916/8564 [00:17<00:02, 618.63 examples/s]Tokenizing train dataset:  81%|████████  | 6905/8564 [00:17<00:02, 615.49 examples/s]Tokenizing train dataset:  81%|████████  | 6949/8564 [00:17<00:02, 623.55 examples/s]Tokenizing train dataset:  82%|████████▏ | 6982/8564 [00:17<00:02, 618.44 examples/s]Tokenizing train dataset:  81%|████████▏ | 6968/8564 [00:17<00:02, 616.30 examples/s]Tokenizing train dataset:  82%|████████▏ | 7031/8564 [00:17<00:02, 587.48 examples/s]Tokenizing train dataset:  83%|████████▎ | 7069/8564 [00:17<00:02, 602.94 examples/s]Tokenizing train dataset:  82%|████████▏ | 7058/8564 [00:17<00:02, 607.42 examples/s]Tokenizing train dataset:  83%|████████▎ | 7102/8564 [00:17<00:02, 615.25 examples/s]Tokenizing train dataset:  83%|████████▎ | 7136/8564 [00:17<00:02, 618.12 examples/s]Tokenizing train dataset:  83%|████████▎ | 7124/8564 [00:17<00:02, 617.20 examples/s]Tokenizing train dataset:  84%|████████▎ | 7167/8564 [00:17<00:02, 622.62 examples/s]Tokenizing train dataset:  84%|████████▍ | 7205/8564 [00:17<00:02, 635.54 examples/s]Tokenizing train dataset:  84%|████████▍ | 7192/8564 [00:18<00:02, 632.72 examples/s]Tokenizing train dataset:  85%|████████▍ | 7259/8564 [00:18<00:02, 616.56 examples/s]Tokenizing train dataset:  85%|████████▌ | 7292/8564 [00:18<00:02, 608.86 examples/s]Tokenizing train dataset:  85%|████████▌ | 7282/8564 [00:18<00:02, 616.28 examples/s]Tokenizing train dataset:  86%|████████▌ | 7324/8564 [00:18<00:02, 619.94 examples/s]Tokenizing train dataset:  86%|████████▌ | 7360/8564 [00:18<00:01, 620.94 examples/s]Tokenizing train dataset:  86%|████████▌ | 7379/8564 [00:18<00:01, 622.48 examples/s]Tokenizing train dataset:  86%|████████▋ | 7388/8564 [00:18<00:01, 620.25 examples/s]Tokenizing train dataset:  87%|████████▋ | 7457/8564 [00:18<00:01, 628.58 examples/s]Tokenizing train dataset:  87%|████████▋ | 7443/8564 [00:18<00:01, 624.47 examples/s]Tokenizing train dataset:  87%|████████▋ | 7458/8564 [00:18<00:01, 641.00 examples/s]Tokenizing train dataset:  88%|████████▊ | 7506/8564 [00:18<00:01, 621.53 examples/s]Tokenizing train dataset:  88%|████████▊ | 7553/8564 [00:18<00:01, 631.13 examples/s]Tokenizing train dataset:  88%|████████▊ | 7555/8564 [00:18<00:01, 640.95 examples/s]Tokenizing train dataset:  89%|████████▊ | 7581/8564 [00:18<00:01, 652.39 examples/s]Tokenizing train dataset:  89%|████████▉ | 7626/8564 [00:18<00:01, 652.87 examples/s]Tokenizing train dataset:  89%|████████▉ | 7627/8564 [00:18<00:01, 657.88 examples/s]Tokenizing train dataset:  90%|████████▉ | 7665/8564 [00:18<00:01, 616.65 examples/s]Tokenizing train dataset:  90%|█████████ | 7709/8564 [00:18<00:01, 615.87 examples/s]Tokenizing train dataset:  90%|█████████ | 7710/8564 [00:18<00:01, 619.52 examples/s]Tokenizing train dataset:  90%|█████████ | 7728/8564 [00:18<00:01, 618.84 examples/s]Tokenizing train dataset:  91%|█████████ | 7791/8564 [00:18<00:01, 592.11 examples/s]Tokenizing train dataset:  91%|█████████ | 7794/8564 [00:18<00:01, 593.30 examples/s]Tokenizing train dataset:  91%|█████████ | 7809/8564 [00:19<00:01, 585.46 examples/s]Tokenizing train dataset:  92%|█████████▏| 7854/8564 [00:19<00:01, 597.63 examples/s]Tokenizing train dataset:  92%|█████████▏| 7859/8564 [00:19<00:01, 599.21 examples/s]Tokenizing train dataset:  92%|█████████▏| 7871/8564 [00:19<00:01, 592.72 examples/s]Tokenizing train dataset:  92%|█████████▏| 7916/8564 [00:19<00:01, 599.87 examples/s]Tokenizing train dataset:  93%|█████████▎| 7923/8564 [00:19<00:01, 606.76 examples/s]Tokenizing train dataset:  93%|█████████▎| 7936/8564 [00:19<00:01, 604.27 examples/s]Tokenizing train dataset:  93%|█████████▎| 7979/8564 [00:19<00:00, 606.27 examples/s]Tokenizing train dataset:  93%|█████████▎| 7987/8564 [00:19<00:00, 610.62 examples/s]Tokenizing train dataset:  93%|█████████▎| 8003/8564 [00:19<00:00, 619.75 examples/s]Tokenizing train dataset:  94%|█████████▍| 8046/8564 [00:19<00:00, 616.76 examples/s]Tokenizing train dataset:  94%|█████████▍| 8050/8564 [00:19<00:00, 611.60 examples/s]Tokenizing train dataset:  94%|█████████▍| 8091/8564 [00:19<00:00, 599.39 examples/s]Tokenizing train dataset:  95%|█████████▍| 8129/8564 [00:19<00:00, 590.79 examples/s]Tokenizing train dataset:  95%|█████████▍| 8134/8564 [00:19<00:00, 588.41 examples/s]Tokenizing train dataset:  96%|█████████▌| 8198/8564 [00:19<00:00, 612.56 examples/s]Tokenizing train dataset:  96%|█████████▌| 8184/8564 [00:19<00:00, 598.93 examples/s]Tokenizing train dataset:  96%|█████████▌| 8204/8564 [00:19<00:00, 606.15 examples/s]Tokenizing train dataset:  96%|█████████▋| 8263/8564 [00:19<00:00, 618.60 examples/s]Tokenizing train dataset:  96%|█████████▋| 8251/8564 [00:19<00:00, 607.87 examples/s]Tokenizing train dataset:  97%|█████████▋| 8277/8564 [00:19<00:00, 637.41 examples/s]Tokenizing train dataset:  97%|█████████▋| 8333/8564 [00:19<00:00, 637.48 examples/s]Tokenizing train dataset:  97%|█████████▋| 8329/8564 [00:19<00:00, 642.70 examples/s]Tokenizing train dataset:  97%|█████████▋| 8342/8564 [00:19<00:00, 636.05 examples/s]Tokenizing train dataset:  98%|█████████▊| 8422/8564 [00:19<00:00, 614.97 examples/s]Tokenizing train dataset:  98%|█████████▊| 8411/8564 [00:20<00:00, 607.40 examples/s]Tokenizing train dataset:  98%|█████████▊| 8430/8564 [00:19<00:00, 614.54 examples/s]Tokenizing train dataset:  99%|█████████▉| 8485/8564 [00:20<00:00, 616.36 examples/s]Tokenizing train dataset:  99%|█████████▉| 8478/8564 [00:20<00:00, 616.91 examples/s]Tokenizing train dataset:  99%|█████████▉| 8494/8564 [00:20<00:00, 615.30 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 612.15 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 424.64 examples/s]
Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 616.33 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 423.92 examples/s]
Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 610.78 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 422.73 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11312.25 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11106.15 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11077.47 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13300.98 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13234.18 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13416.57 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 329.42 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 330.40 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 328.45 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 299.34 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 298.51 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 298.30 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:02, 281.04 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:02, 279.77 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:02, 280.25 examples/s]Tokenizing eval dataset:  17%|█▋        | 161/953 [00:00<00:02, 273.46 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 270.57 examples/s]Tokenizing eval dataset:  17%|█▋        | 161/953 [00:00<00:02, 272.34 examples/s]Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:02, 263.54 examples/s]Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:02, 262.75 examples/s]Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:02, 262.36 examples/s]Tokenizing eval dataset:  25%|██▍       | 238/953 [00:00<00:02, 293.85 examples/s]Tokenizing eval dataset:  25%|██▌       | 240/953 [00:00<00:02, 295.80 examples/s]Tokenizing eval dataset:  25%|██▌       | 240/953 [00:00<00:02, 294.97 examples/s]Tokenizing eval dataset:  32%|███▏      | 305/953 [00:00<00:01, 391.46 examples/s]Tokenizing eval dataset:  32%|███▏      | 309/953 [00:00<00:01, 398.40 examples/s]Tokenizing eval dataset:  32%|███▏      | 308/953 [00:00<00:01, 395.33 examples/s]Tokenizing eval dataset:  39%|███▊      | 368/953 [00:01<00:01, 455.74 examples/s]Tokenizing eval dataset:  39%|███▉      | 371/953 [00:01<00:01, 458.00 examples/s]Tokenizing eval dataset:  39%|███▉      | 370/953 [00:01<00:01, 454.79 examples/s]Tokenizing eval dataset:  46%|████▌     | 437/953 [00:01<00:00, 520.38 examples/s]Tokenizing eval dataset:  46%|████▋     | 442/953 [00:01<00:00, 527.12 examples/s]Tokenizing eval dataset:  46%|████▌     | 440/953 [00:01<00:00, 522.50 examples/s]Tokenizing eval dataset:  53%|█████▎    | 509/953 [00:01<00:00, 566.83 examples/s]Tokenizing eval dataset:  53%|█████▎    | 505/953 [00:01<00:00, 561.49 examples/s]Tokenizing eval dataset:  53%|█████▎    | 508/953 [00:01<00:00, 563.72 examples/s]Tokenizing eval dataset:  60%|██████    | 575/953 [00:01<00:00, 591.00 examples/s]Tokenizing eval dataset:  60%|█████▉    | 570/953 [00:01<00:00, 583.30 examples/s]Tokenizing eval dataset:  60%|██████    | 574/953 [00:01<00:00, 589.26 examples/s]Tokenizing eval dataset:  68%|██████▊   | 644/953 [00:01<00:00, 612.84 examples/s]Tokenizing eval dataset:  67%|██████▋   | 640/953 [00:01<00:00, 612.75 examples/s]Tokenizing eval dataset:  67%|██████▋   | 643/953 [00:01<00:00, 616.96 examples/s]Tokenizing eval dataset:  77%|███████▋  | 730/953 [00:01<00:00, 602.10 examples/s]Tokenizing eval dataset:  77%|███████▋  | 734/953 [00:01<00:00, 600.70 examples/s]Tokenizing eval dataset:  77%|███████▋  | 733/953 [00:01<00:00, 600.79 examples/s]Tokenizing eval dataset:  85%|████████▌ | 811/953 [00:01<00:00, 566.31 examples/s]Tokenizing eval dataset:  85%|████████▍ | 807/953 [00:01<00:00, 565.85 examples/s]Tokenizing eval dataset:  85%|████████▍ | 810/953 [00:01<00:00, 562.50 examples/s]Tokenizing eval dataset:  93%|█████████▎| 883/953 [00:01<00:00, 544.47 examples/s]Tokenizing eval dataset:  93%|█████████▎| 890/953 [00:01<00:00, 548.25 examples/s]Tokenizing eval dataset:  93%|█████████▎| 889/953 [00:01<00:00, 546.09 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 537.93 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 469.55 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 535.21 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 467.57 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 533.05 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 466.76 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
[rank4]: Traceback (most recent call last):
[rank4]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 195, in <module>
[rank4]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank4]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 174, in main
[rank4]:     dpo_trainer.train()
[rank4]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank4]:     return inner_training_loop(
[rank4]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank4]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
[rank4]:     self.accelerator.backward(loss, **kwargs)
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2321, in backward
[rank4]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/deepspeed.py", line 266, in backward
[rank4]:     self.engine.backward(loss, **kwargs)
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 2126, in backward
[rank4]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank4]:     ret_val = func(*args, **kwargs)
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py", line 2284, in backward
[rank4]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank4]:     scaled_loss.backward(retain_graph=retain_graph)
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 626, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 1129, in unpack_hook
[rank4]:     frame.check_recomputed_tensors_match(gid)
[rank4]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 903, in check_recomputed_tensors_match
[rank4]:     raise CheckpointError(
[rank4]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank4]: tensor at position 6:
[rank4]: saved metadata: {'shape': torch.Size([4096, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: tensor at position 12:
[rank4]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: tensor at position 18:
[rank4]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: tensor at position 34:
[rank4]: saved metadata: {'shape': torch.Size([3584, 4096]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: tensor at position 50:
[rank4]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: tensor at position 57:
[rank4]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: tensor at position 65:
[rank4]: saved metadata: {'shape': torch.Size([3584, 14336]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank4]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}

[rank5]: Traceback (most recent call last):
[rank5]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 195, in <module>
[rank5]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank5]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 174, in main
[rank5]:     dpo_trainer.train()
[rank5]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank5]:     return inner_training_loop(
[rank5]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank5]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
[rank5]:     self.accelerator.backward(loss, **kwargs)
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2321, in backward
[rank5]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/deepspeed.py", line 266, in backward
[rank5]:     self.engine.backward(loss, **kwargs)
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank5]:     ret_val = func(*args, **kwargs)
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 2126, in backward
[rank5]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank5]:     ret_val = func(*args, **kwargs)
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py", line 2284, in backward
[rank5]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank5]:     scaled_loss.backward(retain_graph=retain_graph)
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 626, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 1129, in unpack_hook
[rank5]:     frame.check_recomputed_tensors_match(gid)
[rank5]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 903, in check_recomputed_tensors_match
[rank5]:     raise CheckpointError(
[rank5]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank5]: tensor at position 6:
[rank5]: saved metadata: {'shape': torch.Size([4096, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: tensor at position 12:
[rank5]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: tensor at position 18:
[rank5]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: tensor at position 34:
[rank5]: saved metadata: {'shape': torch.Size([3584, 4096]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: tensor at position 50:
[rank5]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: tensor at position 57:
[rank5]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: tensor at position 65:
[rank5]: saved metadata: {'shape': torch.Size([3584, 14336]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank5]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}

[rank6]: Traceback (most recent call last):
[rank6]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 195, in <module>
[rank6]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank6]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 174, in main
[rank6]:     dpo_trainer.train()
[rank6]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank6]:     return inner_training_loop(
[rank6]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank6]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
[rank6]:     self.accelerator.backward(loss, **kwargs)
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2321, in backward
[rank6]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/deepspeed.py", line 266, in backward
[rank6]:     self.engine.backward(loss, **kwargs)
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank6]:     ret_val = func(*args, **kwargs)
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 2126, in backward
[rank6]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank6]:     ret_val = func(*args, **kwargs)
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py", line 2284, in backward
[rank6]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank6]:     scaled_loss.backward(retain_graph=retain_graph)
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 626, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 1129, in unpack_hook
[rank6]:     frame.check_recomputed_tensors_match(gid)
[rank6]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 903, in check_recomputed_tensors_match
[rank6]:     raise CheckpointError(
[rank6]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank6]: tensor at position 6:
[rank6]: saved metadata: {'shape': torch.Size([4096, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: tensor at position 12:
[rank6]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: tensor at position 18:
[rank6]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: tensor at position 34:
[rank6]: saved metadata: {'shape': torch.Size([3584, 4096]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: tensor at position 50:
[rank6]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: tensor at position 57:
[rank6]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: tensor at position 65:
[rank6]: saved metadata: {'shape': torch.Size([3584, 14336]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank6]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}

[rank7]: Traceback (most recent call last):
[rank7]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 195, in <module>
[rank7]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank7]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 174, in main
[rank7]:     dpo_trainer.train()
[rank7]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank7]:     return inner_training_loop(
[rank7]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank7]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
[rank7]:     self.accelerator.backward(loss, **kwargs)
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2321, in backward
[rank7]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/deepspeed.py", line 266, in backward
[rank7]:     self.engine.backward(loss, **kwargs)
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank7]:     ret_val = func(*args, **kwargs)
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/engine.py", line 2126, in backward
[rank7]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank7]:     ret_val = func(*args, **kwargs)
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/zero/stage3.py", line 2284, in backward
[rank7]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank7]:     scaled_loss.backward(retain_graph=retain_graph)
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/torch/_tensor.py", line 626, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py", line 347, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py", line 823, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 1129, in unpack_hook
[rank7]:     frame.check_recomputed_tensors_match(gid)
[rank7]:   File "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py", line 903, in check_recomputed_tensors_match
[rank7]:     raise CheckpointError(
[rank7]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank7]: tensor at position 6:
[rank7]: saved metadata: {'shape': torch.Size([4096, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: tensor at position 12:
[rank7]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: tensor at position 18:
[rank7]: saved metadata: {'shape': torch.Size([2048, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: tensor at position 34:
[rank7]: saved metadata: {'shape': torch.Size([3584, 4096]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: tensor at position 50:
[rank7]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: tensor at position 57:
[rank7]: saved metadata: {'shape': torch.Size([14336, 3584]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: tensor at position 65:
[rank7]: saved metadata: {'shape': torch.Size([3584, 14336]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank7]: recomputed metadata: {'shape': torch.Size([0]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}

W0531 01:14:56.538000 529801 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 529975 closing signal SIGTERM
W0531 01:14:56.538000 529801 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 529976 closing signal SIGTERM
W0531 01:14:56.539000 529801 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 529978 closing signal SIGTERM
E0531 01:14:56.803000 529801 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 529977) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1182, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 861, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-31_01:14:56
  host      : pm5-nod42.vega.pri
  rank      : 6 (local_rank: 2)
  exitcode  : 1 (pid: 529977)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
