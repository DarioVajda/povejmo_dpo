cpu-bind=MASK - gn15, task  1  0 [490274]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 1 ---
Total Nodes: 2
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn11
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 2     --machine_rank 1     --main_process_ip gn11     --main_process_port 29500     --num_processes 8     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62080426     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train.py"     --rank=64 --learning_rate=3e-7 --total_epochs=3 --beta=0.2
-------------------------------------------
[2025-05-30 23:44:53,105] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0530 23:44:54.957000 490327 torch/distributed/run.py:792] 
W0530 23:44:54.957000 490327 torch/distributed/run.py:792] *****************************************
W0530 23:44:54.957000 490327 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0530 23:44:54.957000 490327 torch/distributed/run.py:792] *****************************************
[2025-05-30 23:45:00,367] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-30 23:45:00,416] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-30 23:45:00,428] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-30 23:45:00,429] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
World size: 8
Setting gradient accumulation steps to: 2
[2025-05-30 23:45:03,692] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-30 23:45:03,701] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Steps per epoch: 4282
Eval steps: 2141
[2025-05-30 23:45:03,703] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-30 23:45:03,704] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
[2025-05-30 23:45:05,536] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-30 23:45:05,536] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-30 23:45:05,536] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-30 23:45:05,536] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.64s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.64s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.64s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:21, 10.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:21, 10.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:21, 10.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:21, 10.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.84s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00, 10.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00,  9.92s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00, 10.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00, 10.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00, 10.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00,  9.92s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00,  9.92s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00,  9.92s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
Total Parameters: 216.07M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 100.0000%
Total Parameters: 216.07M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 100.0000%
Total Parameters: 216.07M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 100.0000%
Using LoRA and set up the model
-------------------- CHECKING GRADIENTS --------------------
Trainable parameters:
- base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
Total Parameters: 216.07M- base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)

Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 100.0000%
- base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.40.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.40.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.40.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.40.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.40.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.40.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.40.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.40.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.40.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.40.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.40.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.40.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.40.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.40.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.41.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.41.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.41.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.41.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.41.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.41.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.41.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.41.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.41.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.41.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.41.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.41.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.41.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.41.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
Total trainable parameters: 216072192
------------------------------------------------------------
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   7%|▋         | 566/8564 [00:00<00:01, 5613.64 examples/s]Extracting prompt in train dataset:  16%|█▋        | 1404/8564 [00:00<00:01, 5568.85 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2238/8564 [00:00<00:01, 5562.83 examples/s]Extracting prompt in train dataset:  36%|███▌      | 3050/8564 [00:00<00:01, 5457.67 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3626/8564 [00:00<00:00, 5527.30 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4190/8564 [00:00<00:00, 5543.53 examples/s]Extracting prompt in train dataset:  55%|█████▌    | 4751/8564 [00:00<00:00, 5560.99 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 5344/8564 [00:00<00:00, 5663.72 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 5920/8564 [00:01<00:00, 5668.36 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 6500/8564 [00:01<00:00, 5689.33 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 7080/8564 [00:01<00:00, 5700.15 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 7660/8564 [00:01<00:00, 5711.55 examples/s]Extracting prompt in train dataset:  99%|█████████▉| 8480/8564 [00:01<00:00, 5597.65 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5548.45 examples/s]
Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   3%|▎         | 293/8564 [00:00<00:02, 2904.75 examples/s]Applying chat template to train dataset:   7%|▋         | 602/8564 [00:00<00:02, 3005.74 examples/s]Applying chat template to train dataset:  11%|█         | 911/8564 [00:00<00:02, 3037.24 examples/s]Applying chat template to train dataset:  14%|█▍        | 1221/8564 [00:00<00:02, 3058.29 examples/s]Applying chat template to train dataset:  18%|█▊        | 1530/8564 [00:00<00:02, 3062.78 examples/s]Applying chat template to train dataset:  21%|██▏       | 1841/8564 [00:00<00:02, 3075.88 examples/s]Applying chat template to train dataset:  25%|██▌       | 2154/8564 [00:00<00:02, 3090.27 examples/s]Applying chat template to train dataset:  29%|██▉       | 2467/8564 [00:00<00:01, 3099.70 examples/s]Applying chat template to train dataset:  32%|███▏      | 2779/8564 [00:00<00:01, 3102.88 examples/s]Applying chat template to train dataset:  38%|███▊      | 3237/8564 [00:01<00:01, 3080.73 examples/s]Applying chat template to train dataset:  41%|████▏     | 3551/8564 [00:01<00:01, 3096.34 examples/s]Applying chat template to train dataset:  45%|████▌     | 3862/8564 [00:01<00:01, 3097.35 examples/s]Applying chat template to train dataset:  49%|████▊     | 4173/8564 [00:01<00:01, 3098.79 examples/s]Applying chat template to train dataset:  52%|█████▏    | 4484/8564 [00:01<00:01, 3100.47 examples/s]Applying chat template to train dataset:  56%|█████▌    | 4796/8564 [00:01<00:01, 3098.43 examples/s]Applying chat template to train dataset:  60%|█████▉    | 5113/8564 [00:01<00:01, 3116.60 examples/s]Applying chat template to train dataset:  63%|██████▎   | 5430/8564 [00:01<00:01, 3131.04 examples/s]Applying chat template to train dataset:  69%|██████▊   | 5870/8564 [00:01<00:00, 3051.63 examples/s]Applying chat template to train dataset:  72%|███████▏  | 6190/8564 [00:02<00:00, 3082.80 examples/s]Applying chat template to train dataset:  76%|███████▌  | 6510/8564 [00:02<00:00, 3105.47 examples/s]Applying chat template to train dataset:  80%|███████▉  | 6830/8564 [00:02<00:00, 3126.39 examples/s]Applying chat template to train dataset:  84%|████████▎ | 7159/8564 [00:02<00:00, 3169.09 examples/s]Applying chat template to train dataset:  89%|████████▉ | 7639/8564 [00:02<00:00, 3175.95 examples/s]Applying chat template to train dataset:  94%|█████████▍| 8090/8564 [00:02<00:00, 3114.50 examples/s]Applying chat template to train dataset:  98%|█████████▊| 8411/8564 [00:02<00:00, 3132.30 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3098.65 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 43/8564 [00:00<00:21, 403.83 examples/s]Tokenizing train dataset:   1%|          | 92/8564 [00:00<00:25, 335.74 examples/s]Tokenizing train dataset:   2%|▏         | 140/8564 [00:00<00:26, 322.08 examples/s]Tokenizing train dataset:   2%|▏         | 184/8564 [00:00<00:27, 307.66 examples/s]Tokenizing train dataset:   3%|▎         | 218/8564 [00:00<00:26, 313.69 examples/s]Tokenizing train dataset:   3%|▎         | 251/8564 [00:00<00:26, 316.97 examples/s]Tokenizing train dataset:   3%|▎         | 289/8564 [00:00<00:24, 333.75 examples/s]Tokenizing train dataset:   4%|▍         | 338/8564 [00:01<00:25, 327.11 examples/s]Tokenizing train dataset:   5%|▍         | 389/8564 [00:01<00:25, 325.77 examples/s]Tokenizing train dataset:   5%|▌         | 437/8564 [00:01<00:25, 315.88 examples/s]Tokenizing train dataset:   5%|▌         | 469/8564 [00:01<00:25, 315.54 examples/s]Tokenizing train dataset:   6%|▌         | 512/8564 [00:01<00:26, 304.70 examples/s]Tokenizing train dataset:   6%|▋         | 549/8564 [00:01<00:25, 318.76 examples/s]Tokenizing train dataset:   7%|▋         | 596/8564 [00:01<00:25, 309.39 examples/s]Tokenizing train dataset:   7%|▋         | 634/8564 [00:01<00:24, 323.83 examples/s]Tokenizing train dataset:   8%|▊         | 681/8564 [00:02<00:25, 314.94 examples/s]Tokenizing train dataset:   9%|▊         | 729/8564 [00:02<00:25, 313.29 examples/s]Tokenizing train dataset:   9%|▉         | 763/8564 [00:02<00:24, 314.64 examples/s]Tokenizing train dataset:   9%|▉         | 806/8564 [00:02<00:25, 301.41 examples/s]Tokenizing train dataset:  10%|▉         | 850/8564 [00:02<00:26, 295.60 examples/s]Tokenizing train dataset:  10%|█         | 885/8564 [00:02<00:25, 303.78 examples/s]Tokenizing train dataset:  11%|█         | 920/8564 [00:02<00:24, 308.89 examples/s]Tokenizing train dataset:  11%|█         | 952/8564 [00:03<00:24, 307.08 examples/s]Tokenizing train dataset:  11%|█▏        | 983/8564 [00:03<00:25, 302.58 examples/s]Tokenizing train dataset:  12%|█▏        | 1027/8564 [00:03<00:25, 293.28 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 294.87 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 308.16 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:03<00:25, 295.89 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:03<00:24, 296.32 examples/s]Tokenizing train dataset:  14%|█▍        | 1218/8564 [00:03<00:24, 304.82 examples/s]Tokenizing train dataset:  15%|█▍        | 1252/8564 [00:04<00:23, 312.91 examples/s]Tokenizing train dataset:  15%|█▌        | 1286/8564 [00:04<00:23, 310.73 examples/s]Tokenizing train dataset:  15%|█▌        | 1320/8564 [00:04<00:22, 317.65 examples/s]Tokenizing train dataset:  16%|█▌        | 1353/8564 [00:04<00:22, 314.18 examples/s]Tokenizing train dataset:  16%|█▋        | 1397/8564 [00:04<00:23, 303.84 examples/s]Tokenizing train dataset:  17%|█▋        | 1428/8564 [00:04<00:23, 303.49 examples/s]Tokenizing train dataset:  17%|█▋        | 1473/8564 [00:04<00:23, 298.87 examples/s]Tokenizing train dataset:  18%|█▊        | 1515/8564 [00:04<00:24, 290.35 examples/s]Tokenizing train dataset:  18%|█▊        | 1550/8564 [00:05<00:23, 298.95 examples/s]Tokenizing train dataset:  18%|█▊        | 1581/8564 [00:05<00:23, 298.15 examples/s]Tokenizing train dataset:  19%|█▉        | 1614/8564 [00:05<00:22, 303.86 examples/s]Tokenizing train dataset:  19%|█▉        | 1650/8564 [00:05<00:21, 316.56 examples/s]Tokenizing train dataset:  20%|█▉        | 1686/8564 [00:05<00:21, 322.73 examples/s]Tokenizing train dataset:  20%|██        | 1725/8564 [00:05<00:20, 334.87 examples/s]Tokenizing train dataset:  21%|██        | 1762/8564 [00:05<00:20, 338.43 examples/s]Tokenizing train dataset:  21%|██        | 1809/8564 [00:05<00:20, 324.40 examples/s]Tokenizing train dataset:  22%|██▏       | 1843/8564 [00:05<00:20, 324.30 examples/s]Tokenizing train dataset:  22%|██▏       | 1888/8564 [00:06<00:21, 310.78 examples/s]Tokenizing train dataset:  23%|██▎       | 1932/8564 [00:06<00:19, 341.77 examples/s]Tokenizing train dataset:  23%|██▎       | 1969/8564 [00:06<00:19, 341.99 examples/s]Tokenizing train dataset:  23%|██▎       | 2008/8564 [00:06<00:18, 351.46 examples/s]Tokenizing train dataset:  24%|██▍       | 2048/8564 [00:06<00:18, 361.26 examples/s]Tokenizing train dataset:  24%|██▍       | 2086/8564 [00:06<00:17, 365.58 examples/s]Tokenizing train dataset:  25%|██▍       | 2126/8564 [00:06<00:17, 373.16 examples/s]Tokenizing train dataset:  25%|██▌       | 2180/8564 [00:06<00:17, 361.92 examples/s]Tokenizing train dataset:  26%|██▌       | 2217/8564 [00:06<00:17, 362.83 examples/s]Tokenizing train dataset:  26%|██▋       | 2257/8564 [00:07<00:17, 367.46 examples/s]Tokenizing train dataset:  27%|██▋       | 2300/8564 [00:07<00:16, 382.41 examples/s]Tokenizing train dataset:  27%|██▋       | 2349/8564 [00:07<00:17, 359.53 examples/s]Tokenizing train dataset:  28%|██▊       | 2389/8564 [00:07<00:16, 366.55 examples/s]Tokenizing train dataset:  28%|██▊       | 2431/8564 [00:07<00:16, 373.47 examples/s]Tokenizing train dataset:  29%|██▉       | 2470/8564 [00:07<00:16, 375.62 examples/s]Tokenizing train dataset:  29%|██▉       | 2523/8564 [00:07<00:16, 363.48 examples/s]Tokenizing train dataset:  30%|██▉       | 2565/8564 [00:07<00:15, 375.77 examples/s]Tokenizing train dataset:  30%|███       | 2604/8564 [00:07<00:15, 376.46 examples/s]Tokenizing train dataset:  31%|███       | 2654/8564 [00:08<00:17, 344.70 examples/s]Tokenizing train dataset:  32%|███▏      | 2708/8564 [00:08<00:16, 347.94 examples/s]Tokenizing train dataset:  32%|███▏      | 2759/8564 [00:08<00:16, 343.69 examples/s]Tokenizing train dataset:  33%|███▎      | 2798/8564 [00:08<00:16, 351.60 examples/s]Tokenizing train dataset:  33%|███▎      | 2837/8564 [00:08<00:16, 357.69 examples/s]Tokenizing train dataset:  34%|███▍      | 2893/8564 [00:08<00:15, 357.03 examples/s]Tokenizing train dataset:  34%|███▍      | 2934/8564 [00:08<00:15, 369.17 examples/s]Tokenizing train dataset:  35%|███▍      | 2980/8564 [00:09<00:14, 389.58 examples/s]Tokenizing train dataset:  35%|███▌      | 3036/8564 [00:09<00:14, 377.68 examples/s]Tokenizing train dataset:  36%|███▌      | 3092/8564 [00:09<00:14, 373.88 examples/s]Tokenizing train dataset:  37%|███▋      | 3148/8564 [00:09<00:14, 370.42 examples/s]Tokenizing train dataset:  37%|███▋      | 3204/8564 [00:09<00:14, 367.98 examples/s]Tokenizing train dataset:  38%|███▊      | 3245/8564 [00:09<00:14, 375.15 examples/s]Tokenizing train dataset:  39%|███▊      | 3299/8564 [00:09<00:14, 365.87 examples/s]Tokenizing train dataset:  39%|███▉      | 3352/8564 [00:10<00:14, 359.24 examples/s]Tokenizing train dataset:  40%|███▉      | 3394/8564 [00:10<00:13, 370.16 examples/s]Tokenizing train dataset:  40%|████      | 3435/8564 [00:10<00:13, 378.49 examples/s]Tokenizing train dataset:  41%|████      | 3489/8564 [00:10<00:13, 368.83 examples/s]Tokenizing train dataset:  41%|████      | 3531/8564 [00:10<00:13, 378.13 examples/s]Tokenizing train dataset:  42%|████▏     | 3589/8564 [00:10<00:13, 375.91 examples/s]Tokenizing train dataset:  42%|████▏     | 3630/8564 [00:10<00:13, 376.51 examples/s]Tokenizing train dataset:  43%|████▎     | 3680/8564 [00:10<00:13, 353.96 examples/s]Tokenizing train dataset:  43%|████▎     | 3724/8564 [00:11<00:13, 372.14 examples/s]Tokenizing train dataset:  44%|████▍     | 3780/8564 [00:11<00:12, 370.84 examples/s]Tokenizing train dataset:  45%|████▍     | 3835/8564 [00:11<00:12, 365.72 examples/s]Tokenizing train dataset:  45%|████▌     | 3890/8564 [00:11<00:12, 363.72 examples/s]Tokenizing train dataset:  46%|████▌     | 3941/8564 [00:11<00:13, 353.64 examples/s]Tokenizing train dataset:  46%|████▋     | 3978/8564 [00:11<00:12, 355.47 examples/s]Tokenizing train dataset:  47%|████▋     | 4015/8564 [00:11<00:12, 357.93 examples/s]Tokenizing train dataset:  47%|████▋     | 4056/8564 [00:11<00:12, 366.50 examples/s]Tokenizing train dataset:  48%|████▊     | 4112/8564 [00:12<00:12, 363.62 examples/s]Tokenizing train dataset:  49%|████▊     | 4164/8564 [00:12<00:12, 353.63 examples/s]Tokenizing train dataset:  49%|████▉     | 4206/8564 [00:12<00:11, 367.41 examples/s]Tokenizing train dataset:  50%|████▉     | 4260/8564 [00:12<00:12, 356.58 examples/s]Tokenizing train dataset:  50%|█████     | 4299/8564 [00:12<00:11, 361.85 examples/s]Tokenizing train dataset:  51%|█████     | 4353/8564 [00:12<00:11, 354.87 examples/s]Tokenizing train dataset:  51%|█████▏    | 4393/8564 [00:12<00:11, 362.04 examples/s]Tokenizing train dataset:  52%|█████▏    | 4430/8564 [00:13<00:11, 361.01 examples/s]Tokenizing train dataset:  52%|█████▏    | 4470/8564 [00:13<00:11, 365.06 examples/s]Tokenizing train dataset:  53%|█████▎    | 4507/8564 [00:13<00:11, 359.38 examples/s]Tokenizing train dataset:  53%|█████▎    | 4545/8564 [00:13<00:11, 359.18 examples/s]Tokenizing train dataset:  54%|█████▎    | 4600/8564 [00:13<00:11, 356.01 examples/s]Tokenizing train dataset:  54%|█████▍    | 4636/8564 [00:13<00:11, 349.63 examples/s]Tokenizing train dataset:  55%|█████▍    | 4685/8564 [00:13<00:11, 338.09 examples/s]Tokenizing train dataset:  55%|█████▌    | 4735/8564 [00:13<00:11, 334.53 examples/s]Tokenizing train dataset:  56%|█████▌    | 4781/8564 [00:14<00:11, 321.28 examples/s]Tokenizing train dataset:  56%|█████▋    | 4834/8564 [00:14<00:10, 368.53 examples/s]Tokenizing train dataset:  57%|█████▋    | 4894/8564 [00:14<00:08, 422.63 examples/s]Tokenizing train dataset:  58%|█████▊    | 4959/8564 [00:14<00:07, 479.73 examples/s]Tokenizing train dataset:  59%|█████▊    | 5017/8564 [00:14<00:07, 505.44 examples/s]Tokenizing train dataset:  59%|█████▉    | 5086/8564 [00:14<00:06, 553.19 examples/s]Tokenizing train dataset:  60%|██████    | 5153/8564 [00:14<00:05, 583.65 examples/s]Tokenizing train dataset:  61%|██████    | 5227/8564 [00:14<00:05, 622.18 examples/s]Tokenizing train dataset:  62%|██████▏   | 5303/8564 [00:14<00:04, 657.75 examples/s]Tokenizing train dataset:  63%|██████▎   | 5389/8564 [00:15<00:05, 620.69 examples/s]Tokenizing train dataset:  64%|██████▎   | 5454/8564 [00:15<00:04, 623.62 examples/s]Tokenizing train dataset:  65%|██████▍   | 5547/8564 [00:15<00:04, 612.47 examples/s]Tokenizing train dataset:  66%|██████▌   | 5614/8564 [00:15<00:04, 625.44 examples/s]Tokenizing train dataset:  66%|██████▋   | 5678/8564 [00:15<00:04, 626.08 examples/s]Tokenizing train dataset:  67%|██████▋   | 5743/8564 [00:15<00:04, 630.77 examples/s]Tokenizing train dataset:  68%|██████▊   | 5823/8564 [00:15<00:04, 676.53 examples/s]Tokenizing train dataset:  69%|██████▉   | 5905/8564 [00:15<00:04, 622.47 examples/s]Tokenizing train dataset:  70%|██████▉   | 5974/8564 [00:15<00:04, 638.89 examples/s]Tokenizing train dataset:  71%|███████   | 6058/8564 [00:16<00:04, 601.13 examples/s]Tokenizing train dataset:  72%|███████▏  | 6156/8564 [00:16<00:03, 613.27 examples/s]Tokenizing train dataset:  73%|███████▎  | 6230/8564 [00:16<00:03, 642.16 examples/s]Tokenizing train dataset:  74%|███████▍  | 6327/8564 [00:16<00:03, 636.56 examples/s]Tokenizing train dataset:  75%|███████▍  | 6396/8564 [00:16<00:03, 648.80 examples/s]Tokenizing train dataset:  76%|███████▌  | 6487/8564 [00:16<00:03, 627.20 examples/s]Tokenizing train dataset:  77%|███████▋  | 6577/8564 [00:16<00:03, 615.09 examples/s]Tokenizing train dataset:  78%|███████▊  | 6658/8564 [00:17<00:03, 586.17 examples/s]Tokenizing train dataset:  79%|███████▊  | 6729/8564 [00:17<00:02, 612.49 examples/s]Tokenizing train dataset:  79%|███████▉  | 6795/8564 [00:17<00:02, 621.32 examples/s]Tokenizing train dataset:  80%|████████  | 6885/8564 [00:17<00:02, 604.72 examples/s]Tokenizing train dataset:  81%|████████  | 6952/8564 [00:17<00:02, 618.36 examples/s]Tokenizing train dataset:  82%|████████▏ | 7035/8564 [00:17<00:02, 594.55 examples/s]Tokenizing train dataset:  83%|████████▎ | 7105/8564 [00:17<00:02, 617.48 examples/s]Tokenizing train dataset:  84%|████████▎ | 7171/8564 [00:17<00:02, 626.16 examples/s]Tokenizing train dataset:  85%|████████▍ | 7262/8564 [00:18<00:02, 615.23 examples/s]Tokenizing train dataset:  86%|████████▌ | 7328/8564 [00:18<00:01, 622.96 examples/s]Tokenizing train dataset:  87%|████████▋ | 7424/8564 [00:18<00:01, 623.98 examples/s]Tokenizing train dataset:  87%|████████▋ | 7493/8564 [00:18<00:01, 636.24 examples/s]Tokenizing train dataset:  88%|████████▊ | 7560/8564 [00:18<00:01, 642.58 examples/s]Tokenizing train dataset:  89%|████████▉ | 7633/8564 [00:18<00:01, 653.18 examples/s]Tokenizing train dataset:  90%|█████████ | 7721/8564 [00:18<00:01, 620.87 examples/s]Tokenizing train dataset:  91%|█████████ | 7803/8564 [00:18<00:01, 590.86 examples/s]Tokenizing train dataset:  92%|█████████▏| 7865/8564 [00:19<00:01, 596.95 examples/s]Tokenizing train dataset:  93%|█████████▎| 7930/8564 [00:19<00:01, 604.85 examples/s]Tokenizing train dataset:  94%|█████████▎| 8024/8564 [00:19<00:00, 610.99 examples/s]Tokenizing train dataset:  95%|█████████▍| 8110/8564 [00:19<00:00, 590.01 examples/s]Tokenizing train dataset:  95%|█████████▌| 8174/8564 [00:19<00:00, 599.61 examples/s]Tokenizing train dataset:  96%|█████████▌| 8237/8564 [00:19<00:00, 606.25 examples/s]Tokenizing train dataset:  97%|█████████▋| 8314/8564 [00:19<00:00, 648.59 examples/s]Tokenizing train dataset:  98%|█████████▊| 8397/8564 [00:19<00:00, 610.01 examples/s]Tokenizing train dataset:  99%|█████████▉| 8494/8564 [00:20<00:00, 614.57 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 615.01 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 424.92 examples/s]
Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11399.22 examples/s]
Extracting prompt in train dataset:   7%|▋         | 565/8564 [00:00<00:01, 5564.08 examples/s]Extracting prompt in train dataset:   7%|▋         | 570/8564 [00:00<00:01, 5605.39 examples/s]Extracting prompt in train dataset:   7%|▋         | 570/8564 [00:00<00:01, 5591.07 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1130/8564 [00:00<00:01, 5609.69 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1147/8564 [00:00<00:01, 5691.25 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1151/8564 [00:00<00:01, 5694.70 examples/s]Extracting prompt in train dataset:  20%|██        | 1723/8564 [00:00<00:01, 5703.33 examples/s]Extracting prompt in train dataset:  20%|██        | 1724/8564 [00:00<00:01, 5698.42 examples/s]Extracting prompt in train dataset:  20%|█▉        | 1710/8564 [00:00<00:01, 5630.61 examples/s]Extracting prompt in train dataset:  27%|██▋       | 2310/8564 [00:00<00:01, 5748.83 examples/s]Extracting prompt in train dataset:  27%|██▋       | 2310/8564 [00:00<00:01, 5746.26 examples/s]Extracting prompt in train dataset:  27%|██▋       | 2290/8564 [00:00<00:01, 5680.90 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13841.82 examples/s]
Extracting prompt in train dataset:  34%|███▍      | 2900/8564 [00:00<00:00, 5784.24 examples/s]Extracting prompt in train dataset:  34%|███▍      | 2900/8564 [00:00<00:00, 5786.16 examples/s]Extracting prompt in train dataset:  34%|███▎      | 2870/8564 [00:00<00:00, 5713.45 examples/s]Extracting prompt in train dataset:  44%|████▍     | 3747/8564 [00:00<00:00, 5714.25 examples/s]Extracting prompt in train dataset:  44%|████▍     | 3748/8564 [00:00<00:00, 5718.25 examples/s]Extracting prompt in train dataset:  43%|████▎     | 3690/8564 [00:00<00:00, 5588.86 examples/s]Extracting prompt in train dataset:  51%|█████     | 4337/8564 [00:00<00:00, 5754.86 examples/s]Extracting prompt in train dataset:  51%|█████     | 4336/8564 [00:00<00:00, 5751.27 examples/s]Extracting prompt in train dataset:  53%|█████▎    | 4530/8564 [00:00<00:00, 5569.95 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  57%|█████▋    | 4920/8564 [00:00<00:00, 5774.77 examples/s]Extracting prompt in train dataset:  58%|█████▊    | 4930/8564 [00:00<00:00, 5775.78 examples/s]Extracting prompt in train dataset:  60%|█████▉    | 5110/8564 [00:00<00:00, 5615.15 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 331.31 examples/s]Extracting prompt in train dataset:  64%|██████▍   | 5520/8564 [00:00<00:00, 5835.37 examples/s]Extracting prompt in train dataset:  65%|██████▍   | 5530/8564 [00:00<00:00, 5836.85 examples/s]Extracting prompt in train dataset:  67%|██████▋   | 5710/8564 [00:01<00:00, 5695.50 examples/s]Extracting prompt in train dataset:  71%|███████▏  | 6120/8564 [00:01<00:00, 5878.24 examples/s]Extracting prompt in train dataset:  72%|███████▏  | 6130/8564 [00:01<00:00, 5878.97 examples/s]Extracting prompt in train dataset:  74%|███████▎  | 6310/8564 [00:01<00:00, 5753.05 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 299.39 examples/s]Extracting prompt in train dataset:  78%|███████▊  | 6720/8564 [00:01<00:00, 5907.53 examples/s]Extracting prompt in train dataset:  79%|███████▊  | 6730/8564 [00:01<00:00, 5910.45 examples/s]Extracting prompt in train dataset:  81%|████████  | 6900/8564 [00:01<00:00, 5793.15 examples/s]Extracting prompt in train dataset:  85%|████████▌ | 7320/8564 [00:01<00:00, 5927.44 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 7330/8564 [00:01<00:00, 5931.90 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:02, 280.82 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 7500/8564 [00:01<00:00, 5824.75 examples/s]Extracting prompt in train dataset:  95%|█████████▍| 8125/8564 [00:01<00:00, 5705.66 examples/s]Extracting prompt in train dataset:  95%|█████████▌| 8140/8564 [00:01<00:00, 5711.82 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 270.35 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 8320/8564 [00:01<00:00, 5645.76 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5746.33 examples/s]
Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5741.75 examples/s]
Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5639.19 examples/s]
Tokenizing eval dataset:  21%|██        | 198/953 [00:00<00:02, 261.35 examples/s]Tokenizing eval dataset:  25%|██▍       | 238/953 [00:00<00:02, 293.72 examples/s]Tokenizing eval dataset:  32%|███▏      | 305/953 [00:00<00:01, 391.36 examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing eval dataset:  38%|███▊      | 366/953 [00:01<00:01, 449.66 examples/s]Applying chat template to train dataset:   3%|▎         | 286/8564 [00:00<00:02, 2834.13 examples/s]Applying chat template to train dataset:   3%|▎         | 278/8564 [00:00<00:03, 2752.12 examples/s]Applying chat template to train dataset:   3%|▎         | 290/8564 [00:00<00:02, 2863.20 examples/s]Tokenizing eval dataset:  46%|████▌     | 436/953 [00:01<00:00, 517.38 examples/s]Applying chat template to train dataset:   7%|▋         | 604/8564 [00:00<00:02, 3031.63 examples/s]Applying chat template to train dataset:   7%|▋         | 590/8564 [00:00<00:02, 2963.52 examples/s]Applying chat template to train dataset:   7%|▋         | 608/8564 [00:00<00:02, 3045.35 examples/s]Tokenizing eval dataset:  53%|█████▎    | 504/953 [00:01<00:00, 561.81 examples/s]Applying chat template to train dataset:  11%|█         | 922/8564 [00:00<00:02, 3094.74 examples/s]Applying chat template to train dataset:  11%|█         | 902/8564 [00:00<00:02, 3031.31 examples/s]Applying chat template to train dataset:  11%|█         | 924/8564 [00:00<00:02, 3094.96 examples/s]Tokenizing eval dataset:  60%|█████▉    | 570/953 [00:01<00:00, 583.10 examples/s]Applying chat template to train dataset:  14%|█▍        | 1241/8564 [00:00<00:02, 3129.98 examples/s]Applying chat template to train dataset:  14%|█▍        | 1216/8564 [00:00<00:02, 3069.87 examples/s]Applying chat template to train dataset:  15%|█▍        | 1242/8564 [00:00<00:02, 3126.48 examples/s]Tokenizing eval dataset:  67%|██████▋   | 640/953 [00:01<00:00, 613.07 examples/s]Applying chat template to train dataset:  18%|█▊        | 1557/8564 [00:00<00:02, 3135.25 examples/s]Applying chat template to train dataset:  18%|█▊        | 1526/8564 [00:00<00:02, 3076.08 examples/s]Applying chat template to train dataset:  18%|█▊        | 1556/8564 [00:00<00:02, 3128.37 examples/s]Applying chat template to train dataset:  22%|██▏       | 1872/8564 [00:00<00:02, 3137.52 examples/s]Tokenizing eval dataset:  77%|███████▋  | 730/953 [00:01<00:00, 598.33 examples/s]Applying chat template to train dataset:  23%|██▎       | 1985/8564 [00:00<00:02, 3062.71 examples/s]Applying chat template to train dataset:  24%|██▎       | 2026/8564 [00:00<00:02, 3124.66 examples/s]Applying chat template to train dataset:  26%|██▌       | 2194/8564 [00:00<00:02, 3164.07 examples/s]Tokenizing eval dataset:  85%|████████▍ | 807/953 [00:01<00:00, 564.25 examples/s]Applying chat template to train dataset:  27%|██▋       | 2302/8564 [00:00<00:02, 3089.39 examples/s]Applying chat template to train dataset:  27%|██▋       | 2347/8564 [00:00<00:01, 3148.97 examples/s]Applying chat template to train dataset:  29%|██▉       | 2516/8564 [00:00<00:01, 3179.09 examples/s]Applying chat template to train dataset:  31%|███       | 2619/8564 [00:00<00:01, 3110.47 examples/s]Applying chat template to train dataset:  31%|███       | 2668/8564 [00:00<00:01, 3165.02 examples/s]Applying chat template to train dataset:  33%|███▎      | 2838/8564 [00:00<00:01, 3188.64 examples/s]Tokenizing eval dataset:  93%|█████████▎| 884/953 [00:01<00:00, 544.56 examples/s]Applying chat template to train dataset:  34%|███▍      | 2934/8564 [00:00<00:01, 3120.19 examples/s]Applying chat template to train dataset:  35%|███▍      | 2990/8564 [00:00<00:01, 3174.70 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 533.09 examples/s]Applying chat template to train dataset:  39%|███▊      | 3302/8564 [00:01<00:01, 3147.63 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 466.11 examples/s]
Applying chat template to train dataset:  40%|███▉      | 3389/8564 [00:01<00:01, 3082.31 examples/s]Applying chat template to train dataset:  40%|████      | 3450/8564 [00:01<00:01, 3128.15 examples/s]Applying chat template to train dataset:  42%|████▏     | 3624/8564 [00:01<00:01, 3164.84 examples/s]Applying chat template to train dataset:  43%|████▎     | 3703/8564 [00:01<00:01, 3096.05 examples/s]Applying chat template to train dataset:  44%|████▍     | 3770/8564 [00:01<00:01, 3145.06 examples/s]Applying chat template to train dataset:  46%|████▌     | 3944/8564 [00:01<00:01, 3173.89 examples/s]Applying chat template to train dataset:  47%|████▋     | 4019/8564 [00:01<00:01, 3110.73 examples/s]Applying chat template to train dataset:  48%|████▊     | 4090/8564 [00:01<00:01, 3156.70 examples/s]Applying chat template to train dataset:  50%|████▉     | 4265/8564 [00:01<00:01, 3181.47 examples/s]Applying chat template to train dataset:  51%|█████     | 4333/8564 [00:01<00:01, 3116.07 examples/s]Applying chat template to train dataset:  51%|█████▏    | 4410/8564 [00:01<00:01, 3165.56 examples/s]Applying chat template to train dataset:  54%|█████▎    | 4585/8564 [00:01<00:01, 3185.51 examples/s]Applying chat template to train dataset:  54%|█████▍    | 4648/8564 [00:01<00:01, 3124.77 examples/s]Applying chat template to train dataset:  55%|█████▌    | 4730/8564 [00:01<00:01, 3169.54 examples/s]Applying chat template to train dataset:  57%|█████▋    | 4907/8564 [00:01<00:01, 3192.72 examples/s]Applying chat template to train dataset:  58%|█████▊    | 4965/8564 [00:01<00:01, 3134.68 examples/s]Applying chat template to train dataset:  59%|█████▉    | 5055/8564 [00:01<00:01, 3190.79 examples/s]Applying chat template to train dataset:  61%|██████    | 5235/8564 [00:01<00:01, 3215.49 examples/s]Applying chat template to train dataset:  62%|██████▏   | 5287/8564 [00:01<00:01, 3157.64 examples/s]Applying chat template to train dataset:  63%|██████▎   | 5382/8564 [00:01<00:00, 3210.51 examples/s]Applying chat template to train dataset:  65%|██████▍   | 5562/8564 [00:01<00:00, 3229.66 examples/s]Applying chat template to train dataset:  65%|██████▌   | 5608/8564 [00:01<00:00, 3169.88 examples/s]Applying chat template to train dataset:  67%|██████▋   | 5709/8564 [00:01<00:00, 3226.33 examples/s]Applying chat template to train dataset:  69%|██████▉   | 5890/8564 [00:01<00:00, 3240.02 examples/s]Applying chat template to train dataset:  69%|██████▉   | 5930/8564 [00:01<00:00, 3178.21 examples/s]Applying chat template to train dataset:  70%|███████   | 6034/8564 [00:01<00:00, 3232.30 examples/s]Applying chat template to train dataset:  73%|███████▎  | 6219/8564 [00:01<00:00, 3251.61 examples/s]Applying chat template to train dataset:  73%|███████▎  | 6253/8564 [00:02<00:00, 3188.53 examples/s]Applying chat template to train dataset:  74%|███████▍  | 6361/8564 [00:02<00:00, 3240.20 examples/s]Applying chat template to train dataset:  76%|███████▋  | 6546/8564 [00:02<00:00, 3254.60 examples/s]Applying chat template to train dataset:  77%|███████▋  | 6575/8564 [00:02<00:00, 3193.59 examples/s]Applying chat template to train dataset:  78%|███████▊  | 6687/8564 [00:02<00:00, 3243.27 examples/s]Applying chat template to train dataset:  80%|████████  | 6872/8564 [00:02<00:00, 3253.99 examples/s]Applying chat template to train dataset:  81%|████████  | 6896/8564 [00:02<00:00, 3197.43 examples/s]Applying chat template to train dataset:  82%|████████▏ | 7014/8564 [00:02<00:00, 3244.72 examples/s]Applying chat template to train dataset:  84%|████████▍ | 7200/8564 [00:02<00:00, 3259.26 examples/s]Applying chat template to train dataset:  84%|████████▍ | 7219/8564 [00:02<00:00, 3202.30 examples/s]Applying chat template to train dataset:  86%|████████▌ | 7340/8564 [00:02<00:00, 3246.06 examples/s]Applying chat template to train dataset:  88%|████████▊ | 7529/8564 [00:02<00:00, 3265.09 examples/s]Applying chat template to train dataset:  88%|████████▊ | 7540/8564 [00:02<00:00, 3198.82 examples/s]Applying chat template to train dataset:  90%|████████▉ | 7666/8564 [00:02<00:00, 3248.93 examples/s]Applying chat template to train dataset:  93%|█████████▎| 7976/8564 [00:02<00:00, 3150.17 examples/s]Applying chat template to train dataset:  93%|█████████▎| 7973/8564 [00:02<00:00, 3075.67 examples/s]Applying chat template to train dataset:  95%|█████████▍| 8108/8564 [00:02<00:00, 3128.78 examples/s]Applying chat template to train dataset:  97%|█████████▋| 8305/8564 [00:02<00:00, 3183.43 examples/s]Applying chat template to train dataset:  97%|█████████▋| 8295/8564 [00:02<00:00, 3113.41 examples/s]Applying chat template to train dataset:  98%|█████████▊| 8434/8564 [00:02<00:00, 3162.89 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3180.04 examples/s]
Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3165.86 examples/s]
Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3114.34 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 41/8564 [00:00<00:21, 399.85 examples/s]Tokenizing train dataset:   0%|          | 42/8564 [00:00<00:20, 407.99 examples/s]Tokenizing train dataset:   1%|          | 43/8564 [00:00<00:21, 404.46 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:25, 337.84 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:24, 339.32 examples/s]Tokenizing train dataset:   1%|          | 92/8564 [00:00<00:25, 335.85 examples/s]Tokenizing train dataset:   2%|▏         | 133/8564 [00:00<00:27, 306.19 examples/s]Tokenizing train dataset:   2%|▏         | 139/8564 [00:00<00:26, 320.82 examples/s]Tokenizing train dataset:   2%|▏         | 140/8564 [00:00<00:26, 322.24 examples/s]Tokenizing train dataset:   2%|▏         | 165/8564 [00:00<00:27, 306.65 examples/s]Tokenizing train dataset:   2%|▏         | 182/8564 [00:00<00:27, 303.05 examples/s]Tokenizing train dataset:   2%|▏         | 184/8564 [00:00<00:27, 303.98 examples/s]Tokenizing train dataset:   2%|▏         | 212/8564 [00:00<00:27, 303.84 examples/s]Tokenizing train dataset:   3%|▎         | 218/8564 [00:00<00:26, 313.48 examples/s]Tokenizing train dataset:   3%|▎         | 218/8564 [00:00<00:26, 310.92 examples/s]Tokenizing train dataset:   3%|▎         | 248/8564 [00:00<00:26, 317.97 examples/s]Tokenizing train dataset:   3%|▎         | 252/8564 [00:00<00:26, 318.30 examples/s]Tokenizing train dataset:   3%|▎         | 252/8564 [00:00<00:26, 316.72 examples/s]Tokenizing train dataset:   3%|▎         | 283/8564 [00:00<00:25, 325.99 examples/s]Tokenizing train dataset:   3%|▎         | 290/8564 [00:00<00:24, 332.06 examples/s]Tokenizing train dataset:   3%|▎         | 290/8564 [00:00<00:24, 331.17 examples/s]Tokenizing train dataset:   4%|▎         | 318/8564 [00:00<00:25, 327.09 examples/s]Tokenizing train dataset:   4%|▍         | 324/8564 [00:00<00:24, 331.06 examples/s]Tokenizing train dataset:   4%|▍         | 324/8564 [00:00<00:24, 330.58 examples/s]Tokenizing train dataset:   4%|▍         | 367/8564 [00:01<00:25, 318.78 examples/s]Tokenizing train dataset:   4%|▍         | 370/8564 [00:01<00:25, 318.30 examples/s]Tokenizing train dataset:   4%|▍         | 370/8564 [00:01<00:25, 317.97 examples/s]Tokenizing train dataset:   5%|▍         | 400/8564 [00:01<00:25, 318.37 examples/s]Tokenizing train dataset:   5%|▌         | 433/8564 [00:01<00:25, 317.40 examples/s]Tokenizing train dataset:   5%|▍         | 420/8564 [00:01<00:25, 316.97 examples/s]Tokenizing train dataset:   5%|▍         | 420/8564 [00:01<00:25, 316.88 examples/s]Tokenizing train dataset:   6%|▌         | 480/8564 [00:01<00:26, 309.23 examples/s]Tokenizing train dataset:   5%|▌         | 469/8564 [00:01<00:25, 315.14 examples/s]Tokenizing train dataset:   5%|▌         | 469/8564 [00:01<00:25, 315.48 examples/s]Tokenizing train dataset:   6%|▌         | 527/8564 [00:01<00:26, 308.86 examples/s]Tokenizing train dataset:   6%|▌         | 516/8564 [00:01<00:26, 308.85 examples/s]Tokenizing train dataset:   6%|▌         | 516/8564 [00:01<00:26, 309.29 examples/s]Tokenizing train dataset:   7%|▋         | 561/8564 [00:01<00:25, 314.38 examples/s]Tokenizing train dataset:   6%|▋         | 550/8564 [00:01<00:25, 314.02 examples/s]Tokenizing train dataset:   6%|▋         | 550/8564 [00:01<00:25, 314.66 examples/s]Tokenizing train dataset:   7%|▋         | 605/8564 [00:01<00:26, 303.65 examples/s]Tokenizing train dataset:   7%|▋         | 596/8564 [00:01<00:25, 309.25 examples/s]Tokenizing train dataset:   7%|▋         | 596/8564 [00:01<00:25, 309.93 examples/s]Tokenizing train dataset:   8%|▊         | 644/8564 [00:02<00:24, 322.05 examples/s]Tokenizing train dataset:   7%|▋         | 634/8564 [00:01<00:24, 323.52 examples/s]Tokenizing train dataset:   7%|▋         | 634/8564 [00:01<00:24, 324.14 examples/s]Tokenizing train dataset:   8%|▊         | 690/8564 [00:02<00:25, 311.33 examples/s]Tokenizing train dataset:   8%|▊         | 681/8564 [00:02<00:25, 314.79 examples/s]Tokenizing train dataset:   8%|▊         | 681/8564 [00:02<00:25, 315.08 examples/s]Tokenizing train dataset:   8%|▊         | 723/8564 [00:02<00:24, 314.35 examples/s]Tokenizing train dataset:   9%|▊         | 729/8564 [00:02<00:25, 313.07 examples/s]Tokenizing train dataset:   9%|▊         | 729/8564 [00:02<00:25, 313.08 examples/s]Tokenizing train dataset:   9%|▉         | 759/8564 [00:02<00:24, 321.08 examples/s]Tokenizing train dataset:   9%|▉         | 761/8564 [00:02<00:24, 313.91 examples/s]Tokenizing train dataset:   9%|▉         | 763/8564 [00:02<00:24, 314.48 examples/s]Tokenizing train dataset:   9%|▉         | 802/8564 [00:02<00:25, 303.21 examples/s]Tokenizing train dataset:   9%|▉         | 806/8564 [00:02<00:25, 301.11 examples/s]Tokenizing train dataset:   9%|▉         | 806/8564 [00:02<00:25, 301.33 examples/s]Tokenizing train dataset:  10%|▉         | 847/8564 [00:02<00:25, 300.06 examples/s]Tokenizing train dataset:  10%|▉         | 850/8564 [00:02<00:26, 295.25 examples/s]Tokenizing train dataset:  10%|▉         | 850/8564 [00:02<00:26, 295.66 examples/s]Tokenizing train dataset:  10%|█         | 879/8564 [00:02<00:25, 300.73 examples/s]Tokenizing train dataset:  10%|█         | 885/8564 [00:02<00:25, 303.10 examples/s]Tokenizing train dataset:  10%|█         | 885/8564 [00:02<00:25, 303.65 examples/s]Tokenizing train dataset:  11%|█         | 917/8564 [00:02<00:24, 315.50 examples/s]Tokenizing train dataset:  11%|█         | 920/8564 [00:02<00:24, 308.29 examples/s]Tokenizing train dataset:  11%|█         | 920/8564 [00:02<00:24, 309.11 examples/s]Tokenizing train dataset:  11%|█▏        | 965/8564 [00:03<00:24, 312.52 examples/s]Tokenizing train dataset:  11%|█         | 952/8564 [00:03<00:24, 306.48 examples/s]Tokenizing train dataset:  11%|█         | 952/8564 [00:03<00:24, 307.24 examples/s]Tokenizing train dataset:  11%|█▏        | 983/8564 [00:03<00:25, 302.19 examples/s]Tokenizing train dataset:  11%|█▏        | 983/8564 [00:03<00:25, 302.88 examples/s]Tokenizing train dataset:  12%|█▏        | 1008/8564 [00:03<00:25, 300.10 examples/s]Tokenizing train dataset:  12%|█▏        | 1027/8564 [00:03<00:25, 292.86 examples/s]Tokenizing train dataset:  12%|█▏        | 1027/8564 [00:03<00:25, 293.29 examples/s]Tokenizing train dataset:  12%|█▏        | 1051/8564 [00:03<00:25, 291.43 examples/s]Tokenizing train dataset:  13%|█▎        | 1085/8564 [00:03<00:24, 300.62 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 294.26 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 294.95 examples/s]Tokenizing train dataset:  13%|█▎        | 1119/8564 [00:03<00:24, 308.88 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 307.48 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 308.28 examples/s]Tokenizing train dataset:  14%|█▎        | 1160/8564 [00:03<00:25, 292.86 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:03<00:25, 295.07 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:03<00:25, 295.98 examples/s]Tokenizing train dataset:  14%|█▍        | 1193/8564 [00:03<00:24, 300.73 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:03<00:24, 295.40 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:03<00:24, 296.40 examples/s]Tokenizing train dataset:  14%|█▍        | 1227/8564 [00:03<00:24, 305.36 examples/s]Tokenizing train dataset:  14%|█▍        | 1218/8564 [00:03<00:24, 303.72 examples/s]Tokenizing train dataset:  14%|█▍        | 1218/8564 [00:03<00:24, 305.13 examples/s]Tokenizing train dataset:  15%|█▍        | 1264/8564 [00:04<00:22, 318.62 examples/s]Tokenizing train dataset:  15%|█▍        | 1252/8564 [00:04<00:23, 311.67 examples/s]Tokenizing train dataset:  15%|█▍        | 1252/8564 [00:04<00:23, 313.17 examples/s]Tokenizing train dataset:  15%|█▌        | 1286/8564 [00:04<00:23, 309.41 examples/s]Tokenizing train dataset:  15%|█▌        | 1312/8564 [00:04<00:22, 317.71 examples/s]Tokenizing train dataset:  15%|█▌        | 1286/8564 [00:04<00:23, 311.27 examples/s]Tokenizing train dataset:  15%|█▌        | 1320/8564 [00:04<00:22, 316.62 examples/s]Tokenizing train dataset:  16%|█▌        | 1345/8564 [00:04<00:22, 318.00 examples/s]Tokenizing train dataset:  15%|█▌        | 1320/8564 [00:04<00:22, 318.45 examples/s]Tokenizing train dataset:  16%|█▌        | 1353/8564 [00:04<00:23, 313.30 examples/s]Tokenizing train dataset:  16%|█▌        | 1353/8564 [00:04<00:22, 315.06 examples/s]Tokenizing train dataset:  16%|█▌        | 1387/8564 [00:04<00:23, 301.48 examples/s]Tokenizing train dataset:  16%|█▋        | 1397/8564 [00:04<00:23, 303.64 examples/s]Tokenizing train dataset:  17%|█▋        | 1418/8564 [00:04<00:23, 302.79 examples/s]Tokenizing train dataset:  16%|█▋        | 1397/8564 [00:04<00:23, 305.02 examples/s]Tokenizing train dataset:  17%|█▋        | 1428/8564 [00:04<00:23, 303.38 examples/s]Tokenizing train dataset:  17%|█▋        | 1428/8564 [00:04<00:23, 304.26 examples/s]Tokenizing train dataset:  17%|█▋        | 1462/8564 [00:04<00:24, 295.83 examples/s]Tokenizing train dataset:  17%|█▋        | 1473/8564 [00:04<00:23, 298.91 examples/s]Tokenizing train dataset:  17%|█▋        | 1495/8564 [00:04<00:23, 295.02 examples/s]Tokenizing train dataset:  17%|█▋        | 1474/8564 [00:04<00:23, 298.05 examples/s]Tokenizing train dataset:  18%|█▊        | 1527/8564 [00:04<00:23, 297.18 examples/s]Tokenizing train dataset:  18%|█▊        | 1505/8564 [00:04<00:23, 294.48 examples/s]Tokenizing train dataset:  18%|█▊        | 1515/8564 [00:04<00:24, 290.49 examples/s]Tokenizing train dataset:  18%|█▊        | 1559/8564 [00:05<00:23, 301.16 examples/s]Tokenizing train dataset:  18%|█▊        | 1538/8564 [00:04<00:23, 301.35 examples/s]Tokenizing train dataset:  18%|█▊        | 1550/8564 [00:05<00:23, 299.17 examples/s]Tokenizing train dataset:  18%|█▊        | 1581/8564 [00:05<00:23, 298.71 examples/s]Tokenizing train dataset:  19%|█▊        | 1605/8564 [00:05<00:23, 300.00 examples/s]Tokenizing train dataset:  18%|█▊        | 1582/8564 [00:05<00:23, 297.94 examples/s]Tokenizing train dataset:  19%|█▉        | 1614/8564 [00:05<00:22, 304.06 examples/s]Tokenizing train dataset:  19%|█▉        | 1614/8564 [00:05<00:22, 303.02 examples/s]Tokenizing train dataset:  19%|█▉        | 1641/8564 [00:05<00:22, 310.36 examples/s]Tokenizing train dataset:  19%|█▉        | 1650/8564 [00:05<00:21, 316.40 examples/s]Tokenizing train dataset:  19%|█▉        | 1650/8564 [00:05<00:21, 315.75 examples/s]Tokenizing train dataset:  20%|█▉        | 1676/8564 [00:05<00:21, 316.80 examples/s]Tokenizing train dataset:  20%|█▉        | 1686/8564 [00:05<00:21, 322.09 examples/s]Tokenizing train dataset:  20%|██        | 1718/8564 [00:05<00:20, 339.15 examples/s]Tokenizing train dataset:  20%|█▉        | 1705/8564 [00:05<00:20, 333.35 examples/s]Tokenizing train dataset:  20%|██        | 1725/8564 [00:05<00:20, 334.26 examples/s]Tokenizing train dataset:  21%|██        | 1769/8564 [00:05<00:20, 334.04 examples/s]Tokenizing train dataset:  20%|██        | 1754/8564 [00:05<00:20, 326.92 examples/s]Tokenizing train dataset:  21%|██        | 1762/8564 [00:05<00:20, 338.08 examples/s]Tokenizing train dataset:  21%|██        | 1816/8564 [00:05<00:20, 321.68 examples/s]Tokenizing train dataset:  21%|██        | 1802/8564 [00:05<00:21, 321.57 examples/s]Tokenizing train dataset:  21%|██        | 1809/8564 [00:05<00:20, 324.26 examples/s]Tokenizing train dataset:  22%|██▏       | 1850/8564 [00:05<00:20, 320.66 examples/s]Tokenizing train dataset:  21%|██▏       | 1836/8564 [00:05<00:20, 325.61 examples/s]Tokenizing train dataset:  22%|██▏       | 1843/8564 [00:05<00:20, 324.29 examples/s]Tokenizing train dataset:  22%|██▏       | 1896/8564 [00:06<00:21, 311.12 examples/s]Tokenizing train dataset:  22%|██▏       | 1880/8564 [00:06<00:21, 309.05 examples/s]Tokenizing train dataset:  22%|██▏       | 1888/8564 [00:06<00:21, 310.76 examples/s]Tokenizing train dataset:  23%|██▎       | 1942/8564 [00:06<00:19, 343.86 examples/s]Tokenizing train dataset:  22%|██▏       | 1921/8564 [00:06<00:20, 331.18 examples/s]Tokenizing train dataset:  23%|██▎       | 1933/8564 [00:06<00:19, 342.38 examples/s]Tokenizing train dataset:  23%|██▎       | 1980/8564 [00:06<00:19, 346.40 examples/s]Tokenizing train dataset:  23%|██▎       | 1960/8564 [00:06<00:19, 339.39 examples/s]Tokenizing train dataset:  23%|██▎       | 1969/8564 [00:06<00:19, 341.96 examples/s]Tokenizing train dataset:  24%|██▎       | 2018/8564 [00:06<00:18, 353.00 examples/s]Tokenizing train dataset:  23%|██▎       | 1999/8564 [00:06<00:18, 348.69 examples/s]Tokenizing train dataset:  23%|██▎       | 2008/8564 [00:06<00:18, 351.56 examples/s]Tokenizing train dataset:  24%|██▍       | 2060/8564 [00:06<00:17, 365.69 examples/s]Tokenizing train dataset:  24%|██▍       | 2039/8564 [00:06<00:18, 358.11 examples/s]Tokenizing train dataset:  24%|██▍       | 2048/8564 [00:06<00:18, 361.25 examples/s]Tokenizing train dataset:  24%|██▍       | 2077/8564 [00:06<00:17, 360.60 examples/s]Tokenizing train dataset:  24%|██▍       | 2086/8564 [00:06<00:17, 365.50 examples/s]Tokenizing train dataset:  25%|██▍       | 2116/8564 [00:06<00:17, 364.06 examples/s]Tokenizing train dataset:  25%|██▍       | 2116/8564 [00:06<00:17, 365.09 examples/s]Tokenizing train dataset:  25%|██▍       | 2126/8564 [00:06<00:17, 372.87 examples/s]Tokenizing train dataset:  25%|██▌       | 2154/8564 [00:06<00:17, 364.74 examples/s]Tokenizing train dataset:  25%|██▌       | 2154/8564 [00:06<00:17, 365.58 examples/s]Tokenizing train dataset:  26%|██▌       | 2192/8564 [00:06<00:17, 364.61 examples/s]Tokenizing train dataset:  25%|██▌       | 2180/8564 [00:06<00:17, 361.76 examples/s]Tokenizing train dataset:  26%|██▌       | 2192/8564 [00:06<00:17, 365.14 examples/s]Tokenizing train dataset:  26%|██▌       | 2230/8564 [00:06<00:17, 365.03 examples/s]Tokenizing train dataset:  26%|██▌       | 2217/8564 [00:06<00:17, 362.89 examples/s]Tokenizing train dataset:  26%|██▌       | 2230/8564 [00:06<00:17, 365.02 examples/s]Tokenizing train dataset:  27%|██▋       | 2270/8564 [00:07<00:16, 373.22 examples/s]Tokenizing train dataset:  26%|██▋       | 2257/8564 [00:07<00:17, 367.72 examples/s]Tokenizing train dataset:  27%|██▋       | 2270/8564 [00:07<00:16, 373.13 examples/s]Tokenizing train dataset:  27%|██▋       | 2309/8564 [00:07<00:16, 373.16 examples/s]Tokenizing train dataset:  27%|██▋       | 2300/8564 [00:07<00:16, 382.82 examples/s]Tokenizing train dataset:  27%|██▋       | 2309/8564 [00:07<00:16, 372.87 examples/s]Tokenizing train dataset:  28%|██▊       | 2360/8564 [00:07<00:17, 354.48 examples/s]Tokenizing train dataset:  27%|██▋       | 2349/8564 [00:07<00:17, 359.82 examples/s]Tokenizing train dataset:  28%|██▊       | 2359/8564 [00:07<00:17, 353.96 examples/s]Tokenizing train dataset:  28%|██▊       | 2404/8564 [00:07<00:16, 375.02 examples/s]Tokenizing train dataset:  28%|██▊       | 2389/8564 [00:07<00:16, 366.75 examples/s]Tokenizing train dataset:  28%|██▊       | 2403/8564 [00:07<00:16, 374.10 examples/s]Tokenizing train dataset:  28%|██▊       | 2431/8564 [00:07<00:16, 373.36 examples/s]Tokenizing train dataset:  29%|██▉       | 2463/8564 [00:07<00:16, 378.03 examples/s]Tokenizing train dataset:  29%|██▊       | 2441/8564 [00:07<00:16, 370.78 examples/s]Tokenizing train dataset:  29%|██▉       | 2470/8564 [00:07<00:16, 375.12 examples/s]Tokenizing train dataset:  29%|██▉       | 2480/8564 [00:07<00:16, 370.11 examples/s]Tokenizing train dataset:  29%|██▉       | 2514/8564 [00:07<00:16, 359.30 examples/s]Tokenizing train dataset:  29%|██▉       | 2523/8564 [00:07<00:16, 362.89 examples/s]Tokenizing train dataset:  30%|██▉       | 2559/8564 [00:07<00:15, 379.49 examples/s]Tokenizing train dataset:  30%|██▉       | 2538/8564 [00:07<00:16, 370.73 examples/s]Tokenizing train dataset:  30%|██▉       | 2565/8564 [00:07<00:15, 375.24 examples/s]Tokenizing train dataset:  30%|███       | 2578/8564 [00:07<00:15, 377.17 examples/s]Tokenizing train dataset:  31%|███       | 2615/8564 [00:08<00:15, 371.87 examples/s]Tokenizing train dataset:  30%|███       | 2604/8564 [00:07<00:15, 375.93 examples/s]Tokenizing train dataset:  31%|███       | 2625/8564 [00:08<00:17, 349.29 examples/s]Tokenizing train dataset:  31%|███       | 2662/8564 [00:08<00:16, 349.30 examples/s]Tokenizing train dataset:  31%|███       | 2654/8564 [00:08<00:17, 342.45 examples/s]Tokenizing train dataset:  31%|███       | 2661/8564 [00:08<00:16, 348.00 examples/s]Tokenizing train dataset:  32%|███▏      | 2717/8564 [00:08<00:16, 352.47 examples/s]Tokenizing train dataset:  32%|███▏      | 2708/8564 [00:08<00:16, 346.40 examples/s]Tokenizing train dataset:  32%|███▏      | 2716/8564 [00:08<00:16, 349.51 examples/s]Tokenizing train dataset:  32%|███▏      | 2769/8564 [00:08<00:16, 346.50 examples/s]Tokenizing train dataset:  32%|███▏      | 2759/8564 [00:08<00:16, 342.85 examples/s]Tokenizing train dataset:  32%|███▏      | 2768/8564 [00:08<00:16, 343.63 examples/s]Tokenizing train dataset:  33%|███▎      | 2809/8564 [00:08<00:16, 356.74 examples/s]Tokenizing train dataset:  33%|███▎      | 2798/8564 [00:08<00:16, 351.33 examples/s]Tokenizing train dataset:  33%|███▎      | 2808/8564 [00:08<00:16, 354.95 examples/s]Tokenizing train dataset:  33%|███▎      | 2848/8564 [00:08<00:15, 359.11 examples/s]Tokenizing train dataset:  33%|███▎      | 2837/8564 [00:08<00:16, 357.63 examples/s]Tokenizing train dataset:  33%|███▎      | 2844/8564 [00:08<00:16, 354.32 examples/s]Tokenizing train dataset:  34%|███▍      | 2906/8564 [00:08<00:15, 363.43 examples/s]Tokenizing train dataset:  34%|███▍      | 2893/8564 [00:08<00:15, 357.30 examples/s]Tokenizing train dataset:  34%|███▍      | 2900/8564 [00:08<00:15, 355.14 examples/s]Tokenizing train dataset:  34%|███▍      | 2949/8564 [00:08<00:14, 376.17 examples/s]Tokenizing train dataset:  34%|███▍      | 2934/8564 [00:08<00:15, 369.65 examples/s]Tokenizing train dataset:  34%|███▍      | 2944/8564 [00:08<00:14, 374.89 examples/s]Tokenizing train dataset:  35%|███▍      | 2991/8564 [00:09<00:14, 386.80 examples/s]Tokenizing train dataset:  35%|███▍      | 2980/8564 [00:09<00:14, 390.16 examples/s]Tokenizing train dataset:  35%|███▍      | 2989/8564 [00:09<00:14, 391.07 examples/s]Tokenizing train dataset:  36%|███▌      | 3046/8564 [00:09<00:14, 374.99 examples/s]Tokenizing train dataset:  35%|███▌      | 3036/8564 [00:09<00:14, 377.29 examples/s]Tokenizing train dataset:  36%|███▌      | 3043/8564 [00:09<00:14, 376.99 examples/s]Tokenizing train dataset:  36%|███▌      | 3102/8564 [00:09<00:14, 370.69 examples/s]Tokenizing train dataset:  36%|███▌      | 3092/8564 [00:09<00:14, 373.18 examples/s]Tokenizing train dataset:  36%|███▌      | 3098/8564 [00:09<00:14, 371.56 examples/s]Tokenizing train dataset:  37%|███▋      | 3140/8564 [00:09<00:14, 367.08 examples/s]Tokenizing train dataset:  37%|███▋      | 3148/8564 [00:09<00:14, 369.80 examples/s]Tokenizing train dataset:  37%|███▋      | 3153/8564 [00:09<00:14, 367.94 examples/s]Tokenizing train dataset:  37%|███▋      | 3195/8564 [00:09<00:14, 365.20 examples/s]Tokenizing train dataset:  37%|███▋      | 3204/8564 [00:09<00:14, 367.31 examples/s]Tokenizing train dataset:  38%|███▊      | 3234/8564 [00:09<00:14, 369.66 examples/s]Tokenizing train dataset:  37%|███▋      | 3209/8564 [00:09<00:14, 365.20 examples/s]Tokenizing train dataset:  38%|███▊      | 3245/8564 [00:09<00:14, 374.30 examples/s]Tokenizing train dataset:  38%|███▊      | 3274/8564 [00:09<00:14, 374.64 examples/s]Tokenizing train dataset:  38%|███▊      | 3251/8564 [00:09<00:14, 374.07 examples/s]Tokenizing train dataset:  39%|███▊      | 3299/8564 [00:09<00:14, 365.42 examples/s]Tokenizing train dataset:  39%|███▉      | 3326/8564 [00:09<00:14, 358.62 examples/s]Tokenizing train dataset:  39%|███▊      | 3304/8564 [00:09<00:14, 362.24 examples/s]Tokenizing train dataset:  39%|███▉      | 3364/8564 [00:10<00:14, 360.52 examples/s]Tokenizing train dataset:  39%|███▉      | 3352/8564 [00:10<00:14, 358.92 examples/s]Tokenizing train dataset:  39%|███▉      | 3360/8564 [00:10<00:14, 359.88 examples/s]Tokenizing train dataset:  40%|███▉      | 3408/8564 [00:10<00:13, 378.57 examples/s]Tokenizing train dataset:  40%|███▉      | 3394/8564 [00:10<00:13, 370.07 examples/s]Tokenizing train dataset:  40%|███▉      | 3402/8564 [00:10<00:13, 372.63 examples/s]Tokenizing train dataset:  40%|████      | 3447/8564 [00:10<00:13, 377.92 examples/s]Tokenizing train dataset:  40%|████      | 3435/8564 [00:10<00:13, 378.42 examples/s]Tokenizing train dataset:  40%|████      | 3440/8564 [00:10<00:13, 370.81 examples/s]Tokenizing train dataset:  41%|████      | 3504/8564 [00:10<00:13, 376.53 examples/s]Tokenizing train dataset:  41%|████      | 3479/8564 [00:10<00:13, 372.50 examples/s]Tokenizing train dataset:  41%|████      | 3489/8564 [00:10<00:13, 368.80 examples/s]Tokenizing train dataset:  41%|████▏     | 3542/8564 [00:10<00:13, 375.90 examples/s]Tokenizing train dataset:  41%|████      | 3520/8564 [00:10<00:13, 376.99 examples/s]Tokenizing train dataset:  41%|████▏     | 3536/8564 [00:10<00:14, 347.68 examples/s]Tokenizing train dataset:  42%|████▏     | 3599/8564 [00:10<00:13, 374.84 examples/s]Tokenizing train dataset:  42%|████▏     | 3573/8564 [00:10<00:13, 366.32 examples/s]Tokenizing train dataset:  42%|████▏     | 3592/8564 [00:10<00:14, 354.38 examples/s]Tokenizing train dataset:  42%|████▏     | 3637/8564 [00:10<00:13, 371.77 examples/s]Tokenizing train dataset:  42%|████▏     | 3614/8564 [00:10<00:13, 373.86 examples/s]Tokenizing train dataset:  42%|████▏     | 3631/8564 [00:10<00:13, 360.48 examples/s]Tokenizing train dataset:  43%|████▎     | 3689/8564 [00:10<00:13, 359.40 examples/s]Tokenizing train dataset:  43%|████▎     | 3666/8564 [00:10<00:13, 362.52 examples/s]Tokenizing train dataset:  43%|████▎     | 3680/8564 [00:10<00:14, 344.70 examples/s]Tokenizing train dataset:  44%|████▎     | 3729/8564 [00:11<00:13, 366.08 examples/s]Tokenizing train dataset:  44%|████▎     | 3726/8564 [00:11<00:13, 370.08 examples/s]Tokenizing train dataset:  43%|████▎     | 3724/8564 [00:11<00:13, 364.82 examples/s]Tokenizing train dataset:  44%|████▍     | 3768/8564 [00:11<00:12, 370.12 examples/s]Tokenizing train dataset:  44%|████▍     | 3764/8564 [00:11<00:13, 368.53 examples/s]Tokenizing train dataset:  44%|████▍     | 3781/8564 [00:11<00:13, 367.34 examples/s]Tokenizing train dataset:  45%|████▍     | 3823/8564 [00:11<00:12, 365.56 examples/s]Tokenizing train dataset:  44%|████▍     | 3802/8564 [00:11<00:12, 369.19 examples/s]Tokenizing train dataset:  45%|████▍     | 3835/8564 [00:11<00:13, 362.02 examples/s]Tokenizing train dataset:  45%|████▌     | 3879/8564 [00:11<00:12, 364.72 examples/s]Tokenizing train dataset:  45%|████▌     | 3856/8564 [00:11<00:13, 358.86 examples/s]Tokenizing train dataset:  45%|████▌     | 3894/8564 [00:11<00:12, 360.44 examples/s]Tokenizing train dataset:  45%|████▌     | 3890/8564 [00:11<00:12, 361.10 examples/s]Tokenizing train dataset:  46%|████▌     | 3930/8564 [00:11<00:13, 352.92 examples/s]Tokenizing train dataset:  46%|████▋     | 3967/8564 [00:11<00:12, 356.17 examples/s]Tokenizing train dataset:  46%|████▌     | 3946/8564 [00:11<00:13, 351.76 examples/s]Tokenizing train dataset:  46%|████▌     | 3941/8564 [00:11<00:13, 351.63 examples/s]Tokenizing train dataset:  47%|████▋     | 4004/8564 [00:11<00:12, 354.94 examples/s]Tokenizing train dataset:  46%|████▋     | 3982/8564 [00:11<00:13, 350.04 examples/s]Tokenizing train dataset:  46%|████▋     | 3978/8564 [00:11<00:12, 353.81 examples/s]Tokenizing train dataset:  47%|████▋     | 4045/8564 [00:11<00:12, 366.54 examples/s]Tokenizing train dataset:  47%|████▋     | 4024/8564 [00:11<00:12, 363.02 examples/s]Tokenizing train dataset:  47%|████▋     | 4015/8564 [00:11<00:12, 356.67 examples/s]Tokenizing train dataset:  48%|████▊     | 4082/8564 [00:12<00:12, 364.80 examples/s]Tokenizing train dataset:  47%|████▋     | 4063/8564 [00:12<00:12, 366.43 examples/s]Tokenizing train dataset:  47%|████▋     | 4056/8564 [00:11<00:12, 365.60 examples/s]Tokenizing train dataset:  48%|████▊     | 4119/8564 [00:12<00:12, 363.98 examples/s]Tokenizing train dataset:  48%|████▊     | 4117/8564 [00:12<00:12, 359.94 examples/s]Tokenizing train dataset:  48%|████▊     | 4110/8564 [00:12<00:12, 360.62 examples/s]Tokenizing train dataset:  49%|████▊     | 4170/8564 [00:12<00:12, 350.72 examples/s]Tokenizing train dataset:  49%|████▊     | 4170/8564 [00:12<00:12, 351.08 examples/s]Tokenizing train dataset:  49%|████▊     | 4162/8564 [00:12<00:12, 354.05 examples/s]Tokenizing train dataset:  49%|████▉     | 4211/8564 [00:12<00:12, 361.28 examples/s]Tokenizing train dataset:  49%|████▉     | 4211/8564 [00:12<00:12, 360.27 examples/s]Tokenizing train dataset:  49%|████▉     | 4203/8564 [00:12<00:11, 364.56 examples/s]Tokenizing train dataset:  50%|████▉     | 4266/8564 [00:12<00:11, 358.88 examples/s]Tokenizing train dataset:  50%|████▉     | 4266/8564 [00:12<00:12, 357.69 examples/s]Tokenizing train dataset:  50%|████▉     | 4257/8564 [00:12<00:11, 359.63 examples/s]Tokenizing train dataset:  50%|█████     | 4304/8564 [00:12<00:11, 361.56 examples/s]Tokenizing train dataset:  50%|█████     | 4304/8564 [00:12<00:11, 360.06 examples/s]Tokenizing train dataset:  50%|█████     | 4295/8564 [00:12<00:11, 358.01 examples/s]Tokenizing train dataset:  51%|█████     | 4359/8564 [00:12<00:11, 358.54 examples/s]Tokenizing train dataset:  51%|█████     | 4332/8564 [00:12<00:11, 358.22 examples/s]Tokenizing train dataset:  51%|█████     | 4357/8564 [00:12<00:11, 354.94 examples/s]Tokenizing train dataset:  51%|█████▏    | 4398/8564 [00:12<00:11, 364.52 examples/s]Tokenizing train dataset:  51%|█████     | 4370/8564 [00:12<00:11, 358.10 examples/s]Tokenizing train dataset:  51%|█████▏    | 4398/8564 [00:12<00:11, 363.92 examples/s]Tokenizing train dataset:  52%|█████▏    | 4435/8564 [00:13<00:11, 364.35 examples/s]Tokenizing train dataset:  51%|█████▏    | 4409/8564 [00:12<00:11, 363.43 examples/s]Tokenizing train dataset:  52%|█████▏    | 4435/8564 [00:13<00:11, 363.35 examples/s]Tokenizing train dataset:  52%|█████▏    | 4472/8564 [00:13<00:11, 362.66 examples/s]Tokenizing train dataset:  52%|█████▏    | 4448/8564 [00:13<00:11, 367.43 examples/s]Tokenizing train dataset:  52%|█████▏    | 4472/8564 [00:13<00:11, 362.01 examples/s]Tokenizing train dataset:  52%|█████▏    | 4486/8564 [00:13<00:11, 366.41 examples/s]Tokenizing train dataset:  53%|█████▎    | 4525/8564 [00:13<00:11, 354.16 examples/s]Tokenizing train dataset:  53%|█████▎    | 4525/8564 [00:13<00:11, 353.33 examples/s]Tokenizing train dataset:  53%|█████▎    | 4566/8564 [00:13<00:10, 366.54 examples/s]Tokenizing train dataset:  53%|█████▎    | 4541/8564 [00:13<00:11, 359.78 examples/s]Tokenizing train dataset:  53%|█████▎    | 4565/8564 [00:13<00:11, 363.37 examples/s]Tokenizing train dataset:  54%|█████▍    | 4620/8564 [00:13<00:11, 355.74 examples/s]Tokenizing train dataset:  54%|█████▎    | 4593/8564 [00:13<00:11, 353.83 examples/s]Tokenizing train dataset:  54%|█████▍    | 4620/8564 [00:13<00:11, 354.60 examples/s]Tokenizing train dataset:  54%|█████▍    | 4630/8564 [00:13<00:11, 357.13 examples/s]Tokenizing train dataset:  55%|█████▍    | 4670/8564 [00:13<00:11, 341.32 examples/s]Tokenizing train dataset:  54%|█████▍    | 4666/8564 [00:13<00:11, 337.17 examples/s]Tokenizing train dataset:  55%|█████▍    | 4677/8564 [00:13<00:11, 338.91 examples/s]Tokenizing train dataset:  55%|█████▌    | 4718/8564 [00:13<00:11, 331.00 examples/s]Tokenizing train dataset:  55%|█████▌    | 4716/8564 [00:13<00:11, 330.69 examples/s]Tokenizing train dataset:  55%|█████▌    | 4726/8564 [00:13<00:11, 329.60 examples/s]Tokenizing train dataset:  56%|█████▌    | 4767/8564 [00:14<00:11, 326.32 examples/s]Tokenizing train dataset:  55%|█████▌    | 4750/8564 [00:13<00:11, 327.79 examples/s]Tokenizing train dataset:  56%|█████▌    | 4804/8564 [00:14<00:11, 333.67 examples/s]Tokenizing train dataset:  56%|█████▌    | 4773/8564 [00:14<00:11, 321.38 examples/s]Tokenizing train dataset:  56%|█████▌    | 4800/8564 [00:14<00:11, 326.64 examples/s]Tokenizing train dataset:  57%|█████▋    | 4868/8564 [00:14<00:09, 407.44 examples/s]Tokenizing train dataset:  56%|█████▋    | 4821/8564 [00:14<00:10, 354.29 examples/s]Tokenizing train dataset:  57%|█████▋    | 4865/8564 [00:14<00:09, 399.26 examples/s]Tokenizing train dataset:  58%|█████▊    | 4932/8564 [00:14<00:07, 460.13 examples/s]Tokenizing train dataset:  57%|█████▋    | 4883/8564 [00:14<00:08, 415.47 examples/s]Tokenizing train dataset:  58%|█████▊    | 4930/8564 [00:14<00:07, 459.83 examples/s]Tokenizing train dataset:  58%|█████▊    | 4993/8564 [00:14<00:07, 492.42 examples/s]Tokenizing train dataset:  58%|█████▊    | 4946/8564 [00:14<00:07, 470.39 examples/s]Tokenizing train dataset:  58%|█████▊    | 4989/8564 [00:14<00:07, 489.37 examples/s]Tokenizing train dataset:  59%|█████▉    | 5060/8564 [00:14<00:06, 533.47 examples/s]Tokenizing train dataset:  58%|█████▊    | 5007/8564 [00:14<00:07, 506.11 examples/s]Tokenizing train dataset:  59%|█████▉    | 5049/8564 [00:14<00:06, 518.30 examples/s]Tokenizing train dataset:  60%|█████▉    | 5127/8564 [00:14<00:06, 570.51 examples/s]Tokenizing train dataset:  59%|█████▉    | 5072/8564 [00:14<00:06, 542.37 examples/s]Tokenizing train dataset:  60%|█████▉    | 5118/8564 [00:14<00:06, 560.34 examples/s]Tokenizing train dataset:  61%|██████    | 5200/8564 [00:14<00:05, 606.16 examples/s]Tokenizing train dataset:  60%|██████    | 5140/8564 [00:14<00:05, 574.34 examples/s]Tokenizing train dataset:  61%|██████    | 5189/8564 [00:14<00:05, 601.13 examples/s]Tokenizing train dataset:  62%|██████▏   | 5279/8564 [00:14<00:05, 653.67 examples/s]Tokenizing train dataset:  61%|██████    | 5214/8564 [00:14<00:05, 618.67 examples/s]Tokenizing train dataset:  61%|██████▏   | 5262/8564 [00:14<00:05, 636.14 examples/s]Tokenizing train dataset:  62%|██████▏   | 5289/8564 [00:14<00:05, 649.94 examples/s]Tokenizing train dataset:  63%|██████▎   | 5371/8564 [00:14<00:05, 632.09 examples/s]Tokenizing train dataset:  63%|██████▎   | 5359/8564 [00:15<00:05, 633.79 examples/s]Tokenizing train dataset:  63%|██████▎   | 5436/8564 [00:15<00:04, 633.77 examples/s]Tokenizing train dataset:  63%|██████▎   | 5380/8564 [00:15<00:05, 624.47 examples/s]Tokenizing train dataset:  64%|██████▎   | 5453/8564 [00:15<00:04, 624.59 examples/s]Tokenizing train dataset:  64%|██████▎   | 5448/8564 [00:15<00:04, 635.73 examples/s]Tokenizing train dataset:  65%|██████▍   | 5526/8564 [00:15<00:04, 613.13 examples/s]Tokenizing train dataset:  65%|██████▍   | 5540/8564 [00:15<00:04, 606.73 examples/s]Tokenizing train dataset:  65%|██████▍   | 5535/8564 [00:15<00:04, 613.52 examples/s]Tokenizing train dataset:  66%|██████▌   | 5622/8564 [00:15<00:04, 620.03 examples/s]Tokenizing train dataset:  65%|██████▌   | 5605/8564 [00:15<00:04, 612.74 examples/s]Tokenizing train dataset:  66%|██████▋   | 5688/8564 [00:15<00:04, 628.30 examples/s]Tokenizing train dataset:  66%|██████▌   | 5631/8564 [00:15<00:04, 615.03 examples/s]Tokenizing train dataset:  66%|██████▌   | 5673/8564 [00:15<00:04, 627.19 examples/s]Tokenizing train dataset:  67%|██████▋   | 5754/8564 [00:15<00:04, 634.44 examples/s]Tokenizing train dataset:  67%|██████▋   | 5698/8564 [00:15<00:04, 627.23 examples/s]Tokenizing train dataset:  68%|██████▊   | 5830/8564 [00:15<00:04, 661.65 examples/s]Tokenizing train dataset:  67%|██████▋   | 5778/8564 [00:15<00:04, 646.95 examples/s]Tokenizing train dataset:  68%|██████▊   | 5801/8564 [00:15<00:04, 643.04 examples/s]Tokenizing train dataset:  68%|██████▊   | 5846/8564 [00:15<00:04, 649.07 examples/s]Tokenizing train dataset:  69%|██████▉   | 5918/8564 [00:15<00:04, 626.67 examples/s]Tokenizing train dataset:  69%|██████▉   | 5889/8564 [00:15<00:04, 618.89 examples/s]Tokenizing train dataset:  69%|██████▉   | 5935/8564 [00:15<00:04, 617.26 examples/s]Tokenizing train dataset:  70%|███████   | 6016/8564 [00:16<00:04, 607.92 examples/s]Tokenizing train dataset:  70%|██████▉   | 5980/8564 [00:16<00:04, 587.58 examples/s]Tokenizing train dataset:  71%|███████   | 6078/8564 [00:16<00:04, 605.02 examples/s]Tokenizing train dataset:  70%|███████   | 6021/8564 [00:16<00:04, 593.74 examples/s]Tokenizing train dataset:  71%|███████   | 6043/8564 [00:16<00:04, 596.51 examples/s]Tokenizing train dataset:  72%|███████▏  | 6140/8564 [00:16<00:03, 606.86 examples/s]Tokenizing train dataset:  71%|███████▏  | 6111/8564 [00:16<00:04, 592.42 examples/s]Tokenizing train dataset:  73%|███████▎  | 6219/8564 [00:16<00:03, 644.61 examples/s]Tokenizing train dataset:  72%|███████▏  | 6133/8564 [00:16<00:04, 596.97 examples/s]Tokenizing train dataset:  72%|███████▏  | 6185/8564 [00:16<00:03, 620.09 examples/s]Tokenizing train dataset:  73%|███████▎  | 6286/8564 [00:16<00:03, 650.24 examples/s]Tokenizing train dataset:  72%|███████▏  | 6208/8564 [00:16<00:03, 631.93 examples/s]Tokenizing train dataset:  73%|███████▎  | 6259/8564 [00:16<00:03, 648.73 examples/s]Tokenizing train dataset:  74%|███████▍  | 6355/8564 [00:16<00:03, 656.24 examples/s]Tokenizing train dataset:  73%|███████▎  | 6277/8564 [00:16<00:03, 644.38 examples/s]Tokenizing train dataset:  74%|███████▍  | 6357/8564 [00:16<00:03, 644.27 examples/s]Tokenizing train dataset:  75%|███████▌  | 6452/8564 [00:16<00:03, 643.05 examples/s]Tokenizing train dataset:  74%|███████▍  | 6376/8564 [00:16<00:03, 642.06 examples/s]Tokenizing train dataset:  75%|███████▌  | 6453/8564 [00:16<00:03, 639.13 examples/s]Tokenizing train dataset:  76%|███████▋  | 6541/8564 [00:16<00:03, 625.13 examples/s]Tokenizing train dataset:  76%|███████▌  | 6468/8564 [00:16<00:03, 631.23 examples/s]Tokenizing train dataset:  76%|███████▋  | 6544/8564 [00:16<00:03, 621.87 examples/s]Tokenizing train dataset:  77%|███████▋  | 6619/8564 [00:17<00:03, 585.07 examples/s]Tokenizing train dataset:  77%|███████▋  | 6560/8564 [00:16<00:03, 613.94 examples/s]Tokenizing train dataset:  78%|███████▊  | 6687/8564 [00:17<00:03, 603.75 examples/s]Tokenizing train dataset:  77%|███████▋  | 6622/8564 [00:17<00:03, 585.69 examples/s]Tokenizing train dataset:  78%|███████▊  | 6640/8564 [00:17<00:03, 582.96 examples/s]Tokenizing train dataset:  79%|███████▉  | 6756/8564 [00:17<00:02, 623.31 examples/s]Tokenizing train dataset:  78%|███████▊  | 6689/8564 [00:17<00:03, 603.66 examples/s]Tokenizing train dataset:  78%|███████▊  | 6713/8564 [00:17<00:03, 612.49 examples/s]Tokenizing train dataset:  79%|███████▉  | 6759/8564 [00:17<00:02, 621.03 examples/s]Tokenizing train dataset:  80%|███████▉  | 6846/8564 [00:17<00:02, 605.08 examples/s]Tokenizing train dataset:  79%|███████▉  | 6776/8564 [00:17<00:02, 614.88 examples/s]Tokenizing train dataset:  81%|████████  | 6914/8564 [00:17<00:02, 622.19 examples/s]Tokenizing train dataset:  80%|███████▉  | 6846/8564 [00:17<00:02, 601.48 examples/s]Tokenizing train dataset:  80%|████████  | 6866/8564 [00:17<00:02, 601.80 examples/s]Tokenizing train dataset:  82%|████████▏ | 6980/8564 [00:17<00:02, 624.61 examples/s]Tokenizing train dataset:  81%|████████  | 6913/8564 [00:17<00:02, 616.68 examples/s]Tokenizing train dataset:  81%|████████  | 6940/8564 [00:17<00:02, 630.91 examples/s]Tokenizing train dataset:  82%|████████▏ | 6980/8564 [00:17<00:02, 620.30 examples/s]Tokenizing train dataset:  83%|████████▎ | 7067/8564 [00:17<00:02, 604.68 examples/s]Tokenizing train dataset:  82%|████████▏ | 7020/8564 [00:17<00:02, 584.97 examples/s]Tokenizing train dataset:  83%|████████▎ | 7135/8564 [00:17<00:02, 620.67 examples/s]Tokenizing train dataset:  83%|████████▎ | 7066/8564 [00:17<00:02, 599.48 examples/s]Tokenizing train dataset:  83%|████████▎ | 7091/8564 [00:17<00:02, 612.78 examples/s]Tokenizing train dataset:  84%|████████▍ | 7205/8564 [00:17<00:02, 640.08 examples/s]Tokenizing train dataset:  83%|████████▎ | 7135/8564 [00:17<00:02, 616.64 examples/s]Tokenizing train dataset:  84%|████████▍ | 7205/8564 [00:17<00:02, 636.01 examples/s]Tokenizing train dataset:  84%|████████▍ | 7192/8564 [00:17<00:02, 628.83 examples/s]Tokenizing train dataset:  85%|████████▌ | 7292/8564 [00:18<00:02, 613.17 examples/s]Tokenizing train dataset:  86%|████████▌ | 7360/8564 [00:18<00:01, 624.86 examples/s]Tokenizing train dataset:  85%|████████▌ | 7292/8564 [00:18<00:02, 609.42 examples/s]Tokenizing train dataset:  85%|████████▌ | 7282/8564 [00:18<00:02, 614.95 examples/s]Tokenizing train dataset:  86%|████████▌ | 7360/8564 [00:18<00:01, 621.27 examples/s]Tokenizing train dataset:  87%|████████▋ | 7460/8564 [00:18<00:01, 634.58 examples/s]Tokenizing train dataset:  86%|████████▌ | 7379/8564 [00:18<00:01, 619.99 examples/s]Tokenizing train dataset:  88%|████████▊ | 7524/8564 [00:18<00:01, 632.36 examples/s]Tokenizing train dataset:  87%|████████▋ | 7457/8564 [00:18<00:01, 628.36 examples/s]Tokenizing train dataset:  87%|████████▋ | 7443/8564 [00:18<00:01, 621.76 examples/s]Tokenizing train dataset:  89%|████████▊ | 7596/8564 [00:18<00:01, 650.88 examples/s]Tokenizing train dataset:  88%|████████▊ | 7506/8564 [00:18<00:01, 618.34 examples/s]Tokenizing train dataset:  88%|████████▊ | 7555/8564 [00:18<00:01, 632.65 examples/s]Tokenizing train dataset:  89%|████████▊ | 7581/8564 [00:18<00:01, 648.85 examples/s]Tokenizing train dataset:  90%|████████▉ | 7683/8564 [00:18<00:01, 619.32 examples/s]Tokenizing train dataset:  89%|████████▉ | 7627/8564 [00:18<00:01, 649.23 examples/s]Tokenizing train dataset:  90%|████████▉ | 7668/8564 [00:18<00:01, 615.92 examples/s]Tokenizing train dataset:  91%|█████████ | 7770/8564 [00:18<00:01, 599.40 examples/s]Tokenizing train dataset:  90%|█████████ | 7710/8564 [00:18<00:01, 615.20 examples/s]Tokenizing train dataset:  90%|█████████ | 7731/8564 [00:18<00:01, 612.21 examples/s]Tokenizing train dataset:  92%|█████████▏| 7861/8564 [00:19<00:01, 598.18 examples/s]Tokenizing train dataset:  91%|█████████ | 7794/8564 [00:18<00:01, 590.41 examples/s]Tokenizing train dataset:  91%|█████████ | 7812/8564 [00:19<00:01, 585.71 examples/s]Tokenizing train dataset:  93%|█████████▎| 7926/8564 [00:19<00:01, 605.81 examples/s]Tokenizing train dataset:  92%|█████████▏| 7859/8564 [00:19<00:01, 596.14 examples/s]Tokenizing train dataset:  92%|█████████▏| 7880/8564 [00:19<00:01, 597.33 examples/s]Tokenizing train dataset:  93%|█████████▎| 7990/8564 [00:19<00:00, 613.66 examples/s]Tokenizing train dataset:  93%|█████████▎| 7922/8564 [00:19<00:01, 602.73 examples/s]Tokenizing train dataset:  93%|█████████▎| 7943/8564 [00:19<00:01, 601.79 examples/s]Tokenizing train dataset:  94%|█████████▍| 8053/8564 [00:19<00:00, 615.60 examples/s]Tokenizing train dataset:  93%|█████████▎| 7986/8564 [00:19<00:00, 607.43 examples/s]Tokenizing train dataset:  94%|█████████▎| 8013/8564 [00:19<00:00, 621.42 examples/s]Tokenizing train dataset:  94%|█████████▍| 8049/8564 [00:19<00:00, 610.36 examples/s]Tokenizing train dataset:  95%|█████████▌| 8140/8564 [00:19<00:00, 594.33 examples/s]Tokenizing train dataset:  95%|█████████▍| 8101/8564 [00:19<00:00, 599.51 examples/s]Tokenizing train dataset:  96%|█████████▌| 8205/8564 [00:19<00:00, 601.72 examples/s]Tokenizing train dataset:  95%|█████████▍| 8132/8564 [00:19<00:00, 585.66 examples/s]Tokenizing train dataset:  97%|█████████▋| 8280/8564 [00:19<00:00, 635.76 examples/s]Tokenizing train dataset:  96%|█████████▌| 8200/8564 [00:19<00:00, 605.36 examples/s]Tokenizing train dataset:  96%|█████████▌| 8198/8564 [00:19<00:00, 610.42 examples/s]Tokenizing train dataset:  97%|█████████▋| 8345/8564 [00:19<00:00, 628.38 examples/s]Tokenizing train dataset:  97%|█████████▋| 8269/8564 [00:19<00:00, 625.60 examples/s]Tokenizing train dataset:  96%|█████████▋| 8262/8564 [00:19<00:00, 614.68 examples/s]Tokenizing train dataset:  97%|█████████▋| 8339/8564 [00:19<00:00, 637.78 examples/s]Tokenizing train dataset:  97%|█████████▋| 8331/8564 [00:19<00:00, 631.50 examples/s]Tokenizing train dataset:  98%|█████████▊| 8433/8564 [00:19<00:00, 608.12 examples/s]Tokenizing train dataset:  99%|█████████▉| 8498/8564 [00:20<00:00, 617.74 examples/s]Tokenizing train dataset:  98%|█████████▊| 8427/8564 [00:19<00:00, 616.22 examples/s]Tokenizing train dataset:  98%|█████████▊| 8418/8564 [00:19<00:00, 609.95 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 616.99 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 425.00 examples/s]
Tokenizing train dataset:  99%|█████████▉| 8480/8564 [00:20<00:00, 605.71 examples/s]Tokenizing train dataset:  99%|█████████▉| 8519/8564 [00:20<00:00, 612.40 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 423.34 examples/s]
Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 600.61 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 423.06 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11504.31 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11463.82 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11383.90 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13918.16 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13800.10 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13357.70 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 325.59 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 326.98 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 325.60 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 297.71 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 297.94 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 296.34 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:02, 279.70 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:02, 279.83 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:02, 278.34 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 269.45 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 268.14 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 268.28 examples/s]Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:02, 262.18 examples/s]Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:02, 261.45 examples/s]Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:02, 260.70 examples/s]Tokenizing eval dataset:  25%|██▌       | 240/953 [00:00<00:02, 294.58 examples/s]Tokenizing eval dataset:  25%|██▌       | 240/953 [00:00<00:02, 293.91 examples/s]Tokenizing eval dataset:  25%|██▍       | 238/953 [00:00<00:02, 292.14 examples/s]Tokenizing eval dataset:  32%|███▏      | 309/953 [00:00<00:01, 396.97 examples/s]Tokenizing eval dataset:  32%|███▏      | 309/953 [00:00<00:01, 396.19 examples/s]Tokenizing eval dataset:  32%|███▏      | 304/953 [00:00<00:01, 387.33 examples/s]Tokenizing eval dataset:  39%|███▉      | 371/953 [00:01<00:01, 456.65 examples/s]Tokenizing eval dataset:  39%|███▉      | 371/953 [00:01<00:01, 455.88 examples/s]Tokenizing eval dataset:  39%|███▊      | 368/953 [00:01<00:01, 454.15 examples/s]Tokenizing eval dataset:  46%|████▋     | 442/953 [00:01<00:00, 525.77 examples/s]Tokenizing eval dataset:  46%|████▋     | 442/953 [00:01<00:00, 524.81 examples/s]Tokenizing eval dataset:  46%|████▌     | 437/953 [00:01<00:00, 518.62 examples/s]Tokenizing eval dataset:  53%|█████▎    | 509/953 [00:01<00:00, 565.67 examples/s]Tokenizing eval dataset:  53%|█████▎    | 509/953 [00:01<00:00, 564.36 examples/s]Tokenizing eval dataset:  53%|█████▎    | 504/953 [00:01<00:00, 560.80 examples/s]Tokenizing eval dataset:  60%|██████    | 575/953 [00:01<00:00, 590.17 examples/s]Tokenizing eval dataset:  60%|██████    | 575/953 [00:01<00:00, 588.89 examples/s]Tokenizing eval dataset:  60%|█████▉    | 569/953 [00:01<00:00, 583.53 examples/s]Tokenizing eval dataset:  68%|██████▊   | 644/953 [00:01<00:00, 611.91 examples/s]Tokenizing eval dataset:  68%|██████▊   | 644/953 [00:01<00:00, 610.58 examples/s]Tokenizing eval dataset:  67%|██████▋   | 640/953 [00:01<00:00, 609.12 examples/s]Tokenizing eval dataset:  77%|███████▋  | 734/953 [00:01<00:00, 599.90 examples/s]Tokenizing eval dataset:  77%|███████▋  | 734/953 [00:01<00:00, 598.56 examples/s]Tokenizing eval dataset:  77%|███████▋  | 730/953 [00:01<00:00, 599.18 examples/s]Tokenizing eval dataset:  85%|████████▌ | 811/953 [00:01<00:00, 565.70 examples/s]Tokenizing eval dataset:  85%|████████▌ | 811/953 [00:01<00:00, 564.13 examples/s]Tokenizing eval dataset:  85%|████████▍ | 807/953 [00:01<00:00, 563.54 examples/s]Tokenizing eval dataset:  93%|█████████▎| 890/953 [00:01<00:00, 547.85 examples/s]Tokenizing eval dataset:  93%|█████████▎| 890/953 [00:01<00:00, 546.47 examples/s]Tokenizing eval dataset:  93%|█████████▎| 883/953 [00:01<00:00, 542.17 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 537.31 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 468.09 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 536.22 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 467.14 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 531.13 examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 464.45 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
