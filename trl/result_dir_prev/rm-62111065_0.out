cpu-bind=MASK - gn01, task  0  0 [1229354]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 0 ---
Total Nodes: 4
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 4     --machine_rank 0     --main_process_ip gn01     --main_process_port 29500     --num_processes 16     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62111065     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train.py"     --rank=64 --learning_rate=3e-7 --total_epochs=3 --beta=0.2
-------------------------------------------
[2025-05-31 11:15:23,076] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0531 11:15:24.682000 1229409 torch/distributed/run.py:792] 
W0531 11:15:24.682000 1229409 torch/distributed/run.py:792] *****************************************
W0531 11:15:24.682000 1229409 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0531 11:15:24.682000 1229409 torch/distributed/run.py:792] *****************************************
[2025-05-31 11:15:30,652] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 11:15:30,697] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 11:15:30,713] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 11:15:30,723] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
World size: 16
Setting gradient accumulation steps to: 1
[2025-05-31 11:15:33,075] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-31 11:15:33,081] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Steps per epoch: 8564
Eval steps: 4282
[2025-05-31 11:15:33,083] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-31 11:15:33,085] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-31 11:15:33,085] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Set up DPO configuration
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:59, 19.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:59, 19.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:59, 19.97s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:19<00:59, 19.86s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:38, 19.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:38, 19.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:38, 19.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:38<00:38, 19.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:18, 18.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:06<00:00, 15.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:06<00:00, 15.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:06<00:00, 15.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:06<00:00, 15.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:06<00:00, 16.65s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:06<00:00, 16.65s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:06<00:00, 16.65s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:06<00:00, 16.68s/it]
/transformers/src/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/transformers/src/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
Using LoRA and set up the model
Loaded tokenizer
[rank3]:[W531 11:17:35.100283378 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W531 11:17:35.273841917 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   6%|▋         | 537/8564 [00:00<00:01, 5331.00 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1080/8564 [00:00<00:01, 5387.23 examples/s]Extracting prompt in train dataset:  19%|█▉        | 1630/8564 [00:00<00:01, 5421.34 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2192/8564 [00:00<00:01, 5482.61 examples/s]Extracting prompt in train dataset:  32%|███▏      | 2750/8564 [00:00<00:01, 5509.68 examples/s]Extracting prompt in train dataset:  41%|████▏     | 3550/8564 [00:00<00:00, 5416.10 examples/s]Extracting prompt in train dataset:  48%|████▊     | 4110/8564 [00:00<00:00, 5461.53 examples/s]Extracting prompt in train dataset:  55%|█████▍    | 4670/8564 [00:00<00:00, 5492.89 examples/s]Extracting prompt in train dataset:  61%|██████    | 5240/8564 [00:00<00:00, 5541.29 examples/s]Extracting prompt in train dataset:  68%|██████▊   | 5820/8564 [00:01<00:00, 5591.16 examples/s]Extracting prompt in train dataset:  75%|███████▍  | 6400/8564 [00:01<00:00, 5626.27 examples/s]Extracting prompt in train dataset:  82%|████████▏ | 6980/8564 [00:01<00:00, 5648.33 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 7560/8564 [00:01<00:00, 5663.52 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 8310/8564 [00:01<00:00, 5382.27 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5436.79 examples/s]
Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   3%|▎         | 280/8564 [00:00<00:02, 2762.77 examples/s]Applying chat template to train dataset:   7%|▋         | 586/8564 [00:00<00:02, 2928.99 examples/s]Applying chat template to train dataset:  10%|█         | 889/8564 [00:00<00:02, 2974.37 examples/s]Applying chat template to train dataset:  14%|█▍        | 1193/8564 [00:00<00:02, 2997.23 examples/s]Applying chat template to train dataset:  17%|█▋        | 1493/8564 [00:00<00:02, 2997.70 examples/s]Applying chat template to train dataset:  21%|██        | 1800/8564 [00:00<00:02, 3014.24 examples/s]Applying chat template to train dataset:  25%|██▍       | 2109/8564 [00:00<00:02, 3035.99 examples/s]Applying chat template to train dataset:  28%|██▊       | 2417/8564 [00:00<00:02, 3045.66 examples/s]Applying chat template to train dataset:  32%|███▏      | 2724/8564 [00:00<00:01, 3051.48 examples/s]Applying chat template to train dataset:  35%|███▌      | 3032/8564 [00:01<00:01, 3057.58 examples/s]Applying chat template to train dataset:  40%|████      | 3465/8564 [00:01<00:01, 2985.60 examples/s]Applying chat template to train dataset:  44%|████▍     | 3770/8564 [00:01<00:01, 3002.70 examples/s]Applying chat template to train dataset:  48%|████▊     | 4076/8564 [00:01<00:01, 3016.82 examples/s]Applying chat template to train dataset:  51%|█████     | 4381/8564 [00:01<00:01, 3023.55 examples/s]Applying chat template to train dataset:  55%|█████▍    | 4688/8564 [00:01<00:01, 3033.26 examples/s]Applying chat template to train dataset:  58%|█████▊    | 4997/8564 [00:01<00:01, 3049.24 examples/s]Applying chat template to train dataset:  62%|██████▏   | 5310/8564 [00:01<00:01, 3071.16 examples/s]Applying chat template to train dataset:  66%|██████▌   | 5623/8564 [00:01<00:00, 3086.95 examples/s]Applying chat template to train dataset:  69%|██████▉   | 5938/8564 [00:01<00:00, 3101.49 examples/s]Applying chat template to train dataset:  73%|███████▎  | 6251/8564 [00:02<00:00, 3106.87 examples/s]Applying chat template to train dataset:  77%|███████▋  | 6565/8564 [00:02<00:00, 3112.69 examples/s]Applying chat template to train dataset:  80%|████████  | 6879/8564 [00:02<00:00, 3116.44 examples/s]Applying chat template to train dataset:  84%|████████▍ | 7192/8564 [00:02<00:00, 3115.69 examples/s]Applying chat template to train dataset:  88%|████████▊ | 7506/8564 [00:02<00:00, 3119.86 examples/s]Applying chat template to train dataset:  92%|█████████▏| 7912/8564 [00:02<00:00, 2954.71 examples/s]Applying chat template to train dataset:  96%|█████████▌| 8226/8564 [00:02<00:00, 3002.92 examples/s]Applying chat template to train dataset: 100%|█████████▉| 8540/8564 [00:02<00:00, 3038.74 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3033.53 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 41/8564 [00:00<00:21, 394.50 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:25, 334.61 examples/s]Tokenizing train dataset:   2%|▏         | 138/8564 [00:00<00:26, 318.21 examples/s]Tokenizing train dataset:   2%|▏         | 182/8564 [00:00<00:27, 302.52 examples/s]Tokenizing train dataset:   3%|▎         | 217/8564 [00:00<00:26, 312.22 examples/s]Tokenizing train dataset:   3%|▎         | 251/8564 [00:00<00:26, 315.50 examples/s]Tokenizing train dataset:   3%|▎         | 289/8564 [00:00<00:24, 331.04 examples/s]Tokenizing train dataset:   4%|▍         | 337/8564 [00:01<00:25, 322.87 examples/s]Tokenizing train dataset:   5%|▍         | 387/8564 [00:01<00:25, 321.36 examples/s]Tokenizing train dataset:   5%|▌         | 434/8564 [00:01<00:25, 313.16 examples/s]Tokenizing train dataset:   6%|▌         | 480/8564 [00:01<00:26, 306.63 examples/s]Tokenizing train dataset:   6%|▌         | 527/8564 [00:01<00:26, 305.66 examples/s]Tokenizing train dataset:   7%|▋         | 561/8564 [00:01<00:25, 310.66 examples/s]Tokenizing train dataset:   7%|▋         | 605/8564 [00:01<00:26, 300.20 examples/s]Tokenizing train dataset:   8%|▊         | 644/8564 [00:02<00:24, 317.32 examples/s]Tokenizing train dataset:   8%|▊         | 689/8564 [00:02<00:25, 309.01 examples/s]Tokenizing train dataset:   9%|▊         | 740/8564 [00:02<00:24, 315.15 examples/s]Tokenizing train dataset:   9%|▉         | 785/8564 [00:02<00:25, 305.75 examples/s]Tokenizing train dataset:  10%|▉         | 827/8564 [00:02<00:26, 292.53 examples/s]Tokenizing train dataset:  10%|█         | 860/8564 [00:02<00:25, 297.62 examples/s]Tokenizing train dataset:  10%|█         | 896/8564 [00:02<00:24, 312.33 examples/s]Tokenizing train dataset:  11%|█         | 940/8564 [00:03<00:25, 300.73 examples/s]Tokenizing train dataset:  11%|█▏        | 972/8564 [00:03<00:24, 304.16 examples/s]Tokenizing train dataset:  12%|█▏        | 1014/8564 [00:03<00:26, 288.81 examples/s]Tokenizing train dataset:  12%|█▏        | 1060/8564 [00:03<00:25, 289.36 examples/s]Tokenizing train dataset:  13%|█▎        | 1095/8564 [00:03<00:24, 302.12 examples/s]Tokenizing train dataset:  13%|█▎        | 1137/8564 [00:03<00:25, 289.41 examples/s]Tokenizing train dataset:  14%|█▎        | 1167/8564 [00:03<00:25, 289.78 examples/s]Tokenizing train dataset:  14%|█▍        | 1202/8564 [00:03<00:24, 300.43 examples/s]Tokenizing train dataset:  14%|█▍        | 1233/8564 [00:04<00:24, 302.41 examples/s]Tokenizing train dataset:  15%|█▍        | 1270/8564 [00:04<00:23, 307.05 examples/s]Tokenizing train dataset:  15%|█▌        | 1304/8564 [00:04<00:23, 311.40 examples/s]Tokenizing train dataset:  16%|█▌        | 1338/8564 [00:04<00:22, 315.61 examples/s]Tokenizing train dataset:  16%|█▌        | 1379/8564 [00:04<00:24, 294.19 examples/s]Tokenizing train dataset:  16%|█▋        | 1410/8564 [00:04<00:24, 294.20 examples/s]Tokenizing train dataset:  17%|█▋        | 1440/8564 [00:04<00:24, 292.29 examples/s]Tokenizing train dataset:  17%|█▋        | 1472/8564 [00:04<00:23, 297.60 examples/s]Tokenizing train dataset:  18%|█▊        | 1514/8564 [00:04<00:24, 285.63 examples/s]Tokenizing train dataset:  18%|█▊        | 1548/8564 [00:05<00:23, 297.45 examples/s]Tokenizing train dataset:  19%|█▊        | 1591/8564 [00:05<00:23, 291.07 examples/s]Tokenizing train dataset:  19%|█▉        | 1627/8564 [00:05<00:23, 300.74 examples/s]Tokenizing train dataset:  19%|█▉        | 1661/8564 [00:05<00:22, 308.44 examples/s]Tokenizing train dataset:  20%|█▉        | 1702/8564 [00:05<00:20, 334.30 examples/s]Tokenizing train dataset:  20%|██        | 1750/8564 [00:05<00:21, 323.69 examples/s]Tokenizing train dataset:  21%|██        | 1783/8564 [00:05<00:21, 321.02 examples/s]Tokenizing train dataset:  21%|██▏       | 1830/8564 [00:05<00:21, 316.12 examples/s]Tokenizing train dataset:  22%|██▏       | 1875/8564 [00:06<00:21, 307.01 examples/s]Tokenizing train dataset:  22%|██▏       | 1914/8564 [00:06<00:20, 324.25 examples/s]Tokenizing train dataset:  23%|██▎       | 1953/8564 [00:06<00:19, 337.75 examples/s]Tokenizing train dataset:  23%|██▎       | 1990/8564 [00:06<00:19, 339.63 examples/s]Tokenizing train dataset:  24%|██▎       | 2029/8564 [00:06<00:18, 350.67 examples/s]Tokenizing train dataset:  24%|██▍       | 2067/8564 [00:06<00:18, 357.37 examples/s]Tokenizing train dataset:  25%|██▍       | 2125/8564 [00:06<00:17, 365.05 examples/s]Tokenizing train dataset:  25%|██▌       | 2180/8564 [00:06<00:17, 355.65 examples/s]Tokenizing train dataset:  26%|██▌       | 2217/8564 [00:07<00:17, 356.50 examples/s]Tokenizing train dataset:  26%|██▋       | 2256/8564 [00:07<00:17, 362.94 examples/s]Tokenizing train dataset:  27%|██▋       | 2299/8564 [00:07<00:16, 376.68 examples/s]Tokenizing train dataset:  27%|██▋       | 2348/8564 [00:07<00:17, 353.43 examples/s]Tokenizing train dataset:  28%|██▊       | 2385/8564 [00:07<00:17, 356.49 examples/s]Tokenizing train dataset:  28%|██▊       | 2426/8564 [00:07<00:16, 367.72 examples/s]Tokenizing train dataset:  29%|██▉       | 2464/8564 [00:07<00:16, 369.97 examples/s]Tokenizing train dataset:  29%|██▉       | 2512/8564 [00:07<00:17, 349.99 examples/s]Tokenizing train dataset:  30%|██▉       | 2555/8564 [00:07<00:16, 367.87 examples/s]Tokenizing train dataset:  30%|███       | 2593/8564 [00:08<00:16, 366.60 examples/s]Tokenizing train dataset:  31%|███       | 2640/8564 [00:08<00:17, 338.53 examples/s]Tokenizing train dataset:  31%|███▏      | 2689/8564 [00:08<00:17, 332.66 examples/s]Tokenizing train dataset:  32%|███▏      | 2729/8564 [00:08<00:16, 344.41 examples/s]Tokenizing train dataset:  32%|███▏      | 2781/8564 [00:08<00:17, 338.83 examples/s]Tokenizing train dataset:  33%|███▎      | 2822/8564 [00:08<00:16, 352.58 examples/s]Tokenizing train dataset:  34%|███▎      | 2870/8564 [00:08<00:16, 337.55 examples/s]Tokenizing train dataset:  34%|███▍      | 2914/8564 [00:09<00:15, 359.93 examples/s]Tokenizing train dataset:  35%|███▍      | 2955/8564 [00:09<00:15, 371.11 examples/s]Tokenizing train dataset:  35%|███▌      | 2998/8564 [00:09<00:14, 385.54 examples/s]Tokenizing train dataset:  36%|███▌      | 3050/8564 [00:09<00:14, 368.78 examples/s]Tokenizing train dataset:  36%|███▋      | 3106/8564 [00:09<00:14, 365.13 examples/s]Tokenizing train dataset:  37%|███▋      | 3162/8564 [00:09<00:14, 363.20 examples/s]Tokenizing train dataset:  38%|███▊      | 3216/8564 [00:09<00:14, 360.24 examples/s]Tokenizing train dataset:  38%|███▊      | 3256/8564 [00:09<00:14, 367.40 examples/s]Tokenizing train dataset:  39%|███▊      | 3308/8564 [00:10<00:14, 357.95 examples/s]Tokenizing train dataset:  39%|███▉      | 3361/8564 [00:10<00:14, 352.61 examples/s]Tokenizing train dataset:  40%|███▉      | 3404/8564 [00:10<00:14, 367.42 examples/s]Tokenizing train dataset:  40%|████      | 3461/8564 [00:10<00:13, 369.57 examples/s]Tokenizing train dataset:  41%|████      | 3519/8564 [00:10<00:13, 373.29 examples/s]Tokenizing train dataset:  42%|████▏     | 3570/8564 [00:10<00:13, 358.97 examples/s]Tokenizing train dataset:  42%|████▏     | 3610/8564 [00:10<00:13, 365.36 examples/s]Tokenizing train dataset:  43%|████▎     | 3664/8564 [00:11<00:13, 359.70 examples/s]Tokenizing train dataset:  43%|████▎     | 3724/8564 [00:11<00:13, 363.73 examples/s][rank1]:[W531 11:17:51.705019762 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Tokenizing train dataset:  44%|████▍     | 3780/8564 [00:11<00:13, 362.65 examples/s]Tokenizing train dataset:  45%|████▍     | 3834/8564 [00:11<00:13, 357.33 examples/s]Tokenizing train dataset:  45%|████▌     | 3870/8564 [00:11<00:13, 354.69 examples/s]Tokenizing train dataset:  46%|████▌     | 3906/8564 [00:11<00:13, 351.49 examples/s]Tokenizing train dataset:  46%|████▌     | 3960/8564 [00:11<00:13, 348.82 examples/s]Tokenizing train dataset:  47%|████▋     | 3997/8564 [00:12<00:13, 350.21 examples/s]Tokenizing train dataset:  47%|████▋     | 4036/8564 [00:12<00:12, 356.16 examples/s]Tokenizing train dataset:  48%|████▊     | 4073/8564 [00:12<00:12, 354.77 examples/s]Tokenizing train dataset:  48%|████▊     | 4110/8564 [00:12<00:12, 355.04 examples/s]Tokenizing train dataset:  49%|████▊     | 4162/8564 [00:12<00:12, 347.52 examples/s]Tokenizing train dataset:  49%|████▉     | 4203/8564 [00:12<00:12, 358.54 examples/s]Tokenizing train dataset:  50%|████▉     | 4257/8564 [00:12<00:12, 353.59 examples/s]Tokenizing train dataset:  50%|█████     | 4293/8564 [00:12<00:12, 352.46 examples/s]Tokenizing train dataset:  51%|█████     | 4330/8564 [00:12<00:11, 352.94 examples/s]Tokenizing train dataset:  51%|█████     | 4385/8564 [00:13<00:11, 355.25 examples/s]Tokenizing train dataset:  52%|█████▏    | 4423/8564 [00:13<00:11, 358.30 examples/s]Tokenizing train dataset:  52%|█████▏    | 4460/8564 [00:13<00:11, 354.50 examples/s]Tokenizing train dataset:  53%|█████▎    | 4498/8564 [00:13<00:11, 359.40 examples/s]Tokenizing train dataset:  53%|█████▎    | 4552/8564 [00:13<00:11, 356.06 examples/s]Tokenizing train dataset:  54%|█████▍    | 4604/8564 [00:13<00:11, 346.94 examples/s]Tokenizing train dataset:  54%|█████▍    | 4654/8564 [00:13<00:11, 339.78 examples/s]Tokenizing train dataset:  55%|█████▍    | 4702/8564 [00:14<00:11, 327.92 examples/s]Tokenizing train dataset:  55%|█████▌    | 4738/8564 [00:14<00:11, 331.34 examples/s]Tokenizing train dataset:  56%|█████▌    | 4782/8564 [00:14<00:11, 315.73 examples/s]Tokenizing train dataset:  56%|█████▋    | 4835/8564 [00:14<00:10, 365.50 examples/s]Tokenizing train dataset:  57%|█████▋    | 4894/8564 [00:14<00:08, 417.70 examples/s]Tokenizing train dataset:  58%|█████▊    | 4959/8564 [00:14<00:07, 474.36 examples/s]Tokenizing train dataset:  59%|█████▊    | 5017/8564 [00:14<00:07, 499.44 examples/s]Tokenizing train dataset:  59%|█████▉    | 5085/8564 [00:14<00:06, 547.91 examples/s]Tokenizing train dataset:  60%|██████    | 5150/8564 [00:14<00:05, 570.79 examples/s]Tokenizing train dataset:  61%|██████    | 5223/8564 [00:15<00:05, 612.13 examples/s]Tokenizing train dataset:  62%|██████▏   | 5298/8564 [00:15<00:05, 648.37 examples/s]Tokenizing train dataset:  63%|██████▎   | 5383/8564 [00:15<00:05, 612.47 examples/s]Tokenizing train dataset:  64%|██████▎   | 5451/8564 [00:15<00:04, 622.89 examples/s]Tokenizing train dataset:  65%|██████▍   | 5537/8564 [00:15<00:05, 599.94 examples/s]Tokenizing train dataset:  65%|██████▌   | 5601/8564 [00:15<00:04, 604.02 examples/s]Tokenizing train dataset:  66%|██████▌   | 5669/8564 [00:15<00:04, 619.89 examples/s]Tokenizing train dataset:  67%|██████▋   | 5772/8564 [00:15<00:04, 639.08 examples/s]Tokenizing train dataset:  68%|██████▊   | 5842/8564 [00:16<00:04, 644.73 examples/s]Tokenizing train dataset:  69%|██████▉   | 5928/8564 [00:16<00:04, 616.98 examples/s]Tokenizing train dataset:  70%|███████   | 6014/8564 [00:16<00:04, 581.46 examples/s]Tokenizing train dataset:  71%|███████   | 6074/8564 [00:16<00:04, 582.69 examples/s]Tokenizing train dataset:  72%|███████▏  | 6133/8564 [00:16<00:04, 582.09 examples/s]Tokenizing train dataset:  73%|███████▎  | 6209/8564 [00:16<00:03, 625.77 examples/s]Tokenizing train dataset:  73%|███████▎  | 6277/8564 [00:16<00:03, 638.76 examples/s]Tokenizing train dataset:  74%|███████▍  | 6379/8564 [00:16<00:03, 648.31 examples/s]Tokenizing train dataset:  76%|███████▌  | 6470/8564 [00:17<00:03, 625.95 examples/s]Tokenizing train dataset:  76%|███████▋  | 6535/8564 [00:17<00:03, 621.14 examples/s]Tokenizing train dataset:  77%|███████▋  | 6613/8564 [00:17<00:03, 574.26 examples/s]Tokenizing train dataset:  78%|███████▊  | 6678/8564 [00:17<00:03, 590.97 examples/s]Tokenizing train dataset:  79%|███████▉  | 6749/8564 [00:17<00:02, 614.59 examples/s]Tokenizing train dataset:  80%|███████▉  | 6835/8564 [00:17<00:02, 595.85 examples/s]Tokenizing train dataset:  81%|████████  | 6899/8564 [00:17<00:02, 605.13 examples/s]Tokenizing train dataset:  81%|████████▏ | 6964/8564 [00:17<00:02, 610.14 examples/s]Tokenizing train dataset:  82%|████████▏ | 7051/8564 [00:18<00:02, 593.88 examples/s]Tokenizing train dataset:  83%|████████▎ | 7119/8564 [00:18<00:02, 611.17 examples/s]Tokenizing train dataset:  84%|████████▍ | 7187/8564 [00:18<00:02, 620.68 examples/s]Tokenizing train dataset:  85%|████████▌ | 7280/8564 [00:18<00:02, 611.99 examples/s]Tokenizing train dataset:  86%|████████▌ | 7374/8564 [00:18<00:01, 614.18 examples/s]Tokenizing train dataset:  87%|████████▋ | 7438/8564 [00:18<00:01, 616.04 examples/s]Tokenizing train dataset:  88%|████████▊ | 7504/8564 [00:18<00:01, 618.28 examples/s]Tokenizing train dataset:  88%|████████▊ | 7575/8564 [00:18<00:01, 639.44 examples/s]Tokenizing train dataset:  89%|████████▉ | 7663/8564 [00:19<00:01, 614.56 examples/s]Tokenizing train dataset:  91%|█████████ | 7753/8564 [00:19<00:01, 599.20 examples/s]Tokenizing train dataset:  92%|█████████▏| 7839/8564 [00:19<00:01, 588.47 examples/s]Tokenizing train dataset:  92%|█████████▏| 7902/8564 [00:19<00:01, 592.78 examples/s]Tokenizing train dataset:  93%|█████████▎| 7963/8564 [00:19<00:01, 591.66 examples/s]Tokenizing train dataset:  94%|█████████▍| 8029/8564 [00:19<00:00, 607.12 examples/s]Tokenizing train dataset:  95%|█████████▍| 8115/8564 [00:19<00:00, 589.21 examples/s]Tokenizing train dataset:  95%|█████████▌| 8178/8564 [00:19<00:00, 595.26 examples/s]Tokenizing train dataset:  96%|█████████▌| 8240/8564 [00:19<00:00, 599.75 examples/s]Tokenizing train dataset:  97%|█████████▋| 8318/8564 [00:20<00:00, 641.49 examples/s]Tokenizing train dataset:  98%|█████████▊| 8400/8564 [00:20<00:00, 600.76 examples/s]Tokenizing train dataset:  99%|█████████▉| 8496/8564 [00:20<00:00, 606.18 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 603.84 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 417.49 examples/s]
[rank0]:[W531 11:18:01.166329664 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 10812.31 examples/s]
Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5423.05 examples/s]Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5389.96 examples/s]Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5357.73 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1108/8564 [00:00<00:01, 5496.93 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1110/8564 [00:00<00:01, 5495.37 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1101/8564 [00:00<00:01, 5443.86 examples/s]Extracting prompt in train dataset:  20%|█▉        | 1670/8564 [00:00<00:01, 5533.03 examples/s]Extracting prompt in train dataset:  19%|█▉        | 1655/8564 [00:00<00:01, 5482.76 examples/s]Extracting prompt in train dataset:  19%|█▉        | 1660/8564 [00:00<00:01, 5485.36 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2213/8564 [00:00<00:01, 5518.09 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2224/8564 [00:00<00:01, 5540.39 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2240/8564 [00:00<00:01, 5584.12 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  32%|███▏      | 2771/8564 [00:00<00:01, 5537.04 examples/s]Extracting prompt in train dataset:  33%|███▎      | 2784/8564 [00:00<00:01, 5559.69 examples/s]Extracting prompt in train dataset:  33%|███▎      | 2810/8564 [00:00<00:01, 5610.05 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13272.59 examples/s]
Extracting prompt in train dataset:  39%|███▉      | 3330/8564 [00:00<00:00, 5363.39 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3618/8564 [00:00<00:00, 5473.82 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3611/8564 [00:00<00:00, 5404.23 examples/s]Extracting prompt in train dataset:  45%|████▌     | 3891/8564 [00:00<00:00, 5439.31 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4189/8564 [00:00<00:00, 5529.92 examples/s]Extracting prompt in train dataset:  49%|████▊     | 4173/8564 [00:00<00:00, 5465.25 examples/s]Extracting prompt in train dataset:  52%|█████▏    | 4451/8564 [00:00<00:00, 5487.60 examples/s]Extracting prompt in train dataset:  55%|█████▌    | 4752/8564 [00:00<00:00, 5547.65 examples/s]Extracting prompt in train dataset:  55%|█████▌    | 4731/8564 [00:00<00:00, 5497.58 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  59%|█████▊    | 5020/8564 [00:00<00:00, 5524.72 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 5332/8564 [00:00<00:00, 5613.57 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 5310/8564 [00:00<00:00, 5564.63 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 320.96 examples/s]Extracting prompt in train dataset:  65%|██████▌   | 5600/8564 [00:01<00:00, 5588.22 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 5911/8564 [00:01<00:00, 5661.28 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 5890/8564 [00:01<00:00, 5613.00 examples/s]Extracting prompt in train dataset:  72%|███████▏  | 6180/8564 [00:01<00:00, 5632.53 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 6493/8564 [00:01<00:00, 5705.62 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 289.92 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 6470/8564 [00:01<00:00, 5652.10 examples/s]Extracting prompt in train dataset:  79%|███████▉  | 6760/8564 [00:01<00:00, 5663.12 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 7070/8564 [00:01<00:00, 5721.64 examples/s]Extracting prompt in train dataset:  82%|████████▏ | 7050/8564 [00:01<00:00, 5676.69 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 277.16 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 7340/8564 [00:01<00:00, 5678.40 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 7651/8564 [00:01<00:00, 5743.58 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 7630/8564 [00:01<00:00, 5695.92 examples/s]Extracting prompt in train dataset:  94%|█████████▍| 8060/8564 [00:01<00:00, 5320.76 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.53 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 8399/8564 [00:01<00:00, 5446.61 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 8350/8564 [00:01<00:00, 5335.89 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5526.38 examples/s]
Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5466.05 examples/s]
Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5453.85 examples/s]
Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.03 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 269.39 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:00<00:01, 370.55 examples/s]Tokenizing eval dataset:  37%|███▋      | 357/953 [00:01<00:01, 441.95 examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 487.21 examples/s]Applying chat template to train dataset:   3%|▎         | 288/8564 [00:00<00:02, 2853.03 examples/s]Applying chat template to train dataset:   3%|▎         | 285/8564 [00:00<00:02, 2824.35 examples/s]Applying chat template to train dataset:   3%|▎         | 286/8564 [00:00<00:02, 2833.56 examples/s]Applying chat template to train dataset:   7%|▋         | 600/8564 [00:00<00:02, 3003.85 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 541.08 examples/s]Applying chat template to train dataset:   7%|▋         | 597/8564 [00:00<00:02, 2986.55 examples/s]Applying chat template to train dataset:   7%|▋         | 597/8564 [00:00<00:02, 2985.94 examples/s]Applying chat template to train dataset:  11%|█         | 913/8564 [00:00<00:02, 3056.04 examples/s]Tokenizing eval dataset:  58%|█████▊    | 555/953 [00:01<00:00, 576.02 examples/s]Applying chat template to train dataset:  11%|█         | 908/8564 [00:00<00:02, 3035.29 examples/s]Applying chat template to train dataset:  11%|█         | 906/8564 [00:00<00:02, 3029.69 examples/s]Applying chat template to train dataset:  14%|█▍        | 1225/8564 [00:00<00:02, 3073.85 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 581.38 examples/s]Applying chat template to train dataset:  16%|█▌        | 1360/8564 [00:00<00:02, 3014.78 examples/s]Applying chat template to train dataset:  16%|█▌        | 1350/8564 [00:00<00:02, 2985.08 examples/s]Tokenizing eval dataset:  71%|███████   | 676/953 [00:01<00:00, 581.24 examples/s]Applying chat template to train dataset:  20%|█▉        | 1686/8564 [00:00<00:02, 3067.08 examples/s]Applying chat template to train dataset:  19%|█▉        | 1668/8564 [00:00<00:02, 3032.65 examples/s]Applying chat template to train dataset:  21%|██        | 1790/8564 [00:00<00:02, 2960.00 examples/s]Applying chat template to train dataset:  23%|██▎       | 1999/8564 [00:00<00:02, 3083.42 examples/s]Tokenizing eval dataset:  80%|███████▉  | 758/953 [00:01<00:00, 562.64 examples/s]Applying chat template to train dataset:  23%|██▎       | 1976/8564 [00:00<00:02, 3045.84 examples/s]Applying chat template to train dataset:  24%|██▍       | 2093/8564 [00:00<00:02, 2978.08 examples/s]Applying chat template to train dataset:  27%|██▋       | 2313/8564 [00:00<00:02, 3099.58 examples/s]Applying chat template to train dataset:  27%|██▋       | 2289/8564 [00:00<00:02, 3068.10 examples/s]Tokenizing eval dataset:  87%|████████▋ | 832/953 [00:01<00:00, 534.30 examples/s]Applying chat template to train dataset:  28%|██▊       | 2404/8564 [00:00<00:02, 3014.38 examples/s]Applying chat template to train dataset:  31%|███       | 2629/8564 [00:00<00:01, 3115.58 examples/s]Applying chat template to train dataset:  30%|███       | 2600/8564 [00:00<00:01, 3080.07 examples/s]Applying chat template to train dataset:  32%|███▏      | 2717/8564 [00:00<00:01, 3043.50 examples/s]Applying chat template to train dataset:  34%|███▍      | 2952/8564 [00:00<00:01, 3144.71 examples/s]Applying chat template to train dataset:  34%|███▍      | 2913/8564 [00:00<00:01, 3092.36 examples/s]Tokenizing eval dataset:  95%|█████████▌| 908/953 [00:02<00:00, 518.22 examples/s]Applying chat template to train dataset:  35%|███▌      | 3030/8564 [00:01<00:01, 3060.83 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 452.42 examples/s]
Applying chat template to train dataset:  40%|███▉      | 3402/8564 [00:01<00:01, 3085.43 examples/s]Applying chat template to train dataset:  39%|███▉      | 3361/8564 [00:01<00:01, 3028.56 examples/s]Applying chat template to train dataset:  41%|████      | 3469/8564 [00:01<00:01, 3007.46 examples/s]Applying chat template to train dataset:  43%|████▎     | 3720/8564 [00:01<00:01, 3107.49 examples/s]Applying chat template to train dataset:  43%|████▎     | 3670/8564 [00:01<00:01, 3039.23 examples/s]Applying chat template to train dataset:  44%|████▍     | 3780/8564 [00:01<00:01, 3029.62 examples/s]Applying chat template to train dataset:  47%|████▋     | 4036/8564 [00:01<00:01, 3119.39 examples/s]Applying chat template to train dataset:  46%|████▋     | 3976/8564 [00:01<00:01, 3042.30 examples/s]Applying chat template to train dataset:  48%|████▊     | 4090/8564 [00:01<00:01, 3047.60 examples/s]Applying chat template to train dataset:  51%|█████     | 4350/8564 [00:01<00:01, 3122.50 examples/s]Applying chat template to train dataset:  52%|█████▏    | 4439/8564 [00:01<00:01, 3053.74 examples/s]Applying chat template to train dataset:  51%|█████▏    | 4400/8564 [00:01<00:01, 3060.38 examples/s]Applying chat template to train dataset:  54%|█████▍    | 4665/8564 [00:01<00:01, 3128.11 examples/s]Applying chat template to train dataset:  56%|█████▌    | 4755/8564 [00:01<00:01, 3078.53 examples/s]Applying chat template to train dataset:  55%|█████▍    | 4710/8564 [00:01<00:01, 3067.59 examples/s]Applying chat template to train dataset:  58%|█████▊    | 4983/8564 [00:01<00:01, 3140.13 examples/s]Applying chat template to train dataset:  59%|█████▉    | 5076/8564 [00:01<00:01, 3112.26 examples/s]Applying chat template to train dataset:  59%|█████▊    | 5025/8564 [00:01<00:01, 3090.60 examples/s]Applying chat template to train dataset:  62%|██████▏   | 5304/8564 [00:01<00:01, 3160.37 examples/s]Applying chat template to train dataset:  63%|██████▎   | 5394/8564 [00:01<00:01, 3129.74 examples/s]Applying chat template to train dataset:  62%|██████▏   | 5343/8564 [00:01<00:01, 3114.17 examples/s]Applying chat template to train dataset:  66%|██████▌   | 5625/8564 [00:01<00:00, 3171.80 examples/s]Applying chat template to train dataset:  67%|██████▋   | 5713/8564 [00:01<00:00, 3142.93 examples/s]Applying chat template to train dataset:  66%|██████▌   | 5660/8564 [00:01<00:00, 3129.98 examples/s]Applying chat template to train dataset:  69%|██████▉   | 5947/8564 [00:01<00:00, 3182.80 examples/s]Applying chat template to train dataset:  70%|███████   | 6031/8564 [00:01<00:00, 3152.19 examples/s]Applying chat template to train dataset:  70%|██████▉   | 5980/8564 [00:01<00:00, 3143.74 examples/s]Applying chat template to train dataset:  73%|███████▎  | 6269/8564 [00:02<00:00, 3191.30 examples/s]Applying chat template to train dataset:  74%|███████▍  | 6350/8564 [00:02<00:00, 3158.47 examples/s]Applying chat template to train dataset:  74%|███████▎  | 6300/8564 [00:02<00:00, 3151.85 examples/s]Applying chat template to train dataset:  77%|███████▋  | 6590/8564 [00:02<00:00, 3191.08 examples/s]Applying chat template to train dataset:  78%|███████▊  | 6670/8564 [00:02<00:00, 3161.59 examples/s]Applying chat template to train dataset:  77%|███████▋  | 6619/8564 [00:02<00:00, 3158.41 examples/s]Applying chat template to train dataset:  81%|████████  | 6911/8564 [00:02<00:00, 3194.77 examples/s]Applying chat template to train dataset:  82%|████████▏ | 6990/8564 [00:02<00:00, 3165.14 examples/s]Applying chat template to train dataset:  81%|████████  | 6937/8564 [00:02<00:00, 3160.20 examples/s]Applying chat template to train dataset:  84%|████████▍ | 7232/8564 [00:02<00:00, 3197.63 examples/s]Applying chat template to train dataset:  85%|████████▌ | 7310/8564 [00:02<00:00, 3168.40 examples/s]Applying chat template to train dataset:  85%|████████▍ | 7254/8564 [00:02<00:00, 3160.28 examples/s]Applying chat template to train dataset:  88%|████████▊ | 7554/8564 [00:02<00:00, 3201.82 examples/s]Applying chat template to train dataset:  89%|████████▉ | 7630/8564 [00:02<00:00, 3172.73 examples/s]Applying chat template to train dataset:  88%|████████▊ | 7571/8564 [00:02<00:00, 3159.09 examples/s]Applying chat template to train dataset:  93%|█████████▎| 7972/8564 [00:02<00:00, 3036.78 examples/s]Applying chat template to train dataset:  93%|█████████▎| 7981/8564 [00:02<00:00, 2989.38 examples/s]Applying chat template to train dataset:  94%|█████████▍| 8069/8564 [00:02<00:00, 3011.10 examples/s]Applying chat template to train dataset:  97%|█████████▋| 8302/8564 [00:02<00:00, 3106.44 examples/s]Applying chat template to train dataset:  97%|█████████▋| 8300/8564 [00:02<00:00, 3039.84 examples/s]Applying chat template to train dataset:  98%|█████████▊| 8388/8564 [00:02<00:00, 3056.76 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3120.76 examples/s]
Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3075.88 examples/s]
Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3057.73 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 41/8564 [00:00<00:21, 397.14 examples/s]Tokenizing train dataset:   0%|          | 41/8564 [00:00<00:21, 402.28 examples/s]Tokenizing train dataset:   0%|          | 42/8564 [00:00<00:21, 403.65 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:25, 334.98 examples/s]Tokenizing train dataset:   1%|          | 88/8564 [00:00<00:25, 334.76 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:25, 337.70 examples/s]Tokenizing train dataset:   2%|▏         | 135/8564 [00:00<00:26, 317.81 examples/s]Tokenizing train dataset:   2%|▏         | 133/8564 [00:00<00:26, 312.57 examples/s]Tokenizing train dataset:   2%|▏         | 139/8564 [00:00<00:26, 319.56 examples/s]Tokenizing train dataset:   2%|▏         | 165/8564 [00:00<00:27, 310.73 examples/s]Tokenizing train dataset:   2%|▏         | 180/8564 [00:00<00:27, 304.02 examples/s]Tokenizing train dataset:   2%|▏         | 184/8564 [00:00<00:27, 307.45 examples/s]Tokenizing train dataset:   2%|▏         | 214/8564 [00:00<00:26, 310.59 examples/s]Tokenizing train dataset:   2%|▏         | 212/8564 [00:00<00:27, 307.47 examples/s]Tokenizing train dataset:   3%|▎         | 218/8564 [00:00<00:26, 313.04 examples/s]Tokenizing train dataset:   3%|▎         | 250/8564 [00:00<00:26, 315.92 examples/s]Tokenizing train dataset:   3%|▎         | 248/8564 [00:00<00:25, 320.04 examples/s]Tokenizing train dataset:   3%|▎         | 251/8564 [00:00<00:26, 316.13 examples/s]Tokenizing train dataset:   3%|▎         | 288/8564 [00:00<00:25, 330.68 examples/s]Tokenizing train dataset:   3%|▎         | 283/8564 [00:00<00:25, 326.47 examples/s]Tokenizing train dataset:   3%|▎         | 289/8564 [00:00<00:24, 331.84 examples/s]Tokenizing train dataset:   4%|▎         | 318/8564 [00:00<00:25, 326.73 examples/s]Tokenizing train dataset:   4%|▍         | 336/8564 [00:01<00:25, 324.02 examples/s]Tokenizing train dataset:   4%|▍         | 337/8564 [00:01<00:25, 324.10 examples/s]Tokenizing train dataset:   4%|▍         | 366/8564 [00:01<00:25, 318.70 examples/s]Tokenizing train dataset:   5%|▍         | 387/8564 [00:01<00:25, 321.89 examples/s]Tokenizing train dataset:   5%|▍         | 387/8564 [00:01<00:25, 322.52 examples/s]Tokenizing train dataset:   5%|▍         | 399/8564 [00:01<00:25, 318.35 examples/s]Tokenizing train dataset:   5%|▌         | 434/8564 [00:01<00:25, 313.51 examples/s]Tokenizing train dataset:   5%|▌         | 432/8564 [00:01<00:25, 316.12 examples/s]Tokenizing train dataset:   5%|▌         | 434/8564 [00:01<00:25, 314.03 examples/s]Tokenizing train dataset:   6%|▌         | 480/8564 [00:01<00:26, 307.05 examples/s]Tokenizing train dataset:   6%|▌         | 478/8564 [00:01<00:26, 308.03 examples/s]Tokenizing train dataset:   6%|▌         | 480/8564 [00:01<00:26, 307.86 examples/s]Tokenizing train dataset:   6%|▌         | 527/8564 [00:01<00:26, 305.98 examples/s]Tokenizing train dataset:   6%|▌         | 524/8564 [00:01<00:26, 304.65 examples/s]Tokenizing train dataset:   6%|▌         | 527/8564 [00:01<00:26, 306.65 examples/s]Tokenizing train dataset:   7%|▋         | 561/8564 [00:01<00:25, 310.96 examples/s]Tokenizing train dataset:   7%|▋         | 557/8564 [00:01<00:25, 309.52 examples/s]Tokenizing train dataset:   7%|▋         | 561/8564 [00:01<00:25, 311.56 examples/s]Tokenizing train dataset:   7%|▋         | 605/8564 [00:01<00:26, 300.32 examples/s]Tokenizing train dataset:   7%|▋         | 602/8564 [00:01<00:26, 303.21 examples/s]Tokenizing train dataset:   7%|▋         | 605/8564 [00:01<00:26, 300.97 examples/s]Tokenizing train dataset:   8%|▊         | 643/8564 [00:02<00:24, 318.33 examples/s]Tokenizing train dataset:   7%|▋         | 640/8564 [00:02<00:24, 318.02 examples/s]Tokenizing train dataset:   8%|▊         | 644/8564 [00:02<00:24, 317.89 examples/s]Tokenizing train dataset:   8%|▊         | 689/8564 [00:02<00:25, 309.51 examples/s]Tokenizing train dataset:   8%|▊         | 686/8564 [00:02<00:25, 311.99 examples/s]Tokenizing train dataset:   8%|▊         | 689/8564 [00:02<00:25, 309.89 examples/s]Tokenizing train dataset:   9%|▊         | 740/8564 [00:02<00:24, 316.14 examples/s]Tokenizing train dataset:   9%|▊         | 736/8564 [00:02<00:24, 313.38 examples/s]Tokenizing train dataset:   9%|▊         | 740/8564 [00:02<00:24, 316.00 examples/s]Tokenizing train dataset:   9%|▉         | 768/8564 [00:02<00:25, 311.71 examples/s]Tokenizing train dataset:   9%|▉         | 785/8564 [00:02<00:25, 306.76 examples/s]Tokenizing train dataset:   9%|▉         | 785/8564 [00:02<00:25, 306.68 examples/s]Tokenizing train dataset:   9%|▉         | 810/8564 [00:02<00:26, 297.90 examples/s]Tokenizing train dataset:  10%|▉         | 827/8564 [00:02<00:26, 293.30 examples/s]Tokenizing train dataset:  10%|▉         | 827/8564 [00:02<00:26, 293.15 examples/s]Tokenizing train dataset:  10%|█         | 860/8564 [00:02<00:25, 298.43 examples/s]Tokenizing train dataset:  10%|▉         | 854/8564 [00:02<00:26, 291.82 examples/s]Tokenizing train dataset:  10%|█         | 860/8564 [00:02<00:25, 298.36 examples/s]Tokenizing train dataset:  10%|█         | 897/8564 [00:02<00:24, 313.86 examples/s]Tokenizing train dataset:  10%|█         | 890/8564 [00:02<00:25, 304.96 examples/s]Tokenizing train dataset:  10%|█         | 897/8564 [00:02<00:24, 313.94 examples/s]Tokenizing train dataset:  11%|█         | 922/8564 [00:02<00:25, 301.64 examples/s]Tokenizing train dataset:  11%|█         | 940/8564 [00:03<00:25, 301.49 examples/s]Tokenizing train dataset:  11%|█         | 940/8564 [00:03<00:25, 301.72 examples/s]Tokenizing train dataset:  11%|█         | 953/8564 [00:03<00:25, 302.97 examples/s]Tokenizing train dataset:  11%|█▏        | 972/8564 [00:03<00:24, 304.88 examples/s]Tokenizing train dataset:  11%|█▏        | 972/8564 [00:03<00:24, 305.35 examples/s]Tokenizing train dataset:  12%|█▏        | 997/8564 [00:03<00:25, 294.05 examples/s]Tokenizing train dataset:  12%|█▏        | 1014/8564 [00:03<00:26, 289.51 examples/s]Tokenizing train dataset:  12%|█▏        | 1014/8564 [00:03<00:26, 290.27 examples/s]Tokenizing train dataset:  12%|█▏        | 1027/8564 [00:03<00:26, 289.28 examples/s]Tokenizing train dataset:  12%|█▏        | 1060/8564 [00:03<00:25, 289.95 examples/s]Tokenizing train dataset:  12%|█▏        | 1060/8564 [00:03<00:25, 290.74 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 291.15 examples/s]Tokenizing train dataset:  13%|█▎        | 1095/8564 [00:03<00:24, 302.55 examples/s]Tokenizing train dataset:  13%|█▎        | 1095/8564 [00:03<00:24, 303.50 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 304.59 examples/s]Tokenizing train dataset:  13%|█▎        | 1137/8564 [00:03<00:25, 290.03 examples/s]Tokenizing train dataset:  13%|█▎        | 1137/8564 [00:03<00:25, 290.87 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:03<00:25, 292.31 examples/s]Tokenizing train dataset:  14%|█▎        | 1167/8564 [00:03<00:25, 290.30 examples/s]Tokenizing train dataset:  14%|█▎        | 1167/8564 [00:03<00:25, 291.25 examples/s]Tokenizing train dataset:  14%|█▍        | 1181/8564 [00:03<00:25, 293.38 examples/s]Tokenizing train dataset:  14%|█▍        | 1202/8564 [00:03<00:24, 301.10 examples/s]Tokenizing train dataset:  14%|█▍        | 1202/8564 [00:03<00:24, 302.12 examples/s]Tokenizing train dataset:  14%|█▍        | 1214/8564 [00:03<00:24, 297.66 examples/s]Tokenizing train dataset:  14%|█▍        | 1233/8564 [00:04<00:24, 302.99 examples/s]Tokenizing train dataset:  14%|█▍        | 1236/8564 [00:04<00:23, 307.72 examples/s]Tokenizing train dataset:  15%|█▍        | 1250/8564 [00:04<00:23, 311.84 examples/s]Tokenizing train dataset:  15%|█▍        | 1270/8564 [00:04<00:23, 310.01 examples/s]Tokenizing train dataset:  15%|█▍        | 1270/8564 [00:04<00:23, 307.76 examples/s]Tokenizing train dataset:  15%|█▌        | 1304/8564 [00:04<00:23, 313.90 examples/s]Tokenizing train dataset:  15%|█▌        | 1299/8564 [00:04<00:23, 312.08 examples/s]Tokenizing train dataset:  15%|█▌        | 1304/8564 [00:04<00:23, 312.55 examples/s]Tokenizing train dataset:  16%|█▌        | 1338/8564 [00:04<00:22, 317.77 examples/s]Tokenizing train dataset:  16%|█▌        | 1331/8564 [00:04<00:23, 311.30 examples/s]Tokenizing train dataset:  16%|█▌        | 1339/8564 [00:04<00:22, 316.47 examples/s]Tokenizing train dataset:  16%|█▌        | 1379/8564 [00:04<00:24, 295.93 examples/s]Tokenizing train dataset:  16%|█▌        | 1373/8564 [00:04<00:24, 295.80 examples/s]Tokenizing train dataset:  16%|█▌        | 1379/8564 [00:04<00:24, 296.36 examples/s]Tokenizing train dataset:  16%|█▋        | 1410/8564 [00:04<00:24, 295.78 examples/s]Tokenizing train dataset:  16%|█▋        | 1407/8564 [00:04<00:23, 302.25 examples/s]Tokenizing train dataset:  16%|█▋        | 1410/8564 [00:04<00:24, 296.33 examples/s]Tokenizing train dataset:  17%|█▋        | 1440/8564 [00:04<00:24, 293.44 examples/s]Tokenizing train dataset:  17%|█▋        | 1440/8564 [00:04<00:24, 294.37 examples/s]Tokenizing train dataset:  17%|█▋        | 1453/8564 [00:04<00:23, 298.52 examples/s]Tokenizing train dataset:  17%|█▋        | 1472/8564 [00:04<00:23, 298.26 examples/s]Tokenizing train dataset:  17%|█▋        | 1472/8564 [00:04<00:23, 299.55 examples/s]Tokenizing train dataset:  17%|█▋        | 1498/8564 [00:04<00:23, 295.48 examples/s]Tokenizing train dataset:  18%|█▊        | 1514/8564 [00:04<00:24, 286.07 examples/s]Tokenizing train dataset:  18%|█▊        | 1514/8564 [00:04<00:24, 287.46 examples/s]Tokenizing train dataset:  18%|█▊        | 1530/8564 [00:05<00:23, 295.51 examples/s]Tokenizing train dataset:  18%|█▊        | 1548/8564 [00:05<00:23, 297.86 examples/s]Tokenizing train dataset:  18%|█▊        | 1548/8564 [00:05<00:23, 299.26 examples/s]Tokenizing train dataset:  18%|█▊        | 1561/8564 [00:05<00:23, 296.21 examples/s]Tokenizing train dataset:  19%|█▊        | 1591/8564 [00:05<00:23, 291.69 examples/s]Tokenizing train dataset:  19%|█▊        | 1591/8564 [00:05<00:23, 292.14 examples/s]Tokenizing train dataset:  19%|█▊        | 1591/8564 [00:05<00:23, 292.83 examples/s]Tokenizing train dataset:  19%|█▉        | 1627/8564 [00:05<00:23, 301.10 examples/s]Tokenizing train dataset:  19%|█▉        | 1627/8564 [00:05<00:22, 301.98 examples/s]Tokenizing train dataset:  19%|█▉        | 1627/8564 [00:05<00:22, 302.56 examples/s]Tokenizing train dataset:  19%|█▉        | 1661/8564 [00:05<00:22, 308.71 examples/s]Tokenizing train dataset:  19%|█▉        | 1661/8564 [00:05<00:22, 310.18 examples/s]Tokenizing train dataset:  19%|█▉        | 1661/8564 [00:05<00:22, 310.15 examples/s]Tokenizing train dataset:  20%|█▉        | 1702/8564 [00:05<00:20, 335.16 examples/s]Tokenizing train dataset:  20%|█▉        | 1702/8564 [00:05<00:20, 337.68 examples/s]Tokenizing train dataset:  20%|█▉        | 1703/8564 [00:05<00:20, 336.74 examples/s]Tokenizing train dataset:  20%|██        | 1751/8564 [00:05<00:20, 324.71 examples/s]Tokenizing train dataset:  20%|██        | 1751/8564 [00:05<00:20, 326.41 examples/s]Tokenizing train dataset:  20%|██        | 1753/8564 [00:05<00:20, 326.31 examples/s]Tokenizing train dataset:  21%|██        | 1799/8564 [00:05<00:21, 320.51 examples/s]Tokenizing train dataset:  21%|██        | 1800/8564 [00:05<00:21, 320.09 examples/s]Tokenizing train dataset:  21%|██        | 1800/8564 [00:05<00:21, 320.21 examples/s]Tokenizing train dataset:  21%|██▏       | 1833/8564 [00:05<00:20, 321.72 examples/s]Tokenizing train dataset:  22%|██▏       | 1848/8564 [00:06<00:21, 317.99 examples/s]Tokenizing train dataset:  21%|██▏       | 1833/8564 [00:05<00:20, 321.71 examples/s]Tokenizing train dataset:  22%|██▏       | 1877/8564 [00:06<00:21, 308.04 examples/s]Tokenizing train dataset:  22%|██▏       | 1893/8564 [00:06<00:21, 306.63 examples/s]Tokenizing train dataset:  22%|██▏       | 1877/8564 [00:06<00:21, 308.12 examples/s]Tokenizing train dataset:  22%|██▏       | 1919/8564 [00:06<00:20, 331.82 examples/s]Tokenizing train dataset:  23%|██▎       | 1938/8564 [00:06<00:19, 337.00 examples/s]Tokenizing train dataset:  22%|██▏       | 1919/8564 [00:06<00:20, 331.90 examples/s]Tokenizing train dataset:  23%|██▎       | 1956/8564 [00:06<00:19, 338.38 examples/s]Tokenizing train dataset:  23%|██▎       | 1974/8564 [00:06<00:19, 340.05 examples/s]Tokenizing train dataset:  23%|██▎       | 1956/8564 [00:06<00:19, 338.35 examples/s]Tokenizing train dataset:  23%|██▎       | 1991/8564 [00:06<00:19, 340.33 examples/s]Tokenizing train dataset:  23%|██▎       | 2011/8564 [00:06<00:18, 345.46 examples/s]Tokenizing train dataset:  23%|██▎       | 1991/8564 [00:06<00:19, 340.34 examples/s]Tokenizing train dataset:  24%|██▎       | 2030/8564 [00:06<00:18, 351.77 examples/s]Tokenizing train dataset:  24%|██▍       | 2050/8564 [00:06<00:18, 355.23 examples/s]Tokenizing train dataset:  24%|██▎       | 2030/8564 [00:06<00:18, 351.27 examples/s]Tokenizing train dataset:  24%|██▍       | 2069/8564 [00:06<00:18, 356.85 examples/s]Tokenizing train dataset:  24%|██▍       | 2088/8564 [00:06<00:18, 357.93 examples/s]Tokenizing train dataset:  24%|██▍       | 2069/8564 [00:06<00:18, 356.54 examples/s]Tokenizing train dataset:  25%|██▍       | 2106/8564 [00:06<00:18, 358.52 examples/s]Tokenizing train dataset:  25%|██▍       | 2128/8564 [00:06<00:17, 365.73 examples/s]Tokenizing train dataset:  25%|██▍       | 2106/8564 [00:06<00:18, 358.13 examples/s]Tokenizing train dataset:  25%|██▌       | 2146/8564 [00:06<00:17, 367.81 examples/s]Tokenizing train dataset:  25%|██▌       | 2146/8564 [00:06<00:17, 367.48 examples/s]Tokenizing train dataset:  25%|██▌       | 2180/8564 [00:06<00:17, 357.70 examples/s]Tokenizing train dataset:  26%|██▌       | 2199/8564 [00:06<00:17, 360.40 examples/s]Tokenizing train dataset:  26%|██▌       | 2217/8564 [00:07<00:17, 358.22 examples/s]Tokenizing train dataset:  26%|██▌       | 2199/8564 [00:06<00:17, 360.16 examples/s]Tokenizing train dataset:  26%|██▌       | 2237/8564 [00:07<00:17, 362.93 examples/s]Tokenizing train dataset:  26%|██▋       | 2256/8564 [00:07<00:17, 364.49 examples/s]Tokenizing train dataset:  26%|██▌       | 2237/8564 [00:07<00:17, 362.80 examples/s]Tokenizing train dataset:  27%|██▋       | 2278/8564 [00:07<00:16, 374.34 examples/s]Tokenizing train dataset:  27%|██▋       | 2299/8564 [00:07<00:16, 378.46 examples/s]Tokenizing train dataset:  27%|██▋       | 2278/8564 [00:07<00:16, 374.62 examples/s]Tokenizing train dataset:  27%|██▋       | 2332/8564 [00:07<00:17, 362.62 examples/s]Tokenizing train dataset:  27%|██▋       | 2348/8564 [00:07<00:17, 354.08 examples/s]Tokenizing train dataset:  27%|██▋       | 2332/8564 [00:07<00:17, 362.88 examples/s]Tokenizing train dataset:  28%|██▊       | 2385/8564 [00:07<00:17, 356.92 examples/s]Tokenizing train dataset:  28%|██▊       | 2385/8564 [00:07<00:17, 358.30 examples/s]Tokenizing train dataset:  28%|██▊       | 2385/8564 [00:07<00:17, 358.63 examples/s]Tokenizing train dataset:  28%|██▊       | 2426/8564 [00:07<00:16, 367.75 examples/s]Tokenizing train dataset:  28%|██▊       | 2426/8564 [00:07<00:16, 368.43 examples/s]Tokenizing train dataset:  28%|██▊       | 2426/8564 [00:07<00:16, 369.01 examples/s]Tokenizing train dataset:  29%|██▉       | 2465/8564 [00:07<00:16, 370.89 examples/s]Tokenizing train dataset:  29%|██▉       | 2465/8564 [00:07<00:16, 371.29 examples/s]Tokenizing train dataset:  29%|██▉       | 2465/8564 [00:07<00:16, 371.92 examples/s]Tokenizing train dataset:  29%|██▉       | 2514/8564 [00:07<00:17, 350.16 examples/s]Tokenizing train dataset:  29%|██▉       | 2514/8564 [00:07<00:17, 351.37 examples/s]Tokenizing train dataset:  29%|██▉       | 2514/8564 [00:07<00:17, 352.21 examples/s]Tokenizing train dataset:  30%|██▉       | 2558/8564 [00:07<00:16, 370.61 examples/s]Tokenizing train dataset:  30%|██▉       | 2558/8564 [00:07<00:16, 371.17 examples/s]Tokenizing train dataset:  30%|██▉       | 2558/8564 [00:07<00:16, 371.96 examples/s]Tokenizing train dataset:  30%|███       | 2596/8564 [00:08<00:16, 369.80 examples/s]Tokenizing train dataset:  30%|███       | 2596/8564 [00:08<00:16, 370.24 examples/s]Tokenizing train dataset:  30%|███       | 2596/8564 [00:08<00:16, 371.01 examples/s]Tokenizing train dataset:  31%|███       | 2640/8564 [00:08<00:17, 338.04 examples/s]Tokenizing train dataset:  31%|███       | 2641/8564 [00:08<00:17, 341.20 examples/s]Tokenizing train dataset:  31%|███       | 2640/8564 [00:08<00:17, 338.33 examples/s]Tokenizing train dataset:  31%|███▏      | 2689/8564 [00:08<00:17, 332.47 examples/s]Tokenizing train dataset:  31%|███▏      | 2693/8564 [00:08<00:17, 334.84 examples/s]Tokenizing train dataset:  31%|███▏      | 2689/8564 [00:08<00:17, 333.34 examples/s]Tokenizing train dataset:  32%|███▏      | 2729/8564 [00:08<00:16, 344.29 examples/s]Tokenizing train dataset:  32%|███▏      | 2731/8564 [00:08<00:17, 342.80 examples/s]Tokenizing train dataset:  32%|███▏      | 2729/8564 [00:08<00:16, 345.20 examples/s]Tokenizing train dataset:  32%|███▏      | 2781/8564 [00:08<00:17, 338.77 examples/s]Tokenizing train dataset:  33%|███▎      | 2785/8564 [00:08<00:16, 343.37 examples/s]Tokenizing train dataset:  32%|███▏      | 2781/8564 [00:08<00:16, 340.23 examples/s]Tokenizing train dataset:  33%|███▎      | 2822/8564 [00:08<00:16, 352.66 examples/s]Tokenizing train dataset:  33%|███▎      | 2825/8564 [00:08<00:16, 355.07 examples/s]Tokenizing train dataset:  33%|███▎      | 2822/8564 [00:08<00:16, 354.03 examples/s]Tokenizing train dataset:  34%|███▎      | 2870/8564 [00:08<00:16, 338.06 examples/s]Tokenizing train dataset:  34%|███▎      | 2873/8564 [00:08<00:16, 339.61 examples/s]Tokenizing train dataset:  34%|███▎      | 2870/8564 [00:08<00:16, 339.22 examples/s]Tokenizing train dataset:  34%|███▍      | 2914/8564 [00:08<00:15, 360.73 examples/s]Tokenizing train dataset:  34%|███▍      | 2919/8564 [00:08<00:15, 368.06 examples/s]Tokenizing train dataset:  34%|███▍      | 2914/8564 [00:08<00:15, 361.96 examples/s]Tokenizing train dataset:  35%|███▍      | 2956/8564 [00:09<00:14, 373.94 examples/s]Tokenizing train dataset:  35%|███▍      | 2960/8564 [00:09<00:15, 371.89 examples/s]Tokenizing train dataset:  35%|███▍      | 2956/8564 [00:09<00:14, 375.28 examples/s]Tokenizing train dataset:  35%|███▌      | 2999/8564 [00:09<00:14, 385.53 examples/s]Tokenizing train dataset:  35%|███▌      | 3005/8564 [00:09<00:14, 387.78 examples/s]Tokenizing train dataset:  35%|███▌      | 2999/8564 [00:09<00:14, 386.79 examples/s]Tokenizing train dataset:  36%|███▌      | 3052/8564 [00:09<00:14, 369.59 examples/s]Tokenizing train dataset:  36%|███▌      | 3057/8564 [00:09<00:15, 366.35 examples/s]Tokenizing train dataset:  36%|███▌      | 3052/8564 [00:09<00:14, 370.76 examples/s]Tokenizing train dataset:  36%|███▌      | 3097/8564 [00:09<00:14, 370.83 examples/s]Tokenizing train dataset:  36%|███▋      | 3110/8564 [00:09<00:14, 367.90 examples/s]Tokenizing train dataset:  36%|███▋      | 3110/8564 [00:09<00:14, 368.87 examples/s]Tokenizing train dataset:  37%|███▋      | 3150/8564 [00:09<00:14, 362.48 examples/s]Tokenizing train dataset:  37%|███▋      | 3164/8564 [00:09<00:14, 363.19 examples/s]Tokenizing train dataset:  37%|███▋      | 3165/8564 [00:09<00:14, 363.51 examples/s]Tokenizing train dataset:  37%|███▋      | 3205/8564 [00:09<00:14, 360.72 examples/s]Tokenizing train dataset:  38%|███▊      | 3220/8564 [00:09<00:14, 360.96 examples/s]Tokenizing train dataset:  38%|███▊      | 3220/8564 [00:09<00:14, 361.88 examples/s]Tokenizing train dataset:  38%|███▊      | 3247/8564 [00:09<00:14, 370.84 examples/s]Tokenizing train dataset:  38%|███▊      | 3261/8564 [00:09<00:14, 370.55 examples/s]Tokenizing train dataset:  38%|███▊      | 3261/8564 [00:09<00:14, 371.42 examples/s]Tokenizing train dataset:  39%|███▊      | 3300/8564 [00:10<00:14, 358.09 examples/s]Tokenizing train dataset:  39%|███▊      | 3313/8564 [00:10<00:14, 357.83 examples/s]Tokenizing train dataset:  39%|███▊      | 3313/8564 [00:10<00:14, 358.71 examples/s]Tokenizing train dataset:  39%|███▉      | 3355/8564 [00:10<00:14, 354.87 examples/s]Tokenizing train dataset:  39%|███▉      | 3366/8564 [00:10<00:14, 352.41 examples/s]Tokenizing train dataset:  39%|███▉      | 3366/8564 [00:10<00:14, 353.27 examples/s]Tokenizing train dataset:  40%|███▉      | 3409/8564 [00:10<00:13, 369.33 examples/s]Tokenizing train dataset:  40%|███▉      | 3396/8564 [00:10<00:14, 366.11 examples/s]Tokenizing train dataset:  40%|███▉      | 3410/8564 [00:10<00:13, 368.82 examples/s]Tokenizing train dataset:  40%|████      | 3437/8564 [00:10<00:13, 372.16 examples/s]Tokenizing train dataset:  40%|████      | 3465/8564 [00:10<00:13, 367.83 examples/s]Tokenizing train dataset:  40%|████      | 3450/8564 [00:10<00:13, 368.08 examples/s]Tokenizing train dataset:  41%|████      | 3504/8564 [00:10<00:13, 369.33 examples/s]Tokenizing train dataset:  41%|████      | 3491/8564 [00:10<00:13, 362.83 examples/s]Tokenizing train dataset:  41%|████      | 3488/8564 [00:10<00:13, 362.70 examples/s]Tokenizing train dataset:  41%|████▏     | 3542/8564 [00:10<00:13, 369.40 examples/s]Tokenizing train dataset:  41%|████▏     | 3533/8564 [00:10<00:13, 373.60 examples/s]Tokenizing train dataset:  41%|████      | 3530/8564 [00:10<00:13, 374.03 examples/s]Tokenizing train dataset:  42%|████▏     | 3598/8564 [00:10<00:13, 368.51 examples/s]Tokenizing train dataset:  42%|████▏     | 3592/8564 [00:10<00:13, 370.12 examples/s]Tokenizing train dataset:  42%|████▏     | 3586/8564 [00:10<00:13, 371.74 examples/s]Tokenizing train dataset:  42%|████▏     | 3636/8564 [00:10<00:13, 367.16 examples/s]Tokenizing train dataset:  42%|████▏     | 3631/8564 [00:10<00:13, 370.73 examples/s]Tokenizing train dataset:  42%|████▏     | 3624/8564 [00:10<00:13, 371.46 examples/s]Tokenizing train dataset:  43%|████▎     | 3685/8564 [00:11<00:13, 351.82 examples/s]Tokenizing train dataset:  43%|████▎     | 3680/8564 [00:11<00:13, 349.33 examples/s]Tokenizing train dataset:  43%|████▎     | 3674/8564 [00:11<00:13, 354.06 examples/s]Tokenizing train dataset:  44%|████▎     | 3726/8564 [00:11<00:13, 364.21 examples/s]Tokenizing train dataset:  43%|████▎     | 3724/8564 [00:11<00:13, 367.21 examples/s]Tokenizing train dataset:  43%|████▎     | 3712/8564 [00:11<00:13, 357.15 examples/s]Tokenizing train dataset:  44%|████▍     | 3763/8564 [00:11<00:13, 363.90 examples/s]Tokenizing train dataset:  44%|████▍     | 3752/8564 [00:11<00:13, 364.95 examples/s]Tokenizing train dataset:  44%|████▍     | 3780/8564 [00:11<00:13, 365.89 examples/s]Tokenizing train dataset:  44%|████▍     | 3800/8564 [00:11<00:13, 362.12 examples/s]Tokenizing train dataset:  44%|████▍     | 3790/8564 [00:11<00:13, 367.02 examples/s]Tokenizing train dataset:  45%|████▍     | 3834/8564 [00:11<00:13, 360.35 examples/s]Tokenizing train dataset:  45%|████▍     | 3852/8564 [00:11<00:13, 353.05 examples/s]Tokenizing train dataset:  45%|████▍     | 3842/8564 [00:11<00:13, 357.01 examples/s]Tokenizing train dataset:  45%|████▌     | 3891/8564 [00:11<00:13, 359.39 examples/s]Tokenizing train dataset:  45%|████▌     | 3880/8564 [00:11<00:12, 360.98 examples/s]Tokenizing train dataset:  45%|████▌     | 3890/8564 [00:11<00:13, 358.85 examples/s]Tokenizing train dataset:  46%|████▌     | 3942/8564 [00:11<00:13, 346.55 examples/s]Tokenizing train dataset:  46%|████▌     | 3930/8564 [00:11<00:13, 348.14 examples/s]Tokenizing train dataset:  46%|████▌     | 3940/8564 [00:11<00:13, 349.08 examples/s]Tokenizing train dataset:  46%|████▋     | 3979/8564 [00:11<00:13, 348.44 examples/s]Tokenizing train dataset:  46%|████▋     | 3967/8564 [00:11<00:13, 351.83 examples/s]Tokenizing train dataset:  46%|████▋     | 3978/8564 [00:11<00:13, 350.49 examples/s]Tokenizing train dataset:  47%|████▋     | 4018/8564 [00:12<00:12, 355.36 examples/s]Tokenizing train dataset:  47%|████▋     | 4004/8564 [00:11<00:13, 350.63 examples/s]Tokenizing train dataset:  47%|████▋     | 4014/8564 [00:12<00:12, 351.94 examples/s]Tokenizing train dataset:  47%|████▋     | 4057/8564 [00:12<00:12, 360.29 examples/s]Tokenizing train dataset:  47%|████▋     | 4044/8564 [00:12<00:12, 362.48 examples/s]Tokenizing train dataset:  47%|████▋     | 4054/8564 [00:12<00:12, 360.48 examples/s]Tokenizing train dataset:  48%|████▊     | 4111/8564 [00:12<00:12, 356.41 examples/s]Tokenizing train dataset:  48%|████▊     | 4095/8564 [00:12<00:12, 352.31 examples/s]Tokenizing train dataset:  48%|████▊     | 4110/8564 [00:12<00:12, 355.74 examples/s]Tokenizing train dataset:  48%|████▊     | 4132/8564 [00:12<00:12, 353.37 examples/s]Tokenizing train dataset:  49%|████▊     | 4162/8564 [00:12<00:12, 347.05 examples/s]Tokenizing train dataset:  49%|████▊     | 4162/8564 [00:12<00:12, 348.78 examples/s]Tokenizing train dataset:  49%|████▊     | 4169/8564 [00:12<00:12, 351.54 examples/s]Tokenizing train dataset:  49%|████▉     | 4202/8564 [00:12<00:12, 357.74 examples/s]Tokenizing train dataset:  49%|████▉     | 4203/8564 [00:12<00:12, 358.83 examples/s]Tokenizing train dataset:  49%|████▉     | 4210/8564 [00:12<00:12, 359.36 examples/s]Tokenizing train dataset:  49%|████▉     | 4239/8564 [00:12<00:12, 353.01 examples/s]Tokenizing train dataset:  50%|████▉     | 4257/8564 [00:12<00:12, 353.29 examples/s]Tokenizing train dataset:  50%|████▉     | 4276/8564 [00:12<00:12, 354.65 examples/s]Tokenizing train dataset:  50%|████▉     | 4264/8564 [00:12<00:12, 353.73 examples/s]Tokenizing train dataset:  50%|█████     | 4312/8564 [00:12<00:11, 354.87 examples/s]Tokenizing train dataset:  50%|█████     | 4301/8564 [00:12<00:12, 353.87 examples/s]Tokenizing train dataset:  50%|█████     | 4312/8564 [00:12<00:11, 356.29 examples/s]Tokenizing train dataset:  51%|█████     | 4337/8564 [00:12<00:12, 352.04 examples/s]Tokenizing train dataset:  51%|█████     | 4364/8564 [00:13<00:12, 345.18 examples/s]Tokenizing train dataset:  51%|█████     | 4364/8564 [00:13<00:12, 348.04 examples/s]Tokenizing train dataset:  51%|█████     | 4373/8564 [00:13<00:11, 353.74 examples/s]Tokenizing train dataset:  51%|█████▏    | 4404/8564 [00:13<00:11, 355.95 examples/s]Tokenizing train dataset:  51%|█████▏    | 4404/8564 [00:13<00:11, 356.96 examples/s]Tokenizing train dataset:  52%|█████▏    | 4411/8564 [00:13<00:11, 357.70 examples/s]Tokenizing train dataset:  52%|█████▏    | 4442/8564 [00:13<00:11, 358.17 examples/s]Tokenizing train dataset:  52%|█████▏    | 4442/8564 [00:13<00:11, 359.22 examples/s]Tokenizing train dataset:  52%|█████▏    | 4450/8564 [00:13<00:11, 359.13 examples/s]Tokenizing train dataset:  52%|█████▏    | 4480/8564 [00:13<00:11, 357.23 examples/s]Tokenizing train dataset:  52%|█████▏    | 4480/8564 [00:13<00:11, 357.95 examples/s]Tokenizing train dataset:  52%|█████▏    | 4489/8564 [00:13<00:11, 362.86 examples/s]Tokenizing train dataset:  53%|█████▎    | 4532/8564 [00:13<00:11, 350.35 examples/s]Tokenizing train dataset:  53%|█████▎    | 4532/8564 [00:13<00:11, 351.65 examples/s]Tokenizing train dataset:  53%|█████▎    | 4542/8564 [00:13<00:11, 352.84 examples/s]Tokenizing train dataset:  53%|█████▎    | 4570/8564 [00:13<00:11, 351.36 examples/s]Tokenizing train dataset:  53%|█████▎    | 4570/8564 [00:13<00:11, 352.04 examples/s]Tokenizing train dataset:  54%|█████▎    | 4595/8564 [00:13<00:11, 349.07 examples/s]Tokenizing train dataset:  54%|█████▍    | 4623/8564 [00:13<00:11, 349.72 examples/s]Tokenizing train dataset:  54%|█████▍    | 4623/8564 [00:13<00:11, 349.88 examples/s]Tokenizing train dataset:  54%|█████▍    | 4631/8564 [00:13<00:11, 349.18 examples/s]Tokenizing train dataset:  55%|█████▍    | 4671/8564 [00:13<00:11, 334.63 examples/s]Tokenizing train dataset:  55%|█████▍    | 4671/8564 [00:13<00:11, 334.92 examples/s]Tokenizing train dataset:  55%|█████▍    | 4678/8564 [00:13<00:11, 332.49 examples/s]Tokenizing train dataset:  55%|█████▌    | 4719/8564 [00:14<00:11, 324.31 examples/s]Tokenizing train dataset:  55%|█████▌    | 4718/8564 [00:14<00:11, 324.14 examples/s]Tokenizing train dataset:  55%|█████▌    | 4726/8564 [00:14<00:11, 324.19 examples/s]Tokenizing train dataset:  56%|█████▌    | 4767/8564 [00:14<00:11, 320.01 examples/s]Tokenizing train dataset:  56%|█████▌    | 4759/8564 [00:14<00:11, 322.82 examples/s]Tokenizing train dataset:  56%|█████▌    | 4766/8564 [00:14<00:11, 320.26 examples/s]Tokenizing train dataset:  56%|█████▌    | 4803/8564 [00:14<00:11, 327.16 examples/s]Tokenizing train dataset:  56%|█████▌    | 4800/8564 [00:14<00:11, 321.38 examples/s]Tokenizing train dataset:  56%|█████▌    | 4816/8564 [00:14<00:11, 339.93 examples/s]Tokenizing train dataset:  57%|█████▋    | 4865/8564 [00:14<00:09, 395.78 examples/s]Tokenizing train dataset:  57%|█████▋    | 4863/8564 [00:14<00:09, 394.64 examples/s]Tokenizing train dataset:  57%|█████▋    | 4879/8564 [00:14<00:09, 404.16 examples/s]Tokenizing train dataset:  58%|█████▊    | 4930/8564 [00:14<00:07, 457.10 examples/s]Tokenizing train dataset:  57%|█████▋    | 4922/8564 [00:14<00:08, 444.55 examples/s]Tokenizing train dataset:  58%|█████▊    | 4943/8564 [00:14<00:07, 457.33 examples/s]Tokenizing train dataset:  58%|█████▊    | 4989/8564 [00:14<00:07, 487.56 examples/s]Tokenizing train dataset:  58%|█████▊    | 4984/8564 [00:14<00:07, 485.54 examples/s]Tokenizing train dataset:  58%|█████▊    | 5004/8564 [00:14<00:07, 493.38 examples/s]Tokenizing train dataset:  59%|█████▉    | 5049/8564 [00:14<00:06, 515.64 examples/s]Tokenizing train dataset:  59%|█████▉    | 5045/8564 [00:14<00:06, 514.12 examples/s]Tokenizing train dataset:  59%|█████▉    | 5068/8564 [00:14<00:06, 532.04 examples/s]Tokenizing train dataset:  60%|█████▉    | 5115/8564 [00:14<00:06, 554.33 examples/s]Tokenizing train dataset:  60%|█████▉    | 5112/8564 [00:14<00:06, 554.01 examples/s]Tokenizing train dataset:  60%|█████▉    | 5132/8564 [00:14<00:06, 559.59 examples/s]Tokenizing train dataset:  61%|██████    | 5186/8564 [00:14<00:05, 588.82 examples/s]Tokenizing train dataset:  61%|██████    | 5182/8564 [00:14<00:05, 591.76 examples/s]Tokenizing train dataset:  61%|██████    | 5203/8564 [00:14<00:05, 597.01 examples/s]Tokenizing train dataset:  61%|██████▏   | 5260/8564 [00:15<00:05, 627.08 examples/s]Tokenizing train dataset:  61%|██████▏   | 5251/8564 [00:15<00:05, 616.06 examples/s]Tokenizing train dataset:  62%|██████▏   | 5279/8564 [00:15<00:05, 639.82 examples/s]Tokenizing train dataset:  62%|██████▏   | 5324/8564 [00:15<00:05, 629.10 examples/s]Tokenizing train dataset:  62%|██████▏   | 5320/8564 [00:15<00:05, 634.84 examples/s]Tokenizing train dataset:  63%|██████▎   | 5369/8564 [00:15<00:05, 621.19 examples/s]Tokenizing train dataset:  63%|██████▎   | 5409/8564 [00:15<00:05, 599.03 examples/s]Tokenizing train dataset:  63%|██████▎   | 5402/8564 [00:15<00:05, 598.40 examples/s]Tokenizing train dataset:  64%|██████▍   | 5473/8564 [00:15<00:05, 604.75 examples/s]Tokenizing train dataset:  64%|██████▍   | 5461/8564 [00:15<00:05, 613.50 examples/s]Tokenizing train dataset:  64%|██████▍   | 5470/8564 [00:15<00:05, 611.97 examples/s]Tokenizing train dataset:  65%|██████▍   | 5536/8564 [00:15<00:04, 609.16 examples/s]Tokenizing train dataset:  65%|██████▍   | 5533/8564 [00:15<00:04, 609.56 examples/s]Tokenizing train dataset:  65%|██████▍   | 5548/8564 [00:15<00:05, 600.56 examples/s]Tokenizing train dataset:  66%|██████▌   | 5631/8564 [00:15<00:04, 612.74 examples/s]Tokenizing train dataset:  66%|██████▌   | 5612/8564 [00:15<00:04, 609.19 examples/s]Tokenizing train dataset:  66%|██████▌   | 5627/8564 [00:15<00:04, 614.29 examples/s]Tokenizing train dataset:  67%|██████▋   | 5698/8564 [00:15<00:04, 621.81 examples/s]Tokenizing train dataset:  66%|██████▋   | 5677/8564 [00:15<00:04, 614.55 examples/s]Tokenizing train dataset:  66%|██████▋   | 5691/8564 [00:15<00:04, 614.73 examples/s]Tokenizing train dataset:  67%|██████▋   | 5766/8564 [00:15<00:04, 634.54 examples/s]Tokenizing train dataset:  67%|██████▋   | 5741/8564 [00:15<00:04, 619.08 examples/s]Tokenizing train dataset:  67%|██████▋   | 5760/8564 [00:15<00:04, 628.45 examples/s]Tokenizing train dataset:  68%|██████▊   | 5836/8564 [00:15<00:04, 650.08 examples/s]Tokenizing train dataset:  68%|██████▊   | 5820/8564 [00:15<00:04, 662.47 examples/s]Tokenizing train dataset:  68%|██████▊   | 5835/8564 [00:15<00:04, 652.95 examples/s]Tokenizing train dataset:  69%|██████▉   | 5921/8564 [00:16<00:04, 616.76 examples/s]Tokenizing train dataset:  69%|██████▉   | 5900/8564 [00:16<00:04, 607.97 examples/s]Tokenizing train dataset:  69%|██████▉   | 5921/8564 [00:16<00:04, 617.98 examples/s]Tokenizing train dataset:  70%|██████▉   | 5970/8564 [00:16<00:04, 628.92 examples/s]Tokenizing train dataset:  70%|███████   | 6014/8564 [00:16<00:04, 579.42 examples/s]Tokenizing train dataset:  70%|███████   | 6014/8564 [00:16<00:04, 574.77 examples/s]Tokenizing train dataset:  71%|███████   | 6042/8564 [00:16<00:04, 568.59 examples/s]Tokenizing train dataset:  71%|███████   | 6074/8564 [00:16<00:04, 580.97 examples/s]Tokenizing train dataset:  71%|███████   | 6074/8564 [00:16<00:04, 577.44 examples/s]Tokenizing train dataset:  72%|███████▏  | 6132/8564 [00:16<00:04, 576.00 examples/s]Tokenizing train dataset:  72%|███████▏  | 6173/8564 [00:16<00:03, 604.90 examples/s]Tokenizing train dataset:  72%|███████▏  | 6173/8564 [00:16<00:03, 602.46 examples/s]Tokenizing train dataset:  72%|███████▏  | 6208/8564 [00:16<00:03, 617.84 examples/s]Tokenizing train dataset:  73%|███████▎  | 6241/8564 [00:16<00:03, 619.46 examples/s]Tokenizing train dataset:  73%|███████▎  | 6241/8564 [00:16<00:03, 617.69 examples/s]Tokenizing train dataset:  73%|███████▎  | 6277/8564 [00:16<00:03, 632.95 examples/s]Tokenizing train dataset:  74%|███████▎  | 6306/8564 [00:16<00:03, 624.90 examples/s]Tokenizing train dataset:  74%|███████▎  | 6306/8564 [00:16<00:03, 623.76 examples/s]Tokenizing train dataset:  74%|███████▍  | 6377/8564 [00:16<00:03, 646.60 examples/s]Tokenizing train dataset:  74%|███████▍  | 6377/8564 [00:16<00:03, 645.44 examples/s]Tokenizing train dataset:  74%|███████▍  | 6379/8564 [00:16<00:03, 644.19 examples/s]Tokenizing train dataset:  76%|███████▌  | 6469/8564 [00:17<00:03, 630.22 examples/s]Tokenizing train dataset:  76%|███████▌  | 6469/8564 [00:16<00:03, 629.57 examples/s]Tokenizing train dataset:  76%|███████▌  | 6470/8564 [00:16<00:03, 623.26 examples/s]Tokenizing train dataset:  77%|███████▋  | 6559/8564 [00:17<00:03, 618.81 examples/s]Tokenizing train dataset:  76%|███████▋  | 6535/8564 [00:17<00:03, 619.41 examples/s]Tokenizing train dataset:  77%|███████▋  | 6559/8564 [00:17<00:03, 618.55 examples/s]Tokenizing train dataset:  77%|███████▋  | 6634/8564 [00:17<00:03, 574.30 examples/s]Tokenizing train dataset:  77%|███████▋  | 6613/8564 [00:17<00:03, 574.15 examples/s]Tokenizing train dataset:  77%|███████▋  | 6634/8564 [00:17<00:03, 574.61 examples/s]Tokenizing train dataset:  78%|███████▊  | 6705/8564 [00:17<00:03, 602.25 examples/s]Tokenizing train dataset:  78%|███████▊  | 6678/8564 [00:17<00:03, 591.45 examples/s]Tokenizing train dataset:  78%|███████▊  | 6705/8564 [00:17<00:03, 602.83 examples/s]Tokenizing train dataset:  79%|███████▉  | 6771/8564 [00:17<00:02, 609.11 examples/s]Tokenizing train dataset:  79%|███████▉  | 6749/8564 [00:17<00:02, 615.00 examples/s]Tokenizing train dataset:  79%|███████▉  | 6771/8564 [00:17<00:02, 609.71 examples/s]Tokenizing train dataset:  80%|████████  | 6854/8564 [00:17<00:02, 585.84 examples/s]Tokenizing train dataset:  80%|███████▉  | 6835/8564 [00:17<00:02, 596.23 examples/s]Tokenizing train dataset:  80%|████████  | 6854/8564 [00:17<00:02, 586.70 examples/s]Tokenizing train dataset:  81%|████████  | 6925/8564 [00:17<00:02, 613.67 examples/s]Tokenizing train dataset:  81%|████████  | 6899/8564 [00:17<00:02, 605.14 examples/s]Tokenizing train dataset:  81%|████████  | 6925/8564 [00:17<00:02, 614.67 examples/s]Tokenizing train dataset:  81%|████████▏ | 6964/8564 [00:17<00:02, 610.45 examples/s]Tokenizing train dataset:  82%|████████▏ | 7013/8564 [00:17<00:02, 598.55 examples/s]Tokenizing train dataset:  82%|████████▏ | 7013/8564 [00:17<00:02, 599.45 examples/s]Tokenizing train dataset:  83%|████████▎ | 7074/8564 [00:18<00:02, 599.03 examples/s]Tokenizing train dataset:  82%|████████▏ | 7051/8564 [00:17<00:02, 594.62 examples/s]Tokenizing train dataset:  83%|████████▎ | 7108/8564 [00:18<00:02, 606.45 examples/s]Tokenizing train dataset:  83%|████████▎ | 7141/8564 [00:18<00:02, 613.72 examples/s]Tokenizing train dataset:  83%|████████▎ | 7119/8564 [00:18<00:02, 611.75 examples/s]Tokenizing train dataset:  84%|████████▍ | 7173/8564 [00:18<00:02, 612.88 examples/s]Tokenizing train dataset:  84%|████████▍ | 7210/8564 [00:18<00:02, 629.29 examples/s]Tokenizing train dataset:  84%|████████▍ | 7187/8564 [00:18<00:02, 621.91 examples/s]Tokenizing train dataset:  85%|████████▍ | 7262/8564 [00:18<00:02, 603.25 examples/s]Tokenizing train dataset:  85%|████████▌ | 7294/8564 [00:18<00:02, 601.38 examples/s]Tokenizing train dataset:  85%|████████▌ | 7280/8564 [00:18<00:02, 613.08 examples/s]Tokenizing train dataset:  86%|████████▌ | 7326/8564 [00:18<00:02, 608.82 examples/s]Tokenizing train dataset:  86%|████████▌ | 7360/8564 [00:18<00:01, 613.12 examples/s]Tokenizing train dataset:  86%|████████▌ | 7376/8564 [00:18<00:01, 612.50 examples/s]Tokenizing train dataset:  87%|████████▋ | 7421/8564 [00:18<00:01, 613.07 examples/s]Tokenizing train dataset:  87%|████████▋ | 7456/8564 [00:18<00:01, 618.02 examples/s]Tokenizing train dataset:  87%|████████▋ | 7440/8564 [00:18<00:01, 618.57 examples/s]Tokenizing train dataset:  87%|████████▋ | 7485/8564 [00:18<00:01, 617.71 examples/s]Tokenizing train dataset:  88%|████████▊ | 7503/8564 [00:18<00:01, 617.32 examples/s]Tokenizing train dataset:  88%|████████▊ | 7553/8564 [00:18<00:01, 621.72 examples/s]Tokenizing train dataset:  88%|████████▊ | 7553/8564 [00:18<00:01, 627.36 examples/s]Tokenizing train dataset:  88%|████████▊ | 7575/8564 [00:18<00:01, 638.96 examples/s]Tokenizing train dataset:  89%|████████▉ | 7625/8564 [00:18<00:01, 642.56 examples/s]Tokenizing train dataset:  89%|████████▉ | 7625/8564 [00:18<00:01, 649.03 examples/s]Tokenizing train dataset:  89%|████████▉ | 7661/8564 [00:18<00:01, 614.54 examples/s]Tokenizing train dataset:  90%|████████▉ | 7707/8564 [00:19<00:01, 607.29 examples/s]Tokenizing train dataset:  90%|████████▉ | 7707/8564 [00:19<00:01, 609.21 examples/s]Tokenizing train dataset:  90%|█████████ | 7750/8564 [00:19<00:01, 603.44 examples/s]Tokenizing train dataset:  91%|█████████ | 7790/8564 [00:19<00:01, 582.39 examples/s]Tokenizing train dataset:  91%|█████████ | 7790/8564 [00:19<00:01, 583.00 examples/s]Tokenizing train dataset:  92%|█████████▏| 7854/8564 [00:19<00:01, 588.77 examples/s]Tokenizing train dataset:  91%|█████████▏| 7832/8564 [00:19<00:01, 580.67 examples/s]Tokenizing train dataset:  92%|█████████▏| 7854/8564 [00:19<00:01, 589.68 examples/s]Tokenizing train dataset:  92%|█████████▏| 7915/8564 [00:19<00:01, 590.99 examples/s]Tokenizing train dataset:  92%|█████████▏| 7896/8564 [00:19<00:01, 594.33 examples/s]Tokenizing train dataset:  92%|█████████▏| 7916/8564 [00:19<00:01, 591.71 examples/s]Tokenizing train dataset:  93%|█████████▎| 7975/8564 [00:19<00:00, 591.90 examples/s]Tokenizing train dataset:  93%|█████████▎| 7977/8564 [00:19<00:00, 596.13 examples/s]Tokenizing train dataset:  93%|█████████▎| 7990/8564 [00:19<00:00, 600.61 examples/s]Tokenizing train dataset:  94%|█████████▍| 8041/8564 [00:19<00:00, 608.25 examples/s]Tokenizing train dataset:  94%|█████████▍| 8045/8564 [00:19<00:00, 612.55 examples/s]Tokenizing train dataset:  94%|█████████▍| 8052/8564 [00:19<00:00, 601.90 examples/s]Tokenizing train dataset:  95%|█████████▍| 8123/8564 [00:19<00:00, 577.39 examples/s]Tokenizing train dataset:  95%|█████████▍| 8123/8564 [00:19<00:00, 576.81 examples/s]Tokenizing train dataset:  95%|█████████▍| 8134/8564 [00:19<00:00, 576.94 examples/s]Tokenizing train dataset:  96%|█████████▌| 8190/8564 [00:19<00:00, 599.28 examples/s]Tokenizing train dataset:  96%|█████████▌| 8190/8564 [00:19<00:00, 599.00 examples/s]Tokenizing train dataset:  96%|█████████▌| 8201/8564 [00:19<00:00, 596.03 examples/s]Tokenizing train dataset:  96%|█████████▋| 8251/8564 [00:19<00:00, 600.45 examples/s]Tokenizing train dataset:  96%|█████████▋| 8253/8564 [00:19<00:00, 602.60 examples/s]Tokenizing train dataset:  97%|█████████▋| 8270/8564 [00:19<00:00, 615.69 examples/s]Tokenizing train dataset:  97%|█████████▋| 8327/8564 [00:20<00:00, 642.57 examples/s]Tokenizing train dataset:  97%|█████████▋| 8329/8564 [00:20<00:00, 636.71 examples/s]Tokenizing train dataset:  97%|█████████▋| 8339/8564 [00:20<00:00, 629.41 examples/s]Tokenizing train dataset:  98%|█████████▊| 8410/8564 [00:20<00:00, 600.24 examples/s]Tokenizing train dataset:  98%|█████████▊| 8411/8564 [00:20<00:00, 599.51 examples/s]Tokenizing train dataset:  98%|█████████▊| 8426/8564 [00:20<00:00, 608.00 examples/s]Tokenizing train dataset:  99%|█████████▉| 8474/8564 [00:20<00:00, 607.91 examples/s]Tokenizing train dataset:  99%|█████████▉| 8477/8564 [00:20<00:00, 609.93 examples/s]Tokenizing train dataset:  99%|█████████▉| 8519/8564 [00:20<00:00, 607.27 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 604.24 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 415.31 examples/s]
Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 415.64 examples/s]
Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 396.35 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 413.39 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s][2025-05-31 11:18:40,780] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s][2025-05-31 11:18:40,827] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 16
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 10579.59 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 10590.08 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 10407.43 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13008.70 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13013.19 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 12877.36 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 322.82 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 323.06 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 323.97 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 292.84 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 290.67 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 291.43 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:03, 274.58 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 277.56 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 278.06 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 264.97 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.52 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 268.02 examples/s]Tokenizing eval dataset:  21%|██        | 196/953 [00:00<00:02, 254.27 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 252.85 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.29 examples/s]Tokenizing eval dataset:  25%|██▍       | 235/953 [00:00<00:02, 284.50 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:02, 277.45 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:02, 277.83 examples/s]Tokenizing eval dataset:  32%|███▏      | 303/953 [00:00<00:01, 383.50 examples/s]Tokenizing eval dataset:  31%|███       | 296/953 [00:00<00:01, 374.61 examples/s]Tokenizing eval dataset:  31%|███       | 297/953 [00:00<00:01, 376.64 examples/s]Tokenizing eval dataset:  38%|███▊      | 363/953 [00:01<00:01, 440.92 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 440.04 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 440.51 examples/s]Tokenizing eval dataset:  45%|████▌     | 432/953 [00:01<00:01, 505.63 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 498.81 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 499.35 examples/s]Tokenizing eval dataset:  52%|█████▏    | 500/953 [00:01<00:00, 547.72 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 538.72 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 539.34 examples/s]Tokenizing eval dataset:  59%|█████▉    | 564/953 [00:01<00:00, 571.62 examples/s]Tokenizing eval dataset:  59%|█████▉    | 561/953 [00:01<00:00, 574.21 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 571.62 examples/s]Tokenizing eval dataset:  66%|██████▌   | 630/953 [00:01<00:00, 593.89 examples/s]Tokenizing eval dataset:  66%|██████▌   | 626/953 [00:01<00:00, 593.45 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 594.85 examples/s]Tokenizing eval dataset:  73%|███████▎  | 693/953 [00:01<00:00, 599.57 examples/s]Tokenizing eval dataset:  72%|███████▏  | 687/953 [00:01<00:00, 594.58 examples/s]Tokenizing eval dataset:  72%|███████▏  | 689/953 [00:01<00:00, 589.61 examples/s]Tokenizing eval dataset:  81%|████████  | 769/953 [00:01<00:00, 554.86 examples/s]Tokenizing eval dataset:  80%|███████▉  | 758/953 [00:01<00:00, 542.25 examples/s]Tokenizing eval dataset:  80%|███████▉  | 762/953 [00:01<00:00, 543.19 examples/s]Tokenizing eval dataset:  88%|████████▊ | 834/953 [00:01<00:00, 504.76 examples/s]Tokenizing eval dataset:  87%|████████▋ | 825/953 [00:01<00:00, 506.69 examples/s]Tokenizing eval dataset:  87%|████████▋ | 828/953 [00:01<00:00, 502.49 examples/s]Tokenizing eval dataset:  95%|█████████▍| 905/953 [00:02<00:00, 484.36 examples/s]Tokenizing eval dataset:  93%|█████████▎| 891/953 [00:02<00:00, 481.15 examples/s]Tokenizing eval dataset:  94%|█████████▍| 899/953 [00:02<00:00, 485.86 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 443.51 examples/s]
[2025-05-31 11:18:43,924] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 16
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 469.87 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 441.21 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 468.46 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 441.41 examples/s]
[2025-05-31 11:18:43,993] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 16
[2025-05-31 11:18:44,008] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 16
[2025-05-31 11:18:57,226] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-05-31 11:18:57,228] [INFO] [logging.py:128:log_dist] [Rank 0] Creating BF16 optimizer
[2025-05-31 11:18:57,415] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer
[2025-05-31 11:18:57,416] [INFO] [utils.py:782:see_memory_usage] MA 17.21 GB         Max_MA 17.21 GB         CA 17.21 GB         Max_CA 17 GB 
[2025-05-31 11:18:57,416] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 170.21 GB, percent = 33.8%
[2025-05-31 11:18:57,601] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer
[2025-05-31 11:18:57,602] [INFO] [utils.py:782:see_memory_usage] MA 17.21 GB         Max_MA 17.21 GB         CA 17.21 GB         Max_CA 17 GB 
[2025-05-31 11:18:57,602] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 170.22 GB, percent = 33.8%
[2025-05-31 11:18:57,604] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-05-31 11:18:57,604] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2025-05-31 11:18:57,604] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-05-31 11:18:57,604] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-05-31 11:18:57,604] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-05-31 11:18:57,604] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2025-05-31 11:18:57,604] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-05-31 11:18:57,604] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-05-31 11:18:57,604] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fdf68d5e9b0>
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-05-31 11:18:57,605] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 1
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-05-31 11:18:57,606] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   optimizer_name ............... None
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   optimizer_params ............. None
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   scheduler_name ............... None
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   scheduler_params ............. None
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-05-31 11:18:57,607] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   train_batch_size ............. 16
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  1
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   world_size ................... 16
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  True
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   zero_enabled ................. False
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-05-31 11:18:57,608] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 0
[2025-05-31 11:18:57,608] [INFO] [config.py:991:print_user_config]   json = {
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 0, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true, 
        "round_robin_gradients": true
    }, 
    "steps_per_print": inf
}
Set up DPO trainer
[rank3]: Traceback (most recent call last):
[rank3]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 207, in <module>
[rank3]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank3]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 186, in main
[rank3]:     dpo_trainer.train()
[rank3]:   File "/transformers/src/transformers/trainer.py", line 2189, in train
[rank3]:     self._move_model_to_device(self.model, args.device)
[rank3]:   File "/transformers/src/transformers/trainer.py", line 901, in _move_model_to_device
[rank3]:     model = model.to(device)
[rank3]:   File "/transformers/src/transformers/modeling_utils.py", line 3260, in to
[rank3]:     return super().to(*args, **kwargs)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1343, in to
[rank3]:     return self._apply(convert)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank3]:     module._apply(fn)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank3]:     module._apply(fn)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank3]:     module._apply(fn)
[rank3]:   [Previous line repeated 2 more times]
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 930, in _apply
[rank3]:     param_applied = fn(param)
[rank3]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1329, in convert
[rank3]:     return t.to(
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 3 has a total capacity of 39.50 GiB of which 56.12 MiB is free. Including non-PyTorch memory, this process has 39.43 GiB memory in use. Of the allocated memory 38.71 GiB is allocated by PyTorch, and 1.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 207, in <module>
[rank1]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank1]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 186, in main
[rank1]:     dpo_trainer.train()
[rank1]:   File "/transformers/src/transformers/trainer.py", line 2189, in train
[rank1]:     self._move_model_to_device(self.model, args.device)
[rank1]:   File "/transformers/src/transformers/trainer.py", line 901, in _move_model_to_device
[rank1]:     model = model.to(device)
[rank1]:   File "/transformers/src/transformers/modeling_utils.py", line 3260, in to
[rank1]:     return super().to(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1343, in to
[rank1]:     return self._apply(convert)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank1]:     module._apply(fn)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank1]:     module._apply(fn)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank1]:     module._apply(fn)
[rank1]:   [Previous line repeated 2 more times]
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 930, in _apply
[rank1]:     param_applied = fn(param)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1329, in convert
[rank1]:     return t.to(
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 1 has a total capacity of 39.50 GiB of which 8.12 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.71 GiB is allocated by PyTorch, and 1.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 207, in <module>
[rank0]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank0]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 186, in main
[rank0]:     dpo_trainer.train()
[rank0]:   File "/transformers/src/transformers/trainer.py", line 2189, in train
[rank0]:     self._move_model_to_device(self.model, args.device)
[rank0]:   File "/transformers/src/transformers/trainer.py", line 901, in _move_model_to_device
[rank0]:     model = model.to(device)
[rank0]:   File "/transformers/src/transformers/modeling_utils.py", line 3260, in to
[rank0]:     return super().to(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1343, in to
[rank0]:     return self._apply(convert)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank0]:     module._apply(fn)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank0]:     module._apply(fn)
[rank0]:   [Previous line repeated 2 more times]
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 930, in _apply
[rank0]:     param_applied = fn(param)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1329, in convert
[rank0]:     return t.to(
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 56.12 MiB is free. Including non-PyTorch memory, this process has 39.43 GiB memory in use. Of the allocated memory 38.71 GiB is allocated by PyTorch, and 1.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 207, in <module>
[rank2]:     main(train_data, val_data, args.rank, args.learning_rate, args.total_epochs, args.beta)
[rank2]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train.py", line 186, in main
[rank2]:     dpo_trainer.train()
[rank2]:   File "/transformers/src/transformers/trainer.py", line 2189, in train
[rank2]:     self._move_model_to_device(self.model, args.device)
[rank2]:   File "/transformers/src/transformers/trainer.py", line 901, in _move_model_to_device
[rank2]:     model = model.to(device)
[rank2]:   File "/transformers/src/transformers/modeling_utils.py", line 3260, in to
[rank2]:     return super().to(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1343, in to
[rank2]:     return self._apply(convert)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank2]:     module._apply(fn)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank2]:     module._apply(fn)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 903, in _apply
[rank2]:     module._apply(fn)
[rank2]:   [Previous line repeated 2 more times]
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 930, in _apply
[rank2]:     param_applied = fn(param)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1329, in convert
[rank2]:     return t.to(
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 2 has a total capacity of 39.50 GiB of which 8.12 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.71 GiB is allocated by PyTorch, and 1.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W531 11:19:27.588301988 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0531 11:19:30.206000 1229409 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1229601 closing signal SIGTERM
W0531 11:19:30.207000 1229409 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1229602 closing signal SIGTERM
W0531 11:19:30.207000 1229409 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1229604 closing signal SIGTERM
E0531 11:19:30.635000 1229409 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 1229603) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1182, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 861, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-31_11:19:30
  host      : pm5-nod00.vega.pri
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1229603)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
