# Note: Thist test was generated by Gemini 2.5 Pro

from transformers import AutoTokenizer
import json

# --- 1. Load your tokenizer (same as in your training script) ---
model_path = "cjvt/GaMS-9B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_path, legacy=False, add_eos_token=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    print(f"Set pad_token to eos_token: {tokenizer.eos_token}")

print(f"Tokenizer: {tokenizer.__class__.__name__}")
print(f"BOS token: {tokenizer.bos_token}, ID: {tokenizer.bos_token_id}")
print(f"EOS token: {tokenizer.eos_token}, ID: {tokenizer.eos_token_id}")
print(f"PAD token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}")
print(f"UNK token: {tokenizer.unk_token}, ID: {tokenizer.unk_token_id}")

# --- 2. Get a sample data point (similar to your load_data.py) ---
# Using a dummy example for illustration. Replace with one from your actual data.
sample_data_json = {
    "prompt": [{"role": "user", "content": "What color is the sky today?"}],
    "chosen": [{"role": "assistant", "content": "The sky is a brilliant blue with a few fluffy clouds."}],
    "rejected": [{"role": "assistant", "content": "It's green, obviously."}]
}
# If loading from your file:
# with open("/path/to/your/bad_lang_examples.jsonl", 'r') as f:
#     sample_data_json = json.loads(f.readline())
#     if "src" in sample_data_json:
#         del sample_data_json["src"]


# --- 3. Test apply_chat_template ---

print("\n--- Testing Chat Template Application ---")

# Check if a chat template is defined
if tokenizer.chat_template:
    print(f"Chat template found:\n{tokenizer.chat_template}\n")
else:
    print("WARNING: No chat_template explicitly found in this tokenizer. "
          "apply_chat_template might use a default or model-specific hardcoded one, "
          "or it might not format complex chats well.\n")

# How DPOTrainer structures data for loss calculation:
# It needs:
# 1. prompt_string (input to the policy model, prompting for generation)
# 2. chosen_string (prompt + chosen response)
# 3. rejected_string (prompt + rejected response)

prompt_messages = sample_data_json["prompt"]
chosen_messages = sample_data_json["prompt"] + sample_data_json["chosen"]
rejected_messages = sample_data_json["prompt"] + sample_data_json["rejected"]

# Format the "prompt" part that the policy model will see to generate a response
# `add_generation_prompt=True` is typically used to add the necessary tokens
# (e.g., assistant prefix) to signal the model to start generating.
try:
    prompt_for_generation_str = tokenizer.apply_chat_template(
        prompt_messages,
        tokenize=False,
        add_generation_prompt=True # Important for cueing the model
    )
    print(f"Formatted 'Prompt for Generation':\n```\n{prompt_for_generation_str}\n```\n")

    # Format the full "chosen" sequence (prompt + chosen response)
    # For completed sequences, add_generation_prompt is usually False (default)
    chosen_full_str = tokenizer.apply_chat_template(
        chosen_messages,
        tokenize=False
    )
    print(f"Formatted 'Chosen Full Sequence':\n```\n{chosen_full_str}\n```\n")

    # Format the full "rejected" sequence (prompt + rejected response)
    rejected_full_str = tokenizer.apply_chat_template(
        rejected_messages,
        tokenize=False
    )
    print(f"Formatted 'Rejected Full Sequence':\n```\n{rejected_full_str}\n```\n")

    print("--- Tokenization Check (first 10 tokens and decoded) ---")
    prompt_tokens = tokenizer.encode(prompt_for_generation_str)
    chosen_tokens = tokenizer.encode(chosen_full_str)
    rejected_tokens = tokenizer.encode(rejected_full_str)

    print(f"Tokenized Prompt for Gen (first 10): {prompt_tokens[:10]}")
    print(f"Decoded from tokens: {tokenizer.decode(prompt_tokens)}\n")

    print(f"Tokenized Chosen Full (first 10): {chosen_tokens[:10]}")
    print(f"Decoded from tokens: {tokenizer.decode(chosen_tokens)}\n")

    print(f"Tokenized Rejected Full (first 10): {rejected_tokens[:10]}")
    print(f"Decoded from tokens: {tokenizer.decode(rejected_tokens)}\n")


except Exception as e:
    print(f"ERROR during apply_chat_template: {e}")
    print("This might indicate an issue with the tokenizer's chat template or "
          "how it handles the provided chat format.")
    print("You may need to manually pre-format your data into flat strings "
          "for 'prompt', 'chosen', and 'rejected' before creating the Dataset.")