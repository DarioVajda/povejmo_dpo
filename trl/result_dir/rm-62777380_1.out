cpu-bind=MASK - gn35, task  1  0 [888565]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 1 ---
Total Nodes: 4
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn34
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 4     --machine_rank 1     --main_process_ip gn34     --main_process_port 29500     --num_processes 16     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62777380     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.2 --curriculum_stage=0
-------------------------------------------
[2025-06-08 19:35:26,831] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0608 19:35:28.616000 888617 torch/distributed/run.py:792] 
W0608 19:35:28.616000 888617 torch/distributed/run.py:792] *****************************************
W0608 19:35:28.616000 888617 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0608 19:35:28.616000 888617 torch/distributed/run.py:792] *****************************************
[2025-06-08 19:35:38,869] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-08 19:35:38,925] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-08 19:35:38,930] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-08 19:35:38,938] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Training data of type 'bad_lang_examples':    3489
Training data of type 'short_examples':       699
Training data of type 'choose_examples':      13379
Training data of type 'bad_format_examples':  3148
**************************************************
Training data of type 'bad_lang_examples':   Training data of type 'bad_lang_examples':     34893489

Training data of type 'short_examples':      Training data of type 'short_examples':        699699

Training data of type 'choose_examples':     Training data of type 'choose_examples':       Training data of type 'bad_lang_examples':   1337913379 

3489Training data of type 'bad_format_examples': Training data of type 'bad_format_examples': 
  Training data of type 'short_examples':      31483148 

699****************************************************************************************************


Training data of type 'choose_examples':      13379
Training data of type 'bad_format_examples':  3148
**************************************************
Evaluation data size: 953
Evaluation data size: 953
Evaluation data size:Evaluation data size:  953953

Curriculum stage 0 training data size: 7336
Curriculum stage 0 training data size: 7336
Curriculum stage 0 training data size: 7336
Curriculum stage 0 training data size: 7336
Curriculum stage 1 training data size: 6689
Curriculum stage 1 training data size: 6689
Curriculum stage 1 training data size:Curriculum stage 1 training data size:  66896689

Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
World size: 16
Setting gradient accumulation steps to: 1
Curriculum stage 2 training data size: 6690
Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
[2025-06-08 19:35:42,852] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 7336
Validation dataset size: 953
Steps per epoch: 458
Evaluate each 229 steps
[2025-06-08 19:35:42,858] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-08 19:35:42,882] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-08 19:35:42,884] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: cjvt/GaMS-9B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:12, 24.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:45<00:44, 22.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:45<00:45, 22.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:45<00:45, 22.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:45<00:45, 22.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:10<00:23, 23.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:10<00:23, 23.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:10<00:23, 23.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:10<00:23, 23.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 20.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.55s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 20.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.61s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 20.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.61s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 20.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.61s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank6]:[W608 19:37:14.241290950 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W608 19:37:14.446339986 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W608 19:37:14.505366334 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Extracting prompt in train dataset:   7%|▋         | 550/7336 [00:00<00:01, 5431.81 examples/s]Extracting prompt in train dataset:  15%|█▌        | 1120/7336 [00:00<00:01, 5561.92 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1690/7336 [00:00<00:01, 5602.15 examples/s]Extracting prompt in train dataset:  31%|███       | 2280/7336 [00:00<00:00, 5698.82 examples/s]Extracting prompt in train dataset:  43%|████▎     | 3120/7336 [00:00<00:00, 5636.23 examples/s]Extracting prompt in train dataset:  51%|█████     | 3707/7336 [00:00<00:00, 5705.77 examples/s]Extracting prompt in train dataset:  59%|█████▊    | 4300/7336 [00:00<00:00, 5755.29 examples/s]Extracting prompt in train dataset:  67%|██████▋   | 4930/7336 [00:00<00:00, 5904.75 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 5558/7336 [00:00<00:00, 6015.75 examples/s]Extracting prompt in train dataset:  84%|████████▍ | 6184/7336 [00:01<00:00, 6073.06 examples/s]Extracting prompt in train dataset:  93%|█████████▎| 6810/7336 [00:01<00:00, 6112.96 examples/s]Extracting prompt in train dataset: 100%|██████████| 7336/7336 [00:01<00:00, 5825.99 examples/s]
Applying chat template to train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 290/7336 [00:00<00:02, 2858.38 examples/s]Applying chat template to train dataset:   8%|▊         | 611/7336 [00:00<00:02, 3058.61 examples/s]Applying chat template to train dataset:  13%|█▎        | 934/7336 [00:00<00:02, 3132.90 examples/s]Applying chat template to train dataset:  17%|█▋        | 1251/7336 [00:00<00:01, 3143.29 examples/s]Applying chat template to train dataset:  21%|██▏       | 1566/7336 [00:00<00:01, 3143.14 examples/s]Applying chat template to train dataset:  26%|██▌       | 1891/7336 [00:00<00:01, 3176.29 examples/s]Applying chat template to train dataset:  30%|███       | 2216/7336 [00:00<00:01, 3199.08 examples/s]Applying chat template to train dataset:  35%|███▍      | 2544/7336 [00:00<00:01, 3218.17 examples/s]Applying chat template to train dataset:  41%|████      | 3010/7336 [00:00<00:01, 3166.65 examples/s]Applying chat template to train dataset:  45%|████▌     | 3335/7336 [00:01<00:01, 3188.16 examples/s]Applying chat template to train dataset:  50%|████▉     | 3660/7336 [00:01<00:01, 3198.21 examples/s]Applying chat template to train dataset:  54%|█████▍    | 3987/7336 [00:01<00:01, 3217.78 examples/s]Applying chat template to train dataset:  59%|█████▉    | 4320/7336 [00:01<00:00, 3243.59 examples/s]Applying chat template to train dataset:  64%|██████▎   | 4664/7336 [00:01<00:00, 3297.48 examples/s]Applying chat template to train dataset:  68%|██████▊   | 5008/7336 [00:01<00:00, 3337.54 examples/s]Applying chat template to train dataset:  73%|███████▎  | 5351/7336 [00:01<00:00, 3359.31 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5694/7336 [00:01<00:00, 3377.64 examples/s]Applying chat template to train dataset:  82%|████████▏ | 6038/7336 [00:01<00:00, 3392.71 examples/s]Applying chat template to train dataset:  87%|████████▋ | 6382/7336 [00:01<00:00, 3399.42 examples/s]Applying chat template to train dataset:  94%|█████████▍| 6879/7336 [00:02<00:00, 3360.40 examples/s]Applying chat template to train dataset:  98%|█████████▊| 7218/7336 [00:02<00:00, 3361.49 examples/s]Applying chat template to train dataset: 100%|██████████| 7336/7336 [00:02<00:00, 3258.63 examples/s]
Tokenizing train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 41/7336 [00:00<00:18, 393.07 examples/s]Tokenizing train dataset:   1%|          | 89/7336 [00:00<00:21, 337.36 examples/s]Tokenizing train dataset:   2%|▏         | 133/7336 [00:00<00:23, 312.17 examples/s]Tokenizing train dataset:   2%|▏         | 165/7336 [00:00<00:23, 308.82 examples/s]Tokenizing train dataset:   3%|▎         | 212/7336 [00:00<00:23, 304.24 examples/s]Tokenizing train dataset:   3%|▎         | 247/7336 [00:00<00:22, 314.81 examples/s]Tokenizing train dataset:   4%|▍         | 281/7336 [00:00<00:21, 321.15 examples/s]Tokenizing train dataset:   4%|▍         | 314/7336 [00:00<00:21, 319.80 examples/s]Tokenizing train dataset:   5%|▍         | 362/7336 [00:01<00:22, 313.24 examples/s]Tokenizing train dataset:   5%|▌         | 397/7336 [00:01<00:21, 315.60 examples/s]Tokenizing train dataset:   6%|▌         | 430/7336 [00:01<00:21, 316.46 examples/s]Tokenizing train dataset:   6%|▋         | 474/7336 [00:01<00:22, 305.23 examples/s]Tokenizing train dataset:   7%|▋         | 517/7336 [00:01<00:22, 297.83 examples/s]Tokenizing train dataset:   8%|▊         | 551/7336 [00:01<00:22, 304.00 examples/s]Tokenizing train dataset:   8%|▊         | 584/7336 [00:01<00:22, 305.60 examples/s]Tokenizing train dataset:   9%|▊         | 636/7336 [00:02<00:21, 313.25 examples/s]Tokenizing train dataset:   9%|▉         | 680/7336 [00:02<00:21, 303.93 examples/s]Tokenizing train dataset:  10%|▉         | 727/7336 [00:02<00:21, 303.48 examples/s]Tokenizing train dataset:  10%|█         | 760/7336 [00:02<00:21, 306.84 examples/s]Tokenizing train dataset:  11%|█         | 800/7336 [00:02<00:22, 291.62 examples/s]Tokenizing train dataset:  12%|█▏        | 844/7336 [00:02<00:22, 289.92 examples/s]Tokenizing train dataset:  12%|█▏        | 874/7336 [00:02<00:22, 290.99 examples/s]Tokenizing train dataset:  12%|█▏        | 911/7336 [00:02<00:20, 308.86 examples/s]Tokenizing train dataset:  13%|█▎        | 954/7336 [00:03<00:21, 298.99 examples/s]Tokenizing train dataset:  14%|█▎        | 998/7336 [00:03<00:21, 290.91 examples/s]Tokenizing train dataset:  14%|█▍        | 1040/7336 [00:03<00:22, 282.23 examples/s]Tokenizing train dataset:  15%|█▍        | 1073/7336 [00:03<00:21, 289.50 examples/s]Tokenizing train dataset:  15%|█▌        | 1106/7336 [00:03<00:20, 299.08 examples/s]Tokenizing train dataset:  16%|█▌        | 1150/7336 [00:03<00:21, 291.02 examples/s]Tokenizing train dataset:  16%|█▌        | 1180/7336 [00:03<00:21, 289.04 examples/s]Tokenizing train dataset:  17%|█▋        | 1213/7336 [00:04<00:20, 294.47 examples/s]Tokenizing train dataset:  17%|█▋        | 1250/7336 [00:04<00:19, 308.06 examples/s]Tokenizing train dataset:  18%|█▊        | 1298/7336 [00:04<00:19, 308.51 examples/s]Tokenizing train dataset:  18%|█▊        | 1330/7336 [00:04<00:19, 308.29 examples/s]Tokenizing train dataset:  19%|█▊        | 1371/7336 [00:04<00:20, 294.75 examples/s]Tokenizing train dataset:  19%|█▉        | 1402/7336 [00:04<00:20, 296.43 examples/s]Tokenizing train dataset:  20%|█▉        | 1433/7336 [00:04<00:19, 296.82 examples/s]Tokenizing train dataset:  20%|██        | 1479/7336 [00:04<00:19, 294.68 examples/s]Tokenizing train dataset:  21%|██        | 1520/7336 [00:05<00:20, 282.26 examples/s]Tokenizing train dataset:  21%|██        | 1553/7336 [00:05<00:19, 290.20 examples/s]Tokenizing train dataset:  22%|██▏       | 1583/7336 [00:05<00:19, 290.41 examples/s]Tokenizing train dataset:  22%|██▏       | 1619/7336 [00:05<00:19, 300.62 examples/s]Tokenizing train dataset:  23%|██▎       | 1654/7336 [00:05<00:18, 307.02 examples/s]Tokenizing train dataset:  23%|██▎       | 1695/7336 [00:05<00:17, 329.43 examples/s]Tokenizing train dataset:  24%|██▎       | 1742/7336 [00:05<00:17, 320.38 examples/s]Tokenizing train dataset:  24%|██▍       | 1778/7336 [00:05<00:17, 326.19 examples/s]Tokenizing train dataset:  25%|██▍       | 1824/7336 [00:06<00:17, 310.52 examples/s]Tokenizing train dataset:  25%|██▌       | 1868/7336 [00:06<00:18, 303.59 examples/s]Tokenizing train dataset:  26%|██▌       | 1904/7336 [00:06<00:19, 280.78 examples/s]Tokenizing train dataset:  26%|██▋       | 1944/7336 [00:06<00:19, 274.26 examples/s]Tokenizing train dataset:  27%|██▋       | 1985/7336 [00:06<00:19, 272.81 examples/s]Tokenizing train dataset:  28%|██▊       | 2020/7336 [00:06<00:18, 287.22 examples/s]Tokenizing train dataset:  28%|██▊       | 2051/7336 [00:06<00:18, 291.32 examples/s]Tokenizing train dataset:  28%|██▊       | 2086/7336 [00:06<00:17, 304.94 examples/s]Tokenizing train dataset:  29%|██▉       | 2119/7336 [00:07<00:16, 309.87 examples/s]Tokenizing train dataset:  29%|██▉       | 2164/7336 [00:07<00:17, 296.47 examples/s]Tokenizing train dataset:  30%|███       | 2206/7336 [00:07<00:17, 288.41 examples/s]Tokenizing train dataset:  31%|███       | 2243/7336 [00:07<00:16, 306.39 examples/s]Tokenizing train dataset:  31%|███       | 2275/7336 [00:07<00:16, 308.72 examples/s]Tokenizing train dataset:  32%|███▏      | 2321/7336 [00:07<00:16, 301.50 examples/s]Tokenizing train dataset:  32%|███▏      | 2365/7336 [00:07<00:17, 292.10 examples/s]Tokenizing train dataset:  33%|███▎      | 2410/7336 [00:08<00:17, 288.65 examples/s]Tokenizing train dataset:  33%|███▎      | 2442/7336 [00:08<00:16, 293.54 examples/s]Tokenizing train dataset:  34%|███▍      | 2487/7336 [00:08<00:16, 293.40 examples/s]Tokenizing train dataset:  34%|███▍      | 2524/7336 [00:08<00:18, 267.30 examples/s]Tokenizing train dataset:  35%|███▍      | 2556/7336 [00:08<00:17, 278.12 examples/s]Tokenizing train dataset:  35%|███▌      | 2591/7336 [00:08<00:16, 291.33 examples/s]Tokenizing train dataset:  36%|███▌      | 2626/7336 [00:08<00:15, 302.07 examples/s]Tokenizing train dataset:  36%|███▌      | 2659/7336 [00:08<00:15, 305.64 examples/s]Tokenizing train dataset:  37%|███▋      | 2692/7336 [00:08<00:14, 311.25 examples/s]Tokenizing train dataset:  37%|███▋      | 2725/7336 [00:09<00:14, 311.38 examples/s]Tokenizing train dataset:  38%|███▊      | 2767/7336 [00:09<00:15, 297.48 examples/s]Tokenizing train dataset:  38%|███▊      | 2800/7336 [00:09<00:15, 302.38 examples/s]Tokenizing train dataset:  39%|███▉      | 2845/7336 [00:09<00:15, 294.86 examples/s]Tokenizing train dataset:  39%|███▉      | 2875/7336 [00:09<00:15, 295.30 examples/s]Tokenizing train dataset:  40%|███▉      | 2906/7336 [00:09<00:14, 296.36 examples/s]Tokenizing train dataset:  40%|████      | 2937/7336 [00:09<00:14, 295.56 examples/s]Tokenizing train dataset:  41%|████      | 2982/7336 [00:09<00:14, 294.67 examples/s]Tokenizing train dataset:  41%|████      | 3013/7336 [00:10<00:14, 295.85 examples/s]Tokenizing train dataset:  42%|████▏     | 3056/7336 [00:10<00:14, 286.05 examples/s]Tokenizing train dataset:  42%|████▏     | 3086/7336 [00:10<00:14, 284.72 examples/s]Tokenizing train dataset:  43%|████▎     | 3119/7336 [00:10<00:14, 289.59 examples/s]Tokenizing train dataset:  43%|████▎     | 3160/7336 [00:10<00:15, 277.82 examples/s]Tokenizing train dataset:  44%|████▎     | 3197/7336 [00:10<00:14, 294.18 examples/s]Tokenizing train dataset:  44%|████▍     | 3242/7336 [00:10<00:14, 290.05 examples/s]Tokenizing train dataset:  45%|████▍     | 3273/7336 [00:10<00:13, 293.96 examples/s]Tokenizing train dataset:  45%|████▌     | 3304/7336 [00:11<00:13, 295.77 examples/s]Tokenizing train dataset:  46%|████▌     | 3350/7336 [00:11<00:13, 295.97 examples/s]Tokenizing train dataset:  46%|████▌     | 3387/7336 [00:11<00:14, 277.76 examples/s]Tokenizing train dataset:  47%|████▋     | 3426/7336 [00:11<00:14, 270.59 examples/s]Tokenizing train dataset:  47%|████▋     | 3462/7336 [00:11<00:15, 256.77 examples/s]Tokenizing train dataset:  48%|████▊     | 3497/7336 [00:11<00:15, 246.26 examples/s]Tokenizing train dataset:  48%|████▊     | 3537/7336 [00:11<00:13, 279.11 examples/s]Tokenizing train dataset:  49%|████▊     | 3568/7336 [00:12<00:13, 285.28 examples/s]Tokenizing train dataset:  49%|████▉     | 3599/7336 [00:12<00:12, 289.31 examples/s]Tokenizing train dataset:  49%|████▉     | 3631/7336 [00:12<00:12, 292.53 examples/s]Tokenizing train dataset:  50%|████▉     | 3663/7336 [00:12<00:12, 297.40 examples/s]Tokenizing train dataset:  51%|█████     | 3707/7336 [00:12<00:12, 289.68 examples/s]Tokenizing train dataset:  51%|█████     | 3750/7336 [00:12<00:11, 322.71 examples/s]Tokenizing train dataset:  52%|█████▏    | 3800/7336 [00:12<00:11, 319.58 examples/s]Tokenizing train dataset:  52%|█████▏    | 3837/7336 [00:12<00:10, 328.97 examples/s]Tokenizing train dataset:  53%|█████▎    | 3883/7336 [00:13<00:10, 316.79 examples/s]Tokenizing train dataset:  53%|█████▎    | 3922/7336 [00:13<00:11, 291.60 examples/s]Tokenizing train dataset:  54%|█████▍    | 3973/7336 [00:13<00:11, 302.18 examples/s]Tokenizing train dataset:  55%|█████▍    | 4011/7336 [00:13<00:10, 318.80 examples/s]Tokenizing train dataset:  55%|█████▌    | 4055/7336 [00:13<00:10, 304.28 examples/s]Tokenizing train dataset:  56%|█████▌    | 4088/7336 [00:13<00:10, 310.15 examples/s]Tokenizing train dataset:  56%|█████▋    | 4130/7336 [00:13<00:10, 296.01 examples/s]Tokenizing train dataset:  57%|█████▋    | 4174/7336 [00:14<00:10, 289.91 examples/s]Tokenizing train dataset:  58%|█████▊    | 4249/7336 [00:14<00:07, 394.21 examples/s]Tokenizing train dataset:  60%|█████▉    | 4377/7336 [00:14<00:04, 612.53 examples/s]Tokenizing train dataset:  61%|██████▏   | 4502/7336 [00:14<00:03, 777.62 examples/s]Tokenizing train dataset:  63%|██████▎   | 4627/7336 [00:14<00:02, 903.63 examples/s]Tokenizing train dataset:  65%|██████▍   | 4752/7336 [00:14<00:02, 997.16 examples/s]Tokenizing train dataset:  67%|██████▋   | 4880/7336 [00:14<00:02, 1074.26 examples/s]Tokenizing train dataset:  68%|██████▊   | 5004/7336 [00:14<00:02, 1120.33 examples/s]Tokenizing train dataset:  70%|██████▉   | 5126/7336 [00:14<00:01, 1147.95 examples/s]Tokenizing train dataset:  71%|███████▏  | 5245/7336 [00:14<00:01, 1156.95 examples/s]Tokenizing train dataset:  74%|███████▍  | 5421/7336 [00:15<00:01, 1155.63 examples/s]Tokenizing train dataset:  76%|███████▌  | 5545/7336 [00:15<00:01, 1175.91 examples/s]Tokenizing train dataset:  78%|███████▊  | 5713/7336 [00:15<00:01, 1151.03 examples/s]Tokenizing train dataset:  80%|███████▉  | 5833/7336 [00:15<00:01, 1160.56 examples/s]Tokenizing train dataset:  81%|████████  | 5954/7336 [00:15<00:01, 1171.93 examples/s]Tokenizing train dataset:  83%|████████▎ | 6080/7336 [00:15<00:01, 1190.20 examples/s]Tokenizing train dataset:  85%|████████▍ | 6205/7336 [00:15<00:00, 1205.46 examples/s]Tokenizing train dataset:  86%|████████▌ | 6327/7336 [00:15<00:00, 1206.45 examples/s]Tokenizing train dataset:  89%|████████▊ | 6507/7336 [00:16<00:00, 1199.01 examples/s]Tokenizing train dataset:  90%|█████████ | 6633/7336 [00:16<00:00, 1210.33 examples/s]Tokenizing train dataset:  93%|█████████▎| 6813/7336 [00:16<00:00, 1203.56 examples/s]Tokenizing train dataset:  95%|█████████▍| 6937/7336 [00:16<00:00, 1210.40 examples/s]Tokenizing train dataset:  97%|█████████▋| 7114/7336 [00:16<00:00, 1198.48 examples/s]Tokenizing train dataset:  99%|█████████▊| 7238/7336 [00:16<00:00, 1207.63 examples/s]Tokenizing train dataset: 100%|██████████| 7336/7336 [00:16<00:00, 439.23 examples/s] 
[rank4]:[W608 19:37:36.772959588 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Extracting prompt in train dataset:   8%|▊         | 554/7336 [00:00<00:01, 5499.80 examples/s]Extracting prompt in train dataset:   7%|▋         | 550/7336 [00:00<00:01, 5443.96 examples/s]Extracting prompt in train dataset:   7%|▋         | 550/7336 [00:00<00:01, 5441.42 examples/s]Extracting prompt in eval dataset:  61%|██████    | 580/953 [00:00<00:00, 5713.21 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5700.80 examples/s]
Extracting prompt in train dataset:  15%|█▌        | 1120/7336 [00:00<00:01, 5590.24 examples/s]Extracting prompt in train dataset:  15%|█▌        | 1116/7336 [00:00<00:01, 5558.53 examples/s]Extracting prompt in train dataset:  15%|█▌        | 1118/7336 [00:00<00:01, 5567.51 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1674/7336 [00:00<00:01, 5552.93 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1710/7336 [00:00<00:00, 5630.31 examples/s]Extracting prompt in train dataset:  27%|██▋       | 1960/7336 [00:00<00:00, 5566.25 examples/s]Extracting prompt in train dataset:  31%|███       | 2240/7336 [00:00<00:00, 5579.43 examples/s]Extracting prompt in train dataset:  31%|███       | 2280/7336 [00:00<00:00, 5648.36 examples/s]Extracting prompt in train dataset:  34%|███▍      | 2525/7336 [00:00<00:00, 5594.22 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2810/7336 [00:00<00:00, 5474.15 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  42%|████▏     | 3100/7336 [00:00<00:00, 5555.99 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3090/7336 [00:00<00:00, 5487.90 examples/s]Extracting prompt in train dataset:  46%|████▌     | 3375/7336 [00:00<00:00, 5531.25 examples/s]Applying chat template to eval dataset:  33%|███▎      | 315/953 [00:00<00:00, 3115.09 examples/s]Extracting prompt in train dataset:  50%|█████     | 3670/7336 [00:00<00:00, 5585.91 examples/s]Extracting prompt in train dataset:  50%|████▉     | 3650/7336 [00:00<00:00, 5520.89 examples/s]Extracting prompt in train dataset:  54%|█████▎    | 3935/7336 [00:00<00:00, 5551.64 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 641/953 [00:00<00:00, 3192.11 examples/s]Extracting prompt in train dataset:  58%|█████▊    | 4240/7336 [00:00<00:00, 5616.71 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 4213/7336 [00:00<00:00, 5552.45 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 4515/7336 [00:00<00:00, 5629.16 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3182.95 examples/s]
Extracting prompt in train dataset:  66%|██████▌   | 4850/7336 [00:00<00:00, 5745.65 examples/s]Extracting prompt in train dataset:  66%|██████▌   | 4811/7336 [00:00<00:00, 5679.48 examples/s]Extracting prompt in train dataset:  70%|██████▉   | 5110/7336 [00:00<00:00, 5727.61 examples/s]Extracting prompt in train dataset:  74%|███████▍  | 5458/7336 [00:00<00:00, 5844.34 examples/s]Extracting prompt in train dataset:  74%|███████▎  | 5410/7336 [00:00<00:00, 5767.46 examples/s]Extracting prompt in train dataset:  78%|███████▊  | 5710/7336 [00:01<00:00, 5798.91 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 6060/7336 [00:01<00:00, 5868.44 examples/s]Extracting prompt in train dataset:  82%|████████▏ | 6016/7336 [00:01<00:00, 5849.40 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 6300/7336 [00:01<00:00, 5808.97 examples/s]Extracting prompt in train dataset:  91%|█████████ | 6660/7336 [00:01<00:00, 5903.52 examples/s]Extracting prompt in train dataset:  90%|█████████ | 6612/7336 [00:01<00:00, 5881.34 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  94%|█████████▍| 6913/7336 [00:01<00:00, 5905.93 examples/s]Extracting prompt in train dataset:  99%|█████████▉| 7260/7336 [00:01<00:00, 5930.19 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 7210/7336 [00:01<00:00, 5903.91 examples/s]Extracting prompt in train dataset: 100%|██████████| 7336/7336 [00:01<00:00, 5725.57 examples/s]
Extracting prompt in train dataset: 100%|██████████| 7336/7336 [00:01<00:00, 5684.25 examples/s]
Extracting prompt in train dataset: 100%|██████████| 7336/7336 [00:01<00:00, 5683.54 examples/s]
Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 320.62 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 289.68 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 276.87 examples/s]Applying chat template to train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 286/7336 [00:00<00:02, 2825.68 examples/s]Applying chat template to train dataset:   4%|▍         | 282/7336 [00:00<00:02, 2786.83 examples/s]Applying chat template to train dataset:   4%|▍         | 283/7336 [00:00<00:02, 2790.48 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.82 examples/s]Applying chat template to train dataset:   8%|▊         | 599/7336 [00:00<00:02, 2996.68 examples/s]Applying chat template to train dataset:   8%|▊         | 590/7336 [00:00<00:02, 2947.65 examples/s]Applying chat template to train dataset:   8%|▊         | 591/7336 [00:00<00:02, 2952.25 examples/s]Applying chat template to train dataset:  12%|█▏        | 910/7336 [00:00<00:02, 3042.09 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.53 examples/s]Applying chat template to train dataset:  12%|█▏        | 898/7336 [00:00<00:02, 3002.87 examples/s]Applying chat template to train dataset:  12%|█▏        | 899/7336 [00:00<00:02, 3005.50 examples/s]Applying chat template to train dataset:  17%|█▋        | 1223/7336 [00:00<00:01, 3073.95 examples/s]Tokenizing eval dataset:  24%|██▍       | 230/953 [00:00<00:02, 276.52 examples/s]Applying chat template to train dataset:  16%|█▋        | 1205/7336 [00:00<00:02, 3026.31 examples/s]Applying chat template to train dataset:  16%|█▋        | 1207/7336 [00:00<00:02, 3032.17 examples/s]Applying chat template to train dataset:  21%|██        | 1532/7336 [00:00<00:01, 3074.14 examples/s]Tokenizing eval dataset:  31%|███       | 295/953 [00:00<00:01, 373.40 examples/s]Applying chat template to train dataset:  21%|██        | 1510/7336 [00:00<00:01, 3021.89 examples/s]Applying chat template to train dataset:  23%|██▎       | 1666/7336 [00:00<00:01, 3039.80 examples/s]Applying chat template to train dataset:  25%|██▌       | 1845/7336 [00:00<00:01, 3092.46 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 440.78 examples/s]Applying chat template to train dataset:  25%|██▍       | 1820/7336 [00:00<00:01, 3042.43 examples/s]Applying chat template to train dataset:  27%|██▋       | 1975/7336 [00:00<00:01, 3051.59 examples/s]Applying chat template to train dataset:  29%|██▉       | 2160/7336 [00:00<00:01, 3103.62 examples/s]Tokenizing eval dataset:  45%|████▍     | 425/953 [00:01<00:01, 497.70 examples/s]Applying chat template to train dataset:  29%|██▉       | 2129/7336 [00:00<00:01, 3054.84 examples/s]Applying chat template to train dataset:  31%|███       | 2286/7336 [00:00<00:01, 3064.72 examples/s]Applying chat template to train dataset:  34%|███▎      | 2473/7336 [00:00<00:01, 3111.46 examples/s]Applying chat template to train dataset:  33%|███▎      | 2437/7336 [00:00<00:01, 3058.96 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 541.08 examples/s]Applying chat template to train dataset:  35%|███▌      | 2595/7336 [00:00<00:01, 3069.41 examples/s]Applying chat template to train dataset:  38%|███▊      | 2788/7336 [00:00<00:01, 3118.13 examples/s]Applying chat template to train dataset:  37%|███▋      | 2749/7336 [00:00<00:01, 3074.37 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 574.65 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 595.40 examples/s]Applying chat template to train dataset:  41%|████▏     | 3040/7336 [00:01<00:01, 3023.18 examples/s]Applying chat template to train dataset:  44%|████▍     | 3238/7336 [00:01<00:01, 3066.86 examples/s]Applying chat template to train dataset:  44%|████▎     | 3192/7336 [00:01<00:01, 3021.89 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 598.74 examples/s]Applying chat template to train dataset:  46%|████▌     | 3349/7336 [00:01<00:01, 3038.07 examples/s]Applying chat template to train dataset:  48%|████▊     | 3549/7336 [00:01<00:01, 3076.37 examples/s]Applying chat template to train dataset:  48%|████▊     | 3506/7336 [00:01<00:01, 3048.91 examples/s]Applying chat template to train dataset:  50%|████▉     | 3655/7336 [00:01<00:01, 3040.31 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3861/7336 [00:01<00:01, 3086.57 examples/s]Applying chat template to train dataset:  52%|█████▏    | 3821/7336 [00:01<00:01, 3075.99 examples/s]Tokenizing eval dataset:  82%|████████▏ | 777/953 [00:01<00:00, 575.58 examples/s]Applying chat template to train dataset:  54%|█████▍    | 3964/7336 [00:01<00:01, 3049.14 examples/s]Applying chat template to train dataset:  57%|█████▋    | 4174/7336 [00:01<00:01, 3095.19 examples/s]Applying chat template to train dataset:  56%|█████▋    | 4130/7336 [00:01<00:01, 3075.73 examples/s]Applying chat template to train dataset:  58%|█████▊    | 4276/7336 [00:01<00:00, 3066.09 examples/s]Applying chat template to train dataset:  61%|██████▏   | 4500/7336 [00:01<00:00, 3141.86 examples/s]Tokenizing eval dataset:  89%|████████▉ | 850/953 [00:01<00:00, 539.51 examples/s]Applying chat template to train dataset:  61%|██████    | 4450/7336 [00:01<00:00, 3106.53 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4599/7336 [00:01<00:00, 3111.35 examples/s]Applying chat template to train dataset:  66%|██████▌   | 4829/7336 [00:01<00:00, 3182.59 examples/s]Applying chat template to train dataset:  65%|██████▌   | 4773/7336 [00:01<00:00, 3138.67 examples/s]Tokenizing eval dataset:  97%|█████████▋| 923/953 [00:02<00:00, 517.71 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4922/7336 [00:01<00:00, 3141.31 examples/s]Applying chat template to train dataset:  70%|███████   | 5156/7336 [00:01<00:00, 3205.68 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 456.00 examples/s]
Applying chat template to train dataset:  69%|██████▉   | 5095/7336 [00:01<00:00, 3160.06 examples/s]Applying chat template to train dataset:  71%|███████▏  | 5245/7336 [00:01<00:00, 3163.98 examples/s]Applying chat template to train dataset:  75%|███████▍  | 5485/7336 [00:01<00:00, 3225.80 examples/s]Applying chat template to train dataset:  74%|███████▍  | 5417/7336 [00:01<00:00, 3173.29 examples/s]Applying chat template to train dataset:  76%|███████▌  | 5568/7336 [00:01<00:00, 3181.10 examples/s]Applying chat template to train dataset:  79%|███████▉  | 5815/7336 [00:01<00:00, 3240.30 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5739/7336 [00:01<00:00, 3182.87 examples/s]Applying chat template to train dataset:  80%|████████  | 5890/7336 [00:01<00:00, 3188.79 examples/s]Applying chat template to train dataset:  84%|████████▎ | 6143/7336 [00:01<00:00, 3250.51 examples/s]Applying chat template to train dataset:  83%|████████▎ | 6061/7336 [00:01<00:00, 3187.10 examples/s]Applying chat template to train dataset:  85%|████████▍ | 6221/7336 [00:02<00:00, 3221.73 examples/s]Applying chat template to train dataset:  88%|████████▊ | 6472/7336 [00:02<00:00, 3259.48 examples/s]Applying chat template to train dataset:  87%|████████▋ | 6389/7336 [00:02<00:00, 3214.03 examples/s]Applying chat template to train dataset:  89%|████████▉ | 6552/7336 [00:02<00:00, 3244.75 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6801/7336 [00:02<00:00, 3267.91 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6719/7336 [00:02<00:00, 3239.01 examples/s]Applying chat template to train dataset:  94%|█████████▍| 6883/7336 [00:02<00:00, 3261.32 examples/s]Applying chat template to train dataset:  97%|█████████▋| 7130/7336 [00:02<00:00, 3272.99 examples/s]Applying chat template to train dataset:  96%|█████████▌| 7049/7336 [00:02<00:00, 3256.25 examples/s]Applying chat template to train dataset: 100%|██████████| 7336/7336 [00:02<00:00, 3154.10 examples/s]
Applying chat template to train dataset:  98%|█████████▊| 7214/7336 [00:02<00:00, 3274.86 examples/s]Applying chat template to train dataset: 100%|██████████| 7336/7336 [00:02<00:00, 3117.91 examples/s]
Applying chat template to train dataset: 100%|██████████| 7336/7336 [00:02<00:00, 3119.39 examples/s]
Tokenizing train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 42/7336 [00:00<00:17, 405.57 examples/s]Tokenizing train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 42/7336 [00:00<00:17, 408.72 examples/s]Tokenizing train dataset:   1%|          | 42/7336 [00:00<00:17, 410.59 examples/s]Tokenizing train dataset:   1%|          | 90/7336 [00:00<00:21, 341.31 examples/s]Tokenizing train dataset:   1%|          | 90/7336 [00:00<00:21, 340.00 examples/s]Tokenizing train dataset:   1%|          | 90/7336 [00:00<00:21, 340.01 examples/s]Tokenizing train dataset:   2%|▏         | 139/7336 [00:00<00:22, 323.93 examples/s]Tokenizing train dataset:   2%|▏         | 139/7336 [00:00<00:22, 321.19 examples/s]Tokenizing train dataset:   2%|▏         | 139/7336 [00:00<00:22, 321.02 examples/s]Tokenizing train dataset:   3%|▎         | 184/7336 [00:00<00:22, 311.81 examples/s]Tokenizing train dataset:   3%|▎         | 184/7336 [00:00<00:23, 309.07 examples/s]Tokenizing train dataset:   3%|▎         | 218/7336 [00:00<00:22, 317.44 examples/s]Tokenizing train dataset:   3%|▎         | 184/7336 [00:00<00:23, 308.80 examples/s]Tokenizing train dataset:   3%|▎         | 218/7336 [00:00<00:22, 314.19 examples/s]Tokenizing train dataset:   3%|▎         | 252/7336 [00:00<00:22, 321.90 examples/s]Tokenizing train dataset:   3%|▎         | 218/7336 [00:00<00:22, 313.92 examples/s]Tokenizing train dataset:   3%|▎         | 250/7336 [00:00<00:22, 313.46 examples/s]Tokenizing train dataset:   4%|▍         | 290/7336 [00:00<00:21, 335.16 examples/s]Tokenizing train dataset:   3%|▎         | 251/7336 [00:00<00:22, 317.07 examples/s]Tokenizing train dataset:   4%|▍         | 289/7336 [00:00<00:21, 330.54 examples/s]Tokenizing train dataset:   4%|▍         | 324/7336 [00:00<00:21, 333.89 examples/s]Tokenizing train dataset:   4%|▍         | 289/7336 [00:00<00:21, 332.93 examples/s]Tokenizing train dataset:   5%|▍         | 337/7336 [00:01<00:21, 323.72 examples/s]Tokenizing train dataset:   5%|▌         | 370/7336 [00:01<00:21, 320.07 examples/s]Tokenizing train dataset:   5%|▍         | 337/7336 [00:01<00:21, 325.08 examples/s]Tokenizing train dataset:   5%|▌         | 387/7336 [00:01<00:21, 322.48 examples/s]Tokenizing train dataset:   6%|▌         | 420/7336 [00:01<00:21, 318.23 examples/s]Tokenizing train dataset:   5%|▌         | 387/7336 [00:01<00:21, 322.82 examples/s]Tokenizing train dataset:   6%|▌         | 434/7336 [00:01<00:21, 314.38 examples/s]Tokenizing train dataset:   6%|▋         | 469/7336 [00:01<00:21, 316.40 examples/s]Tokenizing train dataset:   6%|▌         | 434/7336 [00:01<00:21, 314.52 examples/s]Tokenizing train dataset:   7%|▋         | 480/7336 [00:01<00:22, 307.97 examples/s]Tokenizing train dataset:   7%|▋         | 516/7336 [00:01<00:22, 309.73 examples/s]Tokenizing train dataset:   7%|▋         | 480/7336 [00:01<00:22, 308.05 examples/s]Tokenizing train dataset:   7%|▋         | 550/7336 [00:01<00:21, 314.87 examples/s]Tokenizing train dataset:   7%|▋         | 527/7336 [00:01<00:22, 307.24 examples/s]Tokenizing train dataset:   7%|▋         | 527/7336 [00:01<00:22, 307.13 examples/s]Tokenizing train dataset:   8%|▊         | 561/7336 [00:01<00:21, 312.25 examples/s]Tokenizing train dataset:   8%|▊         | 596/7336 [00:01<00:21, 310.48 examples/s]Tokenizing train dataset:   8%|▊         | 561/7336 [00:01<00:21, 312.13 examples/s]Tokenizing train dataset:   9%|▊         | 634/7336 [00:01<00:20, 324.95 examples/s]Tokenizing train dataset:   8%|▊         | 605/7336 [00:01<00:22, 301.44 examples/s]Tokenizing train dataset:   8%|▊         | 605/7336 [00:01<00:22, 301.47 examples/s]Tokenizing train dataset:   9%|▉         | 644/7336 [00:02<00:20, 318.83 examples/s]Tokenizing train dataset:   9%|▉         | 681/7336 [00:02<00:21, 316.05 examples/s]Tokenizing train dataset:   9%|▉         | 644/7336 [00:02<00:21, 318.60 examples/s]Tokenizing train dataset:  10%|▉         | 729/7336 [00:02<00:21, 314.36 examples/s]Tokenizing train dataset:   9%|▉         | 690/7336 [00:02<00:21, 307.99 examples/s]Tokenizing train dataset:   9%|▉         | 689/7336 [00:02<00:21, 310.45 examples/s]Tokenizing train dataset:  10%|▉         | 722/7336 [00:02<00:21, 309.12 examples/s]Tokenizing train dataset:  10%|█         | 763/7336 [00:02<00:20, 315.82 examples/s]Tokenizing train dataset:  10%|█         | 740/7336 [00:02<00:20, 317.00 examples/s]Tokenizing train dataset:  10%|█         | 759/7336 [00:02<00:20, 318.21 examples/s]Tokenizing train dataset:  11%|█         | 808/7336 [00:02<00:21, 303.87 examples/s]Tokenizing train dataset:  11%|█         | 785/7336 [00:02<00:21, 307.66 examples/s]Tokenizing train dataset:  11%|█         | 802/7336 [00:02<00:21, 300.75 examples/s]Tokenizing train dataset:  11%|█▏        | 839/7336 [00:02<00:21, 301.11 examples/s]Tokenizing train dataset:  11%|█▏        | 827/7336 [00:02<00:22, 294.21 examples/s]Tokenizing train dataset:  12%|█▏        | 887/7336 [00:02<00:21, 306.19 examples/s]Tokenizing train dataset:  12%|█▏        | 847/7336 [00:02<00:21, 297.83 examples/s]Tokenizing train dataset:  12%|█▏        | 860/7336 [00:02<00:21, 299.29 examples/s]Tokenizing train dataset:  13%|█▎        | 920/7336 [00:02<00:20, 307.40 examples/s]Tokenizing train dataset:  12%|█▏        | 879/7336 [00:02<00:21, 298.31 examples/s]Tokenizing train dataset:  12%|█▏        | 897/7336 [00:02<00:20, 315.11 examples/s]Tokenizing train dataset:  13%|█▎        | 952/7336 [00:03<00:20, 306.29 examples/s]Tokenizing train dataset:  12%|█▎        | 917/7336 [00:02<00:20, 313.16 examples/s]Tokenizing train dataset:  13%|█▎        | 983/7336 [00:03<00:20, 302.76 examples/s]Tokenizing train dataset:  13%|█▎        | 942/7336 [00:03<00:20, 304.81 examples/s]Tokenizing train dataset:  13%|█▎        | 965/7336 [00:03<00:20, 310.24 examples/s]Tokenizing train dataset:  14%|█▍        | 1027/7336 [00:03<00:21, 293.69 examples/s]Tokenizing train dataset:  13%|█▎        | 986/7336 [00:03<00:21, 297.00 examples/s]Tokenizing train dataset:  14%|█▎        | 1008/7336 [00:03<00:21, 298.02 examples/s]Tokenizing train dataset:  14%|█▍        | 1057/7336 [00:03<00:21, 293.12 examples/s]Tokenizing train dataset:  14%|█▍        | 1028/7336 [00:03<00:21, 290.52 examples/s]Tokenizing train dataset:  15%|█▍        | 1093/7336 [00:03<00:20, 308.87 examples/s]Tokenizing train dataset:  14%|█▍        | 1050/7336 [00:03<00:21, 288.27 examples/s]Tokenizing train dataset:  14%|█▍        | 1060/7336 [00:03<00:21, 293.42 examples/s]Tokenizing train dataset:  15%|█▍        | 1085/7336 [00:03<00:20, 298.25 examples/s]Tokenizing train dataset:  15%|█▌        | 1135/7336 [00:03<00:21, 295.26 examples/s]Tokenizing train dataset:  15%|█▍        | 1095/7336 [00:03<00:20, 306.09 examples/s]Tokenizing train dataset:  15%|█▌        | 1119/7336 [00:03<00:20, 306.39 examples/s]Tokenizing train dataset:  16%|█▌        | 1166/7336 [00:03<00:20, 296.12 examples/s]Tokenizing train dataset:  15%|█▌        | 1137/7336 [00:03<00:21, 292.11 examples/s]Tokenizing train dataset:  16%|█▋        | 1202/7336 [00:03<00:19, 306.93 examples/s]Tokenizing train dataset:  16%|█▌        | 1160/7336 [00:03<00:21, 290.60 examples/s]Tokenizing train dataset:  16%|█▌        | 1167/7336 [00:03<00:21, 292.06 examples/s]Tokenizing train dataset:  17%|█▋        | 1236/7336 [00:03<00:19, 312.52 examples/s]Tokenizing train dataset:  16%|█▋        | 1193/7336 [00:03<00:20, 298.14 examples/s]Tokenizing train dataset:  16%|█▋        | 1202/7336 [00:03<00:20, 302.58 examples/s]Tokenizing train dataset:  17%|█▋        | 1270/7336 [00:04<00:19, 314.56 examples/s]Tokenizing train dataset:  17%|█▋        | 1225/7336 [00:03<00:20, 300.46 examples/s]Tokenizing train dataset:  17%|█▋        | 1236/7336 [00:03<00:19, 308.26 examples/s]Tokenizing train dataset:  18%|█▊        | 1304/7336 [00:04<00:18, 318.59 examples/s]Tokenizing train dataset:  17%|█▋        | 1262/7336 [00:04<00:19, 315.22 examples/s]Tokenizing train dataset:  17%|█▋        | 1270/7336 [00:04<00:19, 309.97 examples/s]Tokenizing train dataset:  18%|█▊        | 1339/7336 [00:04<00:18, 321.70 examples/s]Tokenizing train dataset:  18%|█▊        | 1311/7336 [00:04<00:19, 315.81 examples/s]Tokenizing train dataset:  18%|█▊        | 1304/7336 [00:04<00:19, 314.34 examples/s]Tokenizing train dataset:  18%|█▊        | 1344/7336 [00:04<00:18, 316.16 examples/s]Tokenizing train dataset:  19%|█▉        | 1381/7336 [00:04<00:19, 301.69 examples/s]Tokenizing train dataset:  18%|█▊        | 1339/7336 [00:04<00:18, 317.71 examples/s]Tokenizing train dataset:  19%|█▉        | 1412/7336 [00:04<00:19, 302.28 examples/s]Tokenizing train dataset:  19%|█▉        | 1387/7336 [00:04<00:19, 299.08 examples/s]Tokenizing train dataset:  19%|█▉        | 1379/7336 [00:04<00:20, 296.86 examples/s]Tokenizing train dataset:  20%|█▉        | 1458/7336 [00:04<00:19, 298.32 examples/s]Tokenizing train dataset:  19%|█▉        | 1418/7336 [00:04<00:19, 300.29 examples/s]Tokenizing train dataset:  19%|█▉        | 1410/7336 [00:04<00:19, 296.51 examples/s]Tokenizing train dataset:  20%|██        | 1490/7336 [00:04<00:19, 296.79 examples/s]Tokenizing train dataset:  20%|█▉        | 1462/7336 [00:04<00:20, 292.93 examples/s]Tokenizing train dataset:  20%|█▉        | 1456/7336 [00:04<00:19, 296.77 examples/s]Tokenizing train dataset:  20%|██        | 1492/7336 [00:04<00:19, 293.30 examples/s]Tokenizing train dataset:  21%|██        | 1538/7336 [00:04<00:19, 301.02 examples/s]Tokenizing train dataset:  20%|██        | 1486/7336 [00:04<00:19, 292.67 examples/s]Tokenizing train dataset:  22%|██▏       | 1582/7336 [00:05<00:19, 297.82 examples/s]Tokenizing train dataset:  21%|██        | 1539/7336 [00:05<00:19, 296.36 examples/s]Tokenizing train dataset:  21%|██        | 1533/7336 [00:04<00:19, 298.38 examples/s]Tokenizing train dataset:  22%|██▏       | 1614/7336 [00:05<00:18, 302.09 examples/s]Tokenizing train dataset:  21%|██▏       | 1570/7336 [00:05<00:19, 294.05 examples/s]Tokenizing train dataset:  21%|██▏       | 1563/7336 [00:05<00:19, 297.03 examples/s]Tokenizing train dataset:  22%|██▏       | 1650/7336 [00:05<00:18, 314.33 examples/s]Tokenizing train dataset:  22%|██▏       | 1601/7336 [00:05<00:19, 293.58 examples/s]Tokenizing train dataset:  22%|██▏       | 1610/7336 [00:05<00:19, 296.70 examples/s]Tokenizing train dataset:  23%|██▎       | 1685/7336 [00:05<00:17, 320.14 examples/s]Tokenizing train dataset:  22%|██▏       | 1637/7336 [00:05<00:18, 306.01 examples/s]Tokenizing train dataset:  22%|██▏       | 1645/7336 [00:05<00:18, 306.77 examples/s]Tokenizing train dataset:  23%|██▎       | 1722/7336 [00:05<00:16, 330.88 examples/s]Tokenizing train dataset:  23%|██▎       | 1670/7336 [00:05<00:18, 310.03 examples/s]Tokenizing train dataset:  23%|██▎       | 1678/7336 [00:05<00:18, 309.75 examples/s]Tokenizing train dataset:  24%|██▍       | 1756/7336 [00:05<00:16, 332.60 examples/s]Tokenizing train dataset:  23%|██▎       | 1710/7336 [00:05<00:17, 328.09 examples/s]Tokenizing train dataset:  23%|██▎       | 1715/7336 [00:05<00:17, 320.32 examples/s]Tokenizing train dataset:  24%|██▍       | 1792/7336 [00:05<00:16, 335.61 examples/s]Tokenizing train dataset:  24%|██▍       | 1746/7336 [00:05<00:16, 333.50 examples/s]Tokenizing train dataset:  24%|██▍       | 1751/7336 [00:05<00:17, 328.39 examples/s]Tokenizing train dataset:  24%|██▍       | 1781/7336 [00:05<00:16, 333.83 examples/s]Tokenizing train dataset:  25%|██▌       | 1834/7336 [00:05<00:17, 307.84 examples/s]Tokenizing train dataset:  24%|██▍       | 1788/7336 [00:05<00:16, 338.16 examples/s]Tokenizing train dataset:  25%|██▍       | 1826/7336 [00:05<00:17, 314.26 examples/s]Tokenizing train dataset:  26%|██▌       | 1871/7336 [00:06<00:19, 280.78 examples/s]Tokenizing train dataset:  25%|██▌       | 1834/7336 [00:05<00:19, 283.61 examples/s]Tokenizing train dataset:  26%|██▌       | 1872/7336 [00:06<00:17, 306.27 examples/s]Tokenizing train dataset:  26%|██▌       | 1910/7336 [00:06<00:20, 265.10 examples/s]Tokenizing train dataset:  25%|██▌       | 1868/7336 [00:06<00:18, 293.79 examples/s]Tokenizing train dataset:  26%|██▌       | 1910/7336 [00:06<00:19, 282.04 examples/s]Tokenizing train dataset:  27%|██▋       | 1953/7336 [00:06<00:19, 270.14 examples/s]Tokenizing train dataset:  26%|██▌       | 1905/7336 [00:06<00:19, 276.36 examples/s]Tokenizing train dataset:  27%|██▋       | 1982/7336 [00:06<00:19, 273.20 examples/s]Tokenizing train dataset:  27%|██▋       | 1953/7336 [00:06<00:19, 280.92 examples/s]Tokenizing train dataset:  27%|██▋       | 1947/7336 [00:06<00:19, 272.63 examples/s]Tokenizing train dataset:  27%|██▋       | 2015/7336 [00:06<00:18, 283.69 examples/s]Tokenizing train dataset:  27%|██▋       | 1997/7336 [00:06<00:18, 283.07 examples/s]Tokenizing train dataset:  28%|██▊       | 2049/7336 [00:06<00:17, 296.96 examples/s]Tokenizing train dataset:  27%|██▋       | 1991/7336 [00:06<00:19, 274.90 examples/s]Tokenizing train dataset:  28%|██▊       | 2030/7336 [00:06<00:18, 291.91 examples/s]Tokenizing train dataset:  28%|██▊       | 2086/7336 [00:06<00:16, 310.23 examples/s]Tokenizing train dataset:  28%|██▊       | 2025/7336 [00:06<00:18, 289.04 examples/s]Tokenizing train dataset:  28%|██▊       | 2068/7336 [00:06<00:16, 311.76 examples/s]Tokenizing train dataset:  29%|██▉       | 2120/7336 [00:06<00:16, 314.65 examples/s]Tokenizing train dataset:  28%|██▊       | 2061/7336 [00:06<00:17, 299.76 examples/s]Tokenizing train dataset:  29%|██▊       | 2101/7336 [00:06<00:16, 312.74 examples/s]Tokenizing train dataset:  29%|██▊       | 2096/7336 [00:06<00:17, 305.57 examples/s]Tokenizing train dataset:  30%|██▉       | 2167/7336 [00:07<00:16, 306.86 examples/s]Tokenizing train dataset:  29%|██▉       | 2147/7336 [00:06<00:16, 308.80 examples/s]Tokenizing train dataset:  29%|██▉       | 2130/7336 [00:06<00:16, 307.90 examples/s]Tokenizing train dataset:  30%|███       | 2211/7336 [00:07<00:17, 298.02 examples/s]Tokenizing train dataset:  30%|██▉       | 2179/7336 [00:07<00:16, 306.08 examples/s]Tokenizing train dataset:  30%|██▉       | 2176/7336 [00:07<00:16, 304.06 examples/s]Tokenizing train dataset:  31%|███       | 2248/7336 [00:07<00:16, 314.48 examples/s]Tokenizing train dataset:  30%|███       | 2222/7336 [00:07<00:17, 296.79 examples/s]Tokenizing train dataset:  30%|███       | 2221/7336 [00:07<00:17, 297.97 examples/s]Tokenizing train dataset:  31%|███▏      | 2296/7336 [00:07<00:16, 313.94 examples/s]Tokenizing train dataset:  31%|███       | 2259/7336 [00:07<00:16, 312.25 examples/s]Tokenizing train dataset:  31%|███       | 2257/7336 [00:07<00:16, 311.77 examples/s]Tokenizing train dataset:  32%|███▏      | 2339/7336 [00:07<00:16, 300.54 examples/s]Tokenizing train dataset:  31%|███▏      | 2305/7336 [00:07<00:16, 305.03 examples/s]Tokenizing train dataset:  31%|███▏      | 2302/7336 [00:07<00:16, 305.26 examples/s]Tokenizing train dataset:  32%|███▏      | 2370/7336 [00:07<00:16, 298.66 examples/s]Tokenizing train dataset:  32%|███▏      | 2349/7336 [00:07<00:16, 295.71 examples/s]Tokenizing train dataset:  32%|███▏      | 2346/7336 [00:07<00:16, 296.75 examples/s]Tokenizing train dataset:  33%|███▎      | 2416/7336 [00:07<00:16, 300.20 examples/s]Tokenizing train dataset:  32%|███▏      | 2380/7336 [00:07<00:16, 295.33 examples/s]Tokenizing train dataset:  33%|███▎      | 2447/7336 [00:07<00:16, 299.44 examples/s]Tokenizing train dataset:  33%|███▎      | 2410/7336 [00:07<00:16, 294.48 examples/s]Tokenizing train dataset:  33%|███▎      | 2390/7336 [00:07<00:16, 291.77 examples/s]Tokenizing train dataset:  34%|███▍      | 2478/7336 [00:08<00:16, 301.00 examples/s]Tokenizing train dataset:  33%|███▎      | 2443/7336 [00:07<00:16, 298.63 examples/s]Tokenizing train dataset:  33%|███▎      | 2423/7336 [00:07<00:16, 296.41 examples/s]Tokenizing train dataset:  34%|███▍      | 2517/7336 [00:08<00:17, 280.31 examples/s]Tokenizing train dataset:  34%|███▍      | 2491/7336 [00:08<00:16, 300.29 examples/s]Tokenizing train dataset:  34%|███▎      | 2469/7336 [00:08<00:16, 293.32 examples/s]Tokenizing train dataset:  35%|███▍      | 2549/7336 [00:08<00:16, 284.60 examples/s]Tokenizing train dataset:  34%|███▍      | 2500/7336 [00:08<00:16, 287.37 examples/s]Tokenizing train dataset:  34%|███▍      | 2525/7336 [00:08<00:17, 272.25 examples/s]Tokenizing train dataset:  35%|███▌      | 2583/7336 [00:08<00:16, 297.01 examples/s]Tokenizing train dataset:  35%|███▍      | 2558/7336 [00:08<00:16, 283.28 examples/s]Tokenizing train dataset:  35%|███▍      | 2540/7336 [00:08<00:17, 275.96 examples/s]Tokenizing train dataset:  36%|███▌      | 2619/7336 [00:08<00:15, 309.52 examples/s]Tokenizing train dataset:  35%|███▌      | 2593/7336 [00:08<00:15, 298.21 examples/s]Tokenizing train dataset:  35%|███▌      | 2574/7336 [00:08<00:16, 288.83 examples/s]Tokenizing train dataset:  36%|███▌      | 2653/7336 [00:08<00:14, 313.23 examples/s]Tokenizing train dataset:  36%|███▌      | 2627/7336 [00:08<00:15, 305.94 examples/s]Tokenizing train dataset:  36%|███▌      | 2611/7336 [00:08<00:15, 305.43 examples/s]Tokenizing train dataset:  37%|███▋      | 2688/7336 [00:08<00:14, 318.00 examples/s]Tokenizing train dataset:  36%|███▋      | 2660/7336 [00:08<00:15, 310.13 examples/s]Tokenizing train dataset:  36%|███▌      | 2645/7336 [00:08<00:15, 311.74 examples/s]Tokenizing train dataset:  37%|███▋      | 2724/7336 [00:08<00:14, 323.85 examples/s]Tokenizing train dataset:  37%|███▋      | 2695/7336 [00:08<00:14, 317.37 examples/s]Tokenizing train dataset:  37%|███▋      | 2695/7336 [00:08<00:14, 315.91 examples/s]Tokenizing train dataset:  38%|███▊      | 2767/7336 [00:09<00:14, 307.65 examples/s]Tokenizing train dataset:  37%|███▋      | 2740/7336 [00:08<00:14, 307.96 examples/s]Tokenizing train dataset:  37%|███▋      | 2727/7336 [00:08<00:14, 314.31 examples/s]Tokenizing train dataset:  38%|███▊      | 2800/7336 [00:09<00:14, 312.22 examples/s]Tokenizing train dataset:  38%|███▊      | 2787/7336 [00:09<00:14, 306.48 examples/s]Tokenizing train dataset:  38%|███▊      | 2770/7336 [00:09<00:15, 301.67 examples/s]Tokenizing train dataset:  39%|███▉      | 2845/7336 [00:09<00:14, 304.02 examples/s]Tokenizing train dataset:  38%|███▊      | 2818/7336 [00:09<00:14, 304.40 examples/s]Tokenizing train dataset:  38%|███▊      | 2804/7336 [00:09<00:14, 308.35 examples/s]Tokenizing train dataset:  39%|███▉      | 2877/7336 [00:09<00:14, 304.08 examples/s]Tokenizing train dataset:  40%|███▉      | 2908/7336 [00:09<00:14, 304.50 examples/s]Tokenizing train dataset:  39%|███▉      | 2863/7336 [00:09<00:15, 297.07 examples/s]Tokenizing train dataset:  39%|███▉      | 2848/7336 [00:09<00:15, 299.04 examples/s]Tokenizing train dataset:  40%|████      | 2939/7336 [00:09<00:14, 303.92 examples/s]Tokenizing train dataset:  39%|███▉      | 2895/7336 [00:09<00:14, 300.12 examples/s]Tokenizing train dataset:  39%|███▉      | 2879/7336 [00:09<00:14, 297.63 examples/s]Tokenizing train dataset:  41%|████      | 2973/7336 [00:09<00:14, 308.00 examples/s]Tokenizing train dataset:  40%|███▉      | 2910/7336 [00:09<00:14, 299.40 examples/s]Tokenizing train dataset:  40%|████      | 2942/7336 [00:09<00:14, 300.84 examples/s]Tokenizing train dataset:  40%|████      | 2942/7336 [00:09<00:14, 301.36 examples/s]Tokenizing train dataset:  41%|████      | 3020/7336 [00:09<00:14, 306.54 examples/s]Tokenizing train dataset:  41%|████      | 2974/7336 [00:09<00:14, 302.55 examples/s]Tokenizing train dataset:  41%|████      | 2974/7336 [00:09<00:14, 303.07 examples/s]Tokenizing train dataset:  41%|████      | 3005/7336 [00:09<00:14, 299.29 examples/s]Tokenizing train dataset:  42%|████▏     | 3062/7336 [00:09<00:14, 292.95 examples/s]Tokenizing train dataset:  41%|████      | 3005/7336 [00:09<00:14, 299.29 examples/s]Tokenizing train dataset:  42%|████▏     | 3094/7336 [00:10<00:14, 298.44 examples/s]Tokenizing train dataset:  42%|████▏     | 3051/7336 [00:10<00:14, 299.18 examples/s]Tokenizing train dataset:  42%|████▏     | 3051/7336 [00:10<00:14, 299.40 examples/s]Tokenizing train dataset:  43%|████▎     | 3126/7336 [00:10<00:14, 300.14 examples/s]Tokenizing train dataset:  42%|████▏     | 3094/7336 [00:10<00:14, 293.57 examples/s]Tokenizing train dataset:  42%|████▏     | 3094/7336 [00:10<00:14, 293.53 examples/s]Tokenizing train dataset:  43%|████▎     | 3171/7336 [00:10<00:14, 294.16 examples/s]Tokenizing train dataset:  43%|████▎     | 3126/7336 [00:10<00:14, 295.40 examples/s]Tokenizing train dataset:  43%|████▎     | 3126/7336 [00:10<00:14, 295.67 examples/s]Tokenizing train dataset:  44%|████▎     | 3205/7336 [00:10<00:13, 302.95 examples/s]Tokenizing train dataset:  43%|████▎     | 3171/7336 [00:10<00:14, 290.34 examples/s]Tokenizing train dataset:  44%|████▍     | 3237/7336 [00:10<00:13, 303.23 examples/s]Tokenizing train dataset:  43%|████▎     | 3171/7336 [00:10<00:14, 290.34 examples/s]Tokenizing train dataset:  44%|████▎     | 3205/7336 [00:10<00:13, 299.07 examples/s]Tokenizing train dataset:  45%|████▍     | 3271/7336 [00:10<00:13, 307.72 examples/s]Tokenizing train dataset:  44%|████▎     | 3205/7336 [00:10<00:13, 298.81 examples/s]Tokenizing train dataset:  44%|████▍     | 3237/7336 [00:10<00:13, 299.43 examples/s]Tokenizing train dataset:  45%|████▌     | 3302/7336 [00:10<00:13, 306.64 examples/s]Tokenizing train dataset:  44%|████▍     | 3237/7336 [00:10<00:13, 299.02 examples/s]Tokenizing train dataset:  45%|████▍     | 3270/7336 [00:10<00:13, 305.32 examples/s]Tokenizing train dataset:  45%|████▌     | 3333/7336 [00:10<00:13, 302.37 examples/s]Tokenizing train dataset:  45%|████▍     | 3270/7336 [00:10<00:13, 304.72 examples/s]Tokenizing train dataset:  45%|████▌     | 3302/7336 [00:10<00:13, 302.56 examples/s]Tokenizing train dataset:  46%|████▌     | 3365/7336 [00:10<00:13, 302.18 examples/s]Tokenizing train dataset:  45%|████▌     | 3302/7336 [00:10<00:13, 301.78 examples/s]Tokenizing train dataset:  46%|████▌     | 3349/7336 [00:11<00:13, 305.74 examples/s]Tokenizing train dataset:  46%|████▋     | 3402/7336 [00:11<00:14, 277.28 examples/s]Tokenizing train dataset:  46%|████▌     | 3349/7336 [00:11<00:13, 305.12 examples/s]Tokenizing train dataset:  46%|████▌     | 3388/7336 [00:11<00:13, 285.62 examples/s]Tokenizing train dataset:  47%|████▋     | 3441/7336 [00:11<00:14, 268.15 examples/s]Tokenizing train dataset:  46%|████▌     | 3388/7336 [00:11<00:13, 284.85 examples/s]Tokenizing train dataset:  47%|████▋     | 3417/7336 [00:11<00:13, 283.96 examples/s]Tokenizing train dataset:  47%|████▋     | 3417/7336 [00:11<00:13, 283.55 examples/s]Tokenizing train dataset:  47%|████▋     | 3477/7336 [00:11<00:15, 254.00 examples/s]Tokenizing train dataset:  47%|████▋     | 3455/7336 [00:11<00:14, 266.61 examples/s]Tokenizing train dataset:  48%|████▊     | 3508/7336 [00:11<00:14, 264.00 examples/s]Tokenizing train dataset:  47%|████▋     | 3455/7336 [00:11<00:14, 266.48 examples/s]Tokenizing train dataset:  48%|████▊     | 3545/7336 [00:11<00:13, 287.63 examples/s]Tokenizing train dataset:  48%|████▊     | 3489/7336 [00:11<00:15, 250.91 examples/s]Tokenizing train dataset:  48%|████▊     | 3489/7336 [00:11<00:15, 250.51 examples/s]Tokenizing train dataset:  49%|████▉     | 3582/7336 [00:11<00:12, 303.97 examples/s]Tokenizing train dataset:  48%|████▊     | 3530/7336 [00:11<00:13, 286.49 examples/s]Tokenizing train dataset:  48%|████▊     | 3530/7336 [00:11<00:13, 286.07 examples/s]Tokenizing train dataset:  49%|████▊     | 3561/7336 [00:11<00:13, 289.18 examples/s]Tokenizing train dataset:  49%|████▉     | 3629/7336 [00:11<00:12, 303.16 examples/s]Tokenizing train dataset:  49%|████▊     | 3561/7336 [00:11<00:13, 288.61 examples/s]Tokenizing train dataset:  49%|████▉     | 3595/7336 [00:11<00:12, 301.23 examples/s]Tokenizing train dataset:  50%|████▉     | 3663/7336 [00:12<00:11, 308.71 examples/s]Tokenizing train dataset:  49%|████▉     | 3595/7336 [00:11<00:12, 300.68 examples/s]Tokenizing train dataset:  49%|████▉     | 3627/7336 [00:12<00:12, 300.30 examples/s]Tokenizing train dataset:  49%|████▉     | 3627/7336 [00:12<00:12, 299.58 examples/s]Tokenizing train dataset:  51%|█████     | 3708/7336 [00:12<00:12, 301.24 examples/s]Tokenizing train dataset:  50%|████▉     | 3661/7336 [00:12<00:11, 308.25 examples/s]Tokenizing train dataset:  50%|████▉     | 3661/7336 [00:12<00:11, 307.56 examples/s]Tokenizing train dataset:  51%|█████     | 3752/7336 [00:12<00:10, 332.91 examples/s]Tokenizing train dataset:  51%|█████     | 3707/7336 [00:12<00:12, 299.15 examples/s]Tokenizing train dataset:  51%|█████     | 3707/7336 [00:12<00:12, 298.53 examples/s]Tokenizing train dataset:  52%|█████▏    | 3803/7336 [00:12<00:10, 332.02 examples/s]Tokenizing train dataset:  51%|█████     | 3751/7336 [00:12<00:10, 330.94 examples/s]Tokenizing train dataset:  51%|█████     | 3750/7336 [00:12<00:10, 330.71 examples/s]Tokenizing train dataset:  52%|█████▏    | 3840/7336 [00:12<00:10, 337.57 examples/s]Tokenizing train dataset:  52%|█████▏    | 3801/7336 [00:12<00:10, 325.73 examples/s]Tokenizing train dataset:  52%|█████▏    | 3800/7336 [00:12<00:10, 326.44 examples/s]Tokenizing train dataset:  53%|█████▎    | 3888/7336 [00:12<00:10, 330.52 examples/s]Tokenizing train dataset:  52%|█████▏    | 3840/7336 [00:12<00:10, 334.12 examples/s]Tokenizing train dataset:  52%|█████▏    | 3838/7336 [00:12<00:10, 336.32 examples/s]Tokenizing train dataset:  54%|█████▎    | 3929/7336 [00:12<00:11, 309.30 examples/s]Tokenizing train dataset:  53%|█████▎    | 3887/7336 [00:12<00:10, 325.52 examples/s]Tokenizing train dataset:  53%|█████▎    | 3884/7336 [00:12<00:10, 323.56 examples/s]Tokenizing train dataset:  54%|█████▍    | 3980/7336 [00:13<00:10, 314.69 examples/s]Tokenizing train dataset:  54%|█████▎    | 3929/7336 [00:12<00:11, 306.45 examples/s]Tokenizing train dataset:  54%|█████▎    | 3925/7336 [00:12<00:11, 301.90 examples/s]Tokenizing train dataset:  55%|█████▍    | 4020/7336 [00:13<00:10, 330.03 examples/s]Tokenizing train dataset:  54%|█████▍    | 3979/7336 [00:13<00:10, 312.00 examples/s]Tokenizing train dataset:  54%|█████▍    | 3977/7336 [00:13<00:10, 311.36 examples/s]Tokenizing train dataset:  55%|█████▌    | 4067/7336 [00:13<00:10, 318.49 examples/s]Tokenizing train dataset:  55%|█████▍    | 4018/7336 [00:13<00:10, 328.77 examples/s]Tokenizing train dataset:  55%|█████▍    | 4016/7336 [00:13<00:10, 327.26 examples/s]Tokenizing train dataset:  56%|█████▌    | 4100/7336 [00:13<00:10, 317.73 examples/s]Tokenizing train dataset:  55%|█████▌    | 4060/7336 [00:13<00:10, 310.15 examples/s]Tokenizing train dataset:  55%|█████▌    | 4060/7336 [00:13<00:10, 309.91 examples/s]Tokenizing train dataset:  56%|█████▋    | 4142/7336 [00:13<00:10, 302.84 examples/s]Tokenizing train dataset:  56%|█████▌    | 4095/7336 [00:13<00:10, 313.41 examples/s]Tokenizing train dataset:  56%|█████▌    | 4095/7336 [00:13<00:10, 312.93 examples/s]Tokenizing train dataset:  57%|█████▋    | 4174/7336 [00:13<00:10, 302.85 examples/s]Tokenizing train dataset:  56%|█████▋    | 4140/7336 [00:13<00:10, 302.74 examples/s]Tokenizing train dataset:  58%|█████▊    | 4251/7336 [00:13<00:07, 419.57 examples/s]Tokenizing train dataset:  56%|█████▋    | 4140/7336 [00:13<00:10, 302.37 examples/s]Tokenizing train dataset:  60%|█████▉    | 4382/7336 [00:13<00:04, 653.93 examples/s]Tokenizing train dataset:  57%|█████▋    | 4186/7336 [00:13<00:10, 299.17 examples/s]Tokenizing train dataset:  61%|██████▏   | 4510/7336 [00:13<00:03, 823.17 examples/s]Tokenizing train dataset:  57%|█████▋    | 4186/7336 [00:13<00:10, 298.68 examples/s]Tokenizing train dataset:  59%|█████▊    | 4305/7336 [00:13<00:06, 500.91 examples/s]Tokenizing train dataset:  63%|██████▎   | 4638/7336 [00:14<00:02, 948.13 examples/s]Tokenizing train dataset:  59%|█████▊    | 4304/7336 [00:13<00:06, 498.34 examples/s]Tokenizing train dataset:  61%|██████    | 4440/7336 [00:13<00:04, 705.97 examples/s]Tokenizing train dataset:  65%|██████▍   | 4764/7336 [00:14<00:02, 1036.67 examples/s]Tokenizing train dataset:  60%|██████    | 4438/7336 [00:14<00:04, 701.83 examples/s]Tokenizing train dataset:  62%|██████▏   | 4569/7336 [00:14<00:03, 855.15 examples/s]Tokenizing train dataset:  67%|██████▋   | 4893/7336 [00:14<00:02, 1104.98 examples/s]Tokenizing train dataset:  62%|██████▏   | 4565/7336 [00:14<00:03, 846.30 examples/s]Tokenizing train dataset:  64%|██████▍   | 4691/7336 [00:14<00:02, 949.77 examples/s]Tokenizing train dataset:  68%|██████▊   | 5020/7336 [00:14<00:02, 1148.07 examples/s]Tokenizing train dataset:  64%|██████▍   | 4686/7336 [00:14<00:02, 942.15 examples/s]Tokenizing train dataset:  66%|██████▌   | 4824/7336 [00:14<00:02, 1052.44 examples/s]Tokenizing train dataset:  70%|███████   | 5144/7336 [00:14<00:01, 1172.84 examples/s]Tokenizing train dataset:  66%|██████▌   | 4818/7336 [00:14<00:02, 1043.93 examples/s]Tokenizing train dataset:  68%|██████▊   | 4955/7336 [00:14<00:02, 1118.50 examples/s]Tokenizing train dataset:  72%|███████▏  | 5265/7336 [00:14<00:01, 1181.26 examples/s]Tokenizing train dataset:  67%|██████▋   | 4944/7336 [00:14<00:02, 1101.58 examples/s]Tokenizing train dataset:  69%|██████▉   | 5077/7336 [00:14<00:01, 1146.53 examples/s]Tokenizing train dataset:  69%|██████▉   | 5067/7336 [00:14<00:02, 1133.55 examples/s]Tokenizing train dataset:  71%|███████   | 5201/7336 [00:14<00:01, 1169.72 examples/s]Tokenizing train dataset:  74%|███████▍  | 5443/7336 [00:14<00:01, 1179.82 examples/s]Tokenizing train dataset:  71%|███████   | 5191/7336 [00:14<00:01, 1161.99 examples/s]Tokenizing train dataset:  76%|███████▌  | 5565/7336 [00:14<00:01, 1187.32 examples/s]Tokenizing train dataset:  73%|███████▎  | 5379/7336 [00:14<00:01, 1171.69 examples/s]Tokenizing train dataset:  72%|███████▏  | 5310/7336 [00:14<00:01, 1166.68 examples/s]Tokenizing train dataset:  78%|███████▊  | 5688/7336 [00:14<00:01, 1196.01 examples/s]Tokenizing train dataset:  75%|███████▌  | 5504/7336 [00:14<00:01, 1190.11 examples/s]Tokenizing train dataset:  79%|███████▉  | 5810/7336 [00:15<00:01, 1201.40 examples/s]Tokenizing train dataset:  75%|███████▍  | 5493/7336 [00:14<00:01, 1184.51 examples/s]Tokenizing train dataset:  77%|███████▋  | 5684/7336 [00:14<00:01, 1189.66 examples/s]Tokenizing train dataset:  81%|████████  | 5932/7336 [00:15<00:01, 1204.02 examples/s]Tokenizing train dataset:  77%|███████▋  | 5672/7336 [00:15<00:01, 1183.40 examples/s]Tokenizing train dataset:  79%|███████▉  | 5810/7336 [00:15<00:01, 1201.73 examples/s]Tokenizing train dataset:  83%|████████▎ | 6061/7336 [00:15<00:01, 1227.67 examples/s]Tokenizing train dataset:  79%|███████▉  | 5793/7336 [00:15<00:01, 1187.91 examples/s]Tokenizing train dataset:  81%|████████  | 5933/7336 [00:15<00:01, 1208.57 examples/s]Tokenizing train dataset:  85%|████████▌ | 6248/7336 [00:15<00:00, 1230.25 examples/s]Tokenizing train dataset:  81%|████████  | 5915/7336 [00:15<00:01, 1194.03 examples/s]Tokenizing train dataset:  83%|████████▎ | 6063/7336 [00:15<00:01, 1229.77 examples/s]Tokenizing train dataset:  82%|████████▏ | 6043/7336 [00:15<00:01, 1214.98 examples/s]Tokenizing train dataset:  88%|████████▊ | 6428/7336 [00:15<00:00, 1213.50 examples/s]Tokenizing train dataset:  85%|████████▌ | 6248/7336 [00:15<00:00, 1229.39 examples/s]Tokenizing train dataset:  89%|████████▉ | 6552/7336 [00:15<00:00, 1218.56 examples/s]Tokenizing train dataset:  85%|████████▍ | 6228/7336 [00:15<00:00, 1219.55 examples/s]Tokenizing train dataset:  88%|████████▊ | 6427/7336 [00:15<00:00, 1212.46 examples/s]Tokenizing train dataset:  91%|█████████ | 6678/7336 [00:15<00:00, 1226.92 examples/s]Tokenizing train dataset:  87%|████████▋ | 6351/7336 [00:15<00:00, 1216.91 examples/s]Tokenizing train dataset:  89%|████████▉ | 6551/7336 [00:15<00:00, 1215.64 examples/s]Tokenizing train dataset:  94%|█████████▎| 6861/7336 [00:15<00:00, 1221.31 examples/s]Tokenizing train dataset:  89%|████████▉ | 6531/7336 [00:15<00:00, 1205.06 examples/s]Tokenizing train dataset:  91%|█████████ | 6678/7336 [00:15<00:00, 1226.45 examples/s]Tokenizing train dataset:  91%|█████████ | 6655/7336 [00:15<00:00, 1212.41 examples/s]Tokenizing train dataset:  96%|█████████▌| 7043/7336 [00:16<00:00, 1212.63 examples/s]Tokenizing train dataset:  94%|█████████▎| 6861/7336 [00:15<00:00, 1221.52 examples/s]Tokenizing train dataset:  93%|█████████▎| 6838/7336 [00:16<00:00, 1213.44 examples/s]Tokenizing train dataset:  99%|█████████▊| 7226/7336 [00:16<00:00, 1212.81 examples/s]Tokenizing train dataset:  96%|█████████▌| 7042/7336 [00:16<00:00, 1213.25 examples/s]Tokenizing train dataset: 100%|██████████| 7336/7336 [00:16<00:00, 450.95 examples/s] 
Tokenizing train dataset:  96%|█████████▌| 7019/7336 [00:16<00:00, 1208.01 examples/s]Tokenizing train dataset:  99%|█████████▊| 7227/7336 [00:16<00:00, 1215.77 examples/s]Tokenizing train dataset:  98%|█████████▊| 7198/7336 [00:16<00:00, 1201.08 examples/s]Tokenizing train dataset: 100%|██████████| 7336/7336 [00:16<00:00, 448.50 examples/s] 
Tokenizing train dataset: 100%|██████████| 7336/7336 [00:16<00:00, 1194.72 examples/s]Tokenizing train dataset: 100%|██████████| 7336/7336 [00:16<00:00, 446.57 examples/s] 
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  59%|█████▉    | 566/953 [00:00<00:00, 5615.10 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5541.63 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5614.54 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5669.17 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5569.56 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5544.82 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  32%|███▏      | 306/953 [00:00<00:00, 3033.18 examples/s]Applying chat template to eval dataset:  32%|███▏      | 302/953 [00:00<00:00, 2990.99 examples/s]Applying chat template to eval dataset:  32%|███▏      | 303/953 [00:00<00:00, 3004.38 examples/s]Applying chat template to eval dataset:  66%|██████▌   | 625/953 [00:00<00:00, 3120.97 examples/s]Applying chat template to eval dataset:  65%|██████▍   | 617/953 [00:00<00:00, 3079.43 examples/s]Applying chat template to eval dataset:  65%|██████▍   | 619/953 [00:00<00:00, 3090.75 examples/s]Applying chat template to eval dataset:  99%|█████████▉| 943/953 [00:00<00:00, 3143.86 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3109.47 examples/s]
Applying chat template to eval dataset:  98%|█████████▊| 930/953 [00:00<00:00, 3095.74 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3066.60 examples/s]
Applying chat template to eval dataset:  98%|█████████▊| 933/953 [00:00<00:00, 3107.94 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3077.09 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 324.81 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 320.94 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.88 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:02, 292.63 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 288.21 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 287.32 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:02, 280.46 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 276.77 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 275.87 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 271.61 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.53 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 266.71 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 256.49 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:02, 280.84 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.03 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 252.49 examples/s]Tokenizing eval dataset:  31%|███       | 297/953 [00:00<00:01, 379.96 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 269.34 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 268.86 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 443.87 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:00<00:01, 370.77 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:00<00:01, 369.84 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 502.18 examples/s]Tokenizing eval dataset:  37%|███▋      | 357/953 [00:01<00:01, 443.52 examples/s]Tokenizing eval dataset:  37%|███▋      | 357/953 [00:01<00:01, 442.14 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 545.35 examples/s]Tokenizing eval dataset:  44%|████▍     | 419/953 [00:01<00:01, 490.38 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 487.79 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 578.46 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 543.45 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 541.06 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 599.63 examples/s]Tokenizing eval dataset:  58%|█████▊    | 555/953 [00:01<00:00, 577.98 examples/s]Tokenizing eval dataset:  58%|█████▊    | 555/953 [00:01<00:00, 575.20 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 603.13 examples/s]Tokenizing eval dataset:  65%|██████▍   | 617/953 [00:01<00:00, 588.77 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 584.14 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 598.93 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 595.63 examples/s]Tokenizing eval dataset:  81%|████████▏ | 776/953 [00:01<00:00, 580.97 examples/s]Tokenizing eval dataset:  80%|████████  | 767/953 [00:01<00:00, 579.90 examples/s]Tokenizing eval dataset:  80%|████████  | 767/953 [00:01<00:00, 576.64 examples/s]Tokenizing eval dataset:  89%|████████▉ | 850/953 [00:01<00:00, 542.47 examples/s]Tokenizing eval dataset:  88%|████████▊ | 840/953 [00:01<00:00, 542.23 examples/s]Tokenizing eval dataset:  88%|████████▊ | 840/953 [00:01<00:00, 539.08 examples/s]Tokenizing eval dataset:  97%|█████████▋| 924/953 [00:02<00:00, 520.47 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 459.78 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  96%|█████████▌| 913/953 [00:02<00:00, 515.02 examples/s]Tokenizing eval dataset:  96%|█████████▌| 912/953 [00:02<00:00, 515.74 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 455.17 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 453.21 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4470772743225098 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3661067485809326 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3973727226257324 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4613704681396484 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank4]:[W608 21:47:27.851711498 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 1 ---
