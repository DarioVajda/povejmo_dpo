cpu-bind=MASK - gn57, task  2  0 [3908915]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 2 ---
Total Nodes: 4
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn49
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 4     --machine_rank 2     --main_process_ip gn49     --main_process_port 29500     --num_processes 16     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62793424     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.2 --curriculum_stage=1
-------------------------------------------
[2025-06-09 01:29:43,388] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0609 01:29:45.220000 3908968 torch/distributed/run.py:792] 
W0609 01:29:45.220000 3908968 torch/distributed/run.py:792] *****************************************
W0609 01:29:45.220000 3908968 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0609 01:29:45.220000 3908968 torch/distributed/run.py:792] *****************************************
[2025-06-09 01:29:50,241] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-09 01:29:50,265] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-09 01:29:50,308] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-09 01:29:50,316] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Training data of type 'bad_lang_examples':    3489
Training data of type 'short_examples':       699
Training data of type 'choose_examples':      13379
Training data of type 'bad_format_examples':  3148
**************************************************
Training data of type 'bad_lang_examples':   Training data of type 'bad_lang_examples':     34893489

Training data of type 'short_examples':      Training data of type 'short_examples':        699699

Training data of type 'choose_examples':     Training data of type 'choose_examples':       1337913379

Training data of type 'bad_format_examples': Training data of type 'bad_format_examples':   31483148

****************************************************************************************************

Training data of type 'bad_lang_examples':    3489
Training data of type 'short_examples':       699
Training data of type 'choose_examples':      13379
Training data of type 'bad_format_examples':  3148
**************************************************
Evaluation data size: 953
Evaluation data size: 953
Evaluation data size: 953
Evaluation data size: 953
Curriculum stage 0 training data size: 7336
Curriculum stage 0 training data size: 7336
Curriculum stage 0 training data size: 7336
Curriculum stage 0 training data size: 7336
Curriculum stage 1 training data size: 6689
Curriculum stage 1 training data size: 6689
Curriculum stage 1 training data size: 6689
Curriculum stage 1 training data size: 6689
Curriculum stage 2 training data size: 6690
Curriculum stage 2 training data size: 6690
Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
4e-07
World size: 16
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
4e-07
Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
4e-07
Created datasets
Train dataset size: 6689
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-09 01:29:54,246] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-09 01:29:54,248] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-09 01:29:54,250] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-09 01:29:54,250] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curri-0
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:12, 24.31s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.79s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:48<00:47, 23.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:48<00:48, 24.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:48<00:48, 24.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:48<00:48, 24.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:11<00:23, 23.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:11<00:23, 23.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:11<00:23, 23.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:11<00:23, 23.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.21s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 21.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 21.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 21.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.26s/it]
Loaded model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s][rank9]:[W609 01:31:28.762903349 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank11]:[W609 01:31:28.769132454 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   9%|▊         | 572/6689 [00:00<00:01, 5649.95 examples/s][rank10]:[W609 01:31:29.800102467 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  17%|█▋        | 1158/6689 [00:00<00:00, 5770.88 examples/s]Extracting prompt in train dataset:  30%|███       | 2021/6689 [00:00<00:00, 5757.13 examples/s]Extracting prompt in train dataset:  43%|████▎     | 2880/6689 [00:00<00:00, 5737.43 examples/s]Extracting prompt in train dataset:  52%|█████▏    | 3460/6689 [00:00<00:00, 4570.00 examples/s]Extracting prompt in train dataset:  60%|██████    | 4020/6689 [00:00<00:00, 4818.81 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 4600/6689 [00:00<00:00, 5063.15 examples/s]Extracting prompt in train dataset:  77%|███████▋  | 5180/6689 [00:00<00:00, 5249.46 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 5760/6689 [00:01<00:00, 5389.18 examples/s]Extracting prompt in train dataset:  99%|█████████▊| 6590/6689 [00:01<00:00, 5424.10 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5243.67 examples/s]
Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 300/6689 [00:00<00:02, 2959.07 examples/s]Applying chat template to train dataset:   9%|▉         | 629/6689 [00:00<00:01, 3145.20 examples/s]Applying chat template to train dataset:  14%|█▍        | 956/6689 [00:00<00:01, 3194.79 examples/s]Applying chat template to train dataset:  19%|█▉        | 1284/6689 [00:00<00:01, 3222.89 examples/s]Applying chat template to train dataset:  24%|██▍       | 1608/6689 [00:00<00:01, 3223.89 examples/s]Applying chat template to train dataset:  29%|██▉       | 1934/6689 [00:00<00:01, 3230.79 examples/s]Applying chat template to train dataset:  36%|███▌      | 2418/6689 [00:00<00:01, 3225.79 examples/s]Applying chat template to train dataset:  43%|████▎     | 2903/6689 [00:00<00:01, 3225.72 examples/s]Applying chat template to train dataset:  51%|█████     | 3386/6689 [00:01<00:01, 3222.45 examples/s]Applying chat template to train dataset:  55%|█████▌    | 3709/6689 [00:01<00:00, 3221.86 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4190/6689 [00:01<00:00, 3211.44 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4515/6689 [00:01<00:00, 3219.69 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4838/6689 [00:01<00:00, 3220.39 examples/s]Applying chat template to train dataset:  80%|███████▉  | 5322/6689 [00:01<00:00, 3219.04 examples/s]Applying chat template to train dataset:  84%|████████▍ | 5646/6689 [00:01<00:00, 3221.24 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6130/6689 [00:01<00:00, 3215.77 examples/s]Applying chat template to train dataset:  99%|█████████▊| 6593/6689 [00:02<00:00, 3169.71 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3196.19 examples/s]
Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   2%|▏         | 104/6689 [00:00<00:06, 1022.04 examples/s]Tokenizing train dataset:   3%|▎         | 221/6689 [00:00<00:05, 1105.24 examples/s]Tokenizing train dataset:   5%|▌         | 336/6689 [00:00<00:05, 1117.57 examples/s]Tokenizing train dataset:   7%|▋         | 495/6689 [00:00<00:05, 1076.12 examples/s]Tokenizing train dataset:  10%|▉         | 651/6689 [00:00<00:05, 1053.28 examples/s]Tokenizing train dataset:  12%|█▏        | 793/6689 [00:00<00:05, 1009.98 examples/s]Tokenizing train dataset:  14%|█▍        | 924/6689 [00:00<00:06, 960.20 examples/s] Tokenizing train dataset:  16%|█▌        | 1054/6689 [00:01<00:06, 922.72 examples/s]Tokenizing train dataset:  18%|█▊        | 1179/6689 [00:01<00:06, 888.75 examples/s]Tokenizing train dataset:  19%|█▉        | 1295/6689 [00:01<00:06, 846.77 examples/s]Tokenizing train dataset:  21%|██        | 1410/6689 [00:01<00:06, 814.97 examples/s]Tokenizing train dataset:  23%|██▎       | 1513/6689 [00:01<00:06, 772.55 examples/s]Tokenizing train dataset:  24%|██▍       | 1618/6689 [00:01<00:06, 742.38 examples/s]Tokenizing train dataset:  26%|██▌       | 1724/6689 [00:01<00:06, 726.69 examples/s]Tokenizing train dataset:  27%|██▋       | 1822/6689 [00:02<00:06, 700.30 examples/s]Tokenizing train dataset:  29%|██▉       | 1930/6689 [00:02<00:06, 703.20 examples/s]Tokenizing train dataset:  30%|███       | 2029/6689 [00:02<00:06, 684.64 examples/s]Tokenizing train dataset:  31%|███▏      | 2104/6689 [00:02<00:06, 697.03 examples/s]Tokenizing train dataset:  33%|███▎      | 2200/6689 [00:02<00:06, 671.14 examples/s]Tokenizing train dataset:  34%|███▍      | 2299/6689 [00:02<00:06, 662.09 examples/s]Tokenizing train dataset:  35%|███▌      | 2366/6689 [00:02<00:06, 661.12 examples/s]Tokenizing train dataset:  37%|███▋      | 2456/6689 [00:03<00:06, 637.20 examples/s]Tokenizing train dataset:  38%|███▊      | 2547/6689 [00:03<00:06, 625.66 examples/s]Tokenizing train dataset:  39%|███▉      | 2618/6689 [00:03<00:06, 639.81 examples/s]Tokenizing train dataset:  40%|████      | 2684/6689 [00:03<00:06, 637.42 examples/s]Tokenizing train dataset:  41%|████▏     | 2761/6689 [00:03<00:06, 593.87 examples/s]Tokenizing train dataset:  42%|████▏     | 2822/6689 [00:03<00:06, 595.37 examples/s]Tokenizing train dataset:  44%|████▎     | 2912/6689 [00:03<00:06, 591.52 examples/s]Tokenizing train dataset:  45%|████▍     | 2996/6689 [00:04<00:06, 576.26 examples/s]Tokenizing train dataset:  46%|████▌     | 3077/6689 [00:04<00:06, 559.36 examples/s]Tokenizing train dataset:  47%|████▋     | 3134/6689 [00:04<00:06, 560.73 examples/s]Tokenizing train dataset:  48%|████▊     | 3191/6689 [00:04<00:06, 560.60 examples/s]Tokenizing train dataset:  49%|████▉     | 3270/6689 [00:04<00:06, 544.77 examples/s]Tokenizing train dataset:  50%|█████     | 3352/6689 [00:04<00:06, 541.05 examples/s]Tokenizing train dataset:  51%|█████     | 3417/6689 [00:04<00:05, 563.76 examples/s]Tokenizing train dataset:  52%|█████▏    | 3510/6689 [00:04<00:05, 578.56 examples/s]Tokenizing train dataset:  54%|█████▎    | 3583/6689 [00:05<00:05, 541.56 examples/s]Tokenizing train dataset:  55%|█████▍    | 3664/6689 [00:05<00:05, 539.16 examples/s]Tokenizing train dataset:  56%|█████▌    | 3740/6689 [00:05<00:05, 525.91 examples/s]Tokenizing train dataset:  57%|█████▋    | 3793/6689 [00:05<00:05, 524.83 examples/s]Tokenizing train dataset:  58%|█████▊    | 3847/6689 [00:05<00:05, 524.35 examples/s]Tokenizing train dataset:  59%|█████▊    | 3925/6689 [00:05<00:05, 519.57 examples/s]Tokenizing train dataset:  60%|█████▉    | 3999/6689 [00:05<00:05, 500.51 examples/s]Tokenizing train dataset:  61%|██████    | 4071/6689 [00:06<00:05, 492.96 examples/s]Tokenizing train dataset:  62%|██████▏   | 4130/6689 [00:06<00:04, 513.50 examples/s]Tokenizing train dataset:  63%|██████▎   | 4184/6689 [00:06<00:04, 516.53 examples/s]Tokenizing train dataset:  63%|██████▎   | 4241/6689 [00:06<00:04, 530.18 examples/s]Tokenizing train dataset:  64%|██████▍   | 4314/6689 [00:06<00:04, 513.10 examples/s]Tokenizing train dataset:  65%|██████▌   | 4370/6689 [00:06<00:04, 523.75 examples/s]Tokenizing train dataset:  67%|██████▋   | 4451/6689 [00:06<00:04, 525.99 examples/s]Tokenizing train dataset:  68%|██████▊   | 4530/6689 [00:06<00:04, 520.58 examples/s]Tokenizing train dataset:  69%|██████▉   | 4599/6689 [00:07<00:04, 499.84 examples/s]Tokenizing train dataset:  70%|██████▉   | 4670/6689 [00:07<00:04, 489.18 examples/s]Tokenizing train dataset:  71%|███████   | 4727/6689 [00:07<00:03, 504.82 examples/s]Tokenizing train dataset:  72%|███████▏  | 4783/6689 [00:07<00:03, 517.65 examples/s]Tokenizing train dataset:  73%|███████▎  | 4857/6689 [00:07<00:03, 507.54 examples/s]Tokenizing train dataset:  74%|███████▎  | 4930/6689 [00:07<00:03, 496.44 examples/s]Tokenizing train dataset:  75%|███████▍  | 4985/6689 [00:07<00:03, 507.02 examples/s]Tokenizing train dataset:  75%|███████▌  | 5037/6689 [00:07<00:03, 507.80 examples/s]Tokenizing train dataset:  76%|███████▋  | 5107/6689 [00:08<00:03, 484.69 examples/s]Tokenizing train dataset:  77%|███████▋  | 5160/6689 [00:08<00:03, 494.47 examples/s]Tokenizing train dataset:  78%|███████▊  | 5213/6689 [00:08<00:02, 501.49 examples/s]Tokenizing train dataset:  79%|███████▉  | 5271/6689 [00:08<00:02, 519.71 examples/s]Tokenizing train dataset:  80%|███████▉  | 5345/6689 [00:08<00:02, 508.52 examples/s]Tokenizing train dataset:  81%|████████  | 5412/6689 [00:08<00:02, 481.01 examples/s]Tokenizing train dataset:  82%|████████▏ | 5463/6689 [00:08<00:02, 485.70 examples/s]Tokenizing train dataset:  82%|████████▏ | 5517/6689 [00:08<00:02, 492.88 examples/s]Tokenizing train dataset:  83%|████████▎ | 5567/6689 [00:09<00:02, 493.54 examples/s]Tokenizing train dataset:  84%|████████▍ | 5620/6689 [00:09<00:02, 501.62 examples/s]Tokenizing train dataset:  85%|████████▌ | 5695/6689 [00:09<00:02, 496.94 examples/s]Tokenizing train dataset:  86%|████████▋ | 5770/6689 [00:09<00:01, 492.09 examples/s]Tokenizing train dataset:  87%|████████▋ | 5840/6689 [00:09<00:01, 479.74 examples/s]Tokenizing train dataset:  88%|████████▊ | 5910/6689 [00:09<00:01, 470.16 examples/s]Tokenizing train dataset:  89%|████████▉ | 5967/6689 [00:09<00:01, 488.05 examples/s]Tokenizing train dataset:  90%|█████████ | 6037/6689 [00:10<00:01, 476.61 examples/s]Tokenizing train dataset:  91%|█████████ | 6089/6689 [00:10<00:01, 482.91 examples/s]Tokenizing train dataset:  92%|█████████▏| 6146/6689 [00:10<00:01, 503.13 examples/s]Tokenizing train dataset:  93%|█████████▎| 6206/6689 [00:10<00:00, 525.86 examples/s]Tokenizing train dataset:  94%|█████████▎| 6266/6689 [00:10<00:00, 542.37 examples/s]Tokenizing train dataset:  95%|█████████▍| 6338/6689 [00:10<00:00, 511.57 examples/s]Tokenizing train dataset:  96%|█████████▌| 6405/6689 [00:10<00:00, 482.35 examples/s]Tokenizing train dataset:  97%|█████████▋| 6474/6689 [00:10<00:00, 474.50 examples/s]Tokenizing train dataset:  98%|█████████▊| 6550/6689 [00:11<00:00, 482.19 examples/s]Tokenizing train dataset:  99%|█████████▉| 6627/6689 [00:11<00:00, 489.87 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 483.97 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 590.13 examples/s]
[rank8]:[W609 01:31:44.325067537 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▊         | 572/6689 [00:00<00:01, 5674.24 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 563/953 [00:00<00:00, 5589.88 examples/s]Extracting prompt in train dataset:   9%|▊         | 580/6689 [00:00<00:01, 5721.57 examples/s]Extracting prompt in train dataset:   9%|▉         | 590/6689 [00:00<00:01, 5758.08 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5587.07 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1160/6689 [00:00<00:00, 5764.65 examples/s]Extracting prompt in train dataset:  18%|█▊        | 1180/6689 [00:00<00:00, 5827.11 examples/s]Extracting prompt in train dataset:  18%|█▊        | 1190/6689 [00:00<00:00, 5864.42 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1741/6689 [00:00<00:00, 5772.11 examples/s]Extracting prompt in train dataset:  26%|██▋       | 1770/6689 [00:00<00:00, 5845.94 examples/s]Extracting prompt in train dataset:  27%|██▋       | 1790/6689 [00:00<00:00, 5881.23 examples/s]Extracting prompt in train dataset:  35%|███▍      | 2320/6689 [00:00<00:00, 5762.06 examples/s]Extracting prompt in train dataset:  36%|███▌      | 2380/6689 [00:00<00:00, 5865.37 examples/s]Extracting prompt in train dataset:  40%|███▉      | 2646/6689 [00:00<00:00, 5841.58 examples/s]Extracting prompt in train dataset:  43%|████▎     | 2900/6689 [00:00<00:00, 5773.44 examples/s]Extracting prompt in train dataset:  45%|████▍     | 2980/6689 [00:00<00:00, 5871.82 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  52%|█████▏    | 3480/6689 [00:00<00:00, 5777.85 examples/s]Extracting prompt in train dataset:  53%|█████▎    | 3526/6689 [00:00<00:00, 5838.54 examples/s]Extracting prompt in train dataset:  53%|█████▎    | 3570/6689 [00:00<00:00, 5874.47 examples/s]Applying chat template to eval dataset:  33%|███▎      | 313/953 [00:00<00:00, 3105.50 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 638/953 [00:00<00:00, 3183.99 examples/s]Extracting prompt in train dataset:  65%|██████▍   | 4330/6689 [00:00<00:00, 5727.98 examples/s]Extracting prompt in train dataset:  66%|██████▌   | 4387/6689 [00:00<00:00, 5793.44 examples/s]Extracting prompt in train dataset:  66%|██████▋   | 4440/6689 [00:00<00:00, 5825.37 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3166.27 examples/s]
Extracting prompt in train dataset:  73%|███████▎  | 4910/6689 [00:00<00:00, 5737.11 examples/s]Extracting prompt in train dataset:  74%|███████▍  | 4968/6689 [00:00<00:00, 5797.40 examples/s]Extracting prompt in train dataset:  75%|███████▌  | 5030/6689 [00:00<00:00, 5831.25 examples/s]Extracting prompt in train dataset:  82%|████████▏ | 5490/6689 [00:00<00:00, 5739.79 examples/s]Extracting prompt in train dataset:  84%|████████▍ | 5620/6689 [00:00<00:00, 5838.50 examples/s]Extracting prompt in train dataset:  87%|████████▋ | 5836/6689 [00:01<00:00, 5792.20 examples/s]Extracting prompt in train dataset:  91%|█████████ | 6070/6689 [00:01<00:00, 5743.25 examples/s]Extracting prompt in train dataset:  93%|█████████▎| 6210/6689 [00:01<00:00, 5843.60 examples/s]Extracting prompt in train dataset: 100%|█████████▉| 6671/6689 [00:01<00:00, 5710.50 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5778.35 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5733.91 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5642.12 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5679.93 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.91 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 289.19 examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 276.05 examples/s]Applying chat template to train dataset:   5%|▍         | 306/6689 [00:00<00:02, 3024.43 examples/s]Applying chat template to train dataset:   4%|▍         | 289/6689 [00:00<00:02, 2868.57 examples/s]Applying chat template to train dataset:   4%|▍         | 290/6689 [00:00<00:02, 2863.79 examples/s]Applying chat template to train dataset:  10%|▉         | 638/6689 [00:00<00:01, 3192.90 examples/s]Applying chat template to train dataset:   9%|▉         | 607/6689 [00:00<00:01, 3045.00 examples/s]Applying chat template to train dataset:   9%|▉         | 608/6689 [00:00<00:01, 3042.29 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.87 examples/s]Applying chat template to train dataset:  14%|█▍        | 969/6689 [00:00<00:01, 3240.71 examples/s]Applying chat template to train dataset:  14%|█▍        | 924/6689 [00:00<00:01, 3095.11 examples/s]Applying chat template to train dataset:  14%|█▍        | 924/6689 [00:00<00:01, 3092.49 examples/s]Applying chat template to train dataset:  19%|█▉        | 1300/6689 [00:00<00:01, 3259.12 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.89 examples/s]Applying chat template to train dataset:  19%|█▊        | 1240/6689 [00:00<00:01, 3118.48 examples/s]Applying chat template to train dataset:  19%|█▊        | 1240/6689 [00:00<00:01, 3117.74 examples/s]Applying chat template to train dataset:  24%|██▍       | 1628/6689 [00:00<00:01, 3263.70 examples/s]Applying chat template to train dataset:  23%|██▎       | 1554/6689 [00:00<00:01, 3123.92 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:02, 278.04 examples/s]Applying chat template to train dataset:  23%|██▎       | 1554/6689 [00:00<00:01, 3123.02 examples/s]Applying chat template to train dataset:  29%|██▉       | 1956/6689 [00:00<00:01, 3268.74 examples/s]Applying chat template to train dataset:  28%|██▊       | 1870/6689 [00:00<00:01, 3129.62 examples/s]Tokenizing eval dataset:  31%|███       | 296/953 [00:00<00:01, 375.12 examples/s]Applying chat template to train dataset:  28%|██▊       | 1870/6689 [00:00<00:01, 3127.70 examples/s]Applying chat template to train dataset:  33%|███▎      | 2186/6689 [00:00<00:01, 3136.56 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 439.52 examples/s]Applying chat template to train dataset:  33%|███▎      | 2185/6689 [00:00<00:01, 3134.24 examples/s]Applying chat template to train dataset:  37%|███▋      | 2445/6689 [00:00<00:01, 3260.43 examples/s]Tokenizing eval dataset:  44%|████▍     | 424/953 [00:01<00:01, 494.53 examples/s]Applying chat template to train dataset:  41%|████▏     | 2772/6689 [00:00<00:01, 3259.86 examples/s]Applying chat template to train dataset:  40%|███▉      | 2655/6689 [00:00<00:01, 3126.64 examples/s]Applying chat template to train dataset:  40%|███▉      | 2653/6689 [00:00<00:01, 3122.76 examples/s]Tokenizing eval dataset:  52%|█████▏    | 491/953 [00:01<00:00, 542.31 examples/s]Applying chat template to train dataset:  46%|████▋     | 3100/6689 [00:00<00:01, 3257.55 examples/s]Applying chat template to train dataset:  44%|████▍     | 2970/6689 [00:00<00:01, 3125.67 examples/s]Applying chat template to train dataset:  44%|████▍     | 2968/6689 [00:00<00:01, 3124.49 examples/s]Tokenizing eval dataset:  59%|█████▊    | 558/953 [00:01<00:00, 574.38 examples/s]Applying chat template to train dataset:  51%|█████     | 3427/6689 [00:01<00:01, 3259.35 examples/s]Applying chat template to train dataset:  49%|████▉     | 3283/6689 [00:01<00:01, 3126.40 examples/s]Applying chat template to train dataset:  49%|████▉     | 3281/6689 [00:01<00:01, 3121.63 examples/s]Tokenizing eval dataset:  65%|██████▌   | 622/953 [00:01<00:00, 591.34 examples/s]Applying chat template to train dataset:  54%|█████▍    | 3598/6689 [00:01<00:00, 3128.32 examples/s]Applying chat template to train dataset:  54%|█████▎    | 3594/6689 [00:01<00:00, 3120.97 examples/s]Applying chat template to train dataset:  59%|█████▊    | 3914/6689 [00:01<00:00, 3243.14 examples/s]Tokenizing eval dataset:  72%|███████▏  | 685/953 [00:01<00:00, 599.99 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4240/6689 [00:01<00:00, 3244.74 examples/s]Applying chat template to train dataset:  61%|██████    | 4064/6689 [00:01<00:00, 3113.32 examples/s]Applying chat template to train dataset:  61%|██████    | 4058/6689 [00:01<00:00, 3107.18 examples/s]Tokenizing eval dataset:  81%|████████  | 769/953 [00:01<00:00, 577.25 examples/s]Applying chat template to train dataset:  68%|██████▊   | 4567/6689 [00:01<00:00, 3249.53 examples/s]Applying chat template to train dataset:  65%|██████▌   | 4377/6689 [00:01<00:00, 3116.80 examples/s]Applying chat template to train dataset:  65%|██████▌   | 4371/6689 [00:01<00:00, 3109.01 examples/s]Applying chat template to train dataset:  73%|███████▎  | 4893/6689 [00:01<00:00, 3248.57 examples/s]Applying chat template to train dataset:  70%|███████   | 4690/6689 [00:01<00:00, 3115.53 examples/s]Applying chat template to train dataset:  70%|███████   | 4685/6689 [00:01<00:00, 3113.44 examples/s]Tokenizing eval dataset:  88%|████████▊ | 841/953 [00:01<00:00, 539.93 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5219/6689 [00:01<00:00, 3249.76 examples/s]Applying chat template to train dataset:  75%|███████▍  | 5004/6689 [00:01<00:00, 3118.90 examples/s]Applying chat template to train dataset:  75%|███████▍  | 4998/6689 [00:01<00:00, 3116.37 examples/s]Tokenizing eval dataset:  96%|█████████▌| 912/953 [00:02<00:00, 516.40 examples/s]Applying chat template to train dataset:  79%|███████▉  | 5317/6689 [00:01<00:00, 3116.31 examples/s]Applying chat template to train dataset:  79%|███████▉  | 5310/6689 [00:01<00:00, 3112.72 examples/s]Applying chat template to train dataset:  85%|████████▌ | 5707/6689 [00:01<00:00, 3247.33 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 455.19 examples/s]
Applying chat template to train dataset:  84%|████████▍ | 5630/6689 [00:01<00:00, 3115.14 examples/s]Applying chat template to train dataset:  84%|████████▍ | 5623/6689 [00:01<00:00, 3116.14 examples/s]Applying chat template to train dataset:  90%|█████████ | 6033/6689 [00:01<00:00, 3245.02 examples/s]Applying chat template to train dataset:  89%|████████▉ | 5942/6689 [00:01<00:00, 3114.94 examples/s]Applying chat template to train dataset:  89%|████████▉ | 5937/6689 [00:01<00:00, 3117.18 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6503/6689 [00:02<00:00, 3199.36 examples/s]Applying chat template to train dataset:  94%|█████████▎| 6256/6689 [00:02<00:00, 3120.74 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6250/6689 [00:02<00:00, 3118.76 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3229.84 examples/s]
Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3064.90 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3098.85 examples/s]
Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3064.46 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3096.15 examples/s]
Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   2%|▏         | 109/6689 [00:00<00:06, 1072.98 examples/s]Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   2%|▏         | 109/6689 [00:00<00:06, 1078.55 examples/s]Tokenizing train dataset:   3%|▎         | 230/6689 [00:00<00:05, 1145.38 examples/s]Tokenizing train dataset:   2%|▏         | 109/6689 [00:00<00:06, 1079.06 examples/s]Tokenizing train dataset:   3%|▎         | 230/6689 [00:00<00:05, 1152.92 examples/s]Tokenizing train dataset:   5%|▌         | 347/6689 [00:00<00:05, 1147.30 examples/s]Tokenizing train dataset:   3%|▎         | 230/6689 [00:00<00:05, 1150.44 examples/s]Tokenizing train dataset:   5%|▌         | 347/6689 [00:00<00:05, 1151.54 examples/s]Tokenizing train dataset:   5%|▌         | 346/6689 [00:00<00:05, 1148.20 examples/s]Tokenizing train dataset:   8%|▊         | 510/6689 [00:00<00:05, 1111.36 examples/s]Tokenizing train dataset:   8%|▊         | 509/6689 [00:00<00:05, 1114.35 examples/s]Tokenizing train dataset:   8%|▊         | 509/6689 [00:00<00:05, 1108.51 examples/s]Tokenizing train dataset:  10%|▉         | 666/6689 [00:00<00:05, 1075.76 examples/s]Tokenizing train dataset:  10%|▉         | 663/6689 [00:00<00:05, 1075.32 examples/s]Tokenizing train dataset:  10%|▉         | 662/6689 [00:00<00:05, 1067.80 examples/s]Tokenizing train dataset:  12%|█▏        | 810/6689 [00:00<00:05, 1029.15 examples/s]Tokenizing train dataset:  12%|█▏        | 808/6689 [00:00<00:05, 1031.38 examples/s]Tokenizing train dataset:  12%|█▏        | 806/6689 [00:00<00:05, 1024.25 examples/s]Tokenizing train dataset:  14%|█▍        | 941/6689 [00:00<00:05, 970.73 examples/s] Tokenizing train dataset:  14%|█▍        | 940/6689 [00:00<00:05, 973.00 examples/s] Tokenizing train dataset:  14%|█▍        | 936/6689 [00:00<00:05, 967.77 examples/s] Tokenizing train dataset:  16%|█▌        | 1076/6689 [00:01<00:05, 940.46 examples/s]Tokenizing train dataset:  16%|█▌        | 1073/6689 [00:01<00:05, 940.83 examples/s]Tokenizing train dataset:  16%|█▌        | 1070/6689 [00:01<00:05, 937.08 examples/s]Tokenizing train dataset:  18%|█▊        | 1197/6689 [00:01<00:06, 895.05 examples/s]Tokenizing train dataset:  18%|█▊        | 1196/6689 [00:01<00:06, 894.79 examples/s]Tokenizing train dataset:  18%|█▊        | 1190/6689 [00:01<00:06, 887.97 examples/s]Tokenizing train dataset:  20%|█▉        | 1315/6689 [00:01<00:06, 857.39 examples/s]Tokenizing train dataset:  20%|█▉        | 1315/6689 [00:01<00:06, 857.79 examples/s]Tokenizing train dataset:  20%|█▉        | 1310/6689 [00:01<00:06, 854.14 examples/s]Tokenizing train dataset:  21%|██▏       | 1431/6689 [00:01<00:06, 825.21 examples/s]Tokenizing train dataset:  21%|██        | 1414/6689 [00:01<00:06, 795.54 examples/s]Tokenizing train dataset:  21%|██▏       | 1426/6689 [00:01<00:06, 823.24 examples/s]Tokenizing train dataset:  23%|██▎       | 1542/6689 [00:01<00:06, 793.49 examples/s]Tokenizing train dataset:  23%|██▎       | 1526/6689 [00:01<00:06, 779.03 examples/s]Tokenizing train dataset:  23%|██▎       | 1540/6689 [00:01<00:06, 790.95 examples/s]Tokenizing train dataset:  25%|██▍       | 1642/6689 [00:01<00:06, 752.85 examples/s]Tokenizing train dataset:  24%|██▍       | 1628/6689 [00:01<00:06, 744.76 examples/s]Tokenizing train dataset:  26%|██▌       | 1719/6689 [00:01<00:06, 755.11 examples/s]Tokenizing train dataset:  25%|██▍       | 1640/6689 [00:01<00:06, 746.98 examples/s]Tokenizing train dataset:  26%|██▌       | 1716/6689 [00:01<00:06, 748.16 examples/s]Tokenizing train dataset:  26%|██▌       | 1730/6689 [00:01<00:06, 723.39 examples/s]Tokenizing train dataset:  27%|██▋       | 1818/6689 [00:02<00:06, 719.91 examples/s]Tokenizing train dataset:  27%|██▋       | 1815/6689 [00:02<00:06, 715.39 examples/s]Tokenizing train dataset:  27%|██▋       | 1833/6689 [00:02<00:06, 707.37 examples/s]Tokenizing train dataset:  29%|██▉       | 1926/6689 [00:02<00:06, 717.00 examples/s]Tokenizing train dataset:  28%|██▊       | 1887/6689 [00:02<00:06, 709.50 examples/s]Tokenizing train dataset:  29%|██▊       | 1910/6689 [00:02<00:06, 718.33 examples/s]Tokenizing train dataset:  30%|███       | 2029/6689 [00:02<00:06, 699.34 examples/s]Tokenizing train dataset:  29%|██▉       | 1959/6689 [00:02<00:06, 709.80 examples/s]Tokenizing train dataset:  30%|███       | 2013/6689 [00:02<00:06, 706.40 examples/s]Tokenizing train dataset:  32%|███▏      | 2108/6689 [00:02<00:06, 714.62 examples/s]Tokenizing train dataset:  31%|███       | 2062/6689 [00:02<00:06, 699.16 examples/s]Tokenizing train dataset:  32%|███▏      | 2122/6689 [00:02<00:06, 709.81 examples/s]Tokenizing train dataset:  33%|███▎      | 2205/6689 [00:02<00:06, 687.62 examples/s]Tokenizing train dataset:  32%|███▏      | 2137/6689 [00:02<00:06, 706.90 examples/s]Tokenizing train dataset:  33%|███▎      | 2218/6689 [00:02<00:06, 685.45 examples/s]Tokenizing train dataset:  34%|███▍      | 2302/6689 [00:02<00:06, 668.10 examples/s]Tokenizing train dataset:  33%|███▎      | 2230/6689 [00:02<00:06, 670.25 examples/s]Tokenizing train dataset:  35%|███▌      | 2372/6689 [00:02<00:06, 670.12 examples/s]Tokenizing train dataset:  35%|███▍      | 2315/6689 [00:02<00:06, 671.47 examples/s]Tokenizing train dataset:  35%|███▍      | 2332/6689 [00:02<00:06, 669.10 examples/s]Tokenizing train dataset:  37%|███▋      | 2461/6689 [00:03<00:06, 641.22 examples/s]Tokenizing train dataset:  36%|███▌      | 2412/6689 [00:02<00:06, 660.61 examples/s]Tokenizing train dataset:  36%|███▋      | 2429/6689 [00:03<00:06, 657.54 examples/s]Tokenizing train dataset:  38%|███▊      | 2555/6689 [00:03<00:06, 632.51 examples/s]Tokenizing train dataset:  37%|███▋      | 2502/6689 [00:03<00:06, 641.20 examples/s]Tokenizing train dataset:  38%|███▊      | 2514/6689 [00:03<00:06, 626.31 examples/s]Tokenizing train dataset:  39%|███▉      | 2632/6689 [00:03<00:06, 658.39 examples/s]Tokenizing train dataset:  38%|███▊      | 2567/6689 [00:03<00:06, 639.52 examples/s]Tokenizing train dataset:  39%|███▊      | 2583/6689 [00:03<00:06, 636.83 examples/s]Tokenizing train dataset:  39%|███▉      | 2639/6689 [00:03<00:06, 657.69 examples/s]Tokenizing train dataset:  41%|████      | 2721/6689 [00:03<00:06, 631.38 examples/s]Tokenizing train dataset:  40%|███▉      | 2653/6689 [00:03<00:06, 649.04 examples/s]Tokenizing train dataset:  41%|████      | 2726/6689 [00:03<00:06, 628.61 examples/s]Tokenizing train dataset:  42%|████▏     | 2806/6689 [00:03<00:06, 605.73 examples/s]Tokenizing train dataset:  41%|████      | 2738/6689 [00:03<00:06, 611.17 examples/s]Tokenizing train dataset:  42%|████▏     | 2813/6689 [00:03<00:06, 607.68 examples/s]Tokenizing train dataset:  43%|████▎     | 2897/6689 [00:03<00:06, 600.97 examples/s]Tokenizing train dataset:  42%|████▏     | 2830/6689 [00:03<00:06, 605.14 examples/s]Tokenizing train dataset:  43%|████▎     | 2905/6689 [00:03<00:06, 606.30 examples/s]Tokenizing train dataset:  45%|████▍     | 2983/6689 [00:03<00:06, 588.09 examples/s]Tokenizing train dataset:  44%|████▎     | 2920/6689 [00:03<00:06, 597.47 examples/s]Tokenizing train dataset:  45%|████▍     | 2990/6689 [00:03<00:06, 587.22 examples/s]Tokenizing train dataset:  46%|████▌     | 3067/6689 [00:04<00:06, 578.49 examples/s]Tokenizing train dataset:  45%|████▍     | 3005/6689 [00:03<00:06, 585.35 examples/s]Tokenizing train dataset:  47%|████▋     | 3127/6689 [00:04<00:06, 582.14 examples/s]Tokenizing train dataset:  46%|████▌     | 3078/6689 [00:04<00:06, 578.44 examples/s]Tokenizing train dataset:  46%|████▌     | 3087/6689 [00:04<00:06, 570.27 examples/s]Tokenizing train dataset:  48%|████▊     | 3214/6689 [00:04<00:06, 577.85 examples/s]Tokenizing train dataset:  47%|████▋     | 3150/6689 [00:04<00:06, 576.38 examples/s]Tokenizing train dataset:  47%|████▋     | 3164/6689 [00:04<00:06, 574.04 examples/s]Tokenizing train dataset:  48%|████▊     | 3210/6689 [00:04<00:06, 578.95 examples/s]Tokenizing train dataset:  48%|████▊     | 3222/6689 [00:04<00:06, 574.05 examples/s]Tokenizing train dataset:  49%|████▉     | 3291/6689 [00:04<00:06, 553.05 examples/s]Tokenizing train dataset:  49%|████▉     | 3281/6689 [00:04<00:06, 539.78 examples/s]Tokenizing train dataset:  49%|████▉     | 3294/6689 [00:04<00:06, 536.53 examples/s]Tokenizing train dataset:  50%|█████     | 3369/6689 [00:04<00:06, 541.10 examples/s]Tokenizing train dataset:  50%|█████     | 3350/6689 [00:04<00:06, 536.16 examples/s]Tokenizing train dataset:  51%|█████▏    | 3434/6689 [00:04<00:05, 563.57 examples/s]Tokenizing train dataset:  50%|█████     | 3364/6689 [00:04<00:06, 540.92 examples/s]Tokenizing train dataset:  51%|█████     | 3417/6689 [00:04<00:05, 566.06 examples/s]Tokenizing train dataset:  52%|█████▏    | 3497/6689 [00:04<00:05, 571.68 examples/s]Tokenizing train dataset:  51%|█████     | 3427/6689 [00:04<00:05, 559.68 examples/s]Tokenizing train dataset:  52%|█████▏    | 3475/6689 [00:04<00:05, 566.15 examples/s]Tokenizing train dataset:  53%|█████▎    | 3556/6689 [00:04<00:05, 572.67 examples/s]Tokenizing train dataset:  52%|█████▏    | 3489/6689 [00:04<00:05, 573.71 examples/s]Tokenizing train dataset:  53%|█████▎    | 3536/6689 [00:04<00:05, 573.02 examples/s]Tokenizing train dataset:  54%|█████▍    | 3632/6689 [00:05<00:05, 539.39 examples/s]Tokenizing train dataset:  53%|█████▎    | 3572/6689 [00:05<00:05, 561.42 examples/s]Tokenizing train dataset:  54%|█████▍    | 3611/6689 [00:05<00:05, 539.01 examples/s]Tokenizing train dataset:  55%|█████▌    | 3710/6689 [00:05<00:05, 528.21 examples/s]Tokenizing train dataset:  55%|█████▍    | 3649/6689 [00:05<00:05, 540.17 examples/s]Tokenizing train dataset:  55%|█████▍    | 3669/6689 [00:05<00:05, 544.88 examples/s]Tokenizing train dataset:  57%|█████▋    | 3792/6689 [00:05<00:05, 529.52 examples/s]Tokenizing train dataset:  56%|█████▌    | 3726/6689 [00:05<00:05, 528.16 examples/s]Tokenizing train dataset:  56%|█████▌    | 3745/6689 [00:05<00:05, 524.66 examples/s]Tokenizing train dataset:  58%|█████▊    | 3847/6689 [00:05<00:05, 531.84 examples/s]Tokenizing train dataset:  57%|█████▋    | 3780/6689 [00:05<00:05, 526.61 examples/s]Tokenizing train dataset:  57%|█████▋    | 3801/6689 [00:05<00:05, 532.46 examples/s]Tokenizing train dataset:  57%|█████▋    | 3838/6689 [00:05<00:05, 537.09 examples/s]Tokenizing train dataset:  58%|█████▊    | 3857/6689 [00:05<00:05, 536.36 examples/s]Tokenizing train dataset:  59%|█████▊    | 3927/6689 [00:05<00:05, 528.82 examples/s]Tokenizing train dataset:  58%|█████▊    | 3912/6689 [00:05<00:05, 533.98 examples/s]Tokenizing train dataset:  59%|█████▊    | 3918/6689 [00:05<00:05, 528.05 examples/s]Tokenizing train dataset:  60%|█████▉    | 3999/6689 [00:05<00:05, 508.22 examples/s]Tokenizing train dataset:  60%|█████▉    | 3981/6689 [00:05<00:05, 498.79 examples/s]Tokenizing train dataset:  60%|█████▉    | 3987/6689 [00:05<00:05, 501.19 examples/s]Tokenizing train dataset:  61%|██████    | 4075/6689 [00:05<00:05, 501.39 examples/s]Tokenizing train dataset:  61%|██████    | 4057/6689 [00:05<00:05, 491.08 examples/s]Tokenizing train dataset:  62%|██████▏   | 4133/6689 [00:06<00:04, 518.23 examples/s]Tokenizing train dataset:  61%|██████    | 4061/6689 [00:05<00:05, 491.74 examples/s]Tokenizing train dataset:  62%|██████▏   | 4115/6689 [00:06<00:05, 508.93 examples/s]Tokenizing train dataset:  63%|██████▎   | 4186/6689 [00:06<00:04, 519.31 examples/s]Tokenizing train dataset:  62%|██████▏   | 4120/6689 [00:06<00:05, 507.91 examples/s]Tokenizing train dataset:  62%|██████▏   | 4171/6689 [00:06<00:04, 520.61 examples/s]Tokenizing train dataset:  63%|██████▎   | 4243/6689 [00:06<00:04, 529.37 examples/s]Tokenizing train dataset:  62%|██████▏   | 4177/6689 [00:06<00:04, 519.02 examples/s]Tokenizing train dataset:  63%|██████▎   | 4230/6689 [00:06<00:04, 531.55 examples/s]Tokenizing train dataset:  63%|██████▎   | 4233/6689 [00:06<00:04, 526.32 examples/s]Tokenizing train dataset:  65%|██████▍   | 4317/6689 [00:06<00:04, 512.71 examples/s]Tokenizing train dataset:  64%|██████▍   | 4306/6689 [00:06<00:04, 518.02 examples/s]Tokenizing train dataset:  65%|██████▌   | 4374/6689 [00:06<00:04, 521.71 examples/s]Tokenizing train dataset:  64%|██████▍   | 4310/6689 [00:06<00:04, 512.43 examples/s]Tokenizing train dataset:  65%|██████▌   | 4362/6689 [00:06<00:04, 524.39 examples/s]Tokenizing train dataset:  66%|██████▌   | 4429/6689 [00:06<00:04, 526.51 examples/s]Tokenizing train dataset:  65%|██████▌   | 4378/6689 [00:06<00:04, 490.02 examples/s]Tokenizing train dataset:  66%|██████▌   | 4417/6689 [00:06<00:04, 527.77 examples/s]Tokenizing train dataset:  67%|██████▋   | 4507/6689 [00:06<00:04, 520.66 examples/s]Tokenizing train dataset:  66%|██████▋   | 4433/6689 [00:06<00:04, 500.99 examples/s]Tokenizing train dataset:  67%|██████▋   | 4494/6689 [00:06<00:04, 518.45 examples/s]Tokenizing train dataset:  67%|██████▋   | 4486/6689 [00:06<00:04, 503.86 examples/s]Tokenizing train dataset:  68%|██████▊   | 4580/6689 [00:06<00:04, 505.69 examples/s]Tokenizing train dataset:  68%|██████▊   | 4538/6689 [00:06<00:04, 504.76 examples/s]Tokenizing train dataset:  68%|██████▊   | 4568/6689 [00:06<00:04, 506.25 examples/s]Tokenizing train dataset:  70%|██████▉   | 4652/6689 [00:07<00:04, 489.93 examples/s]Tokenizing train dataset:  69%|██████▉   | 4610/6689 [00:07<00:04, 488.60 examples/s]Tokenizing train dataset:  69%|██████▉   | 4639/6689 [00:07<00:04, 494.45 examples/s]Tokenizing train dataset:  70%|███████   | 4711/6689 [00:07<00:03, 505.41 examples/s]Tokenizing train dataset:  70%|██████▉   | 4661/6689 [00:07<00:04, 490.22 examples/s]Tokenizing train dataset:  70%|███████   | 4690/6689 [00:07<00:04, 495.41 examples/s]Tokenizing train dataset:  71%|███████▏  | 4769/6689 [00:07<00:03, 519.05 examples/s]Tokenizing train dataset:  70%|███████   | 4714/6689 [00:07<00:03, 500.29 examples/s]Tokenizing train dataset:  71%|███████   | 4750/6689 [00:07<00:03, 517.61 examples/s]Tokenizing train dataset:  72%|███████▏  | 4845/6689 [00:07<00:03, 509.09 examples/s]Tokenizing train dataset:  71%|███████▏  | 4771/6689 [00:07<00:03, 516.61 examples/s]Tokenizing train dataset:  72%|███████▏  | 4810/6689 [00:07<00:03, 532.65 examples/s]Tokenizing train dataset:  73%|███████▎  | 4897/6689 [00:07<00:03, 506.81 examples/s]Tokenizing train dataset:  72%|███████▏  | 4846/6689 [00:07<00:03, 506.61 examples/s]Tokenizing train dataset:  73%|███████▎  | 4882/6689 [00:07<00:03, 509.36 examples/s]Tokenizing train dataset:  74%|███████▍  | 4973/6689 [00:07<00:03, 500.37 examples/s]Tokenizing train dataset:  73%|███████▎  | 4898/6689 [00:07<00:03, 508.43 examples/s]Tokenizing train dataset:  74%|███████▍  | 4955/6689 [00:07<00:03, 495.89 examples/s]Tokenizing train dataset:  75%|███████▌  | 5029/6689 [00:07<00:03, 511.57 examples/s]Tokenizing train dataset:  74%|███████▍  | 4972/6689 [00:07<00:03, 501.81 examples/s]Tokenizing train dataset:  75%|███████▍  | 5015/6689 [00:07<00:03, 518.18 examples/s]Tokenizing train dataset:  76%|███████▌  | 5100/6689 [00:07<00:03, 495.11 examples/s]Tokenizing train dataset:  75%|███████▌  | 5028/6689 [00:07<00:03, 513.30 examples/s]Tokenizing train dataset:  76%|███████▌  | 5087/6689 [00:07<00:03, 499.19 examples/s]Tokenizing train dataset:  77%|███████▋  | 5152/6689 [00:08<00:03, 496.44 examples/s]Tokenizing train dataset:  76%|███████▌  | 5100/6689 [00:08<00:03, 494.84 examples/s]Tokenizing train dataset:  78%|███████▊  | 5203/6689 [00:08<00:02, 497.87 examples/s]Tokenizing train dataset:  77%|███████▋  | 5163/6689 [00:08<00:03, 497.09 examples/s]Tokenizing train dataset:  77%|███████▋  | 5152/6689 [00:08<00:03, 496.80 examples/s]Tokenizing train dataset:  79%|███████▊  | 5261/6689 [00:08<00:02, 514.06 examples/s]Tokenizing train dataset:  78%|███████▊  | 5216/6689 [00:08<00:02, 499.40 examples/s]Tokenizing train dataset:  78%|███████▊  | 5203/6689 [00:08<00:02, 498.07 examples/s]Tokenizing train dataset:  79%|███████▉  | 5274/6689 [00:08<00:02, 513.60 examples/s]Tokenizing train dataset:  80%|███████▉  | 5341/6689 [00:08<00:02, 514.42 examples/s]Tokenizing train dataset:  79%|███████▊  | 5261/6689 [00:08<00:02, 514.38 examples/s]Tokenizing train dataset:  80%|███████▉  | 5327/6689 [00:08<00:02, 517.13 examples/s]Tokenizing train dataset:  81%|████████  | 5410/6689 [00:08<00:02, 484.49 examples/s]Tokenizing train dataset:  80%|███████▉  | 5341/6689 [00:08<00:02, 515.19 examples/s]Tokenizing train dataset:  81%|████████  | 5400/6689 [00:08<00:02, 505.46 examples/s]Tokenizing train dataset:  82%|████████▏ | 5460/6689 [00:08<00:02, 481.76 examples/s]Tokenizing train dataset:  81%|████████  | 5410/6689 [00:08<00:02, 483.76 examples/s]Tokenizing train dataset:  82%|████████▏ | 5511/6689 [00:08<00:02, 482.52 examples/s]Tokenizing train dataset:  82%|████████▏ | 5464/6689 [00:08<00:02, 471.44 examples/s]Tokenizing train dataset:  83%|████████▎ | 5563/6689 [00:08<00:02, 486.02 examples/s]Tokenizing train dataset:  82%|████████▏ | 5484/6689 [00:08<00:02, 483.54 examples/s]Tokenizing train dataset:  83%|████████▎ | 5519/6689 [00:08<00:02, 483.18 examples/s]Tokenizing train dataset:  84%|████████▍ | 5614/6689 [00:09<00:02, 490.88 examples/s]Tokenizing train dataset:  83%|████████▎ | 5534/6689 [00:08<00:02, 485.27 examples/s]Tokenizing train dataset:  83%|████████▎ | 5570/6689 [00:08<00:02, 483.16 examples/s]Tokenizing train dataset:  85%|████████▍ | 5666/6689 [00:09<00:02, 493.98 examples/s]Tokenizing train dataset:  84%|████████▍ | 5625/6689 [00:09<00:02, 499.66 examples/s]Tokenizing train dataset:  84%|████████▍ | 5610/6689 [00:09<00:02, 487.50 examples/s]Tokenizing train dataset:  85%|████████▌ | 5716/6689 [00:09<00:01, 494.07 examples/s]Tokenizing train dataset:  85%|████████▍ | 5660/6689 [00:09<00:02, 488.73 examples/s]Tokenizing train dataset:  85%|████████▌ | 5698/6689 [00:09<00:02, 491.76 examples/s]Tokenizing train dataset:  87%|████████▋ | 5787/6689 [00:09<00:01, 479.64 examples/s]Tokenizing train dataset:  85%|████████▌ | 5710/6689 [00:09<00:02, 486.54 examples/s]Tokenizing train dataset:  86%|████████▋ | 5774/6689 [00:09<00:01, 490.90 examples/s]Tokenizing train dataset:  86%|████████▌ | 5759/6689 [00:09<00:01, 484.98 examples/s]Tokenizing train dataset:  88%|████████▊ | 5854/6689 [00:09<00:01, 464.88 examples/s]Tokenizing train dataset:  87%|████████▋ | 5810/6689 [00:09<00:01, 485.22 examples/s]Tokenizing train dataset:  87%|████████▋ | 5843/6689 [00:09<00:01, 478.56 examples/s]Tokenizing train dataset:  89%|████████▊ | 5923/6689 [00:09<00:01, 461.93 examples/s]Tokenizing train dataset:  88%|████████▊ | 5880/6689 [00:09<00:01, 472.93 examples/s]Tokenizing train dataset:  88%|████████▊ | 5913/6689 [00:09<00:01, 469.91 examples/s]Tokenizing train dataset:  89%|████████▉ | 5976/6689 [00:09<00:01, 477.13 examples/s]Tokenizing train dataset:  89%|████████▉ | 5969/6689 [00:09<00:01, 487.88 examples/s]Tokenizing train dataset:  89%|████████▉ | 5956/6689 [00:09<00:01, 482.56 examples/s]Tokenizing train dataset:  90%|█████████ | 6049/6689 [00:09<00:01, 474.65 examples/s]Tokenizing train dataset:  90%|█████████ | 6039/6689 [00:09<00:01, 477.03 examples/s]Tokenizing train dataset:  91%|█████████ | 6100/6689 [00:10<00:01, 476.32 examples/s]Tokenizing train dataset:  90%|█████████ | 6027/6689 [00:09<00:01, 473.19 examples/s]Tokenizing train dataset:  91%|█████████ | 6090/6689 [00:10<00:01, 482.31 examples/s]Tokenizing train dataset:  92%|█████████▏| 6163/6689 [00:10<00:01, 512.46 examples/s]Tokenizing train dataset:  91%|█████████ | 6079/6689 [00:10<00:01, 483.68 examples/s]Tokenizing train dataset:  92%|█████████▏| 6149/6689 [00:10<00:01, 505.61 examples/s]Tokenizing train dataset:  93%|█████████▎| 6222/6689 [00:10<00:00, 528.19 examples/s]Tokenizing train dataset:  92%|█████████▏| 6133/6689 [00:10<00:01, 496.16 examples/s]Tokenizing train dataset:  93%|█████████▎| 6209/6689 [00:10<00:00, 529.32 examples/s]Tokenizing train dataset:  94%|█████████▍| 6278/6689 [00:10<00:00, 534.67 examples/s]Tokenizing train dataset:  93%|█████████▎| 6194/6689 [00:10<00:00, 522.25 examples/s]Tokenizing train dataset:  94%|█████████▎| 6267/6689 [00:10<00:00, 538.99 examples/s]Tokenizing train dataset:  93%|█████████▎| 6254/6689 [00:10<00:00, 539.73 examples/s]Tokenizing train dataset:  95%|█████████▍| 6349/6689 [00:10<00:00, 507.34 examples/s]Tokenizing train dataset:  95%|█████████▍| 6338/6689 [00:10<00:00, 508.92 examples/s]Tokenizing train dataset:  95%|█████████▍| 6326/6689 [00:10<00:00, 513.99 examples/s]Tokenizing train dataset:  96%|█████████▌| 6410/6689 [00:10<00:00, 466.05 examples/s]Tokenizing train dataset:  96%|█████████▌| 6405/6689 [00:10<00:00, 479.96 examples/s]Tokenizing train dataset:  97%|█████████▋| 6460/6689 [00:10<00:00, 465.42 examples/s]Tokenizing train dataset:  96%|█████████▌| 6392/6689 [00:10<00:00, 480.64 examples/s]Tokenizing train dataset:  97%|█████████▋| 6508/6689 [00:10<00:00, 467.97 examples/s]Tokenizing train dataset:  97%|█████████▋| 6474/6689 [00:10<00:00, 472.90 examples/s]Tokenizing train dataset:  97%|█████████▋| 6460/6689 [00:10<00:00, 468.21 examples/s]Tokenizing train dataset:  98%|█████████▊| 6560/6689 [00:10<00:00, 480.43 examples/s]Tokenizing train dataset:  97%|█████████▋| 6508/6689 [00:10<00:00, 469.74 examples/s]Tokenizing train dataset:  98%|█████████▊| 6550/6689 [00:10<00:00, 481.03 examples/s]Tokenizing train dataset:  99%|█████████▉| 6612/6689 [00:11<00:00, 488.61 examples/s]Tokenizing train dataset:  98%|█████████▊| 6560/6689 [00:11<00:00, 480.87 examples/s]Tokenizing train dataset:  99%|█████████▉| 6627/6689 [00:11<00:00, 488.83 examples/s]Tokenizing train dataset: 100%|█████████▉| 6683/6689 [00:11<00:00, 480.86 examples/s]Tokenizing train dataset:  99%|█████████▉| 6612/6689 [00:11<00:00, 488.29 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 594.41 examples/s]
Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 482.51 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 594.71 examples/s]
Tokenizing train dataset: 100%|█████████▉| 6683/6689 [00:11<00:00, 481.00 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 592.14 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  58%|█████▊    | 557/953 [00:00<00:00, 5501.85 examples/s]Extracting prompt in eval dataset:  59%|█████▊    | 558/953 [00:00<00:00, 5506.90 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5498.55 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5513.83 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5486.87 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5487.42 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  32%|███▏      | 302/953 [00:00<00:00, 2985.87 examples/s]Applying chat template to eval dataset:  32%|███▏      | 302/953 [00:00<00:00, 2995.32 examples/s]Applying chat template to eval dataset:  33%|███▎      | 316/953 [00:00<00:00, 3135.59 examples/s]Applying chat template to eval dataset:  65%|██████▍   | 616/953 [00:00<00:00, 3062.67 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 643/953 [00:00<00:00, 3212.40 examples/s]Applying chat template to eval dataset:  65%|██████▍   | 616/953 [00:00<00:00, 3075.21 examples/s]Applying chat template to eval dataset:  97%|█████████▋| 927/953 [00:00<00:00, 3083.89 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3199.69 examples/s]
Applying chat template to eval dataset:  97%|█████████▋| 929/953 [00:00<00:00, 3097.67 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3052.66 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3064.21 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 322.31 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 322.27 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 323.95 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 289.92 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 289.05 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 290.07 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 277.31 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 277.42 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:02, 278.77 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 268.43 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 270.03 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 268.16 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 254.26 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 254.02 examples/s]Tokenizing eval dataset:  20%|██        | 195/953 [00:00<00:02, 255.13 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:02, 278.50 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:02, 280.30 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:02, 278.01 examples/s]Tokenizing eval dataset:  31%|███       | 297/953 [00:00<00:01, 377.34 examples/s]Tokenizing eval dataset:  31%|███       | 297/953 [00:00<00:01, 379.71 examples/s]Tokenizing eval dataset:  31%|███       | 297/953 [00:00<00:01, 376.59 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 440.25 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 442.64 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 438.89 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 498.25 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 500.90 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 496.90 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 541.49 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 544.11 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 539.89 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 574.97 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 577.59 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 573.43 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 596.69 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 598.74 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 594.94 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 600.64 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 602.62 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 598.79 examples/s]Tokenizing eval dataset:  81%|████████  | 774/953 [00:01<00:00, 579.22 examples/s]Tokenizing eval dataset:  82%|████████▏ | 777/953 [00:01<00:00, 578.78 examples/s]Tokenizing eval dataset:  81%|████████  | 774/953 [00:01<00:00, 577.67 examples/s]Tokenizing eval dataset:  89%|████████▉ | 848/953 [00:01<00:00, 543.94 examples/s]Tokenizing eval dataset:  89%|████████▉ | 848/953 [00:01<00:00, 542.98 examples/s]Tokenizing eval dataset:  89%|████████▉ | 852/953 [00:01<00:00, 540.90 examples/s]Tokenizing eval dataset:  96%|█████████▋| 918/953 [00:02<00:00, 514.70 examples/s]Tokenizing eval dataset:  96%|█████████▋| 918/953 [00:02<00:00, 513.11 examples/s]Tokenizing eval dataset:  97%|█████████▋| 929/953 [00:02<00:00, 525.83 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 456.99 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 458.82 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 455.96 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.6614954471588135 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3026795387268066 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.33168625831604 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3394429683685303 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank8]:[W609 02:49:56.472062645 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 2 ---
