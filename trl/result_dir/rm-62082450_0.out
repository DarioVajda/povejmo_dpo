cpu-bind=MASK - gn24, task  0  0 [1211898]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 0 ---
Total Nodes: 2
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn24
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 2     --machine_rank 0     --main_process_ip gn24     --main_process_port 29500     --num_processes 8     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62082450     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train.py"     --rank=64 --learning_rate=3e-7 --total_epochs=3 --beta=0.2
-------------------------------------------
[2025-05-31 00:55:08,894] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0531 00:55:10.749000 1211951 torch/distributed/run.py:792] 
W0531 00:55:10.749000 1211951 torch/distributed/run.py:792] *****************************************
W0531 00:55:10.749000 1211951 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0531 00:55:10.749000 1211951 torch/distributed/run.py:792] *****************************************
[2025-05-31 00:55:15,999] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 00:55:16,052] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 00:55:16,061] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-31 00:55:16,064] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
World size: 8
Setting gradient accumulation steps to: 2
[2025-05-31 00:55:19,281] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-31 00:55:19,284] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Steps per epoch: 4282
Eval steps: 2141
[2025-05-31 00:55:19,290] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-31 00:55:19,290] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-31 00:55:19,291] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
[2025-05-31 00:55:20,948] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-31 00:55:20,948] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-31 00:55:20,948] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-31 00:55:20,948] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
hpZeRO group size: 4
[2025-05-31 00:55:36,007] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 465, num_elems = 10.16B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.41s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.42s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.42s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:15<00:47, 15.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:18<00:20, 10.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:18<00:20, 10.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:18<00:20, 10.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:27<00:26, 13.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:29<00:10, 10.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:29<00:10, 10.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:29<00:10, 10.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:38<00:00, 10.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:38<00:00,  9.70s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:38<00:00, 10.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:38<00:00, 10.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:38<00:00,  9.70s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:38<00:00,  9.70s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00, 10.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00, 11.79s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
Using LoRA and set up the model
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5426.51 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1119/8564 [00:00<00:01, 5571.45 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1960/8564 [00:00<00:01, 5565.92 examples/s]Extracting prompt in train dataset:  30%|██▉       | 2540/8564 [00:00<00:01, 5631.41 examples/s]Extracting prompt in train dataset:  39%|███▉      | 3370/8564 [00:00<00:00, 5569.93 examples/s]Extracting prompt in train dataset:  46%|████▌     | 3950/8564 [00:00<00:00, 5619.10 examples/s]Extracting prompt in train dataset:  53%|█████▎    | 4530/8564 [00:00<00:00, 5648.36 examples/s]Extracting prompt in train dataset:  60%|█████▉    | 5119/8564 [00:00<00:00, 5706.90 examples/s]Extracting prompt in train dataset:  67%|██████▋   | 5709/8564 [00:01<00:00, 5754.08 examples/s]Extracting prompt in train dataset:  74%|███████▎  | 6298/8564 [00:01<00:00, 5783.61 examples/s]Extracting prompt in train dataset:  80%|████████  | 6885/8564 [00:01<00:00, 5796.24 examples/s]Extracting prompt in train dataset:  87%|████████▋ | 7478/8564 [00:01<00:00, 5819.33 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 8320/8564 [00:01<00:00, 5623.64 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5616.28 examples/s]
Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   3%|▎         | 281/8564 [00:00<00:02, 2776.81 examples/s]Applying chat template to train dataset:   7%|▋         | 590/8564 [00:00<00:02, 2955.46 examples/s]Applying chat template to train dataset:  10%|█         | 899/8564 [00:00<00:02, 3014.32 examples/s]Applying chat template to train dataset:  14%|█▍        | 1210/8564 [00:00<00:02, 3043.36 examples/s]Applying chat template to train dataset:  18%|█▊        | 1515/8564 [00:00<00:02, 3041.23 examples/s]Applying chat template to train dataset:  21%|██▏       | 1828/8564 [00:00<00:02, 3067.29 examples/s]Applying chat template to train dataset:  25%|██▌       | 2141/8564 [00:00<00:02, 3082.35 examples/s]Applying chat template to train dataset:  29%|██▊       | 2456/8564 [00:00<00:01, 3098.77 examples/s]Applying chat template to train dataset:  32%|███▏      | 2770/8564 [00:00<00:01, 3104.52 examples/s]Applying chat template to train dataset:  38%|███▊      | 3222/8564 [00:01<00:01, 3066.08 examples/s]Applying chat template to train dataset:  41%|████▏     | 3535/8564 [00:01<00:01, 3083.29 examples/s]Applying chat template to train dataset:  45%|████▍     | 3848/8564 [00:01<00:01, 3094.39 examples/s]Applying chat template to train dataset:  49%|████▊     | 4160/8564 [00:01<00:01, 3097.04 examples/s]Applying chat template to train dataset:  52%|█████▏    | 4473/8564 [00:01<00:01, 3103.53 examples/s]Applying chat template to train dataset:  58%|█████▊    | 4942/8564 [00:01<00:01, 3111.83 examples/s]Applying chat template to train dataset:  61%|██████▏   | 5261/8564 [00:01<00:01, 3131.44 examples/s]Applying chat template to train dataset:  65%|██████▌   | 5580/8564 [00:01<00:00, 3145.62 examples/s]Applying chat template to train dataset:  69%|██████▉   | 5900/8564 [00:01<00:00, 3157.08 examples/s]Applying chat template to train dataset:  73%|███████▎  | 6218/8564 [00:02<00:00, 3160.65 examples/s]Applying chat template to train dataset:  78%|███████▊  | 6687/8564 [00:02<00:00, 3143.37 examples/s]Applying chat template to train dataset:  82%|████████▏ | 7006/8564 [00:02<00:00, 3154.12 examples/s]Applying chat template to train dataset:  86%|████████▌ | 7325/8564 [00:02<00:00, 3162.83 examples/s]Applying chat template to train dataset:  89%|████████▉ | 7644/8564 [00:02<00:00, 3168.43 examples/s]Applying chat template to train dataset:  94%|█████████▍| 8080/8564 [00:02<00:00, 3065.93 examples/s]Applying chat template to train dataset:  98%|█████████▊| 8400/8564 [00:02<00:00, 3099.41 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3095.73 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 42/8564 [00:00<00:21, 401.75 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:24, 340.54 examples/s]Tokenizing train dataset:   2%|▏         | 139/8564 [00:00<00:26, 321.08 examples/s]Tokenizing train dataset:   2%|▏         | 183/8564 [00:00<00:27, 307.41 examples/s]Tokenizing train dataset:   3%|▎         | 218/8564 [00:00<00:26, 314.80 examples/s]Tokenizing train dataset:   3%|▎         | 252/8564 [00:00<00:26, 319.36 examples/s]Tokenizing train dataset:   3%|▎         | 290/8564 [00:00<00:24, 332.66 examples/s]Tokenizing train dataset:   4%|▍         | 324/8564 [00:00<00:24, 332.06 examples/s]Tokenizing train dataset:   4%|▍         | 370/8564 [00:01<00:25, 318.46 examples/s]Tokenizing train dataset:   5%|▍         | 420/8564 [00:01<00:25, 317.05 examples/s]Tokenizing train dataset:   5%|▌         | 469/8564 [00:01<00:25, 315.16 examples/s]Tokenizing train dataset:   6%|▌         | 512/8564 [00:01<00:26, 304.68 examples/s]Tokenizing train dataset:   6%|▋         | 549/8564 [00:01<00:25, 317.91 examples/s]Tokenizing train dataset:   7%|▋         | 596/8564 [00:01<00:25, 309.10 examples/s]Tokenizing train dataset:   7%|▋         | 634/8564 [00:01<00:24, 323.39 examples/s]Tokenizing train dataset:   8%|▊         | 681/8564 [00:02<00:25, 314.36 examples/s]Tokenizing train dataset:   9%|▊         | 729/8564 [00:02<00:25, 312.71 examples/s]Tokenizing train dataset:   9%|▉         | 763/8564 [00:02<00:24, 314.17 examples/s]Tokenizing train dataset:   9%|▉         | 808/8564 [00:02<00:25, 302.38 examples/s]Tokenizing train dataset:  10%|▉         | 839/8564 [00:02<00:25, 299.48 examples/s]Tokenizing train dataset:  10%|█         | 887/8564 [00:02<00:25, 304.79 examples/s]Tokenizing train dataset:  11%|█         | 920/8564 [00:02<00:24, 306.01 examples/s]Tokenizing train dataset:  11%|█         | 952/8564 [00:03<00:24, 304.86 examples/s]Tokenizing train dataset:  11%|█▏        | 983/8564 [00:03<00:25, 301.33 examples/s]Tokenizing train dataset:  12%|█▏        | 1027/8564 [00:03<00:25, 292.54 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 294.41 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 307.80 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:03<00:25, 295.57 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:03<00:24, 295.95 examples/s]Tokenizing train dataset:  14%|█▍        | 1218/8564 [00:03<00:24, 304.48 examples/s]Tokenizing train dataset:  15%|█▍        | 1252/8564 [00:04<00:23, 312.69 examples/s]Tokenizing train dataset:  15%|█▌        | 1286/8564 [00:04<00:23, 310.70 examples/s]Tokenizing train dataset:  15%|█▌        | 1320/8564 [00:04<00:23, 313.12 examples/s]Tokenizing train dataset:  16%|█▌        | 1353/8564 [00:04<00:23, 311.18 examples/s]Tokenizing train dataset:  16%|█▋        | 1395/8564 [00:04<00:24, 295.80 examples/s]Tokenizing train dataset:  17%|█▋        | 1427/8564 [00:04<00:23, 299.56 examples/s]Tokenizing train dataset:  17%|█▋        | 1473/8564 [00:04<00:23, 295.66 examples/s]Tokenizing train dataset:  18%|█▊        | 1515/8564 [00:04<00:24, 288.20 examples/s]Tokenizing train dataset:  18%|█▊        | 1550/8564 [00:05<00:23, 297.11 examples/s]Tokenizing train dataset:  18%|█▊        | 1581/8564 [00:05<00:23, 296.57 examples/s]Tokenizing train dataset:  19%|█▉        | 1614/8564 [00:05<00:22, 302.23 examples/s]Tokenizing train dataset:  19%|█▉        | 1650/8564 [00:05<00:21, 315.02 examples/s]Tokenizing train dataset:  20%|█▉        | 1686/8564 [00:05<00:21, 321.52 examples/s]Tokenizing train dataset:  20%|██        | 1725/8564 [00:05<00:20, 333.57 examples/s]Tokenizing train dataset:  21%|██        | 1760/8564 [00:05<00:20, 337.33 examples/s]Tokenizing train dataset:  21%|██        | 1808/8564 [00:05<00:20, 323.38 examples/s]Tokenizing train dataset:  22%|██▏       | 1842/8564 [00:05<00:20, 324.54 examples/s]Tokenizing train dataset:  22%|██▏       | 1887/8564 [00:06<00:21, 311.17 examples/s]Tokenizing train dataset:  23%|██▎       | 1931/8564 [00:06<00:19, 340.50 examples/s]Tokenizing train dataset:  23%|██▎       | 1968/8564 [00:06<00:19, 342.09 examples/s]Tokenizing train dataset:  23%|██▎       | 2007/8564 [00:06<00:18, 350.99 examples/s]Tokenizing train dataset:  24%|██▍       | 2046/8564 [00:06<00:18, 360.23 examples/s]Tokenizing train dataset:  24%|██▍       | 2083/8564 [00:06<00:17, 361.72 examples/s]Tokenizing train dataset:  25%|██▍       | 2121/8564 [00:06<00:17, 364.86 examples/s]Tokenizing train dataset:  25%|██▌       | 2159/8564 [00:06<00:17, 364.61 examples/s]Tokenizing train dataset:  26%|██▌       | 2197/8564 [00:06<00:17, 365.03 examples/s]Tokenizing train dataset:  26%|██▌       | 2236/8564 [00:07<00:17, 366.73 examples/s]Tokenizing train dataset:  27%|██▋       | 2278/8564 [00:07<00:16, 379.64 examples/s]Tokenizing train dataset:  27%|██▋       | 2332/8564 [00:07<00:16, 367.08 examples/s]Tokenizing train dataset:  28%|██▊       | 2387/8564 [00:07<00:16, 363.92 examples/s]Tokenizing train dataset:  28%|██▊       | 2428/8564 [00:07<00:16, 371.36 examples/s]Tokenizing train dataset:  29%|██▉       | 2468/8564 [00:07<00:16, 375.51 examples/s]Tokenizing train dataset:  29%|██▉       | 2520/8564 [00:07<00:16, 362.29 examples/s]Tokenizing train dataset:  30%|██▉       | 2563/8564 [00:07<00:15, 376.29 examples/s]Tokenizing train dataset:  31%|███       | 2618/8564 [00:08<00:16, 367.11 examples/s]Tokenizing train dataset:  31%|███       | 2666/8564 [00:08<00:17, 346.81 examples/s]Tokenizing train dataset:  32%|███▏      | 2720/8564 [00:08<00:16, 348.24 examples/s]Tokenizing train dataset:  32%|███▏      | 2772/8564 [00:08<00:16, 341.33 examples/s]Tokenizing train dataset:  33%|███▎      | 2814/8564 [00:08<00:16, 357.63 examples/s]Tokenizing train dataset:  33%|███▎      | 2866/8564 [00:08<00:16, 350.64 examples/s]Tokenizing train dataset:  34%|███▍      | 2908/8564 [00:08<00:15, 365.99 examples/s]Tokenizing train dataset:  34%|███▍      | 2949/8564 [00:08<00:14, 376.38 examples/s]Tokenizing train dataset:  35%|███▍      | 2991/8564 [00:09<00:14, 387.32 examples/s]Tokenizing train dataset:  36%|███▌      | 3046/8564 [00:09<00:14, 375.16 examples/s]Tokenizing train dataset:  36%|███▌      | 3100/8564 [00:09<00:14, 368.75 examples/s]Tokenizing train dataset:  37%|███▋      | 3157/8564 [00:09<00:14, 370.72 examples/s]Tokenizing train dataset:  38%|███▊      | 3212/8564 [00:09<00:14, 366.27 examples/s]Tokenizing train dataset:  38%|███▊      | 3254/8564 [00:09<00:14, 375.82 examples/s]Tokenizing train dataset:  39%|███▊      | 3307/8564 [00:09<00:14, 365.04 examples/s]Tokenizing train dataset:  39%|███▉      | 3362/8564 [00:10<00:14, 360.37 examples/s]Tokenizing train dataset:  40%|███▉      | 3406/8564 [00:10<00:13, 375.78 examples/s]Tokenizing train dataset:  40%|████      | 3464/8564 [00:10<00:13, 376.35 examples/s]Tokenizing train dataset:  41%|████      | 3523/8564 [00:10<00:13, 379.94 examples/s]Tokenizing train dataset:  42%|████▏     | 3576/8564 [00:10<00:13, 369.08 examples/s]Tokenizing train dataset:  42%|████▏     | 3616/8564 [00:10<00:13, 372.73 examples/s]Tokenizing train dataset:  43%|████▎     | 3670/8564 [00:10<00:13, 362.81 examples/s]Tokenizing train dataset:  43%|████▎     | 3707/8564 [00:11<00:13, 361.02 examples/s]Tokenizing train dataset:  44%|████▍     | 3749/8564 [00:11<00:12, 371.11 examples/s]Tokenizing train dataset:  44%|████▍     | 3788/8564 [00:11<00:12, 373.09 examples/s]Tokenizing train dataset:  45%|████▍     | 3842/8564 [00:11<00:13, 361.57 examples/s]Tokenizing train dataset:  45%|████▌     | 3881/8564 [00:11<00:12, 365.47 examples/s]Tokenizing train dataset:  46%|████▌     | 3933/8564 [00:11<00:13, 352.77 examples/s]Tokenizing train dataset:  46%|████▋     | 3971/8564 [00:11<00:12, 358.72 examples/s]Tokenizing train dataset:  47%|████▋     | 4028/8564 [00:11<00:12, 364.55 examples/s]Tokenizing train dataset:  47%|████▋     | 4065/8564 [00:12<00:12, 364.52 examples/s]Tokenizing train dataset:  48%|████▊     | 4120/8564 [00:12<00:12, 359.11 examples/s]Tokenizing train dataset:  49%|████▊     | 4172/8564 [00:12<00:12, 352.54 examples/s]Tokenizing train dataset:  49%|████▉     | 4212/8564 [00:12<00:12, 360.41 examples/s]Tokenizing train dataset:  50%|████▉     | 4266/8564 [00:12<00:11, 358.53 examples/s]Tokenizing train dataset:  50%|█████     | 4318/8564 [00:12<00:12, 351.63 examples/s]Tokenizing train dataset:  51%|█████     | 4355/8564 [00:12<00:11, 352.02 examples/s]Tokenizing train dataset:  51%|█████▏    | 4395/8564 [00:12<00:11, 360.99 examples/s]Tokenizing train dataset:  52%|█████▏    | 4432/8564 [00:13<00:11, 360.86 examples/s]Tokenizing train dataset:  52%|█████▏    | 4470/8564 [00:13<00:11, 362.52 examples/s]Tokenizing train dataset:  53%|█████▎    | 4507/8564 [00:13<00:11, 357.21 examples/s]Tokenizing train dataset:  53%|█████▎    | 4545/8564 [00:13<00:11, 357.42 examples/s]Tokenizing train dataset:  54%|█████▎    | 4600/8564 [00:13<00:11, 354.71 examples/s]Tokenizing train dataset:  54%|█████▍    | 4636/8564 [00:13<00:11, 348.76 examples/s]Tokenizing train dataset:  55%|█████▍    | 4686/8564 [00:13<00:11, 337.44 examples/s]Tokenizing train dataset:  55%|█████▌    | 4737/8564 [00:13<00:11, 334.15 examples/s]Tokenizing train dataset:  56%|█████▌    | 4782/8564 [00:14<00:11, 320.22 examples/s]Tokenizing train dataset:  57%|█████▋    | 4839/8564 [00:14<00:09, 375.95 examples/s]Tokenizing train dataset:  57%|█████▋    | 4896/8564 [00:14<00:08, 419.79 examples/s]Tokenizing train dataset:  58%|█████▊    | 4960/8564 [00:14<00:07, 475.59 examples/s]Tokenizing train dataset:  59%|█████▊    | 5022/8564 [00:14<00:06, 511.93 examples/s]Tokenizing train dataset:  59%|█████▉    | 5088/8564 [00:14<00:06, 550.70 examples/s]Tokenizing train dataset:  60%|██████    | 5156/8564 [00:14<00:05, 583.73 examples/s]Tokenizing train dataset:  61%|██████    | 5228/8564 [00:14<00:05, 620.22 examples/s]Tokenizing train dataset:  62%|██████▏   | 5304/8564 [00:14<00:04, 656.78 examples/s]Tokenizing train dataset:  63%|██████▎   | 5389/8564 [00:15<00:05, 619.99 examples/s]Tokenizing train dataset:  64%|██████▎   | 5454/8564 [00:15<00:04, 623.01 examples/s]Tokenizing train dataset:  65%|██████▍   | 5547/8564 [00:15<00:04, 612.16 examples/s]Tokenizing train dataset:  66%|██████▌   | 5614/8564 [00:15<00:04, 625.37 examples/s]Tokenizing train dataset:  66%|██████▋   | 5678/8564 [00:15<00:04, 626.40 examples/s]Tokenizing train dataset:  67%|██████▋   | 5744/8564 [00:15<00:04, 629.18 examples/s]Tokenizing train dataset:  68%|██████▊   | 5824/8564 [00:15<00:04, 671.57 examples/s]Tokenizing train dataset:  69%|██████▉   | 5910/8564 [00:15<00:04, 625.67 examples/s]Tokenizing train dataset:  70%|██████▉   | 5978/8564 [00:15<00:04, 635.54 examples/s]Tokenizing train dataset:  71%|███████   | 6060/8564 [00:16<00:04, 601.22 examples/s]Tokenizing train dataset:  72%|███████▏  | 6156/8564 [00:16<00:03, 612.68 examples/s]Tokenizing train dataset:  73%|███████▎  | 6230/8564 [00:16<00:03, 641.95 examples/s]Tokenizing train dataset:  74%|███████▍  | 6329/8564 [00:16<00:03, 640.42 examples/s]Tokenizing train dataset:  75%|███████▍  | 6400/8564 [00:16<00:03, 651.50 examples/s]Tokenizing train dataset:  76%|███████▌  | 6492/8564 [00:16<00:03, 630.18 examples/s]Tokenizing train dataset:  77%|███████▋  | 6582/8564 [00:16<00:03, 615.48 examples/s]Tokenizing train dataset:  78%|███████▊  | 6665/8564 [00:17<00:03, 593.86 examples/s]Tokenizing train dataset:  79%|███████▊  | 6733/8564 [00:17<00:02, 611.56 examples/s]Tokenizing train dataset:  79%|███████▉  | 6801/8564 [00:17<00:02, 623.27 examples/s]Tokenizing train dataset:  80%|████████  | 6891/8564 [00:17<00:02, 612.06 examples/s]Tokenizing train dataset:  81%|████████  | 6958/8564 [00:17<00:02, 623.28 examples/s]Tokenizing train dataset:  82%|████████▏ | 7047/8564 [00:17<00:02, 600.79 examples/s]Tokenizing train dataset:  83%|████████▎ | 7113/8564 [00:17<00:02, 612.78 examples/s]Tokenizing train dataset:  84%|████████▍ | 7183/8564 [00:17<00:02, 627.36 examples/s]Tokenizing train dataset:  85%|████████▍ | 7276/8564 [00:18<00:02, 618.51 examples/s]Tokenizing train dataset:  86%|████████▌ | 7373/8564 [00:18<00:01, 622.07 examples/s]Tokenizing train dataset:  87%|████████▋ | 7438/8564 [00:18<00:01, 624.31 examples/s]Tokenizing train dataset:  88%|████████▊ | 7504/8564 [00:18<00:01, 627.24 examples/s]Tokenizing train dataset:  88%|████████▊ | 7576/8564 [00:18<00:01, 650.65 examples/s]Tokenizing train dataset:  89%|████████▉ | 7664/8564 [00:18<00:01, 619.85 examples/s]Tokenizing train dataset:  91%|█████████ | 7755/8564 [00:18<00:01, 609.05 examples/s]Tokenizing train dataset:  92%|█████████▏| 7844/8564 [00:19<00:01, 602.90 examples/s]Tokenizing train dataset:  92%|█████████▏| 7905/8564 [00:19<00:01, 603.85 examples/s]Tokenizing train dataset:  93%|█████████▎| 7968/8564 [00:19<00:00, 606.73 examples/s]Tokenizing train dataset:  94%|█████████▍| 8032/8564 [00:19<00:00, 612.37 examples/s]Tokenizing train dataset:  95%|█████████▍| 8120/8564 [00:19<00:00, 597.07 examples/s]Tokenizing train dataset:  96%|█████████▌| 8184/8564 [00:19<00:00, 604.93 examples/s]Tokenizing train dataset:  96%|█████████▋| 8251/8564 [00:19<00:00, 614.52 examples/s]Tokenizing train dataset:  97%|█████████▋| 8329/8564 [00:19<00:00, 650.33 examples/s]Tokenizing train dataset:  98%|█████████▊| 8414/8564 [00:19<00:00, 616.06 examples/s]Tokenizing train dataset:  99%|█████████▉| 8478/8564 [00:20<00:00, 620.14 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 614.62 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 424.38 examples/s]
Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11139.36 examples/s]
Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5428.60 examples/s]Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5383.86 examples/s]Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5346.48 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1119/8564 [00:00<00:01, 5568.56 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1114/8564 [00:00<00:01, 5513.77 examples/s]Extracting prompt in train dataset:  13%|█▎        | 1104/8564 [00:00<00:01, 5456.42 examples/s]Extracting prompt in train dataset:  20%|█▉        | 1680/8564 [00:00<00:01, 5551.39 examples/s]Extracting prompt in train dataset:  20%|█▉        | 1670/8564 [00:00<00:01, 5521.18 examples/s]Extracting prompt in train dataset:  19%|█▉        | 1650/8564 [00:00<00:01, 5445.18 examples/s]Extracting prompt in train dataset:  26%|██▋       | 2250/8564 [00:00<00:01, 5605.49 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2218/8564 [00:00<00:01, 5531.00 examples/s]Extracting prompt in train dataset:  26%|██▌       | 2240/8564 [00:00<00:01, 5580.33 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13519.12 examples/s]
Extracting prompt in train dataset:  33%|███▎      | 2786/8564 [00:00<00:01, 5576.75 examples/s]Extracting prompt in train dataset:  33%|███▎      | 2810/8564 [00:00<00:01, 5617.51 examples/s]Extracting prompt in train dataset:  33%|███▎      | 2830/8564 [00:00<00:01, 5638.81 examples/s]Extracting prompt in train dataset:  43%|████▎     | 3640/8564 [00:00<00:00, 5550.54 examples/s]Extracting prompt in train dataset:  43%|████▎     | 3650/8564 [00:00<00:00, 5546.44 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3619/8564 [00:00<00:00, 5514.86 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4210/8564 [00:00<00:00, 5590.87 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4230/8564 [00:00<00:00, 5590.28 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4186/8564 [00:00<00:00, 5545.22 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  56%|█████▌    | 4789/8564 [00:00<00:00, 5630.44 examples/s]Extracting prompt in train dataset:  55%|█████▌    | 4751/8564 [00:00<00:00, 5559.13 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 4810/8564 [00:00<00:00, 5618.74 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 329.52 examples/s]Extracting prompt in train dataset:  63%|██████▎   | 5370/8564 [00:00<00:00, 5667.72 examples/s]Extracting prompt in train dataset:  63%|██████▎   | 5396/8564 [00:00<00:00, 5688.60 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 5330/8564 [00:00<00:00, 5613.14 examples/s]Extracting prompt in train dataset:  70%|██████▉   | 5952/8564 [00:01<00:00, 5709.58 examples/s]Extracting prompt in train dataset:  70%|██████▉   | 5980/8564 [00:01<00:00, 5718.20 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 5910/8564 [00:01<00:00, 5652.83 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 298.59 examples/s]Extracting prompt in train dataset:  76%|███████▋  | 6531/8564 [00:01<00:00, 5733.06 examples/s]Extracting prompt in train dataset:  77%|███████▋  | 6564/8564 [00:01<00:00, 5752.67 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 6490/8564 [00:01<00:00, 5684.54 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 7110/8564 [00:01<00:00, 5746.24 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 7144/8564 [00:01<00:00, 5765.51 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 7070/8564 [00:01<00:00, 5695.80 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:02, 279.59 examples/s]Extracting prompt in train dataset:  90%|████████▉ | 7690/8564 [00:01<00:00, 5754.83 examples/s]Extracting prompt in train dataset:  90%|█████████ | 7724/8564 [00:01<00:00, 5774.22 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 7650/8564 [00:01<00:00, 5714.43 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 270.11 examples/s]Extracting prompt in train dataset:  99%|█████████▉| 8475/8564 [00:01<00:00, 5543.92 examples/s]Extracting prompt in train dataset:  99%|█████████▉| 8519/8564 [00:01<00:00, 5580.50 examples/s]Extracting prompt in train dataset:  99%|█████████▊| 8440/8564 [00:01<00:00, 5515.72 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5597.35 examples/s]
Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5579.00 examples/s]
Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5536.29 examples/s]
Tokenizing eval dataset:  21%|██        | 198/953 [00:00<00:02, 261.08 examples/s]Tokenizing eval dataset:  25%|██▍       | 238/953 [00:00<00:02, 293.13 examples/s]Tokenizing eval dataset:  32%|███▏      | 305/953 [00:00<00:01, 390.42 examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing eval dataset:  39%|███▊      | 368/953 [00:01<00:01, 454.84 examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   3%|▎         | 278/8564 [00:00<00:03, 2745.78 examples/s]Tokenizing eval dataset:  46%|████▌     | 437/953 [00:01<00:00, 519.37 examples/s]Applying chat template to train dataset:   3%|▎         | 283/8564 [00:00<00:02, 2801.48 examples/s]Applying chat template to train dataset:   3%|▎         | 287/8564 [00:00<00:02, 2828.86 examples/s]Applying chat template to train dataset:   7%|▋         | 584/8564 [00:00<00:02, 2925.39 examples/s]Tokenizing eval dataset:  53%|█████▎    | 505/953 [00:01<00:00, 561.30 examples/s]Applying chat template to train dataset:   7%|▋         | 596/8564 [00:00<00:02, 2986.74 examples/s]Applying chat template to train dataset:   7%|▋         | 600/8564 [00:00<00:02, 3001.43 examples/s]Applying chat template to train dataset:  10%|█         | 890/8564 [00:00<00:02, 2981.27 examples/s]Tokenizing eval dataset:  60%|█████▉    | 570/953 [00:01<00:00, 583.79 examples/s]Applying chat template to train dataset:  11%|█         | 908/8564 [00:00<00:02, 3043.97 examples/s]Applying chat template to train dataset:  11%|█         | 914/8564 [00:00<00:02, 3058.42 examples/s]Applying chat template to train dataset:  14%|█▍        | 1199/8564 [00:00<00:02, 3019.14 examples/s]Tokenizing eval dataset:  67%|██████▋   | 640/953 [00:01<00:00, 613.04 examples/s]Applying chat template to train dataset:  14%|█▍        | 1220/8564 [00:00<00:02, 3071.40 examples/s]Applying chat template to train dataset:  14%|█▍        | 1230/8564 [00:00<00:02, 3089.27 examples/s]Applying chat template to train dataset:  18%|█▊        | 1501/8564 [00:00<00:02, 3016.46 examples/s]Applying chat template to train dataset:  19%|█▊        | 1598/8564 [00:00<00:02, 2810.97 examples/s]Tokenizing eval dataset:  75%|███████▍  | 713/953 [00:01<00:00, 560.73 examples/s]Applying chat template to train dataset:  19%|█▉        | 1639/8564 [00:00<00:02, 2916.22 examples/s]Applying chat template to train dataset:  21%|██        | 1810/8564 [00:00<00:02, 3036.40 examples/s]Applying chat template to train dataset:  22%|██▏       | 1912/8564 [00:00<00:02, 2906.73 examples/s]Applying chat template to train dataset:  23%|██▎       | 1956/8564 [00:00<00:02, 2989.16 examples/s]Applying chat template to train dataset:  25%|██▍       | 2120/8564 [00:00<00:02, 3053.29 examples/s]Tokenizing eval dataset:  83%|████████▎ | 791/953 [00:01<00:00, 544.88 examples/s]Applying chat template to train dataset:  26%|██▌       | 2229/8564 [00:00<00:02, 2983.65 examples/s]Applying chat template to train dataset:  27%|██▋       | 2275/8564 [00:00<00:02, 3046.09 examples/s]Applying chat template to train dataset:  28%|██▊       | 2431/8564 [00:00<00:02, 3066.24 examples/s]Applying chat template to train dataset:  30%|██▉       | 2544/8564 [00:00<00:01, 3032.07 examples/s]Tokenizing eval dataset:  91%|█████████ | 866/953 [00:01<00:00, 528.44 examples/s]Applying chat template to train dataset:  30%|███       | 2594/8564 [00:00<00:01, 3085.18 examples/s]Applying chat template to train dataset:  32%|███▏      | 2741/8564 [00:00<00:01, 3072.46 examples/s]Applying chat template to train dataset:  33%|███▎      | 2860/8564 [00:00<00:01, 3066.62 examples/s]Applying chat template to train dataset:  34%|███▍      | 2912/8564 [00:00<00:01, 3109.44 examples/s]Applying chat template to train dataset:  36%|███▌      | 3050/8564 [00:01<00:01, 3029.82 examples/s]Tokenizing eval dataset:  99%|█████████▉| 944/953 [00:02<00:00, 521.36 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 460.11 examples/s]
Applying chat template to train dataset:  39%|███▉      | 3358/8564 [00:01<00:01, 3043.63 examples/s]Applying chat template to train dataset:  39%|███▉      | 3319/8564 [00:01<00:01, 3055.14 examples/s]Applying chat template to train dataset:  39%|███▉      | 3370/8564 [00:01<00:01, 3084.70 examples/s]Applying chat template to train dataset:  43%|████▎     | 3666/8564 [00:01<00:01, 3054.25 examples/s]Applying chat template to train dataset:  42%|████▏     | 3634/8564 [00:01<00:01, 3077.32 examples/s]Applying chat template to train dataset:  43%|████▎     | 3690/8564 [00:01<00:01, 3109.08 examples/s]Applying chat template to train dataset:  46%|████▋     | 3975/8564 [00:01<00:01, 3061.68 examples/s]Applying chat template to train dataset:  46%|████▌     | 3950/8564 [00:01<00:01, 3094.17 examples/s]Applying chat template to train dataset:  47%|████▋     | 4010/8564 [00:01<00:01, 3126.12 examples/s]Applying chat template to train dataset:  50%|█████     | 4285/8564 [00:01<00:01, 3066.96 examples/s]Applying chat template to train dataset:  50%|████▉     | 4265/8564 [00:01<00:01, 3109.56 examples/s]Applying chat template to train dataset:  51%|█████     | 4330/8564 [00:01<00:01, 3138.28 examples/s]Applying chat template to train dataset:  54%|█████▎    | 4595/8564 [00:01<00:01, 3071.49 examples/s]Applying chat template to train dataset:  53%|█████▎    | 4580/8564 [00:01<00:01, 3116.97 examples/s]Applying chat template to train dataset:  54%|█████▍    | 4647/8564 [00:01<00:01, 3146.26 examples/s]Applying chat template to train dataset:  57%|█████▋    | 4905/8564 [00:01<00:01, 3077.22 examples/s]Applying chat template to train dataset:  57%|█████▋    | 4896/8564 [00:01<00:01, 3128.10 examples/s]Applying chat template to train dataset:  58%|█████▊    | 4967/8564 [00:01<00:01, 3160.73 examples/s]Applying chat template to train dataset:  61%|██████    | 5220/8564 [00:01<00:01, 3097.00 examples/s]Applying chat template to train dataset:  61%|██████    | 5219/8564 [00:01<00:01, 3154.25 examples/s]Applying chat template to train dataset:  62%|██████▏   | 5294/8564 [00:01<00:01, 3185.97 examples/s]Applying chat template to train dataset:  65%|██████▍   | 5537/8564 [00:01<00:00, 3114.62 examples/s]Applying chat template to train dataset:  65%|██████▍   | 5541/8564 [00:01<00:00, 3167.44 examples/s]Applying chat template to train dataset:  66%|██████▌   | 5620/8564 [00:01<00:00, 3201.38 examples/s]Applying chat template to train dataset:  68%|██████▊   | 5853/8564 [00:01<00:00, 3124.56 examples/s]Applying chat template to train dataset:  68%|██████▊   | 5865/8564 [00:01<00:00, 3186.02 examples/s]Applying chat template to train dataset:  69%|██████▉   | 5946/8564 [00:01<00:00, 3216.06 examples/s]Applying chat template to train dataset:  72%|███████▏  | 6170/8564 [00:02<00:00, 3131.09 examples/s]Applying chat template to train dataset:  72%|███████▏  | 6187/8564 [00:02<00:00, 3194.30 examples/s]Applying chat template to train dataset:  73%|███████▎  | 6271/8564 [00:02<00:00, 3223.36 examples/s]Applying chat template to train dataset:  76%|███████▌  | 6487/8564 [00:02<00:00, 3139.07 examples/s]Applying chat template to train dataset:  76%|███████▌  | 6509/8564 [00:02<00:00, 3200.75 examples/s]Applying chat template to train dataset:  77%|███████▋  | 6597/8564 [00:02<00:00, 3229.51 examples/s]Applying chat template to train dataset:  79%|███████▉  | 6802/8564 [00:02<00:00, 3138.38 examples/s]Applying chat template to train dataset:  80%|███████▉  | 6830/8564 [00:02<00:00, 3198.64 examples/s]Applying chat template to train dataset:  81%|████████  | 6923/8564 [00:02<00:00, 3231.68 examples/s]Applying chat template to train dataset:  83%|████████▎ | 7119/8564 [00:02<00:00, 3142.95 examples/s]Applying chat template to train dataset:  84%|████████▎ | 7152/8564 [00:02<00:00, 3202.98 examples/s]Applying chat template to train dataset:  85%|████████▍ | 7248/8564 [00:02<00:00, 3236.04 examples/s]Applying chat template to train dataset:  87%|████████▋ | 7475/8564 [00:02<00:00, 3207.36 examples/s]Applying chat template to train dataset:  88%|████████▊ | 7574/8564 [00:02<00:00, 3237.51 examples/s]Applying chat template to train dataset:  89%|████████▊ | 7591/8564 [00:02<00:00, 3141.81 examples/s]Applying chat template to train dataset:  92%|█████████▏| 7916/8564 [00:02<00:00, 3098.78 examples/s]Applying chat template to train dataset:  94%|█████████▎| 8015/8564 [00:02<00:00, 3118.84 examples/s]Applying chat template to train dataset:  94%|█████████▎| 8016/8564 [00:02<00:00, 3027.65 examples/s]Applying chat template to train dataset:  96%|█████████▌| 8233/8564 [00:02<00:00, 3115.68 examples/s]Applying chat template to train dataset:  97%|█████████▋| 8340/8564 [00:02<00:00, 3149.85 examples/s]Applying chat template to train dataset:  97%|█████████▋| 8323/8564 [00:02<00:00, 3032.72 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3127.43 examples/s]
Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3101.22 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3087.14 examples/s]
Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:02<00:00, 3056.46 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 42/8564 [00:00<00:21, 400.69 examples/s]Tokenizing train dataset:   0%|          | 42/8564 [00:00<00:20, 407.06 examples/s]Tokenizing train dataset:   0%|          | 42/8564 [00:00<00:20, 408.13 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:25, 338.50 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:24, 341.32 examples/s]Tokenizing train dataset:   1%|          | 90/8564 [00:00<00:24, 340.04 examples/s]Tokenizing train dataset:   2%|▏         | 139/8564 [00:00<00:26, 319.89 examples/s]Tokenizing train dataset:   2%|▏         | 139/8564 [00:00<00:26, 322.18 examples/s]Tokenizing train dataset:   2%|▏         | 139/8564 [00:00<00:26, 320.37 examples/s]Tokenizing train dataset:   2%|▏         | 183/8564 [00:00<00:27, 306.69 examples/s]Tokenizing train dataset:   2%|▏         | 183/8564 [00:00<00:27, 307.65 examples/s]Tokenizing train dataset:   2%|▏         | 183/8564 [00:00<00:27, 306.12 examples/s]Tokenizing train dataset:   3%|▎         | 218/8564 [00:00<00:26, 314.65 examples/s]Tokenizing train dataset:   3%|▎         | 218/8564 [00:00<00:26, 315.27 examples/s]Tokenizing train dataset:   3%|▎         | 218/8564 [00:00<00:26, 313.57 examples/s]Tokenizing train dataset:   3%|▎         | 252/8564 [00:00<00:26, 319.39 examples/s]Tokenizing train dataset:   3%|▎         | 252/8564 [00:00<00:26, 319.58 examples/s]Tokenizing train dataset:   3%|▎         | 251/8564 [00:00<00:26, 316.34 examples/s]Tokenizing train dataset:   3%|▎         | 290/8564 [00:00<00:24, 332.58 examples/s]Tokenizing train dataset:   3%|▎         | 290/8564 [00:00<00:24, 332.45 examples/s]Tokenizing train dataset:   3%|▎         | 289/8564 [00:00<00:24, 332.59 examples/s]Tokenizing train dataset:   4%|▍         | 324/8564 [00:00<00:24, 331.66 examples/s]Tokenizing train dataset:   4%|▍         | 324/8564 [00:00<00:24, 331.62 examples/s]Tokenizing train dataset:   4%|▍         | 337/8564 [00:01<00:25, 324.83 examples/s]Tokenizing train dataset:   4%|▍         | 370/8564 [00:01<00:25, 318.05 examples/s]Tokenizing train dataset:   4%|▍         | 370/8564 [00:01<00:25, 318.40 examples/s]Tokenizing train dataset:   5%|▍         | 387/8564 [00:01<00:25, 323.09 examples/s]Tokenizing train dataset:   5%|▍         | 420/8564 [00:01<00:25, 316.77 examples/s]Tokenizing train dataset:   5%|▍         | 420/8564 [00:01<00:25, 317.26 examples/s]Tokenizing train dataset:   5%|▌         | 434/8564 [00:01<00:25, 314.79 examples/s]Tokenizing train dataset:   5%|▌         | 469/8564 [00:01<00:25, 315.09 examples/s]Tokenizing train dataset:   5%|▌         | 469/8564 [00:01<00:25, 315.63 examples/s]Tokenizing train dataset:   6%|▌         | 480/8564 [00:01<00:26, 308.00 examples/s]Tokenizing train dataset:   6%|▌         | 512/8564 [00:01<00:26, 304.94 examples/s]Tokenizing train dataset:   6%|▌         | 512/8564 [00:01<00:26, 305.31 examples/s]Tokenizing train dataset:   6%|▌         | 527/8564 [00:01<00:26, 307.38 examples/s]Tokenizing train dataset:   6%|▋         | 549/8564 [00:01<00:25, 318.07 examples/s]Tokenizing train dataset:   6%|▋         | 549/8564 [00:01<00:25, 318.58 examples/s]Tokenizing train dataset:   7%|▋         | 561/8564 [00:01<00:25, 312.15 examples/s]Tokenizing train dataset:   7%|▋         | 596/8564 [00:01<00:25, 309.01 examples/s]Tokenizing train dataset:   7%|▋         | 596/8564 [00:01<00:25, 309.66 examples/s]Tokenizing train dataset:   7%|▋         | 605/8564 [00:01<00:26, 301.77 examples/s]Tokenizing train dataset:   7%|▋         | 634/8564 [00:01<00:24, 323.01 examples/s]Tokenizing train dataset:   7%|▋         | 634/8564 [00:01<00:24, 323.93 examples/s]Tokenizing train dataset:   8%|▊         | 644/8564 [00:02<00:24, 319.08 examples/s]Tokenizing train dataset:   8%|▊         | 681/8564 [00:02<00:25, 313.65 examples/s]Tokenizing train dataset:   8%|▊         | 681/8564 [00:02<00:25, 315.12 examples/s]Tokenizing train dataset:   8%|▊         | 690/8564 [00:02<00:25, 308.42 examples/s]Tokenizing train dataset:   9%|▊         | 729/8564 [00:02<00:25, 312.30 examples/s]Tokenizing train dataset:   9%|▊         | 729/8564 [00:02<00:25, 313.36 examples/s]Tokenizing train dataset:   8%|▊         | 722/8564 [00:02<00:25, 309.18 examples/s]Tokenizing train dataset:   9%|▉         | 763/8564 [00:02<00:24, 313.77 examples/s]Tokenizing train dataset:   9%|▉         | 763/8564 [00:02<00:24, 314.64 examples/s]Tokenizing train dataset:   9%|▉         | 759/8564 [00:02<00:24, 318.44 examples/s]Tokenizing train dataset:   9%|▉         | 806/8564 [00:02<00:25, 300.76 examples/s]Tokenizing train dataset:   9%|▉         | 808/8564 [00:02<00:28, 274.05 examples/s]Tokenizing train dataset:   9%|▉         | 806/8564 [00:02<00:27, 280.75 examples/s]Tokenizing train dataset:  10%|▉         | 850/8564 [00:02<00:26, 295.05 examples/s]Tokenizing train dataset:  10%|▉         | 839/8564 [00:02<00:27, 278.06 examples/s]Tokenizing train dataset:  10%|▉         | 850/8564 [00:02<00:27, 281.09 examples/s]Tokenizing train dataset:  10%|█         | 885/8564 [00:02<00:25, 303.27 examples/s]Tokenizing train dataset:  10%|█         | 868/8564 [00:02<00:27, 279.77 examples/s]Tokenizing train dataset:  10%|█         | 885/8564 [00:02<00:26, 292.05 examples/s]Tokenizing train dataset:  11%|█         | 920/8564 [00:02<00:24, 308.43 examples/s]Tokenizing train dataset:  11%|█         | 907/8564 [00:02<00:24, 306.41 examples/s]Tokenizing train dataset:  11%|█         | 920/8564 [00:02<00:25, 299.68 examples/s]Tokenizing train dataset:  11%|█         | 952/8564 [00:03<00:24, 306.47 examples/s]Tokenizing train dataset:  11%|█         | 952/8564 [00:03<00:25, 295.92 examples/s]Tokenizing train dataset:  11%|█         | 951/8564 [00:03<00:25, 301.65 examples/s]Tokenizing train dataset:  11%|█▏        | 983/8564 [00:03<00:25, 302.05 examples/s]Tokenizing train dataset:  11%|█▏        | 983/8564 [00:03<00:25, 294.78 examples/s]Tokenizing train dataset:  12%|█▏        | 994/8564 [00:03<00:25, 293.22 examples/s]Tokenizing train dataset:  12%|█▏        | 1027/8564 [00:03<00:25, 292.59 examples/s]Tokenizing train dataset:  12%|█▏        | 1027/8564 [00:03<00:26, 288.61 examples/s]Tokenizing train dataset:  12%|█▏        | 1037/8564 [00:03<00:26, 287.83 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 294.03 examples/s]Tokenizing train dataset:  13%|█▎        | 1073/8564 [00:03<00:25, 291.53 examples/s]Tokenizing train dataset:  12%|█▏        | 1068/8564 [00:03<00:25, 292.88 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 307.23 examples/s]Tokenizing train dataset:  13%|█▎        | 1109/8564 [00:03<00:24, 305.20 examples/s]Tokenizing train dataset:  13%|█▎        | 1103/8564 [00:03<00:24, 305.34 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:03<00:25, 295.15 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:03<00:25, 294.07 examples/s]Tokenizing train dataset:  13%|█▎        | 1146/8564 [00:03<00:25, 294.52 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:03<00:24, 295.66 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:03<00:25, 294.96 examples/s]Tokenizing train dataset:  14%|█▍        | 1218/8564 [00:03<00:24, 304.36 examples/s]Tokenizing train dataset:  14%|█▍        | 1193/8564 [00:03<00:24, 298.80 examples/s]Tokenizing train dataset:  14%|█▍        | 1218/8564 [00:03<00:24, 303.70 examples/s]Tokenizing train dataset:  15%|█▍        | 1252/8564 [00:04<00:23, 312.87 examples/s]Tokenizing train dataset:  14%|█▍        | 1225/8564 [00:04<00:24, 301.14 examples/s]Tokenizing train dataset:  15%|█▍        | 1252/8564 [00:04<00:23, 311.98 examples/s]Tokenizing train dataset:  15%|█▌        | 1286/8564 [00:04<00:23, 310.65 examples/s]Tokenizing train dataset:  15%|█▍        | 1262/8564 [00:04<00:23, 314.83 examples/s]Tokenizing train dataset:  15%|█▌        | 1286/8564 [00:04<00:23, 310.12 examples/s]Tokenizing train dataset:  15%|█▌        | 1320/8564 [00:04<00:22, 317.61 examples/s]Tokenizing train dataset:  15%|█▌        | 1311/8564 [00:04<00:22, 315.98 examples/s]Tokenizing train dataset:  15%|█▌        | 1320/8564 [00:04<00:22, 317.06 examples/s]Tokenizing train dataset:  16%|█▌        | 1353/8564 [00:04<00:22, 313.90 examples/s]Tokenizing train dataset:  16%|█▌        | 1344/8564 [00:04<00:22, 316.53 examples/s]Tokenizing train dataset:  16%|█▌        | 1353/8564 [00:04<00:22, 313.92 examples/s]Tokenizing train dataset:  16%|█▋        | 1397/8564 [00:04<00:23, 303.59 examples/s]Tokenizing train dataset:  16%|█▌        | 1387/8564 [00:04<00:23, 300.00 examples/s]Tokenizing train dataset:  16%|█▋        | 1397/8564 [00:04<00:23, 303.66 examples/s]Tokenizing train dataset:  17%|█▋        | 1428/8564 [00:04<00:23, 303.16 examples/s]Tokenizing train dataset:  17%|█▋        | 1418/8564 [00:04<00:23, 301.12 examples/s]Tokenizing train dataset:  17%|█▋        | 1428/8564 [00:04<00:23, 303.23 examples/s]Tokenizing train dataset:  17%|█▋        | 1473/8564 [00:04<00:23, 298.61 examples/s]Tokenizing train dataset:  17%|█▋        | 1462/8564 [00:04<00:24, 294.03 examples/s]Tokenizing train dataset:  17%|█▋        | 1473/8564 [00:04<00:23, 298.59 examples/s]Tokenizing train dataset:  18%|█▊        | 1515/8564 [00:04<00:24, 289.92 examples/s]Tokenizing train dataset:  17%|█▋        | 1492/8564 [00:04<00:24, 293.93 examples/s]Tokenizing train dataset:  18%|█▊        | 1515/8564 [00:04<00:24, 290.02 examples/s]Tokenizing train dataset:  18%|█▊        | 1550/8564 [00:05<00:23, 298.52 examples/s]Tokenizing train dataset:  18%|█▊        | 1539/8564 [00:05<00:23, 296.69 examples/s]Tokenizing train dataset:  18%|█▊        | 1550/8564 [00:05<00:23, 298.52 examples/s]Tokenizing train dataset:  18%|█▊        | 1581/8564 [00:05<00:23, 297.44 examples/s]Tokenizing train dataset:  18%|█▊        | 1570/8564 [00:05<00:23, 294.67 examples/s]Tokenizing train dataset:  18%|█▊        | 1581/8564 [00:05<00:23, 297.75 examples/s]Tokenizing train dataset:  19%|█▉        | 1614/8564 [00:05<00:22, 303.17 examples/s]Tokenizing train dataset:  19%|█▊        | 1601/8564 [00:05<00:23, 294.21 examples/s]Tokenizing train dataset:  19%|█▉        | 1614/8564 [00:05<00:22, 303.48 examples/s]Tokenizing train dataset:  19%|█▉        | 1650/8564 [00:05<00:21, 315.83 examples/s]Tokenizing train dataset:  19%|█▉        | 1637/8564 [00:05<00:22, 306.36 examples/s]Tokenizing train dataset:  19%|█▉        | 1650/8564 [00:05<00:21, 316.08 examples/s]Tokenizing train dataset:  20%|█▉        | 1686/8564 [00:05<00:21, 321.73 examples/s]Tokenizing train dataset:  20%|█▉        | 1672/8564 [00:05<00:21, 316.36 examples/s]Tokenizing train dataset:  20%|█▉        | 1686/8564 [00:05<00:21, 321.84 examples/s]Tokenizing train dataset:  20%|██        | 1725/8564 [00:05<00:20, 333.89 examples/s]Tokenizing train dataset:  20%|██        | 1714/8564 [00:05<00:20, 337.11 examples/s]Tokenizing train dataset:  20%|██        | 1725/8564 [00:05<00:20, 333.60 examples/s]Tokenizing train dataset:  21%|██        | 1760/8564 [00:05<00:20, 337.45 examples/s]Tokenizing train dataset:  21%|██        | 1760/8564 [00:05<00:20, 337.52 examples/s]Tokenizing train dataset:  21%|██        | 1766/8564 [00:05<00:20, 335.22 examples/s]Tokenizing train dataset:  21%|██        | 1808/8564 [00:05<00:20, 323.55 examples/s]Tokenizing train dataset:  21%|██        | 1808/8564 [00:05<00:20, 323.63 examples/s]Tokenizing train dataset:  22%|██▏       | 1842/8564 [00:05<00:20, 324.70 examples/s]Tokenizing train dataset:  21%|██        | 1814/8564 [00:05<00:20, 321.62 examples/s]Tokenizing train dataset:  22%|██▏       | 1842/8564 [00:05<00:20, 324.38 examples/s]Tokenizing train dataset:  22%|██▏       | 1848/8564 [00:05<00:20, 320.26 examples/s]Tokenizing train dataset:  22%|██▏       | 1887/8564 [00:06<00:21, 310.76 examples/s]Tokenizing train dataset:  22%|██▏       | 1887/8564 [00:06<00:21, 310.47 examples/s]Tokenizing train dataset:  23%|██▎       | 1930/8564 [00:06<00:19, 339.87 examples/s]Tokenizing train dataset:  22%|██▏       | 1893/8564 [00:06<00:21, 308.40 examples/s]Tokenizing train dataset:  23%|██▎       | 1930/8564 [00:06<00:19, 339.76 examples/s]Tokenizing train dataset:  23%|██▎       | 1966/8564 [00:06<00:19, 342.16 examples/s]Tokenizing train dataset:  23%|██▎       | 1938/8564 [00:06<00:19, 340.96 examples/s]Tokenizing train dataset:  23%|██▎       | 1966/8564 [00:06<00:19, 342.28 examples/s]Tokenizing train dataset:  23%|██▎       | 2003/8564 [00:06<00:18, 346.51 examples/s]Tokenizing train dataset:  23%|██▎       | 1975/8564 [00:06<00:19, 344.42 examples/s]Tokenizing train dataset:  23%|██▎       | 2003/8564 [00:06<00:18, 346.56 examples/s]Tokenizing train dataset:  24%|██▍       | 2043/8564 [00:06<00:18, 358.67 examples/s]Tokenizing train dataset:  24%|██▎       | 2013/8564 [00:06<00:18, 349.31 examples/s]Tokenizing train dataset:  24%|██▍       | 2043/8564 [00:06<00:18, 359.18 examples/s]Tokenizing train dataset:  24%|██▍       | 2080/8564 [00:06<00:18, 359.31 examples/s]Tokenizing train dataset:  24%|██▍       | 2052/8564 [00:06<00:18, 359.22 examples/s]Tokenizing train dataset:  24%|██▍       | 2080/8564 [00:06<00:18, 360.19 examples/s]Tokenizing train dataset:  25%|██▍       | 2120/8564 [00:06<00:17, 364.01 examples/s]Tokenizing train dataset:  24%|██▍       | 2090/8564 [00:06<00:18, 358.91 examples/s]Tokenizing train dataset:  25%|██▍       | 2120/8564 [00:06<00:17, 365.04 examples/s]Tokenizing train dataset:  25%|██▌       | 2157/8564 [00:06<00:17, 364.74 examples/s]Tokenizing train dataset:  25%|██▍       | 2130/8564 [00:06<00:17, 366.81 examples/s]Tokenizing train dataset:  25%|██▌       | 2157/8564 [00:06<00:17, 365.76 examples/s]Tokenizing train dataset:  26%|██▌       | 2194/8564 [00:06<00:17, 364.33 examples/s]Tokenizing train dataset:  25%|██▌       | 2167/8564 [00:06<00:17, 362.40 examples/s]Tokenizing train dataset:  26%|██▌       | 2194/8564 [00:06<00:17, 365.35 examples/s]Tokenizing train dataset:  26%|██▌       | 2232/8564 [00:06<00:17, 365.93 examples/s]Tokenizing train dataset:  26%|██▌       | 2232/8564 [00:07<00:17, 366.59 examples/s]Tokenizing train dataset:  26%|██▌       | 2224/8564 [00:07<00:17, 366.64 examples/s]Tokenizing train dataset:  27%|██▋       | 2272/8564 [00:07<00:16, 373.17 examples/s]Tokenizing train dataset:  27%|██▋       | 2272/8564 [00:07<00:16, 374.17 examples/s]Tokenizing train dataset:  27%|██▋       | 2284/8564 [00:07<00:16, 377.58 examples/s]Tokenizing train dataset:  27%|██▋       | 2328/8564 [00:07<00:16, 369.73 examples/s]Tokenizing train dataset:  27%|██▋       | 2310/8564 [00:07<00:16, 368.94 examples/s]Tokenizing train dataset:  27%|██▋       | 2336/8564 [00:07<00:17, 365.00 examples/s]Tokenizing train dataset:  28%|██▊       | 2380/8564 [00:07<00:17, 358.22 examples/s]Tokenizing train dataset:  28%|██▊       | 2363/8564 [00:07<00:17, 355.92 examples/s]Tokenizing train dataset:  28%|██▊       | 2421/8564 [00:07<00:16, 369.03 examples/s]Tokenizing train dataset:  28%|██▊       | 2391/8564 [00:07<00:17, 362.95 examples/s]Tokenizing train dataset:  28%|██▊       | 2406/8564 [00:07<00:16, 372.54 examples/s]Tokenizing train dataset:  29%|██▊       | 2461/8564 [00:07<00:16, 376.51 examples/s]Tokenizing train dataset:  28%|██▊       | 2430/8564 [00:07<00:16, 367.79 examples/s]Tokenizing train dataset:  29%|██▊       | 2444/8564 [00:07<00:16, 372.62 examples/s]Tokenizing train dataset:  29%|██▉       | 2470/8564 [00:07<00:16, 371.43 examples/s]Tokenizing train dataset:  29%|██▉       | 2511/8564 [00:07<00:16, 356.79 examples/s]Tokenizing train dataset:  29%|██▉       | 2498/8564 [00:07<00:16, 365.19 examples/s]Tokenizing train dataset:  30%|██▉       | 2555/8564 [00:07<00:16, 373.46 examples/s]Tokenizing train dataset:  30%|██▉       | 2538/8564 [00:07<00:16, 372.51 examples/s]Tokenizing train dataset:  29%|██▉       | 2522/8564 [00:07<00:16, 359.73 examples/s]Tokenizing train dataset:  30%|███       | 2594/8564 [00:07<00:15, 374.44 examples/s]Tokenizing train dataset:  30%|███       | 2578/8564 [00:07<00:15, 379.11 examples/s]Tokenizing train dataset:  30%|██▉       | 2565/8564 [00:07<00:16, 372.07 examples/s]Tokenizing train dataset:  30%|███       | 2604/8564 [00:08<00:15, 373.34 examples/s]Tokenizing train dataset:  31%|███       | 2642/8564 [00:08<00:16, 351.21 examples/s]Tokenizing train dataset:  31%|███       | 2625/8564 [00:08<00:16, 351.75 examples/s]Tokenizing train dataset:  31%|███       | 2661/8564 [00:08<00:16, 349.42 examples/s]Tokenizing train dataset:  31%|███       | 2653/8564 [00:08<00:17, 341.65 examples/s]Tokenizing train dataset:  31%|███▏      | 2693/8564 [00:08<00:17, 341.54 examples/s]Tokenizing train dataset:  32%|███▏      | 2731/8564 [00:08<00:16, 349.13 examples/s]Tokenizing train dataset:  32%|███▏      | 2716/8564 [00:08<00:16, 350.67 examples/s]Tokenizing train dataset:  32%|███▏      | 2705/8564 [00:08<00:17, 342.17 examples/s]Tokenizing train dataset:  32%|███▏      | 2740/8564 [00:08<00:17, 340.52 examples/s]Tokenizing train dataset:  33%|███▎      | 2785/8564 [00:08<00:16, 348.80 examples/s]Tokenizing train dataset:  32%|███▏      | 2768/8564 [00:08<00:16, 344.76 examples/s]Tokenizing train dataset:  32%|███▏      | 2775/8564 [00:08<00:16, 341.82 examples/s]Tokenizing train dataset:  33%|███▎      | 2825/8564 [00:08<00:15, 360.26 examples/s]Tokenizing train dataset:  33%|███▎      | 2808/8564 [00:08<00:16, 355.88 examples/s]Tokenizing train dataset:  33%|███▎      | 2820/8564 [00:08<00:15, 362.65 examples/s]Tokenizing train dataset:  34%|███▎      | 2874/8564 [00:08<00:16, 345.81 examples/s]Tokenizing train dataset:  33%|███▎      | 2860/8564 [00:08<00:16, 349.06 examples/s]Tokenizing train dataset:  34%|███▎      | 2870/8564 [00:08<00:16, 344.56 examples/s]Tokenizing train dataset:  34%|███▍      | 2921/8564 [00:08<00:15, 369.54 examples/s]Tokenizing train dataset:  34%|███▍      | 2900/8564 [00:08<00:15, 357.74 examples/s]Tokenizing train dataset:  34%|███▍      | 2914/8564 [00:08<00:15, 367.26 examples/s]Tokenizing train dataset:  35%|███▍      | 2961/8564 [00:09<00:14, 376.28 examples/s]Tokenizing train dataset:  34%|███▍      | 2944/8564 [00:08<00:14, 377.48 examples/s]Tokenizing train dataset:  35%|███▍      | 2957/8564 [00:09<00:14, 380.63 examples/s]Tokenizing train dataset:  35%|███▌      | 3005/8564 [00:09<00:14, 392.14 examples/s]Tokenizing train dataset:  35%|███▍      | 2989/8564 [00:09<00:14, 393.45 examples/s]Tokenizing train dataset:  35%|███▌      | 3000/8564 [00:09<00:14, 389.34 examples/s]Tokenizing train dataset:  36%|███▌      | 3057/8564 [00:09<00:14, 369.91 examples/s]Tokenizing train dataset:  36%|███▌      | 3043/8564 [00:09<00:14, 377.83 examples/s]Tokenizing train dataset:  36%|███▌      | 3097/8564 [00:09<00:14, 374.32 examples/s]Tokenizing train dataset:  36%|███▌      | 3054/8564 [00:09<00:14, 371.70 examples/s]Tokenizing train dataset:  36%|███▌      | 3098/8564 [00:09<00:14, 371.98 examples/s]Tokenizing train dataset:  37%|███▋      | 3151/8564 [00:09<00:14, 366.82 examples/s]Tokenizing train dataset:  36%|███▋      | 3111/8564 [00:09<00:14, 371.41 examples/s]Tokenizing train dataset:  37%|███▋      | 3153/8564 [00:09<00:14, 367.91 examples/s]Tokenizing train dataset:  37%|███▋      | 3189/8564 [00:09<00:14, 363.57 examples/s]Tokenizing train dataset:  37%|███▋      | 3166/8564 [00:09<00:14, 365.21 examples/s]Tokenizing train dataset:  38%|███▊      | 3228/8564 [00:09<00:14, 369.17 examples/s]Tokenizing train dataset:  37%|███▋      | 3208/8564 [00:09<00:14, 364.26 examples/s]Tokenizing train dataset:  37%|███▋      | 3203/8564 [00:09<00:14, 364.05 examples/s]Tokenizing train dataset:  38%|███▊      | 3269/8564 [00:09<00:14, 375.14 examples/s]Tokenizing train dataset:  38%|███▊      | 3250/8564 [00:09<00:14, 372.82 examples/s]Tokenizing train dataset:  38%|███▊      | 3243/8564 [00:09<00:14, 368.77 examples/s]Tokenizing train dataset:  39%|███▉      | 3320/8564 [00:09<00:14, 359.24 examples/s]Tokenizing train dataset:  39%|███▊      | 3302/8564 [00:09<00:14, 360.06 examples/s]Tokenizing train dataset:  39%|███▊      | 3298/8564 [00:09<00:14, 363.85 examples/s]Tokenizing train dataset:  39%|███▉      | 3357/8564 [00:10<00:14, 359.62 examples/s]Tokenizing train dataset:  39%|███▉      | 3357/8564 [00:10<00:14, 359.70 examples/s]Tokenizing train dataset:  40%|███▉      | 3397/8564 [00:10<00:14, 368.84 examples/s]Tokenizing train dataset:  39%|███▉      | 3350/8564 [00:10<00:14, 354.96 examples/s]Tokenizing train dataset:  40%|███▉      | 3397/8564 [00:10<00:14, 367.02 examples/s]Tokenizing train dataset:  40%|████      | 3437/8564 [00:10<00:13, 376.57 examples/s]Tokenizing train dataset:  40%|███▉      | 3394/8564 [00:10<00:14, 367.08 examples/s]Tokenizing train dataset:  40%|████      | 3437/8564 [00:10<00:13, 374.43 examples/s]Tokenizing train dataset:  40%|████      | 3435/8564 [00:10<00:13, 375.16 examples/s]Tokenizing train dataset:  41%|████      | 3491/8564 [00:10<00:13, 366.31 examples/s]Tokenizing train dataset:  41%|████      | 3491/8564 [00:10<00:13, 365.16 examples/s]Tokenizing train dataset:  41%|████▏     | 3533/8564 [00:10<00:13, 378.00 examples/s]Tokenizing train dataset:  41%|████      | 3489/8564 [00:10<00:13, 365.19 examples/s]Tokenizing train dataset:  41%|████▏     | 3533/8564 [00:10<00:13, 375.00 examples/s]Tokenizing train dataset:  41%|████      | 3530/8564 [00:10<00:13, 369.24 examples/s]Tokenizing train dataset:  42%|████▏     | 3590/8564 [00:10<00:13, 368.60 examples/s]Tokenizing train dataset:  42%|████▏     | 3589/8564 [00:10<00:13, 370.36 examples/s]Tokenizing train dataset:  42%|████▏     | 3631/8564 [00:10<00:13, 371.41 examples/s]Tokenizing train dataset:  42%|████▏     | 3587/8564 [00:10<00:13, 369.32 examples/s]Tokenizing train dataset:  42%|████▏     | 3628/8564 [00:10<00:13, 374.60 examples/s]Tokenizing train dataset:  42%|████▏     | 3627/8564 [00:10<00:13, 372.99 examples/s]Tokenizing train dataset:  43%|████▎     | 3680/8564 [00:10<00:13, 350.38 examples/s]Tokenizing train dataset:  43%|████▎     | 3677/8564 [00:10<00:13, 353.55 examples/s]Tokenizing train dataset:  43%|████▎     | 3724/8564 [00:11<00:13, 368.79 examples/s]Tokenizing train dataset:  43%|████▎     | 3676/8564 [00:11<00:13, 352.17 examples/s]Tokenizing train dataset:  43%|████▎     | 3720/8564 [00:11<00:13, 366.06 examples/s]Tokenizing train dataset:  43%|████▎     | 3714/8564 [00:11<00:13, 356.88 examples/s]Tokenizing train dataset:  44%|████▍     | 3780/8564 [00:11<00:12, 368.40 examples/s]Tokenizing train dataset:  44%|████▍     | 3776/8564 [00:11<00:13, 367.55 examples/s]Tokenizing train dataset:  44%|████▍     | 3755/8564 [00:11<00:13, 364.55 examples/s]Tokenizing train dataset:  45%|████▍     | 3834/8564 [00:11<00:13, 363.10 examples/s]Tokenizing train dataset:  44%|████▍     | 3794/8564 [00:11<00:12, 367.57 examples/s]Tokenizing train dataset:  45%|████▍     | 3831/8564 [00:11<00:13, 362.96 examples/s]Tokenizing train dataset:  45%|████▌     | 3890/8564 [00:11<00:12, 361.33 examples/s]Tokenizing train dataset:  45%|████▌     | 3869/8564 [00:11<00:12, 363.34 examples/s]Tokenizing train dataset:  45%|████▍     | 3848/8564 [00:11<00:13, 357.51 examples/s]Tokenizing train dataset:  45%|████▌     | 3886/8564 [00:11<00:12, 360.22 examples/s]Tokenizing train dataset:  46%|████▌     | 3941/8564 [00:11<00:13, 351.28 examples/s]Tokenizing train dataset:  46%|████▌     | 3919/8564 [00:11<00:13, 350.38 examples/s]Tokenizing train dataset:  46%|████▋     | 3978/8564 [00:11<00:12, 353.12 examples/s]Tokenizing train dataset:  46%|████▌     | 3956/8564 [00:11<00:12, 354.76 examples/s]Tokenizing train dataset:  46%|████▌     | 3939/8564 [00:11<00:13, 350.79 examples/s]Tokenizing train dataset:  47%|████▋     | 4015/8564 [00:11<00:12, 355.59 examples/s]Tokenizing train dataset:  46%|████▋     | 3975/8564 [00:11<00:13, 349.71 examples/s]Tokenizing train dataset:  47%|████▋     | 4010/8564 [00:11<00:12, 354.17 examples/s]Tokenizing train dataset:  47%|████▋     | 4056/8564 [00:12<00:12, 364.23 examples/s]Tokenizing train dataset:  47%|████▋     | 4013/8564 [00:11<00:12, 353.08 examples/s]Tokenizing train dataset:  47%|████▋     | 4050/8564 [00:12<00:12, 362.85 examples/s]Tokenizing train dataset:  47%|████▋     | 4053/8564 [00:12<00:12, 362.75 examples/s]Tokenizing train dataset:  48%|████▊     | 4110/8564 [00:12<00:12, 359.30 examples/s]Tokenizing train dataset:  48%|████▊     | 4104/8564 [00:12<00:12, 358.92 examples/s]Tokenizing train dataset:  48%|████▊     | 4106/8564 [00:12<00:12, 356.70 examples/s]Tokenizing train dataset:  49%|████▊     | 4162/8564 [00:12<00:12, 352.38 examples/s]Tokenizing train dataset:  49%|████▊     | 4158/8564 [00:12<00:12, 353.93 examples/s]Tokenizing train dataset:  49%|████▉     | 4203/8564 [00:12<00:12, 362.83 examples/s]Tokenizing train dataset:  49%|████▊     | 4159/8564 [00:12<00:12, 352.09 examples/s]Tokenizing train dataset:  49%|████▉     | 4198/8564 [00:12<00:12, 362.47 examples/s]Tokenizing train dataset:  49%|████▉     | 4199/8564 [00:12<00:12, 361.82 examples/s]Tokenizing train dataset:  50%|████▉     | 4257/8564 [00:12<00:12, 357.87 examples/s]Tokenizing train dataset:  50%|████▉     | 4250/8564 [00:12<00:12, 353.44 examples/s]Tokenizing train dataset:  50%|████▉     | 4250/8564 [00:12<00:12, 350.72 examples/s]Tokenizing train dataset:  50%|█████     | 4313/8564 [00:12<00:11, 359.62 examples/s]Tokenizing train dataset:  50%|█████     | 4290/8564 [00:12<00:11, 359.75 examples/s]Tokenizing train dataset:  50%|█████     | 4290/8564 [00:12<00:11, 356.87 examples/s]Tokenizing train dataset:  51%|█████     | 4365/8564 [00:12<00:11, 354.12 examples/s]Tokenizing train dataset:  51%|█████     | 4341/8564 [00:12<00:12, 350.13 examples/s]Tokenizing train dataset:  51%|█████     | 4341/8564 [00:12<00:12, 347.61 examples/s]Tokenizing train dataset:  51%|█████▏    | 4405/8564 [00:12<00:11, 361.38 examples/s]Tokenizing train dataset:  51%|█████     | 4380/8564 [00:12<00:11, 358.23 examples/s]Tokenizing train dataset:  51%|█████     | 4380/8564 [00:13<00:11, 356.52 examples/s]Tokenizing train dataset:  52%|█████▏    | 4444/8564 [00:13<00:11, 364.29 examples/s]Tokenizing train dataset:  52%|█████▏    | 4420/8564 [00:13<00:11, 366.01 examples/s]Tokenizing train dataset:  52%|█████▏    | 4420/8564 [00:13<00:11, 364.57 examples/s]Tokenizing train dataset:  52%|█████▏    | 4482/8564 [00:13<00:11, 363.50 examples/s]Tokenizing train dataset:  52%|█████▏    | 4478/8564 [00:13<00:11, 366.96 examples/s]Tokenizing train dataset:  52%|█████▏    | 4476/8564 [00:13<00:11, 365.26 examples/s]Tokenizing train dataset:  53%|█████▎    | 4536/8564 [00:13<00:11, 358.36 examples/s]Tokenizing train dataset:  53%|█████▎    | 4529/8564 [00:13<00:11, 355.86 examples/s]Tokenizing train dataset:  53%|█████▎    | 4529/8564 [00:13<00:11, 354.05 examples/s]Tokenizing train dataset:  54%|█████▎    | 4590/8564 [00:13<00:11, 354.93 examples/s]Tokenizing train dataset:  53%|█████▎    | 4569/8564 [00:13<00:11, 361.69 examples/s]Tokenizing train dataset:  53%|█████▎    | 4569/8564 [00:13<00:11, 359.76 examples/s]Tokenizing train dataset:  54%|█████▍    | 4628/8564 [00:13<00:11, 357.03 examples/s]Tokenizing train dataset:  54%|█████▍    | 4622/8564 [00:13<00:11, 355.40 examples/s]Tokenizing train dataset:  54%|█████▍    | 4621/8564 [00:13<00:11, 353.82 examples/s]Tokenizing train dataset:  55%|█████▍    | 4676/8564 [00:13<00:11, 340.41 examples/s]Tokenizing train dataset:  55%|█████▍    | 4670/8564 [00:13<00:11, 340.87 examples/s]Tokenizing train dataset:  55%|█████▍    | 4668/8564 [00:13<00:11, 338.71 examples/s]Tokenizing train dataset:  55%|█████▌    | 4724/8564 [00:13<00:11, 330.23 examples/s]Tokenizing train dataset:  55%|█████▌    | 4718/8564 [00:13<00:11, 329.71 examples/s]Tokenizing train dataset:  56%|█████▌    | 4758/8564 [00:14<00:11, 328.51 examples/s]Tokenizing train dataset:  55%|█████▌    | 4716/8564 [00:13<00:11, 328.02 examples/s]Tokenizing train dataset:  56%|█████▌    | 4792/8564 [00:14<00:11, 326.56 examples/s]Tokenizing train dataset:  56%|█████▌    | 4767/8564 [00:14<00:11, 324.78 examples/s]Tokenizing train dataset:  56%|█████▌    | 4764/8564 [00:14<00:11, 323.88 examples/s]Tokenizing train dataset:  57%|█████▋    | 4853/8564 [00:14<00:09, 397.38 examples/s]Tokenizing train dataset:  56%|█████▌    | 4804/8564 [00:14<00:11, 332.01 examples/s]Tokenizing train dataset:  56%|█████▌    | 4797/8564 [00:14<00:11, 323.23 examples/s]Tokenizing train dataset:  57%|█████▋    | 4868/8564 [00:14<00:09, 405.23 examples/s]Tokenizing train dataset:  57%|█████▋    | 4912/8564 [00:14<00:08, 442.17 examples/s]Tokenizing train dataset:  57%|█████▋    | 4861/8564 [00:14<00:09, 399.15 examples/s]Tokenizing train dataset:  58%|█████▊    | 4974/8564 [00:14<00:07, 489.55 examples/s]Tokenizing train dataset:  58%|█████▊    | 4931/8564 [00:14<00:07, 458.34 examples/s]Tokenizing train dataset:  57%|█████▋    | 4920/8564 [00:14<00:08, 443.35 examples/s]Tokenizing train dataset:  59%|█████▉    | 5033/8564 [00:14<00:06, 514.33 examples/s]Tokenizing train dataset:  58%|█████▊    | 4990/8564 [00:14<00:07, 489.71 examples/s]Tokenizing train dataset:  58%|█████▊    | 4984/8564 [00:14<00:07, 489.32 examples/s]Tokenizing train dataset:  59%|█████▉    | 5053/8564 [00:14<00:06, 526.93 examples/s]Tokenizing train dataset:  60%|█████▉    | 5106/8564 [00:14<00:06, 569.85 examples/s]Tokenizing train dataset:  59%|█████▉    | 5045/8564 [00:14<00:06, 518.05 examples/s]Tokenizing train dataset:  60%|█████▉    | 5120/8564 [00:14<00:06, 561.29 examples/s]Tokenizing train dataset:  60%|██████    | 5177/8564 [00:14<00:05, 605.04 examples/s]Tokenizing train dataset:  60%|█████▉    | 5113/8564 [00:14<00:06, 561.73 examples/s]Tokenizing train dataset:  61%|██████    | 5193/8564 [00:14<00:05, 607.06 examples/s]Tokenizing train dataset:  61%|██████▏   | 5248/8564 [00:14<00:05, 632.86 examples/s]Tokenizing train dataset:  61%|██████    | 5183/8564 [00:14<00:05, 599.18 examples/s]Tokenizing train dataset:  62%|██████▏   | 5269/8564 [00:14<00:05, 645.62 examples/s]Tokenizing train dataset:  62%|██████▏   | 5320/8564 [00:14<00:05, 647.63 examples/s]Tokenizing train dataset:  61%|██████▏   | 5258/8564 [00:14<00:05, 635.49 examples/s]Tokenizing train dataset:  63%|██████▎   | 5363/8564 [00:15<00:05, 631.56 examples/s]Tokenizing train dataset:  63%|██████▎   | 5407/8564 [00:15<00:05, 615.09 examples/s]Tokenizing train dataset:  62%|██████▏   | 5324/8564 [00:15<00:05, 638.18 examples/s]Tokenizing train dataset:  64%|██████▍   | 5472/8564 [00:15<00:04, 620.16 examples/s]Tokenizing train dataset:  64%|██████▎   | 5454/8564 [00:15<00:05, 621.01 examples/s]Tokenizing train dataset:  63%|██████▎   | 5410/8564 [00:15<00:05, 604.18 examples/s]Tokenizing train dataset:  65%|██████▍   | 5560/8564 [00:15<00:04, 605.23 examples/s]Tokenizing train dataset:  64%|██████▍   | 5475/8564 [00:15<00:05, 613.79 examples/s]Tokenizing train dataset:  65%|██████▍   | 5543/8564 [00:15<00:04, 610.39 examples/s]Tokenizing train dataset:  66%|██████▌   | 5631/8564 [00:15<00:04, 627.98 examples/s]Tokenizing train dataset:  65%|██████▌   | 5608/8564 [00:15<00:04, 617.71 examples/s]Tokenizing train dataset:  65%|██████▍   | 5564/8564 [00:15<00:04, 602.69 examples/s]Tokenizing train dataset:  67%|██████▋   | 5698/8564 [00:15<00:04, 636.93 examples/s]Tokenizing train dataset:  66%|██████▋   | 5676/8564 [00:15<00:04, 627.97 examples/s]Tokenizing train dataset:  66%|██████▌   | 5635/8564 [00:15<00:04, 626.24 examples/s]Tokenizing train dataset:  67%|██████▋   | 5771/8564 [00:15<00:04, 654.90 examples/s]Tokenizing train dataset:  67%|██████▋   | 5741/8564 [00:15<00:04, 629.24 examples/s]Tokenizing train dataset:  67%|██████▋   | 5702/8564 [00:15<00:04, 633.99 examples/s]Tokenizing train dataset:  68%|██████▊   | 5842/8564 [00:15<00:04, 661.01 examples/s]Tokenizing train dataset:  68%|██████▊   | 5821/8564 [00:15<00:04, 669.40 examples/s]Tokenizing train dataset:  67%|██████▋   | 5772/8564 [00:15<00:04, 651.57 examples/s]Tokenizing train dataset:  69%|██████▉   | 5930/8564 [00:15<00:04, 628.33 examples/s]Tokenizing train dataset:  68%|██████▊   | 5842/8564 [00:15<00:04, 655.95 examples/s]Tokenizing train dataset:  69%|██████▉   | 5901/8564 [00:15<00:04, 617.60 examples/s]Tokenizing train dataset:  70%|██████▉   | 5971/8564 [00:16<00:04, 635.58 examples/s]Tokenizing train dataset:  70%|███████   | 6016/8564 [00:16<00:04, 607.44 examples/s]Tokenizing train dataset:  69%|██████▉   | 5928/8564 [00:16<00:04, 624.24 examples/s]Tokenizing train dataset:  71%|███████   | 6078/8564 [00:16<00:04, 605.00 examples/s]Tokenizing train dataset:  71%|███████   | 6055/8564 [00:16<00:04, 604.93 examples/s]Tokenizing train dataset:  70%|███████   | 6015/8564 [00:16<00:04, 603.06 examples/s]Tokenizing train dataset:  72%|███████▏  | 6140/8564 [00:16<00:04, 605.85 examples/s]Tokenizing train dataset:  72%|███████▏  | 6150/8564 [00:16<00:03, 608.23 examples/s]Tokenizing train dataset:  73%|███████▎  | 6213/8564 [00:16<00:03, 638.11 examples/s]Tokenizing train dataset:  71%|███████   | 6099/8564 [00:16<00:04, 587.25 examples/s]Tokenizing train dataset:  73%|███████▎  | 6222/8564 [00:16<00:03, 634.96 examples/s]Tokenizing train dataset:  73%|███████▎  | 6280/8564 [00:16<00:03, 638.96 examples/s]Tokenizing train dataset:  72%|███████▏  | 6174/8564 [00:16<00:03, 624.46 examples/s]Tokenizing train dataset:  73%|███████▎  | 6290/8564 [00:16<00:03, 645.44 examples/s]Tokenizing train dataset:  73%|███████▎  | 6241/8564 [00:16<00:03, 634.98 examples/s]Tokenizing train dataset:  74%|███████▍  | 6346/8564 [00:16<00:03, 639.82 examples/s]Tokenizing train dataset:  74%|███████▍  | 6357/8564 [00:16<00:03, 650.53 examples/s]Tokenizing train dataset:  74%|███████▎  | 6307/8564 [00:16<00:03, 638.59 examples/s]Tokenizing train dataset:  75%|███████▍  | 6412/8564 [00:16<00:03, 641.22 examples/s]Tokenizing train dataset:  74%|███████▍  | 6379/8564 [00:16<00:03, 658.67 examples/s]Tokenizing train dataset:  75%|███████▌  | 6452/8564 [00:16<00:03, 642.32 examples/s]Tokenizing train dataset:  76%|███████▌  | 6501/8564 [00:16<00:03, 617.84 examples/s]Tokenizing train dataset:  76%|███████▌  | 6470/8564 [00:16<00:03, 631.86 examples/s]Tokenizing train dataset:  77%|███████▋  | 6565/8564 [00:16<00:03, 617.51 examples/s]Tokenizing train dataset:  76%|███████▋  | 6541/8564 [00:16<00:03, 623.29 examples/s]Tokenizing train dataset:  76%|███████▋  | 6535/8564 [00:17<00:03, 627.10 examples/s]Tokenizing train dataset:  78%|███████▊  | 6645/8564 [00:17<00:03, 578.76 examples/s]Tokenizing train dataset:  77%|███████▋  | 6618/8564 [00:17<00:03, 582.36 examples/s]Tokenizing train dataset:  78%|███████▊  | 6716/8564 [00:17<00:03, 609.95 examples/s]Tokenizing train dataset:  77%|███████▋  | 6613/8564 [00:17<00:03, 577.83 examples/s]Tokenizing train dataset:  78%|███████▊  | 6683/8564 [00:17<00:03, 597.04 examples/s]Tokenizing train dataset:  79%|███████▉  | 6780/8564 [00:17<00:02, 614.38 examples/s]Tokenizing train dataset:  78%|███████▊  | 6679/8564 [00:17<00:03, 597.22 examples/s]Tokenizing train dataset:  79%|███████▉  | 6755/8564 [00:17<00:02, 621.99 examples/s]Tokenizing train dataset:  79%|███████▉  | 6749/8564 [00:17<00:02, 620.52 examples/s]Tokenizing train dataset:  80%|████████  | 6869/8564 [00:17<00:02, 603.07 examples/s]Tokenizing train dataset:  80%|███████▉  | 6838/8564 [00:17<00:02, 596.87 examples/s]Tokenizing train dataset:  81%|████████  | 6940/8564 [00:17<00:02, 627.87 examples/s]Tokenizing train dataset:  80%|███████▉  | 6835/8564 [00:17<00:02, 601.57 examples/s]Tokenizing train dataset:  81%|████████  | 6907/8564 [00:17<00:02, 616.54 examples/s]Tokenizing train dataset:  81%|████████  | 6900/8564 [00:17<00:02, 610.14 examples/s]Tokenizing train dataset:  81%|████████▏ | 6970/8564 [00:17<00:02, 614.25 examples/s]Tokenizing train dataset:  82%|████████▏ | 7020/8564 [00:17<00:02, 582.00 examples/s]Tokenizing train dataset:  81%|████████▏ | 6965/8564 [00:17<00:02, 614.39 examples/s]Tokenizing train dataset:  83%|████████▎ | 7091/8564 [00:17<00:02, 609.74 examples/s]Tokenizing train dataset:  82%|████████▏ | 7060/8564 [00:17<00:02, 600.86 examples/s]Tokenizing train dataset:  84%|████████▎ | 7154/8564 [00:17<00:02, 611.40 examples/s]Tokenizing train dataset:  82%|████████▏ | 7058/8564 [00:17<00:02, 606.03 examples/s]Tokenizing train dataset:  83%|████████▎ | 7128/8564 [00:17<00:02, 619.11 examples/s]Tokenizing train dataset:  84%|████████▍ | 7221/8564 [00:18<00:02, 624.56 examples/s]Tokenizing train dataset:  83%|████████▎ | 7124/8564 [00:17<00:02, 615.71 examples/s]Tokenizing train dataset:  84%|████████▍ | 7199/8564 [00:18<00:02, 639.24 examples/s]Tokenizing train dataset:  84%|████████▍ | 7192/8564 [00:18<00:02, 631.01 examples/s]Tokenizing train dataset:  85%|████████▌ | 7312/8564 [00:18<00:02, 615.49 examples/s]Tokenizing train dataset:  85%|████████▌ | 7285/8564 [00:18<00:02, 611.24 examples/s]Tokenizing train dataset:  86%|████████▌ | 7376/8564 [00:18<00:01, 616.49 examples/s]Tokenizing train dataset:  85%|████████▌ | 7282/8564 [00:18<00:02, 614.51 examples/s]Tokenizing train dataset:  86%|████████▌ | 7382/8564 [00:18<00:01, 615.25 examples/s]Tokenizing train dataset:  87%|████████▋ | 7442/8564 [00:18<00:01, 626.67 examples/s]Tokenizing train dataset:  86%|████████▌ | 7378/8564 [00:18<00:01, 618.51 examples/s]Tokenizing train dataset:  87%|████████▋ | 7449/8564 [00:18<00:01, 625.07 examples/s]Tokenizing train dataset:  88%|████████▊ | 7540/8564 [00:18<00:01, 629.45 examples/s]Tokenizing train dataset:  87%|████████▋ | 7443/8564 [00:18<00:01, 622.56 examples/s]Tokenizing train dataset:  88%|████████▊ | 7513/8564 [00:18<00:01, 620.34 examples/s]Tokenizing train dataset:  89%|████████▉ | 7609/8564 [00:18<00:01, 643.60 examples/s]Tokenizing train dataset:  88%|████████▊ | 7506/8564 [00:18<00:01, 620.00 examples/s]Tokenizing train dataset:  89%|████████▊ | 7587/8564 [00:18<00:01, 649.92 examples/s]Tokenizing train dataset:  89%|████████▊ | 7581/8564 [00:18<00:01, 649.92 examples/s]Tokenizing train dataset:  90%|████████▉ | 7695/8564 [00:18<00:01, 611.15 examples/s]Tokenizing train dataset:  90%|████████▉ | 7672/8564 [00:18<00:01, 617.16 examples/s]Tokenizing train dataset:  90%|████████▉ | 7665/8564 [00:18<00:01, 613.95 examples/s]Tokenizing train dataset:  91%|█████████ | 7781/8564 [00:18<00:01, 593.24 examples/s]Tokenizing train dataset:  91%|█████████ | 7761/8564 [00:18<00:01, 603.55 examples/s]Tokenizing train dataset:  90%|█████████ | 7728/8564 [00:18<00:01, 616.07 examples/s]Tokenizing train dataset:  92%|█████████▏| 7871/8564 [00:19<00:01, 590.54 examples/s]Tokenizing train dataset:  92%|█████████▏| 7854/8564 [00:19<00:01, 594.99 examples/s]Tokenizing train dataset:  91%|█████████ | 7809/8564 [00:19<00:01, 583.57 examples/s]Tokenizing train dataset:  93%|█████████▎| 7936/8564 [00:19<00:01, 600.13 examples/s]Tokenizing train dataset:  92%|█████████▏| 7916/8564 [00:19<00:01, 596.79 examples/s]Tokenizing train dataset:  92%|█████████▏| 7871/8564 [00:19<00:01, 591.02 examples/s]Tokenizing train dataset:  93%|█████████▎| 8003/8564 [00:19<00:00, 615.03 examples/s]Tokenizing train dataset:  93%|█████████▎| 7978/8564 [00:19<00:00, 601.47 examples/s]Tokenizing train dataset:  93%|█████████▎| 7936/8564 [00:19<00:01, 602.19 examples/s]Tokenizing train dataset:  94%|█████████▍| 8046/8564 [00:19<00:00, 612.77 examples/s]Tokenizing train dataset:  93%|█████████▎| 8002/8564 [00:19<00:00, 616.58 examples/s]Tokenizing train dataset:  94%|█████████▍| 8091/8564 [00:19<00:00, 596.84 examples/s]Tokenizing train dataset:  95%|█████████▍| 8128/8564 [00:19<00:00, 585.17 examples/s]Tokenizing train dataset:  94%|█████████▍| 8091/8564 [00:19<00:00, 597.57 examples/s]Tokenizing train dataset:  96%|█████████▌| 8184/8564 [00:19<00:00, 597.21 examples/s]Tokenizing train dataset:  96%|█████████▌| 8198/8564 [00:19<00:00, 609.31 examples/s]Tokenizing train dataset:  96%|█████████▋| 8251/8564 [00:19<00:00, 606.47 examples/s]Tokenizing train dataset:  96%|█████████▌| 8184/8564 [00:19<00:00, 597.79 examples/s]Tokenizing train dataset:  96%|█████████▋| 8263/8564 [00:19<00:00, 615.78 examples/s]Tokenizing train dataset:  97%|█████████▋| 8327/8564 [00:19<00:00, 644.01 examples/s]Tokenizing train dataset:  96%|█████████▋| 8251/8564 [00:19<00:00, 606.89 examples/s]Tokenizing train dataset:  97%|█████████▋| 8332/8564 [00:19<00:00, 634.30 examples/s]Tokenizing train dataset:  98%|█████████▊| 8410/8564 [00:19<00:00, 605.85 examples/s]Tokenizing train dataset:  97%|█████████▋| 8328/8564 [00:19<00:00, 642.82 examples/s]Tokenizing train dataset:  98%|█████████▊| 8420/8564 [00:20<00:00, 613.37 examples/s]Tokenizing train dataset:  99%|█████████▉| 8474/8564 [00:20<00:00, 613.40 examples/s]Tokenizing train dataset:  98%|█████████▊| 8410/8564 [00:20<00:00, 606.66 examples/s]Tokenizing train dataset:  99%|█████████▉| 8485/8564 [00:20<00:00, 614.53 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 609.37 examples/s]Tokenizing train dataset:  99%|█████████▉| 8474/8564 [00:20<00:00, 613.21 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 423.16 examples/s]
Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 609.24 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 422.50 examples/s]
Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 609.11 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:20<00:00, 420.96 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11216.13 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11077.99 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 10932.73 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13594.07 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13173.16 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13124.93 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 326.25 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 327.46 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 328.66 examples/s]Tokenizing eval dataset:   8%|▊         | 73/953 [00:00<00:03, 271.05 examples/s]Tokenizing eval dataset:  11%|█         | 103/953 [00:00<00:04, 207.40 examples/s]Tokenizing eval dataset:   8%|▊         | 80/953 [00:00<00:05, 171.13 examples/s]Tokenizing eval dataset:   8%|▊         | 80/953 [00:00<00:05, 157.53 examples/s]Tokenizing eval dataset:  11%|█         | 103/953 [00:00<00:04, 174.21 examples/s]Tokenizing eval dataset:  12%|█▏        | 116/953 [00:00<00:04, 191.69 examples/s]Tokenizing eval dataset:  15%|█▍        | 140/953 [00:00<00:03, 217.42 examples/s]Tokenizing eval dataset:  14%|█▎        | 129/953 [00:00<00:04, 194.69 examples/s]Tokenizing eval dataset:  18%|█▊        | 169/953 [00:00<00:03, 205.96 examples/s]Tokenizing eval dataset:  16%|█▌        | 149/953 [00:00<00:04, 197.83 examples/s]Tokenizing eval dataset:  16%|█▌        | 153/953 [00:00<00:03, 201.90 examples/s]Tokenizing eval dataset:  20%|██        | 195/953 [00:00<00:04, 180.79 examples/s]Tokenizing eval dataset:  18%|█▊        | 175/953 [00:00<00:04, 168.31 examples/s]Tokenizing eval dataset:  19%|█▉        | 179/953 [00:00<00:04, 185.79 examples/s]Tokenizing eval dataset:  25%|██▍       | 238/953 [00:01<00:03, 211.41 examples/s]Tokenizing eval dataset:  21%|██        | 202/953 [00:01<00:04, 153.61 examples/s]Tokenizing eval dataset:  31%|███       | 295/953 [00:01<00:02, 289.37 examples/s]Tokenizing eval dataset:  22%|██▏       | 209/953 [00:01<00:04, 159.51 examples/s]Tokenizing eval dataset:  25%|██▍       | 238/953 [00:01<00:03, 192.91 examples/s]Tokenizing eval dataset:  37%|███▋      | 354/953 [00:01<00:01, 358.37 examples/s]Tokenizing eval dataset:  27%|██▋       | 259/953 [00:01<00:03, 230.02 examples/s]Tokenizing eval dataset:  32%|███▏      | 301/953 [00:01<00:02, 287.71 examples/s]Tokenizing eval dataset:  43%|████▎     | 408/953 [00:01<00:01, 402.72 examples/s]Tokenizing eval dataset:  33%|███▎      | 319/953 [00:01<00:02, 313.53 examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 345.37 examples/s]Tokenizing eval dataset:  50%|████▉     | 474/953 [00:01<00:01, 468.73 examples/s]Tokenizing eval dataset:  40%|███▉      | 377/953 [00:01<00:01, 377.04 examples/s]Tokenizing eval dataset:  44%|████▍     | 421/953 [00:01<00:01, 365.94 examples/s]Tokenizing eval dataset:  57%|█████▋    | 540/953 [00:01<00:01, 411.15 examples/s]Tokenizing eval dataset:  47%|████▋     | 452/953 [00:01<00:01, 382.59 examples/s]Tokenizing eval dataset:  50%|█████     | 480/953 [00:01<00:01, 417.38 examples/s]Tokenizing eval dataset:  62%|██████▏   | 591/953 [00:01<00:00, 433.86 examples/s]Tokenizing eval dataset:  52%|█████▏    | 495/953 [00:01<00:01, 392.74 examples/s]Tokenizing eval dataset:  57%|█████▋    | 545/953 [00:01<00:00, 475.43 examples/s]Tokenizing eval dataset:  64%|██████▍   | 608/953 [00:01<00:00, 511.30 examples/s]Tokenizing eval dataset:  69%|██████▉   | 658/953 [00:01<00:00, 436.62 examples/s]Tokenizing eval dataset:  59%|█████▉    | 564/953 [00:01<00:00, 412.25 examples/s]Tokenizing eval dataset:  64%|██████▍   | 609/953 [00:02<00:00, 419.93 examples/s]Tokenizing eval dataset:  72%|███████▏  | 684/953 [00:02<00:00, 505.79 examples/s]Tokenizing eval dataset:  76%|███████▌  | 725/953 [00:02<00:00, 434.33 examples/s]Tokenizing eval dataset:  70%|██████▉   | 663/953 [00:02<00:00, 449.51 examples/s]Tokenizing eval dataset:  78%|███████▊  | 739/953 [00:02<00:00, 416.57 examples/s]Tokenizing eval dataset:  82%|████████▏ | 781/953 [00:02<00:00, 360.41 examples/s]Tokenizing eval dataset:  76%|███████▌  | 725/953 [00:02<00:00, 407.68 examples/s]Tokenizing eval dataset:  83%|████████▎ | 787/953 [00:02<00:00, 428.85 examples/s]Tokenizing eval dataset:  87%|████████▋ | 831/953 [00:02<00:00, 327.37 examples/s]Tokenizing eval dataset:  82%|████████▏ | 782/953 [00:02<00:00, 362.76 examples/s]Tokenizing eval dataset:  88%|████████▊ | 840/953 [00:02<00:00, 392.43 examples/s]Tokenizing eval dataset:  93%|█████████▎| 884/953 [00:02<00:00, 401.81 examples/s]Tokenizing eval dataset:  93%|█████████▎| 887/953 [00:02<00:00, 335.67 examples/s]Tokenizing eval dataset:  88%|████████▊ | 840/953 [00:02<00:00, 366.12 examples/s]Tokenizing eval dataset:  98%|█████████▊| 935/953 [00:02<00:00, 426.11 examples/s]Tokenizing eval dataset:  97%|█████████▋| 929/953 [00:02<00:00, 351.71 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 336.16 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  93%|█████████▎| 891/953 [00:02<00:00, 347.05 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 322.11 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  99%|█████████▉| 942/953 [00:03<00:00, 324.25 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:03<00:00, 306.47 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Parameter Offload: Total persistent parameters: 605696 in 169 params
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
wandb: Currently logged in as: vajdadario (slolama) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
wandb: Tracking run with wandb version 0.19.7
wandb: Run data is saved locally in /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/wandb/run-20250531_005746-3pei7k4z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DPO_r-64_lr-3e-07_e-3_b-0.2
wandb: ⭐️ View project at https://wandb.ai/slolama/GaMS-9B-Translation-DPO
wandb: 🚀 View run at https://wandb.ai/slolama/GaMS-9B-Translation-DPO/runs/3pei7k4z
  0%|          | 0/1605 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  0%|          | 1/1605 [00:17<7:45:07, 17.40s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 0.0, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4416.0, 'logps/rejected': -3968.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.0}
  0%|          | 1/1605 [00:17<7:45:07, 17.40s/it]  0%|          | 2/1605 [00:24<5:03:41, 11.37s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.006071929005137e-11, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4624.0, 'logps/rejected': -4800.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.0}
  0%|          | 2/1605 [00:24<5:03:41, 11.37s/it]  0%|          | 3/1605 [00:32<4:15:56,  9.59s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4012143858010274e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5760.0, 'logps/rejected': -5488.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 3/1605 [00:32<4:15:56,  9.59s/it]  0%|          | 4/1605 [00:39<3:56:09,  8.85s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1018215787015413e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5120.0, 'logps/rejected': -5120.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 4/1605 [00:39<3:56:09,  8.85s/it]  0%|          | 5/1605 [00:47<3:41:44,  8.32s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.802428771602055e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4112.0, 'logps/rejected': -3936.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 5/1605 [00:47<3:41:44,  8.32s/it]  0%|          | 6/1605 [00:55<3:38:19,  8.19s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.5030359645025685e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -8368.0, 'logps/rejected': -6080.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 6/1605 [00:55<3:38:19,  8.19s/it]  0%|          | 7/1605 [01:02<3:32:46,  7.99s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.2036431574030827e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6624.0, 'logps/rejected': -4920.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 7/1605 [01:02<3:32:46,  7.99s/it]  0%|          | 8/1605 [01:10<3:30:03,  7.89s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.904250350303596e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6224.0, 'logps/rejected': -4712.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 8/1605 [01:10<3:30:03,  7.89s/it]  1%|          | 9/1605 [01:18<3:33:43,  8.03s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.60485754320411e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6544.0, 'logps/rejected': -5280.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.02}
  1%|          | 9/1605 [01:18<3:33:43,  8.03s/it]  1%|          | 10/1605 [01:26<3:33:33,  8.03s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.305464736104624e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6768.0, 'logps/rejected': -6112.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.02}
  1%|          | 10/1605 [01:26<3:33:33,  8.03s/it]  1%|          | 11/1605 [01:34<3:30:46,  7.93s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.006071929005137e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6096.0, 'logps/rejected': -5408.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.02}
  1%|          | 11/1605 [01:34<3:30:46,  7.93s/it]  1%|          | 12/1605 [01:41<3:27:31,  7.82s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.706679121905651e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5808.0, 'logps/rejected': -5872.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.02}
  1%|          | 12/1605 [01:42<3:27:31,  7.82s/it]  1%|          | 13/1605 [01:50<3:29:29,  7.90s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.407286314806165e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6256.0, 'logps/rejected': -6032.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.02}
  1%|          | 13/1605 [01:50<3:29:29,  7.90s/it]  1%|          | 14/1605 [01:57<3:29:56,  7.92s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.107893507706679e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5904.0, 'logps/rejected': -5904.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.03}
  1%|          | 14/1605 [01:58<3:29:56,  7.92s/it]  1%|          | 15/1605 [02:05<3:27:13,  7.82s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.808500700607193e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5648.0, 'logps/rejected': -5840.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.03}
  1%|          | 15/1605 [02:05<3:27:13,  7.82s/it]  1%|          | 16/1605 [02:12<3:23:13,  7.67s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0509107893507706e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4464.0, 'logps/rejected': -3208.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.03}
  1%|          | 16/1605 [02:13<3:23:13,  7.67s/it]  1%|          | 17/1605 [02:20<3:20:33,  7.58s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.120971508640822e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5024.0, 'logps/rejected': -5024.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.03}
  1%|          | 17/1605 [02:20<3:20:33,  7.58s/it]  1%|          | 18/1605 [02:28<3:25:26,  7.77s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1910322279308732e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7072.0, 'logps/rejected': -5856.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.03}
  1%|          | 18/1605 [02:28<3:25:26,  7.77s/it]  1%|          | 19/1605 [02:36<3:25:09,  7.76s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2610929472209248e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5232.0, 'logps/rejected': -5024.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|          | 19/1605 [02:36<3:25:09,  7.76s/it]  1%|          | 20/1605 [02:43<3:20:48,  7.60s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3311536665109763e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4800.0, 'logps/rejected': -4960.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|          | 20/1605 [02:43<3:20:48,  7.60s/it]  1%|▏         | 21/1605 [02:52<3:28:16,  7.89s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4012143858010274e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6832.0, 'logps/rejected': -5952.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|▏         | 21/1605 [02:52<3:28:16,  7.89s/it]  1%|▏         | 22/1605 [03:00<3:31:19,  8.01s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.471275105091079e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6464.0, 'logps/rejected': -6368.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|▏         | 22/1605 [03:00<3:31:19,  8.01s/it]  1%|▏         | 23/1605 [03:08<3:31:46,  8.03s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.5413358243811302e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6608.0, 'logps/rejected': -6160.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|▏         | 23/1605 [03:08<3:31:46,  8.03s/it]  1%|▏         | 24/1605 [03:16<3:29:53,  7.97s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.6113965436711816e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5280.0, 'logps/rejected': -3824.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|▏         | 24/1605 [03:16<3:29:53,  7.97s/it]  2%|▏         | 25/1605 [03:23<3:25:50,  7.82s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.681457262961233e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4472.0, 'logps/rejected': -3800.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.05}
  2%|▏         | 25/1605 [03:23<3:25:50,  7.82s/it]  2%|▏         | 26/1605 [03:31<3:24:53,  7.79s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.7515179822512844e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5024.0, 'logps/rejected': -4480.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.05}
  2%|▏         | 26/1605 [03:31<3:24:53,  7.79s/it]  2%|▏         | 27/1605 [03:38<3:22:22,  7.70s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8215787015413357e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5504.0, 'logps/rejected': -5168.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.05}
  2%|▏         | 27/1605 [03:38<3:22:22,  7.70s/it]  2%|▏         | 28/1605 [03:46<3:22:54,  7.72s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.891639420831387e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5536.0, 'logps/rejected': -4320.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.05}
  2%|▏         | 28/1605 [03:46<3:22:54,  7.72s/it]  2%|▏         | 29/1605 [03:54<3:24:26,  7.78s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.9617001401214386e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5920.0, 'logps/rejected': -4968.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.05}
  2%|▏         | 29/1605 [03:54<3:24:26,  7.78s/it]  2%|▏         | 30/1605 [04:02<3:24:35,  7.79s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.03176085941149e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5856.0, 'logps/rejected': -5232.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.06}
  2%|▏         | 30/1605 [04:02<3:24:35,  7.79s/it]  2%|▏         | 31/1605 [04:10<3:24:23,  7.79s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.101821578701541e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5280.0, 'logps/rejected': -4768.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.06}
  2%|▏         | 31/1605 [04:10<3:24:23,  7.79s/it]  2%|▏         | 32/1605 [04:17<3:22:55,  7.74s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1718822979915927e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5168.0, 'logps/rejected': -4608.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.06}
  2%|▏         | 32/1605 [04:18<3:22:55,  7.74s/it]  2%|▏         | 33/1605 [04:25<3:22:14,  7.72s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.241943017281644e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7056.0, 'logps/rejected': -5840.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.06}
  2%|▏         | 33/1605 [04:25<3:22:14,  7.72s/it]  2%|▏         | 34/1605 [04:33<3:25:52,  7.86s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3120037365716953e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7856.0, 'logps/rejected': -6608.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.06}
  2%|▏         | 34/1605 [04:33<3:25:52,  7.86s/it]  2%|▏         | 35/1605 [04:41<3:24:21,  7.81s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3820644558617465e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6448.0, 'logps/rejected': -4736.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 35/1605 [04:41<3:24:21,  7.81s/it]  2%|▏         | 36/1605 [04:49<3:24:18,  7.81s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.4521251751517984e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4848.0, 'logps/rejected': -5056.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 36/1605 [04:49<3:24:18,  7.81s/it]  2%|▏         | 37/1605 [04:56<3:19:58,  7.65s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5221858944418495e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4112.0, 'logps/rejected': -3576.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 37/1605 [04:56<3:19:58,  7.65s/it]  2%|▏         | 38/1605 [05:04<3:21:46,  7.73s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5922466137319006e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6832.0, 'logps/rejected': -4768.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 38/1605 [05:04<3:21:46,  7.73s/it]  2%|▏         | 39/1605 [05:12<3:23:02,  7.78s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6623073330219526e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6320.0, 'logps/rejected': -6080.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 39/1605 [05:12<3:23:02,  7.78s/it]  2%|▏         | 40/1605 [05:19<3:21:27,  7.72s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.7323680523120037e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5120.0, 'logps/rejected': -4528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 40/1605 [05:20<3:21:27,  7.72s/it]  3%|▎         | 41/1605 [05:27<3:21:49,  7.74s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.8024287716020548e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5376.0, 'logps/rejected': -4568.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.08}
  3%|▎         | 41/1605 [05:27<3:21:49,  7.74s/it]  3%|▎         | 42/1605 [05:34<3:18:01,  7.60s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.8724894908921063e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5200.0, 'logps/rejected': -4544.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.08}
  3%|▎         | 42/1605 [05:35<3:18:01,  7.60s/it]  3%|▎         | 43/1605 [05:42<3:18:32,  7.63s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.942550210182158e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5840.0, 'logps/rejected': -4992.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.08}
  3%|▎         | 43/1605 [05:42<3:18:32,  7.63s/it]  3%|▎         | 44/1605 [05:50<3:19:33,  7.67s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.012610929472209e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5392.0, 'logps/rejected': -5008.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.08}
  3%|▎         | 44/1605 [05:50<3:19:33,  7.67s/it]  3%|▎         | 45/1605 [05:58<3:20:47,  7.72s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.0826716487622605e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5376.0, 'logps/rejected': -5128.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.08}
  3%|▎         | 45/1605 [05:58<3:20:47,  7.72s/it]  3%|▎         | 46/1605 [06:05<3:16:37,  7.57s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.152732368052312e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4384.0, 'logps/rejected': -3868.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.09}
  3%|▎         | 46/1605 [06:05<3:16:37,  7.57s/it]  3%|▎         | 47/1605 [06:13<3:19:58,  7.70s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.222793087342363e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -8112.0, 'logps/rejected': -6528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.09}
  3%|▎         | 47/1605 [06:13<3:19:58,  7.70s/it]  3%|▎         | 48/1605 [06:21<3:21:42,  7.77s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.2928538066324146e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5712.0, 'logps/rejected': -5488.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.09}
  3%|▎         | 48/1605 [06:21<3:21:42,  7.77s/it]  3%|▎         | 49/1605 [06:29<3:21:00,  7.75s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.362914525922466e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6256.0, 'logps/rejected': -5424.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.09}
  3%|▎         | 49/1605 [06:29<3:21:00,  7.75s/it]  3%|▎         | 50/1605 [06:36<3:17:41,  7.63s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.4329752452125173e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5088.0, 'logps/rejected': -4992.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.09}
  3%|▎         | 50/1605 [06:36<3:17:41,  7.63s/it]  3%|▎         | 51/1605 [06:44<3:17:46,  7.64s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.5030359645025688e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6368.0, 'logps/rejected': -4672.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 51/1605 [06:44<3:17:46,  7.64s/it]  3%|▎         | 52/1605 [06:51<3:17:58,  7.65s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.5730966837926203e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5632.0, 'logps/rejected': -5136.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 52/1605 [06:51<3:17:58,  7.65s/it]  3%|▎         | 53/1605 [06:59<3:16:13,  7.59s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.6431574030826714e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4728.0, 'logps/rejected': -4544.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 53/1605 [06:59<3:16:13,  7.59s/it]  3%|▎         | 54/1605 [07:06<3:17:19,  7.63s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.713218122372723e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6448.0, 'logps/rejected': -5872.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 54/1605 [07:06<3:17:19,  7.63s/it]  3%|▎         | 55/1605 [07:14<3:19:38,  7.73s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.783278841662774e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4736.0, 'logps/rejected': -4136.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 55/1605 [07:15<3:19:38,  7.73s/it]  3%|▎         | 56/1605 [07:22<3:18:30,  7.69s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.8533395609528256e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5264.0, 'logps/rejected': -5504.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 56/1605 [07:22<3:18:30,  7.69s/it]  4%|▎         | 57/1605 [07:30<3:21:06,  7.80s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.923400280242877e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5696.0, 'logps/rejected': -5104.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.11}
  4%|▎         | 57/1605 [07:30<3:21:06,  7.80s/it]  4%|▎         | 58/1605 [07:38<3:26:07,  7.99s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.993460999532928e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6832.0, 'logps/rejected': -5312.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.11}
  4%|▎         | 58/1605 [07:39<3:26:07,  7.99s/it]  4%|▎         | 59/1605 [07:47<3:31:41,  8.22s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.06352171882298e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7104.0, 'logps/rejected': -6576.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.11}
  4%|▎         | 59/1605 [07:47<3:31:41,  8.22s/it]  4%|▎         | 60/1605 [07:56<3:32:16,  8.24s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.133582438113031e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6336.0, 'logps/rejected': -4560.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.11}
  4%|▎         | 60/1605 [07:56<3:32:16,  8.24s/it]  4%|▍         | 61/1605 [08:03<3:24:57,  7.96s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.203643157403082e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5552.0, 'logps/rejected': -4992.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.11}
  4%|▍         | 61/1605 [08:03<3:24:57,  7.96s/it]  4%|▍         | 62/1605 [08:10<3:22:12,  7.86s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.273703876693134e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5968.0, 'logps/rejected': -5856.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.12}
  4%|▍         | 62/1605 [08:11<3:22:12,  7.86s/it]  4%|▍         | 63/1605 [08:19<3:25:04,  7.98s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.3437645959831854e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6352.0, 'logps/rejected': -5872.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.12}
  4%|▍         | 63/1605 [08:19<3:25:04,  7.98s/it]  4%|▍         | 64/1605 [08:27<3:24:32,  7.96s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.413825315273236e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6192.0, 'logps/rejected': -5936.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.12}
  4%|▍         | 64/1605 [08:27<3:24:32,  7.96s/it]  4%|▍         | 65/1605 [08:34<3:23:37,  7.93s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.483886034563288e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4432.0, 'logps/rejected': -4320.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.12}
  4%|▍         | 65/1605 [08:35<3:23:37,  7.93s/it]  4%|▍         | 66/1605 [08:43<3:26:35,  8.05s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.553946753853339e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7888.0, 'logps/rejected': -6880.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.12}
  4%|▍         | 66/1605 [08:43<3:26:35,  8.05s/it]  4%|▍         | 67/1605 [08:51<3:26:46,  8.07s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.624007473143391e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7456.0, 'logps/rejected': -5896.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 67/1605 [08:51<3:26:46,  8.07s/it]  4%|▍         | 68/1605 [08:58<3:22:23,  7.90s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.694068192433442e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4432.0, 'logps/rejected': -3912.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 68/1605 [08:59<3:22:23,  7.90s/it]  4%|▍         | 69/1605 [09:06<3:21:57,  7.89s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.764128911723493e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4256.0, 'logps/rejected': -4416.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 69/1605 [09:06<3:21:57,  7.89s/it]  4%|▍         | 70/1605 [09:14<3:17:15,  7.71s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.834189631013545e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5360.0, 'logps/rejected': -4448.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 70/1605 [09:14<3:17:15,  7.71s/it]  4%|▍         | 71/1605 [09:21<3:17:42,  7.73s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.904250350303597e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5536.0, 'logps/rejected': -5376.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 71/1605 [09:21<3:17:42,  7.73s/it]  4%|▍         | 72/1605 [09:29<3:18:04,  7.75s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.9743110695936475e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5904.0, 'logps/rejected': -5248.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 72/1605 [09:29<3:18:04,  7.75s/it]  5%|▍         | 73/1605 [09:37<3:17:05,  7.72s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.044371788883699e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5440.0, 'logps/rejected': -5536.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.14}
  5%|▍         | 73/1605 [09:37<3:17:05,  7.72s/it]  5%|▍         | 74/1605 [09:44<3:14:35,  7.63s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.1144325081737505e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4784.0, 'logps/rejected': -4576.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.14}
  5%|▍         | 74/1605 [09:44<3:14:35,  7.63s/it]  5%|▍         | 75/1605 [09:52<3:16:03,  7.69s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.184493227463801e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6240.0, 'logps/rejected': -5536.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.14}
  5%|▍         | 75/1605 [09:52<3:16:03,  7.69s/it]  5%|▍         | 76/1605 [10:00<3:14:59,  7.65s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.254553946753854e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5576.0, 'logps/rejected': -5368.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.14}
  5%|▍         | 76/1605 [10:00<3:14:59,  7.65s/it]  5%|▍         | 77/1605 [10:07<3:12:33,  7.56s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.324614666043905e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4184.0, 'logps/rejected': -4232.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.14}
  5%|▍         | 77/1605 [10:07<3:12:33,  7.56s/it]  5%|▍         | 78/1605 [10:15<3:12:09,  7.55s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.394675385333956e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5664.0, 'logps/rejected': -5520.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▍         | 78/1605 [10:15<3:12:09,  7.55s/it]  5%|▍         | 79/1605 [10:22<3:11:02,  7.51s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.464736104624007e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4960.0, 'logps/rejected': -4528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▍         | 79/1605 [10:22<3:11:02,  7.51s/it]  5%|▍         | 80/1605 [10:30<3:15:53,  7.71s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.534796823914059e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4880.0, 'logps/rejected': -4344.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▍         | 80/1605 [10:30<3:15:53,  7.71s/it]  5%|▌         | 81/1605 [10:38<3:15:00,  7.68s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.6048575432041096e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6896.0, 'logps/rejected': -5824.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▌         | 81/1605 [10:38<3:15:00,  7.68s/it]  5%|▌         | 82/1605 [10:46<3:19:14,  7.85s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.674918262494162e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7312.0, 'logps/rejected': -5584.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▌         | 82/1605 [10:46<3:19:14,  7.85s/it]  5%|▌         | 83/1605 [10:54<3:18:04,  7.81s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.744978981784213e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5184.0, 'logps/rejected': -5056.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▌         | 83/1605 [10:54<3:18:04,  7.81s/it]  5%|▌         | 84/1605 [11:01<3:15:43,  7.72s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.815039701074264e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4144.0, 'logps/rejected': -3936.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.16}
  5%|▌         | 84/1605 [11:01<3:15:43,  7.72s/it]  5%|▌         | 85/1605 [11:09<3:19:13,  7.86s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.885100420364316e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5264.0, 'logps/rejected': -5312.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.16}
  5%|▌         | 85/1605 [11:10<3:19:13,  7.86s/it]  5%|▌         | 86/1605 [11:17<3:19:25,  7.88s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.955161139654366e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5888.0, 'logps/rejected': -3680.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.16}
  5%|▌         | 86/1605 [11:17<3:19:25,  7.88s/it]  5%|▌         | 87/1605 [11:25<3:14:32,  7.69s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.025221858944418e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4752.0, 'logps/rejected': -4608.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.16}
  5%|▌         | 87/1605 [11:25<3:14:32,  7.69s/it]  5%|▌         | 88/1605 [11:32<3:16:00,  7.75s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.09528257823447e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5072.0, 'logps/rejected': -3728.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.16}
  5%|▌         | 88/1605 [11:33<3:16:00,  7.75s/it]  6%|▌         | 89/1605 [11:40<3:16:00,  7.76s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.165343297524521e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6128.0, 'logps/rejected': -6320.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.17}
  6%|▌         | 89/1605 [11:40<3:16:00,  7.76s/it]  6%|▌         | 90/1605 [11:48<3:15:20,  7.74s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.2354040168145725e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5552.0, 'logps/rejected': -5224.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.17}
  6%|▌         | 90/1605 [11:48<3:15:20,  7.74s/it]  6%|▌         | 91/1605 [11:55<3:12:30,  7.63s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.305464736104624e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5520.0, 'logps/rejected': -5360.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.17}
  6%|▌         | 91/1605 [11:55<3:12:30,  7.63s/it]  6%|▌         | 92/1605 [12:03<3:14:04,  7.70s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.375525455394675e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5984.0, 'logps/rejected': -5408.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.17}
  6%|▌         | 92/1605 [12:03<3:14:04,  7.70s/it]  6%|▌         | 93/1605 [12:10<3:10:24,  7.56s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.445586174684726e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4448.0, 'logps/rejected': -4288.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.17}
  6%|▌         | 93/1605 [12:11<3:10:24,  7.56s/it]  6%|▌         | 94/1605 [12:18<3:10:49,  7.58s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.5156468939747786e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5128.0, 'logps/rejected': -4168.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 94/1605 [12:18<3:10:49,  7.58s/it]  6%|▌         | 95/1605 [12:27<3:18:51,  7.90s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.585707613264829e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6176.0, 'logps/rejected': -5248.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 95/1605 [12:27<3:18:51,  7.90s/it]  6%|▌         | 96/1605 [12:34<3:17:29,  7.85s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.655768332554881e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5280.0, 'logps/rejected': -5056.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 96/1605 [12:34<3:17:29,  7.85s/it]  6%|▌         | 97/1605 [12:42<3:17:38,  7.86s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.725829051844932e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6352.0, 'logps/rejected': -5744.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 97/1605 [12:42<3:17:38,  7.86s/it]  6%|▌         | 98/1605 [12:50<3:17:00,  7.84s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.795889771134983e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6048.0, 'logps/rejected': -4304.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 98/1605 [12:50<3:17:00,  7.84s/it]  6%|▌         | 99/1605 [12:57<3:13:09,  7.70s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.8659504904250345e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5736.0, 'logps/rejected': -5208.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 99/1605 [12:58<3:13:09,  7.70s/it]  6%|▌         | 100/1605 [13:05<3:08:38,  7.52s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.936011209715087e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3976.0, 'logps/rejected': -3608.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.19}
  6%|▌         | 100/1605 [13:05<3:08:38,  7.52s/it]  6%|▋         | 101/1605 [13:12<3:11:04,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.0060719290051376e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5736.0, 'logps/rejected': -4768.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.19}
  6%|▋         | 101/1605 [13:13<3:11:04,  7.62s/it]  6%|▋         | 102/1605 [13:20<3:10:03,  7.59s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.076132648295189e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4416.0, 'logps/rejected': -4256.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.19}
  6%|▋         | 102/1605 [13:20<3:10:03,  7.59s/it]  6%|▋         | 103/1605 [13:28<3:12:24,  7.69s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.146193367585241e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6000.0, 'logps/rejected': -4784.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.19}
  6%|▋         | 103/1605 [13:28<3:12:24,  7.69s/it]  6%|▋         | 104/1605 [13:35<3:11:06,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.216254086875291e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4448.0, 'logps/rejected': -4504.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.19}
  6%|▋         | 104/1605 [13:36<3:11:06,  7.64s/it]  7%|▋         | 105/1605 [13:43<3:13:44,  7.75s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.286314806165343e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7248.0, 'logps/rejected': -6128.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.2}
  7%|▋         | 105/1605 [13:44<3:13:44,  7.75s/it]  7%|▋         | 106/1605 [13:51<3:12:56,  7.72s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.3563755254553935e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5808.0, 'logps/rejected': -5968.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.2}
  7%|▋         | 106/1605 [13:51<3:12:56,  7.72s/it]  7%|▋         | 107/1605 [13:58<3:10:43,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.426436244745446e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5232.0, 'logps/rejected': -4464.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.2}
  7%|▋         | 107/1605 [13:59<3:10:43,  7.64s/it]  7%|▋         | 108/1605 [14:07<3:13:47,  7.77s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.496496964035497e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5296.0, 'logps/rejected': -5248.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.2}
  7%|▋         | 108/1605 [14:07<3:13:47,  7.77s/it]  7%|▋         | 109/1605 [14:14<3:12:34,  7.72s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.566557683325548e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4848.0, 'logps/rejected': -4040.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.2}
  7%|▋         | 109/1605 [14:14<3:12:34,  7.72s/it]slurmstepd: error: *** STEP 62082450.0 ON gn24 CANCELLED AT 2025-05-31T01:12:07 ***
