cpu-bind=MASK - gn22, task  1  0 [3213122]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 1 ---
Total Nodes: 4
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn21
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 4     --machine_rank 1     --main_process_ip gn21     --main_process_port 29500     --num_processes 16     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62875861     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.2 --curriculum_stage=0
-------------------------------------------
[2025-06-10 00:51:44,176] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0610 00:51:46.030000 3213175 torch/distributed/run.py:792] 
W0610 00:51:46.030000 3213175 torch/distributed/run.py:792] *****************************************
W0610 00:51:46.030000 3213175 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0610 00:51:46.030000 3213175 torch/distributed/run.py:792] *****************************************
[2025-06-10 00:51:51,599] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 00:51:51,612] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 00:51:51,659] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 00:51:51,667] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
World size: 16
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
[2025-06-10 00:51:57,802] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-10 00:51:57,812] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 4890
Validation dataset size: 953
Steps per epoch: 305
Evaluate each 152 steps
[2025-06-10 00:51:57,817] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-10 00:51:57,867] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: cjvt/GaMS-9B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:18, 26.15s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.58s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.58s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:52, 26.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:52, 26.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:52, 26.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:52, 26.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:18<00:25, 25.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:18<00:26, 26.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:18<00:26, 26.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:18<00:26, 26.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 23.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.37s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 23.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 23.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 23.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.44s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.44s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank6]:[W610 00:53:40.313594079 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s][rank5]:[W610 00:53:40.419851461 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  11%|█         | 550/4890 [00:00<00:00, 5375.32 examples/s][rank7]:[W610 00:53:40.557294856 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  23%|██▎       | 1109/4890 [00:00<00:00, 5497.31 examples/s]Extracting prompt in train dataset:  39%|███▉      | 1923/4890 [00:00<00:00, 5457.04 examples/s]Extracting prompt in train dataset:  51%|█████     | 2470/4890 [00:00<00:00, 5450.91 examples/s]Extracting prompt in train dataset:  67%|██████▋   | 3277/4890 [00:00<00:00, 5413.32 examples/s]Extracting prompt in train dataset:  78%|███████▊  | 3830/4890 [00:00<00:00, 5436.56 examples/s]Extracting prompt in train dataset:  90%|█████████ | 4401/4890 [00:00<00:00, 5501.10 examples/s]Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5435.18 examples/s]
Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   6%|▌         | 285/4890 [00:00<00:01, 2816.30 examples/s]Applying chat template to train dataset:  12%|█▏        | 599/4890 [00:00<00:01, 2992.02 examples/s]Applying chat template to train dataset:  19%|█▊        | 914/4890 [00:00<00:01, 2435.98 examples/s]Applying chat template to train dataset:  25%|██▌       | 1229/4890 [00:00<00:01, 2676.69 examples/s]Applying chat template to train dataset:  31%|███▏      | 1537/4890 [00:00<00:01, 2806.71 examples/s]Applying chat template to train dataset:  38%|███▊      | 1850/4890 [00:00<00:01, 2908.54 examples/s]Applying chat template to train dataset:  44%|████▍     | 2165/4890 [00:00<00:00, 2981.91 examples/s]Applying chat template to train dataset:  51%|█████     | 2479/4890 [00:00<00:00, 3026.52 examples/s]Applying chat template to train dataset:  57%|█████▋    | 2793/4890 [00:00<00:00, 3058.80 examples/s]Applying chat template to train dataset:  66%|██████▋   | 3244/4890 [00:01<00:00, 3033.77 examples/s]Applying chat template to train dataset:  73%|███████▎  | 3557/4890 [00:01<00:00, 3054.91 examples/s]Applying chat template to train dataset:  79%|███████▉  | 3871/4890 [00:01<00:00, 3076.79 examples/s]Applying chat template to train dataset:  86%|████████▌ | 4184/4890 [00:01<00:00, 3090.94 examples/s]Applying chat template to train dataset:  92%|█████████▏| 4513/4890 [00:01<00:00, 3144.20 examples/s]Applying chat template to train dataset:  99%|█████████▉| 4841/4890 [00:01<00:00, 3183.09 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 2997.99 examples/s]
Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 41/4890 [00:00<00:12, 393.75 examples/s]Tokenizing train dataset:   2%|▏         | 89/4890 [00:00<00:14, 337.32 examples/s]Tokenizing train dataset:   3%|▎         | 133/4890 [00:00<00:15, 313.37 examples/s]Tokenizing train dataset:   3%|▎         | 165/4890 [00:00<00:15, 311.02 examples/s]Tokenizing train dataset:   4%|▍         | 213/4890 [00:00<00:15, 310.13 examples/s]Tokenizing train dataset:   5%|▌         | 247/4890 [00:00<00:14, 317.89 examples/s]Tokenizing train dataset:   6%|▌         | 282/4890 [00:00<00:14, 325.94 examples/s]Tokenizing train dataset:   6%|▋         | 315/4890 [00:00<00:14, 322.79 examples/s]Tokenizing train dataset:   7%|▋         | 362/4890 [00:01<00:14, 317.29 examples/s]Tokenizing train dataset:   8%|▊         | 397/4890 [00:01<00:14, 319.81 examples/s]Tokenizing train dataset:   9%|▉         | 430/4890 [00:01<00:13, 320.83 examples/s]Tokenizing train dataset:  10%|▉         | 475/4890 [00:01<00:14, 308.74 examples/s]Tokenizing train dataset:  11%|█         | 519/4890 [00:01<00:14, 301.93 examples/s]Tokenizing train dataset:  11%|█▏        | 553/4890 [00:01<00:14, 309.07 examples/s]Tokenizing train dataset:  12%|█▏        | 585/4890 [00:01<00:13, 308.21 examples/s]Tokenizing train dataset:  13%|█▎        | 637/4890 [00:02<00:13, 315.49 examples/s]Tokenizing train dataset:  14%|█▍        | 684/4890 [00:02<00:13, 311.96 examples/s]Tokenizing train dataset:  15%|█▍        | 732/4890 [00:02<00:13, 309.91 examples/s]Tokenizing train dataset:  16%|█▌        | 764/4890 [00:02<00:13, 311.49 examples/s]Tokenizing train dataset:  16%|█▋        | 806/4890 [00:02<00:13, 295.98 examples/s]Tokenizing train dataset:  17%|█▋        | 850/4890 [00:02<00:13, 290.97 examples/s]Tokenizing train dataset:  18%|█▊        | 885/4890 [00:02<00:13, 299.39 examples/s]Tokenizing train dataset:  19%|█▉        | 920/4890 [00:02<00:13, 304.73 examples/s]Tokenizing train dataset:  19%|█▉        | 951/4890 [00:03<00:12, 305.20 examples/s]Tokenizing train dataset:  20%|██        | 994/4890 [00:03<00:13, 295.46 examples/s]Tokenizing train dataset:  21%|██        | 1037/4890 [00:03<00:13, 288.73 examples/s]Tokenizing train dataset:  22%|██▏       | 1068/4890 [00:03<00:13, 292.97 examples/s]Tokenizing train dataset:  23%|██▎       | 1103/4890 [00:03<00:12, 304.68 examples/s]Tokenizing train dataset:  23%|██▎       | 1145/4890 [00:03<00:12, 293.19 examples/s]Tokenizing train dataset:  24%|██▍       | 1193/4890 [00:03<00:12, 296.71 examples/s]Tokenizing train dataset:  25%|██▌       | 1225/4890 [00:03<00:12, 298.99 examples/s]Tokenizing train dataset:  26%|██▌       | 1262/4890 [00:04<00:11, 312.59 examples/s]Tokenizing train dataset:  27%|██▋       | 1311/4890 [00:04<00:11, 313.83 examples/s]Tokenizing train dataset:  27%|██▋       | 1344/4890 [00:04<00:11, 314.47 examples/s]Tokenizing train dataset:  28%|██▊       | 1387/4890 [00:04<00:11, 297.55 examples/s]Tokenizing train dataset:  29%|██▉       | 1418/4890 [00:04<00:11, 298.46 examples/s]Tokenizing train dataset:  30%|██▉       | 1462/4890 [00:04<00:11, 291.74 examples/s]Tokenizing train dataset:  31%|███       | 1492/4890 [00:04<00:11, 291.82 examples/s]Tokenizing train dataset:  31%|███▏      | 1539/4890 [00:05<00:11, 294.39 examples/s]Tokenizing train dataset:  32%|███▏      | 1569/4890 [00:05<00:11, 294.37 examples/s]Tokenizing train dataset:  33%|███▎      | 1600/4890 [00:05<00:11, 291.48 examples/s]Tokenizing train dataset:  33%|███▎      | 1635/4890 [00:05<00:10, 303.68 examples/s]Tokenizing train dataset:  34%|███▍      | 1670/4890 [00:05<00:10, 307.01 examples/s]Tokenizing train dataset:  35%|███▍      | 1710/4890 [00:05<00:09, 324.78 examples/s]Tokenizing train dataset:  36%|███▌      | 1746/4890 [00:05<00:09, 330.45 examples/s]Tokenizing train dataset:  36%|███▋      | 1781/4890 [00:05<00:09, 331.15 examples/s]Tokenizing train dataset:  37%|███▋      | 1826/4890 [00:05<00:09, 311.97 examples/s]Tokenizing train dataset:  38%|███▊      | 1872/4890 [00:06<00:09, 303.68 examples/s]Tokenizing train dataset:  39%|███▉      | 1910/4890 [00:06<00:10, 279.90 examples/s]Tokenizing train dataset:  40%|███▉      | 1953/4890 [00:06<00:10, 278.78 examples/s]Tokenizing train dataset:  41%|████      | 1997/4890 [00:06<00:10, 280.66 examples/s]Tokenizing train dataset:  42%|████▏     | 2030/4890 [00:06<00:09, 289.63 examples/s]Tokenizing train dataset:  42%|████▏     | 2067/4890 [00:06<00:09, 308.59 examples/s]Tokenizing train dataset:  43%|████▎     | 2100/4890 [00:06<00:08, 310.17 examples/s]Tokenizing train dataset:  44%|████▍     | 2146/4890 [00:07<00:08, 306.10 examples/s]Tokenizing train dataset:  45%|████▍     | 2191/4890 [00:07<00:09, 299.28 examples/s]Tokenizing train dataset:  46%|████▌     | 2240/4890 [00:07<00:08, 304.23 examples/s]Tokenizing train dataset:  46%|████▋     | 2273/4890 [00:07<00:08, 307.55 examples/s]Tokenizing train dataset:  47%|████▋     | 2319/4890 [00:07<00:08, 303.33 examples/s]Tokenizing train dataset:  48%|████▊     | 2360/4890 [00:07<00:08, 291.09 examples/s]Tokenizing train dataset:  49%|████▉     | 2390/4890 [00:07<00:08, 287.29 examples/s]Tokenizing train dataset:  50%|████▉     | 2423/4890 [00:07<00:08, 292.41 examples/s]Tokenizing train dataset:  50%|█████     | 2468/4890 [00:08<00:08, 290.53 examples/s]Tokenizing train dataset:  51%|█████     | 2500/4890 [00:08<00:08, 285.10 examples/s]Tokenizing train dataset:  52%|█████▏    | 2540/4890 [00:08<00:08, 273.82 examples/s]Tokenizing train dataset:  53%|█████▎    | 2569/4890 [00:08<00:08, 272.84 examples/s]Tokenizing train dataset:  53%|█████▎    | 2604/4890 [00:08<00:07, 290.78 examples/s]Tokenizing train dataset:  54%|█████▍    | 2637/4890 [00:08<00:07, 295.32 examples/s]Tokenizing train dataset:  55%|█████▍    | 2670/4890 [00:08<00:07, 299.92 examples/s]Tokenizing train dataset:  55%|█████▌    | 2708/4890 [00:08<00:06, 318.72 examples/s]Tokenizing train dataset:  56%|█████▋    | 2752/4890 [00:09<00:07, 302.43 examples/s]Tokenizing train dataset:  57%|█████▋    | 2798/4890 [00:09<00:06, 303.29 examples/s]Tokenizing train dataset:  58%|█████▊    | 2841/4890 [00:09<00:06, 294.56 examples/s]Tokenizing train dataset:  59%|█████▊    | 2872/4890 [00:09<00:06, 295.00 examples/s]Tokenizing train dataset:  59%|█████▉    | 2903/4890 [00:09<00:06, 296.65 examples/s]Tokenizing train dataset:  60%|██████    | 2950/4890 [00:09<00:06, 299.64 examples/s]Tokenizing train dataset:  61%|██████    | 2994/4890 [00:09<00:06, 296.43 examples/s]Tokenizing train dataset:  62%|██████▏   | 3026/4890 [00:10<00:06, 299.51 examples/s]Tokenizing train dataset:  63%|██████▎   | 3069/4890 [00:10<00:06, 291.73 examples/s]Tokenizing train dataset:  63%|██████▎   | 3100/4890 [00:10<00:06, 293.72 examples/s]Tokenizing train dataset:  64%|██████▍   | 3130/4890 [00:10<00:06, 291.60 examples/s]Tokenizing train dataset:  65%|██████▍   | 3174/4890 [00:10<00:05, 291.42 examples/s]Tokenizing train dataset:  66%|██████▌   | 3206/4890 [00:10<00:05, 294.63 examples/s]Tokenizing train dataset:  66%|██████▌   | 3238/4890 [00:10<00:05, 297.16 examples/s]Tokenizing train dataset:  67%|██████▋   | 3271/4890 [00:10<00:05, 302.34 examples/s]Tokenizing train dataset:  68%|██████▊   | 3302/4890 [00:10<00:05, 301.47 examples/s]Tokenizing train dataset:  68%|██████▊   | 3349/4890 [00:11<00:05, 304.71 examples/s]Tokenizing train dataset:  69%|██████▉   | 3388/4890 [00:11<00:05, 283.93 examples/s]Tokenizing train dataset:  70%|██████▉   | 3417/4890 [00:11<00:05, 282.23 examples/s]Tokenizing train dataset:  71%|███████   | 3455/4890 [00:11<00:05, 264.80 examples/s]Tokenizing train dataset:  71%|███████▏  | 3489/4890 [00:11<00:05, 249.04 examples/s]Tokenizing train dataset:  72%|███████▏  | 3530/4890 [00:11<00:04, 284.28 examples/s]Tokenizing train dataset:  73%|███████▎  | 3561/4890 [00:11<00:04, 287.19 examples/s]Tokenizing train dataset:  74%|███████▎  | 3595/4890 [00:11<00:04, 298.85 examples/s]Tokenizing train dataset:  74%|███████▍  | 3627/4890 [00:12<00:04, 297.85 examples/s]Tokenizing train dataset:  75%|███████▍  | 3661/4890 [00:12<00:04, 305.82 examples/s]Tokenizing train dataset:  76%|███████▌  | 3703/4890 [00:12<00:04, 293.99 examples/s]Tokenizing train dataset:  77%|███████▋  | 3747/4890 [00:12<00:03, 328.99 examples/s]Tokenizing train dataset:  78%|███████▊  | 3797/4890 [00:12<00:03, 326.94 examples/s]Tokenizing train dataset:  78%|███████▊  | 3834/4890 [00:12<00:03, 335.43 examples/s]Tokenizing train dataset:  79%|███████▉  | 3881/4890 [00:12<00:03, 321.62 examples/s]Tokenizing train dataset:  80%|████████  | 3921/4890 [00:13<00:03, 298.67 examples/s]Tokenizing train dataset:  81%|████████  | 3973/4890 [00:13<00:02, 309.03 examples/s]Tokenizing train dataset:  82%|████████▏ | 4013/4890 [00:13<00:02, 326.39 examples/s]Tokenizing train dataset:  83%|████████▎ | 4057/4890 [00:13<00:02, 310.27 examples/s]Tokenizing train dataset:  84%|████████▎ | 4090/4890 [00:13<00:02, 313.52 examples/s]Tokenizing train dataset:  85%|████████▍ | 4134/4890 [00:13<00:02, 303.49 examples/s]Tokenizing train dataset:  85%|████████▌ | 4178/4890 [00:13<00:02, 297.72 examples/s]Tokenizing train dataset:  87%|████████▋ | 4264/4890 [00:13<00:01, 427.66 examples/s]Tokenizing train dataset:  90%|████████▉ | 4391/4890 [00:14<00:00, 636.33 examples/s]Tokenizing train dataset:  92%|█████████▏| 4516/4890 [00:14<00:00, 795.32 examples/s]Tokenizing train dataset:  95%|█████████▍| 4640/4890 [00:14<00:00, 911.63 examples/s]Tokenizing train dataset:  97%|█████████▋| 4764/4890 [00:14<00:00, 1000.97 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 1070.49 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 337.66 examples/s] 
[rank4]:[W610 00:53:58.350197147 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in train dataset:  11%|█         | 540/4890 [00:00<00:00, 5344.78 examples/s]Extracting prompt in train dataset:  11%|█         | 540/4890 [00:00<00:00, 5335.26 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5512.29 examples/s]Extracting prompt in train dataset:  11%|█         | 548/4890 [00:00<00:00, 5390.23 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5526.58 examples/s]Extracting prompt in train dataset:  22%|██▏       | 1099/4890 [00:00<00:00, 5481.47 examples/s]
Extracting prompt in train dataset:  23%|██▎       | 1104/4890 [00:00<00:00, 5460.29 examples/s]Extracting prompt in train dataset:  22%|██▏       | 1100/4890 [00:00<00:00, 5436.15 examples/s]Extracting prompt in train dataset:  34%|███▍      | 1652/4890 [00:00<00:00, 5443.79 examples/s]Extracting prompt in train dataset:  34%|███▍      | 1658/4890 [00:00<00:00, 5475.13 examples/s]Extracting prompt in train dataset:  39%|███▉      | 1911/4890 [00:00<00:00, 5424.57 examples/s]Extracting prompt in train dataset:  45%|████▌     | 2220/4890 [00:00<00:00, 5523.48 examples/s]Extracting prompt in train dataset:  46%|████▌     | 2227/4890 [00:00<00:00, 5539.06 examples/s]Extracting prompt in train dataset:  51%|█████     | 2470/4890 [00:00<00:00, 5467.88 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 2790/4890 [00:00<00:00, 5564.85 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 2800/4890 [00:00<00:00, 5568.95 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  67%|██████▋   | 3273/4890 [00:00<00:00, 5411.02 examples/s]Extracting prompt in train dataset:  74%|███████▎  | 3596/4890 [00:00<00:00, 5481.12 examples/s]Extracting prompt in train dataset:  74%|███████▍  | 3620/4890 [00:00<00:00, 5502.03 examples/s]Applying chat template to eval dataset:  33%|███▎      | 310/953 [00:00<00:00, 3059.07 examples/s]Extracting prompt in train dataset:  78%|███████▊  | 3830/4890 [00:00<00:00, 5438.16 examples/s]Extracting prompt in train dataset:  85%|████████▌ | 4166/4890 [00:00<00:00, 5534.95 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 4186/4890 [00:00<00:00, 5546.27 examples/s]Applying chat template to eval dataset:  66%|██████▋   | 632/953 [00:00<00:00, 3150.83 examples/s]Extracting prompt in train dataset:  90%|████████▉ | 4400/4890 [00:00<00:00, 5494.94 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 4760/4890 [00:00<00:00, 5632.02 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 4785/4890 [00:00<00:00, 5675.44 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3174.85 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3139.66 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5538.24 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5520.73 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5463.37 examples/s]
Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   6%|▌         | 284/4890 [00:00<00:01, 2813.16 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.99 examples/s]Applying chat template to train dataset:   6%|▌         | 280/4890 [00:00<00:01, 2766.67 examples/s]Applying chat template to train dataset:   6%|▌         | 280/4890 [00:00<00:01, 2766.31 examples/s]Applying chat template to train dataset:  12%|█▏        | 600/4890 [00:00<00:01, 2997.51 examples/s]Applying chat template to train dataset:  12%|█▏        | 590/4890 [00:00<00:01, 2950.09 examples/s]Applying chat template to train dataset:  12%|█▏        | 590/4890 [00:00<00:01, 2951.15 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 288.17 examples/s]Applying chat template to train dataset:  19%|█▊        | 916/4890 [00:00<00:01, 3066.34 examples/s]Applying chat template to train dataset:  18%|█▊        | 902/4890 [00:00<00:01, 3021.84 examples/s]Applying chat template to train dataset:  18%|█▊        | 904/4890 [00:00<00:01, 3030.35 examples/s]Applying chat template to train dataset:  25%|██▌       | 1233/4890 [00:00<00:01, 3102.77 examples/s]Applying chat template to train dataset:  25%|██▍       | 1213/4890 [00:00<00:01, 3051.97 examples/s]Applying chat template to train dataset:  25%|██▍       | 1220/4890 [00:00<00:01, 3074.46 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 276.86 examples/s]Applying chat template to train dataset:  32%|███▏      | 1545/4890 [00:00<00:01, 3105.94 examples/s]Applying chat template to train dataset:  31%|███       | 1520/4890 [00:00<00:01, 3049.43 examples/s]Applying chat template to train dataset:  34%|███▍      | 1683/4890 [00:00<00:01, 3079.11 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.33 examples/s]Applying chat template to train dataset:  38%|███▊      | 1864/4890 [00:00<00:00, 3130.46 examples/s]Applying chat template to train dataset:  37%|███▋      | 1832/4890 [00:00<00:00, 3070.53 examples/s]Applying chat template to train dataset:  41%|████      | 1997/4890 [00:00<00:00, 3092.43 examples/s]Applying chat template to train dataset:  45%|████▍     | 2181/4890 [00:00<00:00, 3141.93 examples/s]Applying chat template to train dataset:  44%|████▍     | 2143/4890 [00:00<00:00, 3080.61 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.17 examples/s]Applying chat template to train dataset:  47%|████▋     | 2317/4890 [00:00<00:00, 3120.99 examples/s]Applying chat template to train dataset:  51%|█████     | 2500/4890 [00:00<00:00, 3150.22 examples/s]Applying chat template to train dataset:  50%|█████     | 2453/4890 [00:00<00:00, 3083.75 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 269.13 examples/s]Applying chat template to train dataset:  54%|█████▍    | 2636/4890 [00:00<00:00, 3137.68 examples/s]Applying chat template to train dataset:  57%|█████▋    | 2764/4890 [00:00<00:00, 3089.61 examples/s]Tokenizing eval dataset:  31%|███       | 291/953 [00:00<00:01, 368.34 examples/s]Applying chat template to train dataset:  61%|██████    | 2968/4890 [00:00<00:00, 3110.16 examples/s]Applying chat template to train dataset:  63%|██████▎   | 3091/4890 [00:01<00:00, 3093.18 examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 438.72 examples/s]Applying chat template to train dataset:  67%|██████▋   | 3284/4890 [00:01<00:00, 3121.64 examples/s]Applying chat template to train dataset:  66%|██████▌   | 3215/4890 [00:01<00:00, 3053.31 examples/s]Applying chat template to train dataset:  70%|██████▉   | 3411/4890 [00:01<00:00, 3117.76 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 488.25 examples/s]Applying chat template to train dataset:  74%|███████▎  | 3600/4890 [00:01<00:00, 3127.97 examples/s]Applying chat template to train dataset:  72%|███████▏  | 3527/4890 [00:01<00:00, 3067.93 examples/s]Applying chat template to train dataset:  76%|███████▋  | 3730/4890 [00:01<00:00, 3134.73 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 541.27 examples/s]Applying chat template to train dataset:  80%|████████  | 3920/4890 [00:01<00:00, 3140.12 examples/s]Applying chat template to train dataset:  78%|███████▊  | 3838/4890 [00:01<00:00, 3077.61 examples/s]Applying chat template to train dataset:  83%|████████▎ | 4050/4890 [00:01<00:00, 3148.90 examples/s]Tokenizing eval dataset:  58%|█████▊    | 555/953 [00:01<00:00, 575.23 examples/s]Applying chat template to train dataset:  87%|████████▋ | 4241/4890 [00:01<00:00, 3155.04 examples/s]Applying chat template to train dataset:  85%|████████▍ | 4148/4890 [00:01<00:00, 3080.53 examples/s]Applying chat template to train dataset:  90%|████████▉ | 4379/4890 [00:01<00:00, 3186.69 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 583.89 examples/s]Applying chat template to train dataset:  94%|█████████▎| 4573/4890 [00:01<00:00, 3202.85 examples/s]Applying chat template to train dataset:  91%|█████████ | 4461/4890 [00:01<00:00, 3094.43 examples/s]Applying chat template to train dataset:  96%|█████████▋| 4714/4890 [00:01<00:00, 3232.00 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3138.00 examples/s]
Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 595.03 examples/s]Applying chat template to train dataset:  98%|█████████▊| 4780/4890 [00:01<00:00, 3116.64 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3128.17 examples/s]
Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3065.82 examples/s]
Tokenizing eval dataset:  80%|████████  | 766/953 [00:01<00:00, 574.27 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 540.25 examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:12, 400.09 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 514.49 examples/s]Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:12, 403.90 examples/s]Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:11, 410.00 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 452.81 examples/s]
Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:14, 339.33 examples/s]Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:14, 337.81 examples/s]Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:14, 340.05 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 322.03 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 319.31 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 320.98 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 310.27 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 307.10 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 308.53 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 316.10 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 312.41 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 313.88 examples/s]Tokenizing train dataset:   5%|▌         | 252/4890 [00:00<00:14, 320.56 examples/s]Tokenizing train dataset:   5%|▌         | 251/4890 [00:00<00:14, 315.43 examples/s]Tokenizing train dataset:   5%|▌         | 251/4890 [00:00<00:14, 316.87 examples/s]Tokenizing train dataset:   6%|▌         | 290/4890 [00:00<00:13, 334.04 examples/s]Tokenizing train dataset:   6%|▌         | 289/4890 [00:00<00:13, 331.00 examples/s]Tokenizing train dataset:   6%|▌         | 289/4890 [00:00<00:13, 332.65 examples/s]Tokenizing train dataset:   7%|▋         | 324/4890 [00:00<00:13, 332.99 examples/s]Tokenizing train dataset:   7%|▋         | 337/4890 [00:01<00:14, 323.15 examples/s]Tokenizing train dataset:   7%|▋         | 337/4890 [00:01<00:14, 324.69 examples/s]Tokenizing train dataset:   8%|▊         | 370/4890 [00:01<00:14, 318.78 examples/s]Tokenizing train dataset:   8%|▊         | 387/4890 [00:01<00:14, 321.35 examples/s]Tokenizing train dataset:   8%|▊         | 387/4890 [00:01<00:13, 322.93 examples/s]Tokenizing train dataset:   9%|▊         | 420/4890 [00:01<00:14, 317.01 examples/s]Tokenizing train dataset:   9%|▉         | 434/4890 [00:01<00:14, 313.05 examples/s]Tokenizing train dataset:   9%|▉         | 434/4890 [00:01<00:14, 313.98 examples/s]Tokenizing train dataset:  10%|▉         | 469/4890 [00:01<00:14, 315.31 examples/s]Tokenizing train dataset:  10%|▉         | 480/4890 [00:01<00:14, 306.55 examples/s]Tokenizing train dataset:  10%|▉         | 480/4890 [00:01<00:14, 307.47 examples/s]Tokenizing train dataset:  11%|█         | 516/4890 [00:01<00:14, 308.73 examples/s]Tokenizing train dataset:  11%|█         | 527/4890 [00:01<00:14, 305.62 examples/s]Tokenizing train dataset:  11%|█         | 550/4890 [00:01<00:13, 313.78 examples/s]Tokenizing train dataset:  11%|█         | 527/4890 [00:01<00:14, 306.36 examples/s]Tokenizing train dataset:  11%|█▏        | 561/4890 [00:01<00:13, 310.50 examples/s]Tokenizing train dataset:  11%|█▏        | 561/4890 [00:01<00:13, 311.45 examples/s]Tokenizing train dataset:  12%|█▏        | 596/4890 [00:01<00:13, 309.30 examples/s]Tokenizing train dataset:  13%|█▎        | 634/4890 [00:01<00:13, 323.52 examples/s]Tokenizing train dataset:  12%|█▏        | 605/4890 [00:01<00:14, 300.12 examples/s]Tokenizing train dataset:  12%|█▏        | 605/4890 [00:01<00:14, 301.04 examples/s]Tokenizing train dataset:  13%|█▎        | 643/4890 [00:02<00:13, 317.93 examples/s]Tokenizing train dataset:  13%|█▎        | 644/4890 [00:02<00:13, 318.12 examples/s]Tokenizing train dataset:  14%|█▍        | 681/4890 [00:02<00:13, 314.52 examples/s]Tokenizing train dataset:  14%|█▍        | 689/4890 [00:02<00:13, 308.68 examples/s]Tokenizing train dataset:  14%|█▍        | 689/4890 [00:02<00:13, 310.06 examples/s]Tokenizing train dataset:  15%|█▍        | 732/4890 [00:02<00:14, 278.79 examples/s]Tokenizing train dataset:  15%|█▌        | 740/4890 [00:02<00:13, 315.20 examples/s]Tokenizing train dataset:  15%|█▌        | 740/4890 [00:02<00:13, 316.32 examples/s]Tokenizing train dataset:  16%|█▌        | 766/4890 [00:02<00:14, 287.39 examples/s]Tokenizing train dataset:  16%|█▌        | 785/4890 [00:02<00:13, 305.88 examples/s]Tokenizing train dataset:  16%|█▌        | 785/4890 [00:02<00:13, 307.08 examples/s]Tokenizing train dataset:  17%|█▋        | 810/4890 [00:02<00:14, 283.59 examples/s]Tokenizing train dataset:  17%|█▋        | 827/4890 [00:02<00:13, 292.84 examples/s]Tokenizing train dataset:  17%|█▋        | 827/4890 [00:02<00:13, 293.56 examples/s]Tokenizing train dataset:  17%|█▋        | 840/4890 [00:02<00:14, 283.93 examples/s]Tokenizing train dataset:  18%|█▊        | 860/4890 [00:02<00:13, 297.82 examples/s]Tokenizing train dataset:  18%|█▊        | 860/4890 [00:02<00:13, 298.73 examples/s]Tokenizing train dataset:  18%|█▊        | 870/4890 [00:02<00:14, 285.62 examples/s]Tokenizing train dataset:  18%|█▊        | 897/4890 [00:02<00:12, 313.31 examples/s]Tokenizing train dataset:  18%|█▊        | 897/4890 [00:02<00:12, 313.96 examples/s]Tokenizing train dataset:  19%|█▊        | 910/4890 [00:02<00:12, 309.49 examples/s]Tokenizing train dataset:  19%|█▉        | 940/4890 [00:03<00:13, 300.94 examples/s]Tokenizing train dataset:  19%|█▉        | 940/4890 [00:03<00:13, 301.44 examples/s]Tokenizing train dataset:  20%|█▉        | 954/4890 [00:03<00:13, 300.31 examples/s]Tokenizing train dataset:  20%|█▉        | 972/4890 [00:03<00:12, 304.15 examples/s]Tokenizing train dataset:  20%|█▉        | 972/4890 [00:03<00:12, 304.68 examples/s]Tokenizing train dataset:  20%|██        | 998/4890 [00:03<00:13, 293.88 examples/s]Tokenizing train dataset:  21%|██        | 1014/4890 [00:03<00:13, 289.36 examples/s]Tokenizing train dataset:  21%|██        | 1014/4890 [00:03<00:13, 289.91 examples/s]Tokenizing train dataset:  21%|██▏       | 1040/4890 [00:03<00:13, 286.27 examples/s]Tokenizing train dataset:  22%|██▏       | 1060/4890 [00:03<00:13, 290.17 examples/s]Tokenizing train dataset:  22%|██▏       | 1060/4890 [00:03<00:13, 290.89 examples/s]Tokenizing train dataset:  22%|██▏       | 1073/4890 [00:03<00:12, 294.20 examples/s]Tokenizing train dataset:  22%|██▏       | 1095/4890 [00:03<00:12, 302.58 examples/s]Tokenizing train dataset:  22%|██▏       | 1095/4890 [00:03<00:12, 303.33 examples/s]Tokenizing train dataset:  23%|██▎       | 1109/4890 [00:03<00:12, 307.80 examples/s]Tokenizing train dataset:  23%|██▎       | 1137/4890 [00:03<00:12, 290.21 examples/s]Tokenizing train dataset:  23%|██▎       | 1137/4890 [00:03<00:12, 290.51 examples/s]Tokenizing train dataset:  24%|██▎       | 1151/4890 [00:03<00:12, 295.06 examples/s]Tokenizing train dataset:  24%|██▍       | 1167/4890 [00:03<00:12, 290.35 examples/s]Tokenizing train dataset:  24%|██▍       | 1167/4890 [00:03<00:12, 290.41 examples/s]Tokenizing train dataset:  25%|██▍       | 1200/4890 [00:03<00:12, 303.22 examples/s]Tokenizing train dataset:  25%|██▍       | 1202/4890 [00:03<00:12, 301.11 examples/s]Tokenizing train dataset:  25%|██▍       | 1202/4890 [00:03<00:12, 300.87 examples/s]Tokenizing train dataset:  25%|██▌       | 1232/4890 [00:04<00:11, 304.94 examples/s]Tokenizing train dataset:  25%|██▌       | 1236/4890 [00:04<00:11, 306.84 examples/s]Tokenizing train dataset:  25%|██▌       | 1235/4890 [00:04<00:11, 304.64 examples/s]Tokenizing train dataset:  26%|██▌       | 1269/4890 [00:04<00:11, 317.84 examples/s]Tokenizing train dataset:  26%|██▌       | 1270/4890 [00:04<00:11, 309.01 examples/s]Tokenizing train dataset:  26%|██▌       | 1270/4890 [00:04<00:11, 309.28 examples/s]Tokenizing train dataset:  27%|██▋       | 1304/4890 [00:04<00:11, 313.05 examples/s]Tokenizing train dataset:  27%|██▋       | 1315/4890 [00:04<00:11, 313.05 examples/s]Tokenizing train dataset:  27%|██▋       | 1304/4890 [00:04<00:11, 313.69 examples/s]Tokenizing train dataset:  28%|██▊       | 1348/4890 [00:04<00:11, 315.00 examples/s]Tokenizing train dataset:  27%|██▋       | 1339/4890 [00:04<00:11, 316.13 examples/s]Tokenizing train dataset:  27%|██▋       | 1339/4890 [00:04<00:11, 316.78 examples/s]Tokenizing train dataset:  28%|██▊       | 1379/4890 [00:04<00:11, 295.63 examples/s]Tokenizing train dataset:  28%|██▊       | 1390/4890 [00:04<00:11, 296.90 examples/s]Tokenizing train dataset:  28%|██▊       | 1379/4890 [00:04<00:11, 296.12 examples/s]Tokenizing train dataset:  29%|██▉       | 1422/4890 [00:04<00:11, 301.89 examples/s]Tokenizing train dataset:  29%|██▉       | 1410/4890 [00:04<00:11, 295.26 examples/s]Tokenizing train dataset:  29%|██▉       | 1410/4890 [00:04<00:11, 295.76 examples/s]Tokenizing train dataset:  29%|██▉       | 1440/4890 [00:04<00:11, 292.94 examples/s]Tokenizing train dataset:  30%|██▉       | 1453/4890 [00:04<00:11, 299.45 examples/s]Tokenizing train dataset:  29%|██▉       | 1440/4890 [00:04<00:11, 293.72 examples/s]Tokenizing train dataset:  30%|███       | 1472/4890 [00:04<00:11, 298.29 examples/s]Tokenizing train dataset:  30%|███       | 1472/4890 [00:04<00:11, 298.84 examples/s]Tokenizing train dataset:  31%|███       | 1499/4890 [00:04<00:11, 297.00 examples/s]Tokenizing train dataset:  31%|███       | 1514/4890 [00:04<00:11, 286.46 examples/s]Tokenizing train dataset:  31%|███▏      | 1530/4890 [00:05<00:11, 296.82 examples/s]Tokenizing train dataset:  31%|███       | 1514/4890 [00:04<00:11, 286.76 examples/s]Tokenizing train dataset:  32%|███▏      | 1548/4890 [00:05<00:11, 298.40 examples/s]Tokenizing train dataset:  32%|███▏      | 1561/4890 [00:05<00:11, 297.84 examples/s]Tokenizing train dataset:  32%|███▏      | 1548/4890 [00:05<00:11, 298.67 examples/s]Tokenizing train dataset:  33%|███▎      | 1591/4890 [00:05<00:11, 293.62 examples/s]Tokenizing train dataset:  33%|███▎      | 1591/4890 [00:05<00:11, 291.85 examples/s]Tokenizing train dataset:  33%|███▎      | 1591/4890 [00:05<00:11, 292.26 examples/s]Tokenizing train dataset:  33%|███▎      | 1627/4890 [00:05<00:10, 304.66 examples/s]Tokenizing train dataset:  33%|███▎      | 1627/4890 [00:05<00:10, 301.62 examples/s]Tokenizing train dataset:  33%|███▎      | 1627/4890 [00:05<00:10, 301.77 examples/s]Tokenizing train dataset:  34%|███▍      | 1661/4890 [00:05<00:10, 312.78 examples/s]Tokenizing train dataset:  34%|███▍      | 1661/4890 [00:05<00:10, 309.09 examples/s]Tokenizing train dataset:  34%|███▍      | 1661/4890 [00:05<00:10, 309.14 examples/s]Tokenizing train dataset:  35%|███▍      | 1701/4890 [00:05<00:09, 332.93 examples/s]Tokenizing train dataset:  35%|███▍      | 1701/4890 [00:05<00:09, 328.16 examples/s]Tokenizing train dataset:  35%|███▍      | 1701/4890 [00:05<00:09, 328.22 examples/s]Tokenizing train dataset:  36%|███▌      | 1751/4890 [00:05<00:09, 329.27 examples/s]Tokenizing train dataset:  36%|███▌      | 1751/4890 [00:05<00:09, 325.22 examples/s]Tokenizing train dataset:  36%|███▌      | 1751/4890 [00:05<00:09, 325.57 examples/s]Tokenizing train dataset:  37%|███▋      | 1788/4890 [00:05<00:09, 338.60 examples/s]Tokenizing train dataset:  37%|███▋      | 1788/4890 [00:05<00:09, 334.88 examples/s]Tokenizing train dataset:  37%|███▋      | 1788/4890 [00:05<00:09, 334.84 examples/s]Tokenizing train dataset:  37%|███▋      | 1829/4890 [00:05<00:09, 312.26 examples/s]Tokenizing train dataset:  37%|███▋      | 1829/4890 [00:05<00:09, 308.74 examples/s]Tokenizing train dataset:  37%|███▋      | 1829/4890 [00:05<00:09, 308.98 examples/s]Tokenizing train dataset:  38%|███▊      | 1861/4890 [00:06<00:09, 307.81 examples/s]Tokenizing train dataset:  38%|███▊      | 1874/4890 [00:06<00:09, 303.47 examples/s]Tokenizing train dataset:  38%|███▊      | 1874/4890 [00:06<00:09, 303.98 examples/s]Tokenizing train dataset:  39%|███▉      | 1901/4890 [00:06<00:10, 288.19 examples/s]Tokenizing train dataset:  39%|███▉      | 1910/4890 [00:06<00:10, 279.87 examples/s]Tokenizing train dataset:  39%|███▉      | 1910/4890 [00:06<00:10, 280.50 examples/s]Tokenizing train dataset:  40%|███▉      | 1941/4890 [00:06<00:10, 277.84 examples/s]Tokenizing train dataset:  40%|███▉      | 1953/4890 [00:06<00:10, 278.91 examples/s]Tokenizing train dataset:  40%|███▉      | 1953/4890 [00:06<00:10, 279.29 examples/s]Tokenizing train dataset:  40%|████      | 1971/4890 [00:06<00:10, 279.82 examples/s]Tokenizing train dataset:  41%|████      | 2002/4890 [00:06<00:10, 286.33 examples/s]Tokenizing train dataset:  41%|████      | 1997/4890 [00:06<00:10, 280.78 examples/s]Tokenizing train dataset:  41%|████      | 1997/4890 [00:06<00:10, 281.36 examples/s]Tokenizing train dataset:  42%|████▏     | 2036/4890 [00:06<00:09, 297.87 examples/s]Tokenizing train dataset:  42%|████▏     | 2030/4890 [00:06<00:09, 289.60 examples/s]Tokenizing train dataset:  42%|████▏     | 2030/4890 [00:06<00:09, 290.27 examples/s]Tokenizing train dataset:  42%|████▏     | 2073/4890 [00:06<00:08, 314.34 examples/s]Tokenizing train dataset:  42%|████▏     | 2068/4890 [00:06<00:09, 309.08 examples/s]Tokenizing train dataset:  42%|████▏     | 2068/4890 [00:06<00:09, 309.62 examples/s]Tokenizing train dataset:  43%|████▎     | 2108/4890 [00:06<00:08, 322.79 examples/s]Tokenizing train dataset:  43%|████▎     | 2100/4890 [00:06<00:08, 310.10 examples/s]Tokenizing train dataset:  43%|████▎     | 2101/4890 [00:06<00:08, 310.42 examples/s]Tokenizing train dataset:  44%|████▍     | 2152/4890 [00:07<00:08, 305.54 examples/s]Tokenizing train dataset:  44%|████▍     | 2146/4890 [00:07<00:08, 306.00 examples/s]Tokenizing train dataset:  44%|████▍     | 2147/4890 [00:07<00:08, 306.51 examples/s]Tokenizing train dataset:  45%|████▍     | 2185/4890 [00:07<00:08, 308.86 examples/s]Tokenizing train dataset:  45%|████▍     | 2179/4890 [00:07<00:08, 303.86 examples/s]Tokenizing train dataset:  45%|████▍     | 2191/4890 [00:07<00:09, 299.00 examples/s]Tokenizing train dataset:  46%|████▌     | 2232/4890 [00:07<00:08, 305.62 examples/s]Tokenizing train dataset:  45%|████▌     | 2222/4890 [00:07<00:09, 294.85 examples/s]Tokenizing train dataset:  46%|████▌     | 2240/4890 [00:07<00:08, 303.96 examples/s]Tokenizing train dataset:  46%|████▌     | 2257/4890 [00:07<00:08, 306.05 examples/s]Tokenizing train dataset:  46%|████▋     | 2267/4890 [00:07<00:09, 275.36 examples/s]Tokenizing train dataset:  46%|████▋     | 2273/4890 [00:07<00:08, 307.65 examples/s]Tokenizing train dataset:  47%|████▋     | 2298/4890 [00:07<00:09, 280.59 examples/s]Tokenizing train dataset:  47%|████▋     | 2302/4890 [00:07<00:08, 301.23 examples/s]Tokenizing train dataset:  47%|████▋     | 2319/4890 [00:07<00:08, 303.58 examples/s]Tokenizing train dataset:  48%|████▊     | 2328/4890 [00:07<00:09, 283.67 examples/s]Tokenizing train dataset:  48%|████▊     | 2345/4890 [00:07<00:08, 291.96 examples/s]Tokenizing train dataset:  48%|████▊     | 2360/4890 [00:07<00:08, 291.28 examples/s]Tokenizing train dataset:  48%|████▊     | 2370/4890 [00:07<00:09, 279.08 examples/s]Tokenizing train dataset:  49%|████▉     | 2390/4890 [00:07<00:08, 287.47 examples/s]Tokenizing train dataset:  49%|████▉     | 2390/4890 [00:07<00:08, 289.17 examples/s]Tokenizing train dataset:  49%|████▉     | 2400/4890 [00:07<00:08, 283.05 examples/s]Tokenizing train dataset:  50%|████▉     | 2423/4890 [00:07<00:08, 292.64 examples/s]Tokenizing train dataset:  50%|████▉     | 2431/4890 [00:08<00:08, 288.84 examples/s]Tokenizing train dataset:  50%|████▉     | 2423/4890 [00:07<00:08, 294.01 examples/s]Tokenizing train dataset:  50%|█████     | 2469/4890 [00:08<00:08, 290.13 examples/s]Tokenizing train dataset:  51%|█████     | 2477/4890 [00:08<00:08, 290.63 examples/s]Tokenizing train dataset:  50%|█████     | 2469/4890 [00:08<00:08, 291.41 examples/s]Tokenizing train dataset:  51%|█████     | 2500/4890 [00:08<00:08, 284.68 examples/s]Tokenizing train dataset:  51%|█████     | 2500/4890 [00:08<00:08, 284.33 examples/s]Tokenizing train dataset:  51%|█████▏    | 2516/4890 [00:08<00:08, 274.63 examples/s]Tokenizing train dataset:  52%|█████▏    | 2547/4890 [00:08<00:08, 280.71 examples/s]Tokenizing train dataset:  52%|█████▏    | 2540/4890 [00:08<00:08, 273.17 examples/s]Tokenizing train dataset:  52%|█████▏    | 2540/4890 [00:08<00:08, 273.65 examples/s]Tokenizing train dataset:  53%|█████▎    | 2580/4890 [00:08<00:07, 289.12 examples/s]Tokenizing train dataset:  53%|█████▎    | 2574/4890 [00:08<00:08, 286.36 examples/s]Tokenizing train dataset:  53%|█████▎    | 2574/4890 [00:08<00:08, 286.63 examples/s]Tokenizing train dataset:  54%|█████▎    | 2617/4890 [00:08<00:07, 306.97 examples/s]Tokenizing train dataset:  53%|█████▎    | 2610/4890 [00:08<00:07, 304.09 examples/s]Tokenizing train dataset:  53%|█████▎    | 2610/4890 [00:08<00:07, 304.14 examples/s]Tokenizing train dataset:  54%|█████▍    | 2650/4890 [00:08<00:07, 311.63 examples/s]Tokenizing train dataset:  54%|█████▍    | 2643/4890 [00:08<00:07, 308.17 examples/s]Tokenizing train dataset:  54%|█████▍    | 2643/4890 [00:08<00:07, 308.40 examples/s]Tokenizing train dataset:  55%|█████▍    | 2683/4890 [00:08<00:07, 314.92 examples/s]Tokenizing train dataset:  55%|█████▍    | 2675/4890 [00:08<00:07, 307.52 examples/s]Tokenizing train dataset:  55%|█████▌    | 2692/4890 [00:08<00:07, 312.44 examples/s]Tokenizing train dataset:  56%|█████▌    | 2719/4890 [00:08<00:06, 322.68 examples/s]Tokenizing train dataset:  55%|█████▌    | 2712/4890 [00:08<00:06, 319.50 examples/s]Tokenizing train dataset:  56%|█████▌    | 2725/4890 [00:08<00:06, 313.00 examples/s]Tokenizing train dataset:  57%|█████▋    | 2763/4890 [00:09<00:06, 304.76 examples/s]Tokenizing train dataset:  56%|█████▋    | 2756/4890 [00:09<00:06, 305.38 examples/s]Tokenizing train dataset:  57%|█████▋    | 2767/4890 [00:09<00:07, 299.80 examples/s]Tokenizing train dataset:  57%|█████▋    | 2797/4890 [00:09<00:06, 308.43 examples/s]Tokenizing train dataset:  57%|█████▋    | 2800/4890 [00:09<00:06, 304.08 examples/s]Tokenizing train dataset:  57%|█████▋    | 2804/4890 [00:09<00:06, 305.41 examples/s]Tokenizing train dataset:  58%|█████▊    | 2841/4890 [00:09<00:06, 299.25 examples/s]Tokenizing train dataset:  58%|█████▊    | 2845/4890 [00:09<00:06, 296.51 examples/s]Tokenizing train dataset:  58%|█████▊    | 2848/4890 [00:09<00:06, 296.82 examples/s]Tokenizing train dataset:  59%|█████▊    | 2872/4890 [00:09<00:06, 299.39 examples/s]Tokenizing train dataset:  59%|█████▉    | 2877/4890 [00:09<00:06, 297.30 examples/s]Tokenizing train dataset:  59%|█████▉    | 2879/4890 [00:09<00:06, 296.17 examples/s]Tokenizing train dataset:  59%|█████▉    | 2903/4890 [00:09<00:06, 300.73 examples/s]Tokenizing train dataset:  59%|█████▉    | 2907/4890 [00:09<00:06, 297.27 examples/s]Tokenizing train dataset:  60%|█████▉    | 2910/4890 [00:09<00:06, 297.99 examples/s]Tokenizing train dataset:  60%|██████    | 2951/4890 [00:09<00:06, 302.93 examples/s]Tokenizing train dataset:  60%|██████    | 2938/4890 [00:09<00:06, 297.88 examples/s]Tokenizing train dataset:  60%|██████    | 2942/4890 [00:09<00:06, 299.72 examples/s]Tokenizing train dataset:  61%|██████    | 2982/4890 [00:09<00:06, 302.86 examples/s]Tokenizing train dataset:  61%|██████    | 2969/4890 [00:09<00:06, 300.11 examples/s]Tokenizing train dataset:  61%|██████    | 2974/4890 [00:09<00:06, 301.44 examples/s]Tokenizing train dataset:  62%|██████▏   | 3014/4890 [00:09<00:06, 301.30 examples/s]Tokenizing train dataset:  61%|██████▏   | 3005/4890 [00:09<00:06, 297.76 examples/s]Tokenizing train dataset:  62%|██████▏   | 3015/4890 [00:09<00:06, 296.82 examples/s]Tokenizing train dataset:  63%|██████▎   | 3058/4890 [00:10<00:06, 293.94 examples/s]Tokenizing train dataset:  62%|██████▏   | 3048/4890 [00:10<00:06, 297.70 examples/s]Tokenizing train dataset:  62%|██████▏   | 3051/4890 [00:10<00:06, 297.47 examples/s]Tokenizing train dataset:  63%|██████▎   | 3088/4890 [00:10<00:06, 293.74 examples/s]Tokenizing train dataset:  63%|██████▎   | 3090/4890 [00:10<00:06, 286.22 examples/s]Tokenizing train dataset:  63%|██████▎   | 3094/4890 [00:10<00:06, 292.13 examples/s]Tokenizing train dataset:  64%|██████▍   | 3121/4890 [00:10<00:05, 296.62 examples/s]Tokenizing train dataset:  64%|██████▍   | 3122/4890 [00:10<00:06, 292.28 examples/s]Tokenizing train dataset:  64%|██████▍   | 3126/4890 [00:10<00:05, 294.03 examples/s]Tokenizing train dataset:  65%|██████▍   | 3165/4890 [00:10<00:05, 292.83 examples/s]Tokenizing train dataset:  65%|██████▍   | 3167/4890 [00:10<00:05, 291.37 examples/s]Tokenizing train dataset:  65%|██████▍   | 3170/4890 [00:10<00:05, 289.25 examples/s]Tokenizing train dataset:  65%|██████▌   | 3198/4890 [00:10<00:05, 299.74 examples/s]Tokenizing train dataset:  65%|██████▌   | 3200/4890 [00:10<00:05, 298.19 examples/s]Tokenizing train dataset:  66%|██████▌   | 3204/4890 [00:10<00:05, 298.03 examples/s]Tokenizing train dataset:  66%|██████▋   | 3243/4890 [00:10<00:05, 298.81 examples/s]Tokenizing train dataset:  66%|██████▌   | 3235/4890 [00:10<00:05, 296.89 examples/s]Tokenizing train dataset:  66%|██████▋   | 3247/4890 [00:10<00:05, 296.46 examples/s]Tokenizing train dataset:  67%|██████▋   | 3274/4890 [00:10<00:05, 300.17 examples/s]Tokenizing train dataset:  67%|██████▋   | 3269/4890 [00:10<00:05, 305.30 examples/s]Tokenizing train dataset:  67%|██████▋   | 3279/4890 [00:10<00:05, 297.61 examples/s]Tokenizing train dataset:  68%|██████▊   | 3307/4890 [00:10<00:05, 305.55 examples/s]Tokenizing train dataset:  67%|██████▋   | 3300/4890 [00:10<00:05, 301.95 examples/s]Tokenizing train dataset:  68%|██████▊   | 3311/4890 [00:10<00:05, 300.63 examples/s]Tokenizing train dataset:  69%|██████▊   | 3353/4890 [00:11<00:05, 303.42 examples/s]Tokenizing train dataset:  68%|██████▊   | 3347/4890 [00:11<00:05, 302.34 examples/s]Tokenizing train dataset:  69%|██████▊   | 3357/4890 [00:11<00:05, 299.52 examples/s]Tokenizing train dataset:  69%|██████▉   | 3391/4890 [00:11<00:05, 284.19 examples/s]Tokenizing train dataset:  69%|██████▉   | 3387/4890 [00:11<00:05, 285.20 examples/s]Tokenizing train dataset:  69%|██████▉   | 3397/4890 [00:11<00:05, 284.94 examples/s]Tokenizing train dataset:  70%|██████▉   | 3421/4890 [00:11<00:05, 282.26 examples/s]Tokenizing train dataset:  70%|███████   | 3428/4890 [00:11<00:05, 278.98 examples/s]Tokenizing train dataset:  70%|███████   | 3436/4890 [00:11<00:05, 274.03 examples/s]Tokenizing train dataset:  71%|███████   | 3458/4890 [00:11<00:05, 266.72 examples/s]Tokenizing train dataset:  71%|███████   | 3465/4890 [00:11<00:05, 262.96 examples/s]Tokenizing train dataset:  71%|███████   | 3472/4890 [00:11<00:05, 260.54 examples/s]Tokenizing train dataset:  71%|███████▏  | 3493/4890 [00:11<00:05, 253.59 examples/s]Tokenizing train dataset:  72%|███████▏  | 3506/4890 [00:11<00:05, 261.00 examples/s]Tokenizing train dataset:  72%|███████▏  | 3534/4890 [00:11<00:04, 287.99 examples/s]Tokenizing train dataset:  72%|███████▏  | 3520/4890 [00:11<00:04, 274.09 examples/s]Tokenizing train dataset:  72%|███████▏  | 3545/4890 [00:11<00:04, 284.34 examples/s]Tokenizing train dataset:  73%|███████▎  | 3565/4890 [00:11<00:04, 291.66 examples/s]Tokenizing train dataset:  73%|███████▎  | 3551/4890 [00:11<00:04, 280.87 examples/s]Tokenizing train dataset:  73%|███████▎  | 3580/4890 [00:11<00:04, 298.23 examples/s]Tokenizing train dataset:  74%|███████▎  | 3600/4890 [00:11<00:04, 301.33 examples/s]Tokenizing train dataset:  73%|███████▎  | 3589/4890 [00:11<00:04, 298.18 examples/s]Tokenizing train dataset:  74%|███████▍  | 3634/4890 [00:12<00:04, 304.83 examples/s]Tokenizing train dataset:  74%|███████▍  | 3627/4890 [00:12<00:04, 297.97 examples/s]Tokenizing train dataset:  74%|███████▍  | 3635/4890 [00:12<00:04, 298.01 examples/s]Tokenizing train dataset:  75%|███████▌  | 3668/4890 [00:12<00:03, 307.94 examples/s]Tokenizing train dataset:  75%|███████▍  | 3661/4890 [00:12<00:04, 305.30 examples/s]Tokenizing train dataset:  75%|███████▌  | 3668/4890 [00:12<00:04, 301.96 examples/s]Tokenizing train dataset:  76%|███████▌  | 3713/4890 [00:12<00:03, 299.82 examples/s]Tokenizing train dataset:  76%|███████▌  | 3707/4890 [00:12<00:03, 297.43 examples/s]Tokenizing train dataset:  76%|███████▌  | 3712/4890 [00:12<00:04, 293.86 examples/s]Tokenizing train dataset:  77%|███████▋  | 3756/4890 [00:12<00:03, 330.15 examples/s]Tokenizing train dataset:  77%|███████▋  | 3753/4890 [00:12<00:03, 321.20 examples/s]Tokenizing train dataset:  77%|███████▋  | 3756/4890 [00:12<00:03, 305.44 examples/s]Tokenizing train dataset:  78%|███████▊  | 3790/4890 [00:12<00:03, 326.26 examples/s]Tokenizing train dataset:  77%|███████▋  | 3787/4890 [00:12<00:03, 320.67 examples/s]Tokenizing train dataset:  77%|███████▋  | 3789/4890 [00:12<00:03, 309.01 examples/s]Tokenizing train dataset:  78%|███████▊  | 3826/4890 [00:12<00:03, 334.69 examples/s]Tokenizing train dataset:  78%|███████▊  | 3824/4890 [00:12<00:03, 328.66 examples/s]Tokenizing train dataset:  78%|███████▊  | 3825/4890 [00:12<00:03, 317.95 examples/s]Tokenizing train dataset:  79%|███████▉  | 3860/4890 [00:12<00:03, 330.62 examples/s]Tokenizing train dataset:  79%|███████▉  | 3858/4890 [00:12<00:03, 317.72 examples/s]Tokenizing train dataset:  79%|███████▉  | 3874/4890 [00:12<00:03, 324.58 examples/s]Tokenizing train dataset:  80%|███████▉  | 3904/4890 [00:12<00:03, 311.80 examples/s]Tokenizing train dataset:  80%|███████▉  | 3902/4890 [00:12<00:03, 302.61 examples/s]Tokenizing train dataset:  80%|████████  | 3913/4890 [00:12<00:03, 299.22 examples/s]Tokenizing train dataset:  81%|████████  | 3944/4890 [00:13<00:03, 290.31 examples/s]Tokenizing train dataset:  81%|████████  | 3942/4890 [00:13<00:03, 288.13 examples/s]Tokenizing train dataset:  81%|████████▏ | 3983/4890 [00:13<00:02, 310.65 examples/s]Tokenizing train dataset:  81%|████████  | 3959/4890 [00:13<00:03, 298.42 examples/s]Tokenizing train dataset:  81%|████████▏ | 3980/4890 [00:13<00:02, 307.50 examples/s]Tokenizing train dataset:  82%|████████▏ | 4024/4890 [00:13<00:02, 333.99 examples/s]Tokenizing train dataset:  82%|████████▏ | 4000/4890 [00:13<00:02, 321.08 examples/s]Tokenizing train dataset:  82%|████████▏ | 4020/4890 [00:13<00:02, 324.84 examples/s]Tokenizing train dataset:  82%|████████▏ | 4034/4890 [00:13<00:02, 324.20 examples/s]Tokenizing train dataset:  83%|████████▎ | 4068/4890 [00:13<00:02, 314.15 examples/s]Tokenizing train dataset:  83%|████████▎ | 4067/4890 [00:13<00:02, 312.48 examples/s]Tokenizing train dataset:  84%|████████▍ | 4101/4890 [00:13<00:02, 313.59 examples/s]Tokenizing train dataset:  83%|████████▎ | 4080/4890 [00:13<00:02, 311.61 examples/s]Tokenizing train dataset:  84%|████████▍ | 4100/4890 [00:13<00:02, 312.15 examples/s]Tokenizing train dataset:  85%|████████▍ | 4143/4890 [00:13<00:02, 298.10 examples/s]Tokenizing train dataset:  84%|████████▍ | 4125/4890 [00:13<00:02, 302.63 examples/s]Tokenizing train dataset:  85%|████████▍ | 4141/4890 [00:13<00:02, 297.66 examples/s]Tokenizing train dataset:  85%|████████▌ | 4176/4890 [00:13<00:02, 299.87 examples/s]Tokenizing train dataset:  85%|████████▌ | 4170/4890 [00:13<00:02, 293.54 examples/s]Tokenizing train dataset:  87%|████████▋ | 4260/4890 [00:13<00:01, 435.77 examples/s]Tokenizing train dataset:  86%|████████▌ | 4186/4890 [00:13<00:02, 295.73 examples/s]Tokenizing train dataset:  87%|████████▋ | 4240/4890 [00:13<00:01, 382.91 examples/s]Tokenizing train dataset:  90%|████████▉ | 4390/4890 [00:14<00:00, 663.17 examples/s]Tokenizing train dataset:  88%|████████▊ | 4303/4890 [00:13<00:01, 499.41 examples/s]Tokenizing train dataset:  89%|████████▉ | 4369/4890 [00:14<00:00, 601.87 examples/s]Tokenizing train dataset:  92%|█████████▏| 4517/4890 [00:14<00:00, 827.61 examples/s]Tokenizing train dataset:  91%|█████████ | 4433/4890 [00:14<00:00, 699.31 examples/s]Tokenizing train dataset:  92%|█████████▏| 4497/4890 [00:14<00:00, 775.61 examples/s]Tokenizing train dataset:  95%|█████████▍| 4641/4890 [00:14<00:00, 940.88 examples/s]Tokenizing train dataset:  93%|█████████▎| 4559/4890 [00:14<00:00, 845.85 examples/s]Tokenizing train dataset:  94%|█████████▍| 4621/4890 [00:14<00:00, 898.19 examples/s]Tokenizing train dataset:  98%|█████████▊| 4769/4890 [00:14<00:00, 1035.16 examples/s]Tokenizing train dataset:  96%|█████████▌| 4679/4890 [00:14<00:00, 940.51 examples/s]Tokenizing train dataset:  97%|█████████▋| 4748/4890 [00:14<00:00, 999.39 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 338.98 examples/s] 
Tokenizing train dataset:  98%|█████████▊| 4807/4890 [00:14<00:00, 1035.31 examples/s]Tokenizing train dataset: 100%|█████████▉| 4876/4890 [00:14<00:00, 1077.56 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 338.15 examples/s] 
Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 338.10 examples/s] 
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  59%|█████▊    | 558/953 [00:00<00:00, 5542.04 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5537.97 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5530.41 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5534.29 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5524.92 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5475.23 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  33%|███▎      | 311/953 [00:00<00:00, 3073.99 examples/s]Applying chat template to eval dataset:  32%|███▏      | 306/953 [00:00<00:00, 3027.96 examples/s]Applying chat template to eval dataset:  32%|███▏      | 307/953 [00:00<00:00, 3025.25 examples/s]Applying chat template to eval dataset:  65%|██████▌   | 621/953 [00:00<00:00, 3080.56 examples/s]Applying chat template to eval dataset:  66%|██████▌   | 625/953 [00:00<00:00, 3112.62 examples/s]Applying chat template to eval dataset:  76%|███████▋  | 729/953 [00:00<00:00, 2879.52 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2951.96 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2964.74 examples/s]
Applying chat template to eval dataset:  99%|█████████▊| 941/953 [00:00<00:00, 3128.28 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3096.75 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2857.46 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 25/953 [00:00<00:03, 241.32 examples/s]Tokenizing eval dataset:   3%|▎         | 32/953 [00:00<00:02, 310.08 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   5%|▌         | 50/953 [00:00<00:03, 235.48 examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 258.45 examples/s]Tokenizing eval dataset:   8%|▊         | 72/953 [00:00<00:03, 274.50 examples/s]Tokenizing eval dataset:  11%|█         | 101/953 [00:00<00:03, 274.96 examples/s]Tokenizing eval dataset:   6%|▋         | 61/953 [00:00<00:03, 230.79 examples/s]Tokenizing eval dataset:   9%|▉         | 86/953 [00:00<00:03, 227.86 examples/s]Tokenizing eval dataset:   9%|▉         | 88/953 [00:00<00:03, 242.85 examples/s]Tokenizing eval dataset:  15%|█▍        | 142/953 [00:00<00:03, 270.10 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 214.40 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:03, 223.04 examples/s]Tokenizing eval dataset:  19%|█▊        | 177/953 [00:00<00:03, 252.55 examples/s]Tokenizing eval dataset:  15%|█▌        | 147/953 [00:00<00:03, 205.41 examples/s]Tokenizing eval dataset:  15%|█▌        | 143/953 [00:00<00:03, 223.33 examples/s]Tokenizing eval dataset:  21%|██▏       | 203/953 [00:00<00:02, 252.32 examples/s]Tokenizing eval dataset:  18%|█▊        | 176/953 [00:00<00:03, 196.37 examples/s]Tokenizing eval dataset:  25%|██▌       | 243/953 [00:00<00:02, 292.44 examples/s]Tokenizing eval dataset:  18%|█▊        | 172/953 [00:00<00:03, 209.86 examples/s]Tokenizing eval dataset:  32%|███▏      | 308/953 [00:00<00:01, 390.98 examples/s]Tokenizing eval dataset:  22%|██▏       | 207/953 [00:01<00:03, 196.11 examples/s]Tokenizing eval dataset:  21%|██▏       | 203/953 [00:00<00:03, 207.45 examples/s]Tokenizing eval dataset:  39%|███▊      | 369/953 [00:01<00:01, 449.66 examples/s]Tokenizing eval dataset:  25%|██▌       | 240/953 [00:01<00:03, 223.38 examples/s]Tokenizing eval dataset:  25%|██▍       | 235/953 [00:01<00:03, 233.67 examples/s]Tokenizing eval dataset:  46%|████▌     | 437/953 [00:01<00:01, 513.81 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:01<00:02, 295.79 examples/s]Tokenizing eval dataset:  30%|███       | 290/953 [00:01<00:02, 312.75 examples/s]Tokenizing eval dataset:  53%|█████▎    | 503/953 [00:01<00:00, 555.30 examples/s]Tokenizing eval dataset:  36%|███▌      | 339/953 [00:01<00:01, 335.81 examples/s]Tokenizing eval dataset:  36%|███▌      | 341/953 [00:01<00:01, 363.71 examples/s]Tokenizing eval dataset:  59%|█████▉    | 565/953 [00:01<00:00, 572.93 examples/s]Tokenizing eval dataset:  40%|████      | 384/953 [00:01<00:01, 365.43 examples/s]Tokenizing eval dataset:  41%|████      | 393/953 [00:01<00:01, 405.52 examples/s]Tokenizing eval dataset:  66%|██████▌   | 630/953 [00:01<00:00, 591.05 examples/s]Tokenizing eval dataset:  48%|████▊     | 456/953 [00:01<00:01, 460.97 examples/s]Tokenizing eval dataset:  49%|████▉     | 466/953 [00:01<00:00, 495.14 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 593.98 examples/s]Tokenizing eval dataset:  54%|█████▍    | 516/953 [00:01<00:00, 494.37 examples/s]Tokenizing eval dataset:  55%|█████▌    | 527/953 [00:01<00:00, 516.59 examples/s]Tokenizing eval dataset:  61%|██████    | 581/953 [00:01<00:00, 537.71 examples/s]Tokenizing eval dataset:  81%|████████  | 774/953 [00:01<00:00, 570.33 examples/s]Tokenizing eval dataset:  63%|██████▎   | 596/953 [00:01<00:00, 562.64 examples/s]Tokenizing eval dataset:  68%|██████▊   | 646/953 [00:01<00:00, 566.31 examples/s]Tokenizing eval dataset:  69%|██████▉   | 660/953 [00:01<00:00, 576.35 examples/s]Tokenizing eval dataset:  89%|████████▉ | 846/953 [00:01<00:00, 532.68 examples/s]Tokenizing eval dataset:  74%|███████▍  | 705/953 [00:01<00:00, 572.39 examples/s]Tokenizing eval dataset:  76%|███████▌  | 720/953 [00:01<00:00, 579.79 examples/s]Tokenizing eval dataset:  96%|█████████▌| 914/953 [00:02<00:00, 502.93 examples/s]Tokenizing eval dataset:  82%|████████▏ | 786/953 [00:02<00:00, 548.90 examples/s]Tokenizing eval dataset:  84%|████████▎ | 796/953 [00:02<00:00, 543.78 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 447.76 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  90%|█████████ | 862/953 [00:02<00:00, 524.78 examples/s]Tokenizing eval dataset:  91%|█████████ | 869/953 [00:02<00:00, 520.75 examples/s]Tokenizing eval dataset:  99%|█████████▊| 939/953 [00:02<00:00, 519.26 examples/s]Tokenizing eval dataset:  99%|█████████▉| 942/953 [00:02<00:00, 506.52 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 391.27 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 406.06 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4643850326538086 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.330336093902588 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.37558650970459 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3820412158966064 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank4]:[W610 02:06:46.824313560 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 1 ---
