cpu-bind=MASK - gn11, task  0  0 [261991]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 0 ---
Total Nodes: 2
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn11
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 2     --machine_rank 0     --main_process_ip gn11     --main_process_port 29500     --num_processes 8     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62080426     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train.py"     --rank=64 --learning_rate=3e-7 --total_epochs=3 --beta=0.2
-------------------------------------------
[2025-05-30 23:44:53,096] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0530 23:44:54.951000 262044 torch/distributed/run.py:792] 
W0530 23:44:54.951000 262044 torch/distributed/run.py:792] *****************************************
W0530 23:44:54.951000 262044 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0530 23:44:54.951000 262044 torch/distributed/run.py:792] *****************************************
[2025-05-30 23:45:00,379] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-30 23:45:00,441] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-30 23:45:00,454] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-30 23:45:00,459] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
World size: 8
Setting gradient accumulation steps to: 2
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Number of training examples: 8564
Number of validation examples: 953
Namespace(rank=64, learning_rate=3e-07, total_epochs=3, beta=0.2)
3e-07
Created datasets
Steps per epoch: 4282
Eval steps: 2141
[2025-05-30 23:45:03,680] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-30 23:45:03,680] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-30 23:45:03,681] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-30 23:45:03,684] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-05-30 23:45:03,684] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
[2025-05-30 23:45:05,539] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-30 23:45:05,539] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-30 23:45:05,539] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
[2025-05-30 23:45:05,539] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 8
hpZeRO group size: 4
[2025-05-30 23:45:20,516] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 465, num_elems = 10.16B
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.57s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.58s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.54s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:48, 16.27s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:21, 10.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:21, 10.68s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:19<00:21, 10.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:27<00:26, 13.35s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.85s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:39<00:12, 12.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00, 10.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00, 10.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00, 10.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00,  9.92s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00,  9.93s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00,  9.93s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00, 11.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:47<00:00, 11.99s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
Total Parameters: 216.07M
Total Parameters: 216.07M
Total Parameters: 216.07M
Trainable Parameters (LoRA): 216.07M
Trainable Parameters (LoRA): 216.07M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 100.0000%
Percentage of Trainable Params: 100.0000%
Percentage of Trainable Params: 100.0000%
Total Parameters: 216.07M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 100.0000%
Using LoRA and set up the model
-------------------- CHECKING GRADIENTS --------------------
Trainable parameters:
- base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.40.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.40.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.40.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.40.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.40.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.40.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.40.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.40.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.40.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.40.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.40.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.40.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.40.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.40.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.41.self_attn.q_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.41.self_attn.q_proj.lora_B.default.weight (shape: torch.Size([4096, 64]), numel: 262144)
- base_model.model.model.layers.41.self_attn.k_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.41.self_attn.k_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.41.self_attn.v_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.41.self_attn.v_proj.lora_B.default.weight (shape: torch.Size([2048, 64]), numel: 131072)
- base_model.model.model.layers.41.self_attn.o_proj.lora_A.default.weight (shape: torch.Size([64, 4096]), numel: 262144)
- base_model.model.model.layers.41.self_attn.o_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
- base_model.model.model.layers.41.mlp.gate_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.41.mlp.gate_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.41.mlp.up_proj.lora_A.default.weight (shape: torch.Size([64, 3584]), numel: 229376)
- base_model.model.model.layers.41.mlp.up_proj.lora_B.default.weight (shape: torch.Size([14336, 64]), numel: 917504)
- base_model.model.model.layers.41.mlp.down_proj.lora_A.default.weight (shape: torch.Size([64, 14336]), numel: 917504)
- base_model.model.model.layers.41.mlp.down_proj.lora_B.default.weight (shape: torch.Size([3584, 64]), numel: 229376)
Total trainable parameters: 216072192
------------------------------------------------------------
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   6%|▋         | 550/8564 [00:00<00:01, 5456.46 examples/s]Extracting prompt in train dataset:  16%|█▌        | 1360/8564 [00:00<00:01, 5378.82 examples/s]Extracting prompt in train dataset:  25%|██▍       | 2137/8564 [00:00<00:01, 5270.57 examples/s]Extracting prompt in train dataset:  31%|███▏      | 2690/8564 [00:00<00:01, 5339.58 examples/s]Extracting prompt in train dataset:  41%|████      | 3480/8564 [00:00<00:00, 5301.53 examples/s]Extracting prompt in train dataset:  48%|████▊     | 4108/8564 [00:00<00:00, 4892.24 examples/s]Extracting prompt in train dataset:  54%|█████▍    | 4656/8564 [00:00<00:00, 5032.55 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 5301/8564 [00:01<00:00, 4769.78 examples/s]Extracting prompt in train dataset:  68%|██████▊   | 5820/8564 [00:01<00:00, 4863.32 examples/s]Extracting prompt in train dataset:  75%|███████▍  | 6381/8564 [00:01<00:00, 5054.68 examples/s]Extracting prompt in train dataset:  81%|████████  | 6910/8564 [00:01<00:00, 5112.16 examples/s]Extracting prompt in train dataset:  87%|████████▋ | 7480/8564 [00:01<00:00, 5275.74 examples/s]Extracting prompt in train dataset:  96%|█████████▌| 8220/8564 [00:01<00:00, 5134.80 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5053.01 examples/s]
Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   3%|▎         | 290/8564 [00:00<00:02, 2869.84 examples/s]Applying chat template to train dataset:   7%|▋         | 596/8564 [00:00<00:02, 2973.83 examples/s]Applying chat template to train dataset:  11%|█         | 920/8564 [00:00<00:03, 2492.37 examples/s]Applying chat template to train dataset:  14%|█▍        | 1190/8564 [00:00<00:02, 2559.70 examples/s]Applying chat template to train dataset:  18%|█▊        | 1534/8564 [00:00<00:02, 2443.05 examples/s]Applying chat template to train dataset:  21%|██        | 1814/8564 [00:00<00:02, 2544.28 examples/s]Applying chat template to train dataset:  24%|██▍       | 2075/8564 [00:00<00:02, 2563.02 examples/s]Applying chat template to train dataset:  28%|██▊       | 2364/8564 [00:00<00:02, 2656.00 examples/s]Applying chat template to train dataset:  31%|███       | 2641/8564 [00:01<00:02, 2686.65 examples/s]Applying chat template to train dataset:  34%|███▍      | 2940/8564 [00:01<00:02, 2774.43 examples/s]Applying chat template to train dataset:  38%|███▊      | 3230/8564 [00:01<00:01, 2807.11 examples/s]Applying chat template to train dataset:  41%|████▏     | 3543/8564 [00:01<00:01, 2900.79 examples/s]Applying chat template to train dataset:  45%|████▌     | 3855/8564 [00:01<00:01, 2962.71 examples/s]Applying chat template to train dataset:  49%|████▊     | 4155/8564 [00:01<00:01, 2966.60 examples/s]Applying chat template to train dataset:  52%|█████▏    | 4493/8564 [00:01<00:01, 2684.70 examples/s]Applying chat template to train dataset:  56%|█████▌    | 4780/8564 [00:01<00:01, 2729.59 examples/s]Applying chat template to train dataset:  60%|█████▉    | 5100/8564 [00:01<00:01, 2853.02 examples/s]Applying chat template to train dataset:  64%|██████▎   | 5439/8564 [00:02<00:01, 2147.06 examples/s]Applying chat template to train dataset:  67%|██████▋   | 5753/8564 [00:02<00:01, 2369.11 examples/s]Applying chat template to train dataset:  70%|███████   | 6020/8564 [00:02<00:01, 2436.89 examples/s]Applying chat template to train dataset:  74%|███████▍  | 6330/8564 [00:02<00:00, 2602.55 examples/s]Applying chat template to train dataset:  79%|███████▊  | 6744/8564 [00:02<00:00, 2656.68 examples/s]Applying chat template to train dataset:  83%|████████▎ | 7081/8564 [00:02<00:00, 2448.83 examples/s]Applying chat template to train dataset:  86%|████████▋ | 7397/8564 [00:02<00:00, 2616.49 examples/s]Applying chat template to train dataset:  90%|█████████ | 7735/8564 [00:02<00:00, 2489.31 examples/s]Applying chat template to train dataset:  95%|█████████▌| 8140/8564 [00:03<00:00, 2554.64 examples/s]Applying chat template to train dataset:  99%|█████████▊| 8450/8564 [00:03<00:00, 2680.43 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:03<00:00, 2619.07 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 41/8564 [00:00<00:21, 398.32 examples/s]Tokenizing train dataset:   1%|          | 87/8564 [00:00<00:26, 323.77 examples/s]Tokenizing train dataset:   1%|▏         | 128/8564 [00:00<00:28, 292.16 examples/s]Tokenizing train dataset:   2%|▏         | 171/8564 [00:00<00:29, 286.42 examples/s]Tokenizing train dataset:   3%|▎         | 217/8564 [00:00<00:29, 287.71 examples/s]Tokenizing train dataset:   3%|▎         | 249/8564 [00:00<00:28, 293.25 examples/s]Tokenizing train dataset:   3%|▎         | 282/8564 [00:00<00:27, 300.61 examples/s]Tokenizing train dataset:   4%|▎         | 313/8564 [00:01<00:27, 298.53 examples/s]Tokenizing train dataset:   4%|▍         | 359/8564 [00:01<00:27, 296.87 examples/s]Tokenizing train dataset:   5%|▍         | 389/8564 [00:01<00:27, 296.92 examples/s]Tokenizing train dataset:   5%|▌         | 433/8564 [00:01<00:27, 291.20 examples/s]Tokenizing train dataset:   6%|▌         | 476/8564 [00:01<00:28, 286.22 examples/s]Tokenizing train dataset:   6%|▌         | 510/8564 [00:01<00:30, 261.57 examples/s]Tokenizing train dataset:   6%|▋         | 540/8564 [00:01<00:29, 268.62 examples/s]Tokenizing train dataset:   7%|▋         | 569/8564 [00:01<00:29, 272.93 examples/s]Tokenizing train dataset:   7%|▋         | 597/8564 [00:02<00:29, 271.64 examples/s]Tokenizing train dataset:   8%|▊         | 643/8564 [00:02<00:28, 279.18 examples/s]Tokenizing train dataset:   8%|▊         | 681/8564 [00:02<00:29, 264.06 examples/s]Tokenizing train dataset:   8%|▊         | 719/8564 [00:02<00:30, 257.21 examples/s]Tokenizing train dataset:   9%|▉         | 759/8564 [00:02<00:30, 256.90 examples/s]Tokenizing train dataset:   9%|▉         | 795/8564 [00:02<00:31, 248.24 examples/s]Tokenizing train dataset:  10%|▉         | 824/8564 [00:02<00:30, 253.69 examples/s]Tokenizing train dataset:  10%|█         | 860/8564 [00:03<00:31, 246.14 examples/s]Tokenizing train dataset:  10%|█         | 894/8564 [00:03<00:28, 265.59 examples/s]Tokenizing train dataset:  11%|█         | 922/8564 [00:03<00:28, 266.66 examples/s]Tokenizing train dataset:  11%|█         | 951/8564 [00:03<00:28, 270.72 examples/s]Tokenizing train dataset:  12%|█▏        | 991/8564 [00:03<00:28, 262.20 examples/s]Tokenizing train dataset:  12%|█▏        | 1018/8564 [00:03<00:28, 263.39 examples/s]Tokenizing train dataset:  12%|█▏        | 1061/8564 [00:03<00:28, 267.19 examples/s]Tokenizing train dataset:  13%|█▎        | 1090/8564 [00:03<00:27, 271.51 examples/s]Tokenizing train dataset:  13%|█▎        | 1120/8564 [00:04<00:27, 272.62 examples/s]Tokenizing train dataset:  14%|█▎        | 1161/8564 [00:04<00:27, 267.03 examples/s]Tokenizing train dataset:  14%|█▍        | 1188/8564 [00:04<00:27, 264.04 examples/s]Tokenizing train dataset:  14%|█▍        | 1222/8564 [00:04<00:29, 249.19 examples/s]Tokenizing train dataset:  15%|█▍        | 1250/8564 [00:04<00:29, 251.77 examples/s]Tokenizing train dataset:  15%|█▍        | 1277/8564 [00:04<00:28, 254.78 examples/s]Tokenizing train dataset:  15%|█▌        | 1305/8564 [00:04<00:27, 260.07 examples/s]Tokenizing train dataset:  16%|█▌        | 1338/8564 [00:04<00:26, 273.37 examples/s]Tokenizing train dataset:  16%|█▌        | 1376/8564 [00:05<00:27, 262.19 examples/s]Tokenizing train dataset:  16%|█▋        | 1407/8564 [00:05<00:26, 268.98 examples/s]Tokenizing train dataset:  17%|█▋        | 1435/8564 [00:05<00:26, 270.34 examples/s]Tokenizing train dataset:  17%|█▋        | 1464/8564 [00:05<00:25, 273.11 examples/s]Tokenizing train dataset:  17%|█▋        | 1492/8564 [00:05<00:26, 271.66 examples/s]Tokenizing train dataset:  18%|█▊        | 1535/8564 [00:05<00:25, 273.15 examples/s]Tokenizing train dataset:  18%|█▊        | 1563/8564 [00:05<00:25, 273.42 examples/s]Tokenizing train dataset:  19%|█▊        | 1600/8564 [00:05<00:26, 258.40 examples/s]Tokenizing train dataset:  19%|█▉        | 1631/8564 [00:06<00:25, 269.09 examples/s]Tokenizing train dataset:  19%|█▉        | 1661/8564 [00:06<00:25, 273.06 examples/s]Tokenizing train dataset:  20%|█▉        | 1698/8564 [00:06<00:22, 298.66 examples/s]Tokenizing train dataset:  20%|██        | 1743/8564 [00:06<00:23, 293.30 examples/s]Tokenizing train dataset:  21%|██        | 1776/8564 [00:06<00:22, 298.43 examples/s]Tokenizing train dataset:  21%|██        | 1815/8564 [00:06<00:23, 283.52 examples/s]Tokenizing train dataset:  22%|██▏       | 1845/8564 [00:06<00:23, 286.60 examples/s]Tokenizing train dataset:  22%|██▏       | 1883/8564 [00:06<00:24, 271.93 examples/s]Tokenizing train dataset:  22%|██▏       | 1923/8564 [00:06<00:22, 301.59 examples/s]Tokenizing train dataset:  23%|██▎       | 1972/8564 [00:07<00:21, 307.60 examples/s]Tokenizing train dataset:  23%|██▎       | 2008/8564 [00:07<00:20, 318.45 examples/s]Tokenizing train dataset:  24%|██▍       | 2046/8564 [00:07<00:19, 331.73 examples/s]Tokenizing train dataset:  24%|██▍       | 2088/8564 [00:07<00:20, 313.17 examples/s]Tokenizing train dataset:  25%|██▍       | 2139/8564 [00:07<00:20, 317.70 examples/s]Tokenizing train dataset:  26%|██▌       | 2187/8564 [00:07<00:20, 315.81 examples/s]Tokenizing train dataset:  26%|██▌       | 2235/8564 [00:07<00:20, 313.71 examples/s]Tokenizing train dataset:  27%|██▋       | 2270/8564 [00:08<00:19, 319.38 examples/s]Tokenizing train dataset:  27%|██▋       | 2317/8564 [00:08<00:19, 314.43 examples/s]Tokenizing train dataset:  27%|██▋       | 2349/8564 [00:08<00:19, 313.43 examples/s]Tokenizing train dataset:  28%|██▊       | 2401/8564 [00:08<00:19, 322.85 examples/s]Tokenizing train dataset:  28%|██▊       | 2435/8564 [00:08<00:18, 326.19 examples/s]Tokenizing train dataset:  29%|██▉       | 2483/8564 [00:08<00:18, 322.09 examples/s]Tokenizing train dataset:  29%|██▉       | 2523/8564 [00:08<00:20, 300.10 examples/s]Tokenizing train dataset:  30%|██▉       | 2560/8564 [00:08<00:19, 314.29 examples/s]Tokenizing train dataset:  30%|███       | 2610/8564 [00:09<00:18, 314.88 examples/s]Tokenizing train dataset:  31%|███       | 2652/8564 [00:09<00:19, 301.74 examples/s]Tokenizing train dataset:  31%|███▏      | 2685/8564 [00:09<00:19, 304.38 examples/s]Tokenizing train dataset:  32%|███▏      | 2716/8564 [00:09<00:19, 303.72 examples/s]Tokenizing train dataset:  32%|███▏      | 2760/8564 [00:09<00:19, 296.25 examples/s]Tokenizing train dataset:  33%|███▎      | 2792/8564 [00:09<00:19, 298.20 examples/s]Tokenizing train dataset:  33%|███▎      | 2830/8564 [00:09<00:18, 313.60 examples/s]Tokenizing train dataset:  34%|███▎      | 2877/8564 [00:10<00:18, 310.60 examples/s]Tokenizing train dataset:  34%|███▍      | 2914/8564 [00:10<00:17, 320.83 examples/s]Tokenizing train dataset:  34%|███▍      | 2953/8564 [00:10<00:16, 335.05 examples/s]Tokenizing train dataset:  35%|███▍      | 2994/8564 [00:10<00:15, 352.31 examples/s]Tokenizing train dataset:  35%|███▌      | 3040/8564 [00:10<00:16, 330.82 examples/s]Tokenizing train dataset:  36%|███▌      | 3090/8564 [00:10<00:16, 325.34 examples/s]Tokenizing train dataset:  37%|███▋      | 3137/8564 [00:10<00:16, 320.07 examples/s]Tokenizing train dataset:  37%|███▋      | 3170/8564 [00:10<00:16, 319.26 examples/s]Tokenizing train dataset:  38%|███▊      | 3213/8564 [00:11<00:17, 305.84 examples/s]Tokenizing train dataset:  38%|███▊      | 3252/8564 [00:11<00:16, 323.71 examples/s]Tokenizing train dataset:  38%|███▊      | 3286/8564 [00:11<00:16, 325.81 examples/s]Tokenizing train dataset:  39%|███▉      | 3330/8564 [00:11<00:16, 309.80 examples/s]Tokenizing train dataset:  39%|███▉      | 3367/8564 [00:11<00:16, 323.11 examples/s]Tokenizing train dataset:  40%|███▉      | 3405/8564 [00:11<00:15, 335.63 examples/s]Tokenizing train dataset:  40%|████      | 3452/8564 [00:11<00:15, 323.71 examples/s]Tokenizing train dataset:  41%|████      | 3488/8564 [00:11<00:15, 326.18 examples/s]Tokenizing train dataset:  41%|████      | 3528/8564 [00:12<00:14, 343.94 examples/s]Tokenizing train dataset:  42%|████▏     | 3578/8564 [00:12<00:14, 337.28 examples/s]Tokenizing train dataset:  42%|████▏     | 3631/8564 [00:12<00:14, 335.99 examples/s]Tokenizing train dataset:  43%|████▎     | 3674/8564 [00:12<00:15, 317.91 examples/s]Tokenizing train dataset:  43%|████▎     | 3708/8564 [00:12<00:15, 321.16 examples/s]Tokenizing train dataset:  44%|████▎     | 3746/8564 [00:12<00:14, 333.56 examples/s]Tokenizing train dataset:  44%|████▍     | 3790/8564 [00:12<00:15, 316.39 examples/s]Tokenizing train dataset:  45%|████▍     | 3823/8564 [00:12<00:14, 317.85 examples/s]Tokenizing train dataset:  45%|████▌     | 3873/8564 [00:13<00:14, 320.30 examples/s]Tokenizing train dataset:  46%|████▌     | 3909/8564 [00:13<00:16, 289.48 examples/s]Tokenizing train dataset:  46%|████▌     | 3943/8564 [00:13<00:15, 297.16 examples/s]Tokenizing train dataset:  46%|████▋     | 3975/8564 [00:13<00:15, 300.24 examples/s]Tokenizing train dataset:  47%|████▋     | 4010/8564 [00:13<00:14, 311.19 examples/s]Tokenizing train dataset:  47%|████▋     | 4046/8564 [00:13<00:14, 322.40 examples/s]Tokenizing train dataset:  48%|████▊     | 4093/8564 [00:13<00:14, 315.57 examples/s]Tokenizing train dataset:  48%|████▊     | 4137/8564 [00:13<00:14, 302.38 examples/s]Tokenizing train dataset:  49%|████▉     | 4186/8564 [00:14<00:14, 308.80 examples/s]Tokenizing train dataset:  49%|████▉     | 4218/8564 [00:14<00:13, 310.88 examples/s]Tokenizing train dataset:  50%|████▉     | 4250/8564 [00:14<00:13, 310.60 examples/s]Tokenizing train dataset:  50%|█████     | 4283/8564 [00:14<00:13, 313.92 examples/s]Tokenizing train dataset:  50%|█████     | 4315/8564 [00:14<00:13, 313.77 examples/s]Tokenizing train dataset:  51%|█████     | 4361/8564 [00:14<00:13, 303.76 examples/s]Tokenizing train dataset:  51%|█████▏    | 4398/8564 [00:14<00:13, 316.63 examples/s]Tokenizing train dataset:  52%|█████▏    | 4448/8564 [00:14<00:12, 318.78 examples/s]Tokenizing train dataset:  52%|█████▏    | 4484/8564 [00:15<00:12, 324.93 examples/s]Tokenizing train dataset:  53%|█████▎    | 4536/8564 [00:15<00:12, 326.02 examples/s]Tokenizing train dataset:  53%|█████▎    | 4570/8564 [00:15<00:12, 328.50 examples/s]Tokenizing train dataset:  54%|█████▍    | 4622/8564 [00:15<00:11, 331.15 examples/s]Tokenizing train dataset:  54%|█████▍    | 4662/8564 [00:15<00:12, 305.25 examples/s]Tokenizing train dataset:  55%|█████▍    | 4706/8564 [00:15<00:12, 298.26 examples/s]Tokenizing train dataset:  55%|█████▌    | 4738/8564 [00:15<00:12, 300.92 examples/s]Tokenizing train dataset:  56%|█████▌    | 4780/8564 [00:16<00:12, 292.07 examples/s]Tokenizing train dataset:  56%|█████▋    | 4829/8564 [00:16<00:11, 337.71 examples/s]Tokenizing train dataset:  57%|█████▋    | 4880/8564 [00:16<00:09, 380.39 examples/s]Tokenizing train dataset:  58%|█████▊    | 4936/8564 [00:16<00:08, 426.04 examples/s]Tokenizing train dataset:  58%|█████▊    | 4989/8564 [00:16<00:07, 452.68 examples/s]Tokenizing train dataset:  59%|█████▉    | 5040/8564 [00:16<00:07, 467.34 examples/s]Tokenizing train dataset:  60%|█████▉    | 5098/8564 [00:16<00:07, 494.38 examples/s]Tokenizing train dataset:  60%|██████    | 5160/8564 [00:16<00:06, 526.12 examples/s]Tokenizing train dataset:  61%|██████    | 5223/8564 [00:16<00:06, 553.19 examples/s]Tokenizing train dataset:  62%|██████▏   | 5303/8564 [00:17<00:06, 540.92 examples/s]Tokenizing train dataset:  63%|██████▎   | 5359/8564 [00:17<00:05, 540.55 examples/s]Tokenizing train dataset:  63%|██████▎   | 5433/8564 [00:17<00:06, 519.19 examples/s]Tokenizing train dataset:  64%|██████▍   | 5507/8564 [00:17<00:06, 505.76 examples/s]Tokenizing train dataset:  65%|██████▍   | 5560/8564 [00:17<00:05, 506.04 examples/s]Tokenizing train dataset:  66%|██████▌   | 5645/8564 [00:17<00:05, 520.17 examples/s]Tokenizing train dataset:  67%|██████▋   | 5710/8564 [00:17<00:05, 547.34 examples/s]Tokenizing train dataset:  68%|██████▊   | 5802/8564 [00:17<00:04, 568.46 examples/s]Tokenizing train dataset:  69%|██████▊   | 5876/8564 [00:18<00:04, 539.79 examples/s]Tokenizing train dataset:  69%|██████▉   | 5940/8564 [00:18<00:04, 560.99 examples/s]Tokenizing train dataset:  70%|███████   | 6019/8564 [00:18<00:04, 541.65 examples/s]Tokenizing train dataset:  71%|███████   | 6091/8564 [00:18<00:04, 515.11 examples/s]Tokenizing train dataset:  72%|███████▏  | 6150/8564 [00:18<00:04, 528.55 examples/s]Tokenizing train dataset:  73%|███████▎  | 6234/8564 [00:18<00:04, 531.80 examples/s]Tokenizing train dataset:  74%|███████▎  | 6295/8564 [00:18<00:04, 546.19 examples/s]Tokenizing train dataset:  74%|███████▍  | 6366/8564 [00:18<00:03, 586.03 examples/s]Tokenizing train dataset:  75%|███████▌  | 6457/8564 [00:19<00:03, 586.13 examples/s]Tokenizing train dataset:  76%|███████▌  | 6526/8564 [00:19<00:03, 538.35 examples/s]Tokenizing train dataset:  77%|███████▋  | 6603/8564 [00:19<00:03, 525.98 examples/s]Tokenizing train dataset:  78%|███████▊  | 6688/8564 [00:19<00:03, 536.63 examples/s]Tokenizing train dataset:  79%|███████▉  | 6753/8564 [00:19<00:03, 560.61 examples/s]Tokenizing train dataset:  80%|███████▉  | 6833/8564 [00:19<00:03, 549.09 examples/s]Tokenizing train dataset:  80%|████████  | 6892/8564 [00:19<00:03, 556.33 examples/s]Tokenizing train dataset:  81%|████████  | 6950/8564 [00:20<00:02, 552.60 examples/s]Tokenizing train dataset:  82%|████████▏ | 7020/8564 [00:20<00:02, 517.38 examples/s]Tokenizing train dataset:  83%|████████▎ | 7089/8564 [00:20<00:02, 557.29 examples/s]Tokenizing train dataset:  84%|████████▍ | 7180/8564 [00:20<00:02, 566.93 examples/s]Tokenizing train dataset:  85%|████████▍ | 7253/8564 [00:20<00:02, 537.04 examples/s]Tokenizing train dataset:  86%|████████▌ | 7329/8564 [00:20<00:02, 525.92 examples/s]Tokenizing train dataset:  86%|████████▋ | 7387/8564 [00:20<00:02, 535.37 examples/s]Tokenizing train dataset:  87%|████████▋ | 7447/8564 [00:20<00:02, 547.98 examples/s]Tokenizing train dataset:  88%|████████▊ | 7506/8564 [00:21<00:01, 555.97 examples/s]Tokenizing train dataset:  89%|████████▊ | 7582/8564 [00:21<00:01, 517.34 examples/s]Tokenizing train dataset:  89%|████████▉ | 7659/8564 [00:21<00:01, 507.29 examples/s]Tokenizing train dataset:  90%|█████████ | 7713/8564 [00:21<00:01, 514.13 examples/s]Tokenizing train dataset:  91%|█████████ | 7790/8564 [00:21<00:01, 507.45 examples/s]Tokenizing train dataset:  92%|█████████▏| 7850/8564 [00:21<00:01, 527.10 examples/s]Tokenizing train dataset:  92%|█████████▏| 7907/8564 [00:21<00:01, 537.34 examples/s]Tokenizing train dataset:  93%|█████████▎| 7996/8564 [00:21<00:01, 553.62 examples/s]Tokenizing train dataset:  94%|█████████▍| 8055/8564 [00:22<00:00, 559.25 examples/s]Tokenizing train dataset:  95%|█████████▍| 8134/8564 [00:22<00:00, 543.80 examples/s]Tokenizing train dataset:  96%|█████████▌| 8209/8564 [00:22<00:00, 520.07 examples/s]Tokenizing train dataset:  97%|█████████▋| 8271/8564 [00:22<00:00, 541.22 examples/s]Tokenizing train dataset:  97%|█████████▋| 8329/8564 [00:22<00:00, 549.33 examples/s]Tokenizing train dataset:  98%|█████████▊| 8397/8564 [00:22<00:00, 513.81 examples/s]Tokenizing train dataset:  99%|█████████▉| 8464/8564 [00:22<00:00, 488.91 examples/s]Tokenizing train dataset: 100%|█████████▉| 8541/8564 [00:23<00:00, 493.41 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:23<00:00, 370.13 examples/s]
Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   7%|▋         | 566/8564 [00:00<00:01, 5567.71 examples/s]Extracting prompt in train dataset:   7%|▋         | 570/8564 [00:00<00:01, 5587.05 examples/s]Extracting prompt in train dataset:   6%|▋         | 553/8564 [00:00<00:01, 5479.98 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 9909.76 examples/s]
Extracting prompt in train dataset:  15%|█▌        | 1320/8564 [00:00<00:01, 5171.92 examples/s]Extracting prompt in train dataset:  16%|█▌        | 1341/8564 [00:00<00:01, 5280.02 examples/s]Extracting prompt in train dataset:  14%|█▍        | 1190/8564 [00:00<00:01, 4619.49 examples/s]Extracting prompt in train dataset:  22%|██▏       | 1850/8564 [00:00<00:01, 5210.99 examples/s]Extracting prompt in train dataset:  22%|██▏       | 1880/8564 [00:00<00:01, 5299.69 examples/s]Extracting prompt in train dataset:  21%|██        | 1818/8564 [00:00<00:01, 4396.41 examples/s]Extracting prompt in train dataset:  28%|██▊       | 2400/8564 [00:00<00:01, 5313.33 examples/s]Extracting prompt in train dataset:  31%|███▏      | 2681/8564 [00:00<00:01, 5301.84 examples/s]Extracting prompt in train dataset:  28%|██▊       | 2370/8564 [00:00<00:01, 4744.82 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  36%|███▌      | 3085/8564 [00:00<00:01, 4982.68 examples/s]Extracting prompt in train dataset:  34%|███▍      | 2926/8564 [00:00<00:01, 4991.88 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 12657.69 examples/s]
Extracting prompt in train dataset:  41%|████      | 3488/8564 [00:00<00:00, 5325.97 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3630/8564 [00:00<00:00, 5116.78 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3630/8564 [00:00<00:01, 4835.83 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4180/8564 [00:00<00:00, 5208.74 examples/s]Extracting prompt in train dataset:  50%|████▉     | 4270/8564 [00:00<00:00, 5283.54 examples/s]Extracting prompt in train dataset:  49%|████▉     | 4186/8564 [00:00<00:00, 5022.60 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 4850/8564 [00:00<00:00, 4902.81 examples/s]Extracting prompt in train dataset:  58%|█████▊    | 4966/8564 [00:00<00:00, 5061.11 examples/s]Extracting prompt in train dataset:  58%|█████▊    | 4960/8564 [00:01<00:00, 5067.03 examples/s]Extracting prompt in train dataset:  63%|██████▎   | 5390/8564 [00:01<00:00, 5013.98 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  67%|██████▋   | 5740/8564 [00:01<00:00, 5078.98 examples/s]Extracting prompt in train dataset:  66%|██████▌   | 5658/8564 [00:01<00:00, 4922.65 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 324.54 examples/s]Extracting prompt in train dataset:  72%|███████▏  | 6160/8564 [00:01<00:00, 5033.76 examples/s]Extracting prompt in train dataset:  74%|███████▎  | 6310/8564 [00:01<00:00, 5224.04 examples/s]Extracting prompt in train dataset:  72%|███████▏  | 6201/8564 [00:01<00:00, 5036.99 examples/s]Extracting prompt in train dataset:  79%|███████▊  | 6730/8564 [00:01<00:00, 5183.94 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 285.97 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 7100/8564 [00:01<00:00, 5217.10 examples/s]Extracting prompt in train dataset:  78%|███████▊  | 6721/8564 [00:01<00:00, 5078.56 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 7350/8564 [00:01<00:00, 4795.53 examples/s]Extracting prompt in train dataset:  85%|████████▍ | 7240/8564 [00:01<00:00, 5102.54 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 271.62 examples/s]Extracting prompt in train dataset:  91%|█████████ | 7790/8564 [00:01<00:00, 5008.60 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 8360/8564 [00:01<00:00, 5170.33 examples/s]Extracting prompt in train dataset:  94%|█████████▎| 8027/8564 [00:01<00:00, 4607.28 examples/s]Extracting prompt in train dataset:  92%|█████████▏| 7881/8564 [00:01<00:00, 4788.44 examples/s]Tokenizing eval dataset:  16%|█▋        | 156/953 [00:00<00:03, 260.46 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 5122.67 examples/s]
Extracting prompt in train dataset:  99%|█████████▉| 8520/8564 [00:01<00:00, 4672.20 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 8420/8564 [00:01<00:00, 4933.31 examples/s]Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 4879.71 examples/s]
Extracting prompt in train dataset: 100%|██████████| 8564/8564 [00:01<00:00, 4876.32 examples/s]
Tokenizing eval dataset:  20%|█▉        | 189/953 [00:00<00:03, 244.64 examples/s]Tokenizing eval dataset:  23%|██▎       | 216/953 [00:00<00:02, 248.94 examples/s]Tokenizing eval dataset:  26%|██▌       | 245/953 [00:00<00:02, 245.94 examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing eval dataset:  32%|███▏      | 301/953 [00:01<00:01, 326.12 examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Applying chat template to train dataset:   3%|▎         | 286/8564 [00:00<00:02, 2823.66 examples/s]Applying chat template to train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 381.58 examples/s]Applying chat template to train dataset:   3%|▎         | 292/8564 [00:00<00:02, 2892.99 examples/s]Applying chat template to train dataset:   3%|▎         | 288/8564 [00:00<00:02, 2837.42 examples/s]Applying chat template to train dataset:   8%|▊         | 721/8564 [00:00<00:02, 2869.07 examples/s]Tokenizing eval dataset:  43%|████▎     | 414/953 [00:01<00:01, 438.50 examples/s]Applying chat template to train dataset:   7%|▋         | 597/8564 [00:00<00:02, 2982.87 examples/s]Applying chat template to train dataset:   7%|▋         | 587/8564 [00:00<00:02, 2919.20 examples/s]Applying chat template to train dataset:  12%|█▏        | 1018/8564 [00:00<00:02, 2903.29 examples/s]Tokenizing eval dataset:  50%|█████     | 480/953 [00:01<00:00, 497.42 examples/s]Applying chat template to train dataset:  12%|█▏        | 1010/8564 [00:00<00:02, 2849.04 examples/s]Applying chat template to train dataset:  15%|█▌        | 1316/8564 [00:00<00:02, 2928.72 examples/s]Tokenizing eval dataset:  57%|█████▋    | 546/953 [00:01<00:00, 542.06 examples/s]Applying chat template to train dataset:  12%|█▏        | 1030/8564 [00:00<00:02, 2931.92 examples/s]Applying chat template to train dataset:  15%|█▌        | 1305/8564 [00:00<00:02, 2879.90 examples/s]Applying chat template to train dataset:  19%|█▉        | 1610/8564 [00:00<00:02, 2924.36 examples/s]Tokenizing eval dataset:  64%|██████▍   | 610/953 [00:01<00:00, 568.45 examples/s]Applying chat template to train dataset:  16%|█▌        | 1333/8564 [00:00<00:02, 2961.83 examples/s]Applying chat template to train dataset:  19%|█▊        | 1604/8564 [00:00<00:02, 2913.34 examples/s]Applying chat template to train dataset:  22%|██▏       | 1909/8564 [00:00<00:02, 2941.81 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 554.83 examples/s]Applying chat template to train dataset:  20%|██        | 1747/8564 [00:00<00:02, 2869.47 examples/s]Applying chat template to train dataset:  22%|██▏       | 1902/8564 [00:00<00:02, 2933.71 examples/s]Applying chat template to train dataset:  28%|██▊       | 2361/8564 [00:00<00:02, 2964.63 examples/s]Applying chat template to train dataset:  24%|██▍       | 2037/8564 [00:00<00:02, 2874.27 examples/s]Tokenizing eval dataset:  81%|████████  | 771/953 [00:01<00:00, 544.36 examples/s]Applying chat template to train dataset:  26%|██▋       | 2267/8564 [00:00<00:02, 2727.84 examples/s]Applying chat template to train dataset:  31%|███       | 2660/8564 [00:00<00:01, 2967.43 examples/s]Applying chat template to train dataset:  27%|██▋       | 2340/8564 [00:00<00:02, 2918.09 examples/s]Applying chat template to train dataset:  30%|███       | 2574/8564 [00:00<00:02, 2821.60 examples/s]Applying chat template to train dataset:  35%|███▍      | 2960/8564 [00:01<00:01, 2969.48 examples/s]Tokenizing eval dataset:  88%|████████▊ | 841/953 [00:02<00:00, 514.53 examples/s]Applying chat template to train dataset:  33%|███▎      | 2785/8564 [00:00<00:01, 2933.45 examples/s]Applying chat template to train dataset:  35%|███▍      | 2983/8564 [00:01<00:02, 2784.79 examples/s]Applying chat template to train dataset:  40%|███▉      | 3389/8564 [00:01<00:01, 2921.32 examples/s]Tokenizing eval dataset:  95%|█████████▌| 908/953 [00:02<00:00, 488.45 examples/s]Applying chat template to train dataset:  38%|███▊      | 3236/8564 [00:01<00:01, 2955.12 examples/s]Applying chat template to train dataset:  43%|████▎     | 3685/8564 [00:01<00:01, 2928.32 examples/s]Applying chat template to train dataset:  40%|███▉      | 3418/8564 [00:01<00:01, 2822.60 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 413.27 examples/s]
Applying chat template to train dataset:  43%|████▎     | 3652/8564 [00:01<00:01, 2891.19 examples/s]Applying chat template to train dataset:  47%|████▋     | 4031/8564 [00:01<00:01, 2697.72 examples/s]Applying chat template to train dataset:  44%|████▍     | 3772/8564 [00:01<00:01, 2663.93 examples/s]Applying chat template to train dataset:  51%|█████     | 4336/8564 [00:01<00:01, 2785.27 examples/s]Applying chat template to train dataset:  48%|████▊     | 4081/8564 [00:01<00:01, 2878.27 examples/s]Applying chat template to train dataset:  49%|████▊     | 4156/8564 [00:01<00:01, 2627.17 examples/s]Applying chat template to train dataset:  55%|█████▍    | 4702/8564 [00:01<00:01, 2660.22 examples/s]Applying chat template to train dataset:  52%|█████▏    | 4493/8564 [00:01<00:01, 2830.00 examples/s]Applying chat template to train dataset:  52%|█████▏    | 4460/8564 [00:01<00:01, 2719.48 examples/s]Applying chat template to train dataset:  59%|█████▊    | 5010/8564 [00:01<00:01, 2762.71 examples/s]Applying chat template to train dataset:  57%|█████▋    | 4895/8564 [00:01<00:01, 2778.38 examples/s]Applying chat template to train dataset:  56%|█████▋    | 4838/8564 [00:01<00:01, 2651.06 examples/s]Applying chat template to train dataset:  62%|██████▏   | 5342/8564 [00:01<00:01, 2551.72 examples/s]Applying chat template to train dataset:  62%|██████▏   | 5285/8564 [00:01<00:01, 2718.90 examples/s]Applying chat template to train dataset:  61%|██████    | 5220/8564 [00:01<00:01, 2612.20 examples/s]Applying chat template to train dataset:  65%|██████▌   | 5607/8564 [00:02<00:01, 2574.52 examples/s]Applying chat template to train dataset:  69%|██████▊   | 5871/8564 [00:02<00:01, 2588.83 examples/s]Applying chat template to train dataset:  66%|██████▌   | 5673/8564 [00:02<00:01, 2676.44 examples/s]Applying chat template to train dataset:  65%|██████▌   | 5595/8564 [00:02<00:01, 2574.11 examples/s]Applying chat template to train dataset:  72%|███████▏  | 6189/8564 [00:02<00:00, 2746.97 examples/s]Applying chat template to train dataset:  71%|███████   | 6054/8564 [00:02<00:00, 2634.06 examples/s]Applying chat template to train dataset:  70%|██████▉   | 5990/8564 [00:02<00:00, 2591.19 examples/s]Applying chat template to train dataset:  76%|███████▌  | 6501/8564 [00:02<00:00, 2849.05 examples/s]Applying chat template to train dataset:  74%|███████▍  | 6365/8564 [00:02<00:00, 2738.14 examples/s]Applying chat template to train dataset:  73%|███████▎  | 6283/8564 [00:02<00:00, 2667.56 examples/s]Applying chat template to train dataset:  80%|███████▉  | 6816/8564 [00:02<00:00, 2932.37 examples/s]Applying chat template to train dataset:  83%|████████▎ | 7115/8564 [00:02<00:00, 2943.04 examples/s]Applying chat template to train dataset:  79%|███████▉  | 6794/8564 [00:02<00:00, 2775.79 examples/s]Applying chat template to train dataset:  78%|███████▊  | 6721/8564 [00:02<00:00, 2746.74 examples/s]Applying chat template to train dataset:  88%|████████▊ | 7536/8564 [00:02<00:00, 2884.68 examples/s]Applying chat template to train dataset:  84%|████████▍ | 7230/8564 [00:02<00:00, 2812.98 examples/s]Applying chat template to train dataset:  83%|████████▎ | 7092/8564 [00:02<00:00, 2657.55 examples/s]Applying chat template to train dataset:  92%|█████████▏| 7910/8564 [00:02<00:00, 2682.88 examples/s]Applying chat template to train dataset:  90%|████████▉ | 7674/8564 [00:02<00:00, 2853.95 examples/s]Applying chat template to train dataset:  88%|████████▊ | 7495/8564 [00:02<00:00, 2663.21 examples/s]Applying chat template to train dataset:  97%|█████████▋| 8337/8564 [00:02<00:00, 2734.12 examples/s]Applying chat template to train dataset:  93%|█████████▎| 8007/8564 [00:02<00:00, 2656.29 examples/s]Applying chat template to train dataset:  91%|█████████ | 7785/8564 [00:02<00:00, 2436.64 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:03<00:00, 2791.28 examples/s]
Applying chat template to train dataset:  97%|█████████▋| 8292/8564 [00:02<00:00, 2696.29 examples/s]Applying chat template to train dataset:  94%|█████████▍| 8050/8564 [00:03<00:00, 2481.24 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:03<00:00, 2013.69 examples/s]Applying chat template to train dataset:  98%|█████████▊| 8380/8564 [00:03<00:00, 1946.78 examples/s]Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:03<00:00, 2651.04 examples/s]
Applying chat template to train dataset: 100%|██████████| 8564/8564 [00:03<00:00, 2568.61 examples/s]
Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 41/8564 [00:00<00:21, 401.28 examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/8564 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 41/8564 [00:00<00:21, 401.01 examples/s]Tokenizing train dataset:   1%|          | 85/8564 [00:00<00:26, 323.23 examples/s]Tokenizing train dataset:   0%|          | 41/8564 [00:00<00:21, 404.91 examples/s]Tokenizing train dataset:   1%|          | 88/8564 [00:00<00:25, 330.32 examples/s]Tokenizing train dataset:   1%|▏         | 126/8564 [00:00<00:28, 293.96 examples/s]Tokenizing train dataset:   1%|          | 88/8564 [00:00<00:25, 334.94 examples/s]Tokenizing train dataset:   2%|▏         | 129/8564 [00:00<00:28, 298.17 examples/s]Tokenizing train dataset:   2%|▏         | 169/8564 [00:00<00:29, 284.88 examples/s]Tokenizing train dataset:   2%|▏         | 131/8564 [00:00<00:27, 305.36 examples/s]Tokenizing train dataset:   2%|▏         | 160/8564 [00:00<00:28, 297.33 examples/s]Tokenizing train dataset:   2%|▏         | 200/8564 [00:00<00:29, 284.26 examples/s]Tokenizing train dataset:   2%|▏         | 174/8564 [00:00<00:28, 293.25 examples/s]Tokenizing train dataset:   3%|▎         | 236/8564 [00:00<00:27, 300.42 examples/s]Tokenizing train dataset:   2%|▏         | 204/8564 [00:00<00:29, 287.10 examples/s]Tokenizing train dataset:   3%|▎         | 240/8564 [00:00<00:27, 299.54 examples/s]Tokenizing train dataset:   3%|▎         | 217/8564 [00:00<00:29, 285.71 examples/s]Tokenizing train dataset:   3%|▎         | 282/8564 [00:00<00:27, 296.47 examples/s]Tokenizing train dataset:   3%|▎         | 280/8564 [00:00<00:29, 282.75 examples/s]Tokenizing train dataset:   3%|▎         | 251/8564 [00:00<00:32, 259.72 examples/s]Tokenizing train dataset:   4%|▎         | 318/8564 [00:01<00:30, 266.86 examples/s]Tokenizing train dataset:   4%|▎         | 309/8564 [00:01<00:29, 282.06 examples/s]Tokenizing train dataset:   3%|▎         | 278/8564 [00:00<00:32, 258.28 examples/s]Tokenizing train dataset:   4%|▍         | 350/8564 [00:01<00:33, 244.42 examples/s]Tokenizing train dataset:   4%|▍         | 340/8564 [00:01<00:28, 284.60 examples/s]Tokenizing train dataset:   4%|▎         | 306/8564 [00:01<00:31, 261.57 examples/s]Tokenizing train dataset:   5%|▍         | 386/8564 [00:01<00:30, 267.23 examples/s]Tokenizing train dataset:   4%|▍         | 370/8564 [00:01<00:28, 283.34 examples/s]Tokenizing train dataset:   4%|▍         | 346/8564 [00:01<00:31, 259.88 examples/s]Tokenizing train dataset:   5%|▍         | 415/8564 [00:01<00:30, 269.77 examples/s]Tokenizing train dataset:   5%|▍         | 401/8564 [00:01<00:28, 284.37 examples/s]Tokenizing train dataset:   4%|▍         | 375/8564 [00:01<00:30, 265.82 examples/s]Tokenizing train dataset:   5%|▌         | 431/8564 [00:01<00:28, 285.26 examples/s]Tokenizing train dataset:   5%|▌         | 457/8564 [00:01<00:30, 266.99 examples/s]Tokenizing train dataset:   5%|▍         | 418/8564 [00:01<00:30, 268.20 examples/s]Tokenizing train dataset:   5%|▌         | 470/8564 [00:01<00:29, 271.12 examples/s]Tokenizing train dataset:   6%|▌         | 497/8564 [00:01<00:30, 263.41 examples/s]Tokenizing train dataset:   5%|▌         | 461/8564 [00:01<00:29, 271.90 examples/s]Tokenizing train dataset:   6%|▌         | 499/8564 [00:01<00:29, 275.07 examples/s]Tokenizing train dataset:   6%|▌         | 530/8564 [00:01<00:32, 245.44 examples/s]Tokenizing train dataset:   6%|▌         | 490/8564 [00:01<00:29, 273.10 examples/s]Tokenizing train dataset:   6%|▌         | 528/8564 [00:01<00:29, 276.10 examples/s]Tokenizing train dataset:   7%|▋         | 562/8564 [00:02<00:30, 261.02 examples/s]Tokenizing train dataset:   6%|▌         | 533/8564 [00:01<00:29, 275.02 examples/s]Tokenizing train dataset:   7%|▋         | 570/8564 [00:01<00:28, 275.66 examples/s]Tokenizing train dataset:   7%|▋         | 602/8564 [00:02<00:30, 262.00 examples/s]Tokenizing train dataset:   7%|▋         | 563/8564 [00:02<00:28, 278.60 examples/s]Tokenizing train dataset:   7%|▋         | 614/8564 [00:02<00:28, 275.58 examples/s]Tokenizing train dataset:   7%|▋         | 638/8564 [00:02<00:28, 281.60 examples/s]Tokenizing train dataset:   7%|▋         | 600/8564 [00:02<00:30, 261.57 examples/s]Tokenizing train dataset:   8%|▊         | 650/8564 [00:02<00:27, 291.08 examples/s]Tokenizing train dataset:   8%|▊         | 677/8564 [00:02<00:29, 269.99 examples/s]Tokenizing train dataset:   7%|▋         | 632/8564 [00:02<00:28, 275.10 examples/s]Tokenizing train dataset:   8%|▊         | 688/8564 [00:02<00:28, 272.00 examples/s]Tokenizing train dataset:   8%|▊         | 716/8564 [00:02<00:29, 261.71 examples/s]Tokenizing train dataset:   8%|▊         | 671/8564 [00:02<00:29, 267.72 examples/s]Tokenizing train dataset:   8%|▊         | 719/8564 [00:02<00:31, 246.54 examples/s]Tokenizing train dataset:   9%|▉         | 753/8564 [00:02<00:27, 285.59 examples/s]Tokenizing train dataset:   8%|▊         | 712/8564 [00:02<00:29, 264.02 examples/s]Tokenizing train dataset:   9%|▉         | 754/8564 [00:02<00:29, 268.49 examples/s]Tokenizing train dataset:   9%|▉         | 791/8564 [00:02<00:28, 272.70 examples/s]Tokenizing train dataset:   9%|▉         | 750/8564 [00:02<00:27, 288.34 examples/s]Tokenizing train dataset:   9%|▉         | 794/8564 [00:02<00:29, 264.48 examples/s]Tokenizing train dataset:  10%|▉         | 820/8564 [00:02<00:28, 271.61 examples/s]Tokenizing train dataset:   9%|▉         | 790/8564 [00:02<00:27, 278.49 examples/s]Tokenizing train dataset:  10%|▉         | 831/8564 [00:02<00:30, 256.17 examples/s]Tokenizing train dataset:  10%|█         | 860/8564 [00:03<00:28, 266.57 examples/s]Tokenizing train dataset:  10%|▉         | 819/8564 [00:02<00:27, 280.01 examples/s]Tokenizing train dataset:  10%|█         | 859/8564 [00:03<00:29, 259.42 examples/s]Tokenizing train dataset:  10%|█         | 893/8564 [00:03<00:27, 280.40 examples/s]Tokenizing train dataset:  10%|█         | 862/8564 [00:03<00:27, 279.18 examples/s]Tokenizing train dataset:  11%|█         | 905/8564 [00:03<00:28, 271.16 examples/s]Tokenizing train dataset:  11%|█         | 936/8564 [00:03<00:27, 274.31 examples/s]Tokenizing train dataset:  10%|█         | 894/8564 [00:03<00:26, 287.68 examples/s]Tokenizing train dataset:  11%|█▏        | 965/8564 [00:03<00:27, 276.54 examples/s]Tokenizing train dataset:  11%|█         | 945/8564 [00:03<00:28, 267.34 examples/s]Tokenizing train dataset:  11%|█         | 939/8564 [00:03<00:26, 287.84 examples/s]Tokenizing train dataset:  11%|█▏        | 974/8564 [00:03<00:28, 270.26 examples/s]Tokenizing train dataset:  12%|█▏        | 1007/8564 [00:03<00:27, 270.93 examples/s]Tokenizing train dataset:  11%|█▏        | 970/8564 [00:03<00:26, 287.05 examples/s]Tokenizing train dataset:  12%|█▏        | 1012/8564 [00:03<00:29, 259.55 examples/s]Tokenizing train dataset:  12%|█▏        | 1049/8564 [00:03<00:27, 268.62 examples/s]Tokenizing train dataset:  12%|█▏        | 1011/8564 [00:03<00:27, 275.59 examples/s]Tokenizing train dataset:  13%|█▎        | 1080/8564 [00:03<00:27, 274.57 examples/s]Tokenizing train dataset:  12%|█▏        | 1054/8564 [00:03<00:28, 259.45 examples/s]Tokenizing train dataset:  12%|█▏        | 1045/8564 [00:03<00:29, 256.15 examples/s]Tokenizing train dataset:  13%|█▎        | 1111/8564 [00:04<00:26, 280.44 examples/s]Tokenizing train dataset:  13%|█▎        | 1087/8564 [00:03<00:27, 273.01 examples/s]Tokenizing train dataset:  13%|█▎        | 1085/8564 [00:03<00:29, 253.33 examples/s]Tokenizing train dataset:  13%|█▎        | 1119/8564 [00:04<00:26, 282.08 examples/s]Tokenizing train dataset:  13%|█▎        | 1151/8564 [00:04<00:27, 272.87 examples/s]Tokenizing train dataset:  13%|█▎        | 1117/8564 [00:04<00:27, 267.61 examples/s]Tokenizing train dataset:  13%|█▎        | 1149/8564 [00:04<00:32, 227.27 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:04<00:31, 236.47 examples/s]Tokenizing train dataset:  13%|█▎        | 1155/8564 [00:04<00:28, 258.95 examples/s]Tokenizing train dataset:  14%|█▎        | 1174/8564 [00:04<00:32, 229.59 examples/s]Tokenizing train dataset:  14%|█▍        | 1214/8564 [00:04<00:29, 249.39 examples/s]Tokenizing train dataset:  14%|█▍        | 1183/8564 [00:04<00:28, 262.60 examples/s]Tokenizing train dataset:  14%|█▍        | 1207/8564 [00:04<00:29, 252.43 examples/s]Tokenizing train dataset:  15%|█▍        | 1246/8564 [00:04<00:27, 265.20 examples/s]Tokenizing train dataset:  14%|█▍        | 1222/8564 [00:04<00:28, 255.57 examples/s]Tokenizing train dataset:  14%|█▍        | 1240/8564 [00:04<00:27, 268.94 examples/s]Tokenizing train dataset:  15%|█▍        | 1274/8564 [00:04<00:27, 263.59 examples/s]Tokenizing train dataset:  15%|█▍        | 1255/8564 [00:04<00:26, 272.39 examples/s]Tokenizing train dataset:  15%|█▍        | 1270/8564 [00:04<00:27, 269.99 examples/s]Tokenizing train dataset:  15%|█▌        | 1305/8564 [00:04<00:26, 274.05 examples/s]Tokenizing train dataset:  15%|█▌        | 1285/8564 [00:04<00:26, 274.50 examples/s]Tokenizing train dataset:  15%|█▌        | 1299/8564 [00:04<00:26, 272.99 examples/s]Tokenizing train dataset:  16%|█▌        | 1344/8564 [00:04<00:27, 265.89 examples/s]Tokenizing train dataset:  15%|█▌        | 1318/8564 [00:04<00:25, 286.73 examples/s]Tokenizing train dataset:  16%|█▌        | 1330/8564 [00:04<00:25, 279.82 examples/s]Tokenizing train dataset:  16%|█▌        | 1348/8564 [00:04<00:25, 288.11 examples/s]Tokenizing train dataset:  16%|█▌        | 1386/8564 [00:05<00:27, 263.54 examples/s]Tokenizing train dataset:  16%|█▌        | 1370/8564 [00:05<00:26, 270.25 examples/s]Tokenizing train dataset:  17%|█▋        | 1417/8564 [00:05<00:26, 270.38 examples/s]Tokenizing train dataset:  16%|█▌        | 1387/8564 [00:05<00:26, 272.12 examples/s]Tokenizing train dataset:  16%|█▋        | 1399/8564 [00:05<00:26, 271.47 examples/s]Tokenizing train dataset:  17%|█▋        | 1446/8564 [00:05<00:26, 269.75 examples/s]Tokenizing train dataset:  17%|█▋        | 1427/8564 [00:05<00:26, 271.25 examples/s]Tokenizing train dataset:  17%|█▋        | 1429/8564 [00:05<00:26, 272.64 examples/s]Tokenizing train dataset:  17%|█▋        | 1474/8564 [00:05<00:26, 266.37 examples/s]Tokenizing train dataset:  17%|█▋        | 1457/8564 [00:05<00:26, 271.59 examples/s]Tokenizing train dataset:  17%|█▋        | 1466/8564 [00:05<00:26, 263.27 examples/s]Tokenizing train dataset:  18%|█▊        | 1510/8564 [00:05<00:27, 255.41 examples/s]Tokenizing train dataset:  17%|█▋        | 1486/8564 [00:05<00:26, 270.78 examples/s]Tokenizing train dataset:  17%|█▋        | 1495/8564 [00:05<00:26, 262.01 examples/s]Tokenizing train dataset:  18%|█▊        | 1541/8564 [00:05<00:26, 267.75 examples/s]Tokenizing train dataset:  18%|█▊        | 1526/8564 [00:05<00:26, 269.50 examples/s]Tokenizing train dataset:  18%|█▊        | 1531/8564 [00:05<00:25, 275.20 examples/s]Tokenizing train dataset:  18%|█▊        | 1555/8564 [00:05<00:25, 271.77 examples/s]Tokenizing train dataset:  18%|█▊        | 1581/8564 [00:05<00:26, 261.80 examples/s]Tokenizing train dataset:  18%|█▊        | 1567/8564 [00:05<00:26, 259.62 examples/s]Tokenizing train dataset:  19%|█▉        | 1610/8564 [00:05<00:26, 262.60 examples/s]Tokenizing train dataset:  19%|█▊        | 1598/8564 [00:05<00:25, 272.93 examples/s]Tokenizing train dataset:  19%|█▊        | 1596/8564 [00:05<00:26, 261.79 examples/s]Tokenizing train dataset:  19%|█▉        | 1644/8564 [00:06<00:25, 276.41 examples/s]Tokenizing train dataset:  19%|█▉        | 1630/8564 [00:05<00:24, 280.26 examples/s]Tokenizing train dataset:  19%|█▉        | 1627/8564 [00:05<00:25, 270.84 examples/s]Tokenizing train dataset:  20%|█▉        | 1676/8564 [00:06<00:24, 283.53 examples/s]Tokenizing train dataset:  19%|█▉        | 1660/8564 [00:06<00:24, 281.68 examples/s]Tokenizing train dataset:  20%|█▉        | 1675/8564 [00:06<00:24, 284.39 examples/s]Tokenizing train dataset:  20%|█▉        | 1712/8564 [00:06<00:22, 303.35 examples/s]Tokenizing train dataset:  20%|█▉        | 1700/8564 [00:06<00:22, 305.89 examples/s]Tokenizing train dataset:  20%|██        | 1718/8564 [00:06<00:25, 271.06 examples/s]Tokenizing train dataset:  20%|██        | 1743/8564 [00:06<00:28, 241.40 examples/s]Tokenizing train dataset:  20%|██        | 1749/8564 [00:06<00:22, 309.61 examples/s]Tokenizing train dataset:  20%|██        | 1749/8564 [00:06<00:24, 277.46 examples/s]Tokenizing train dataset:  21%|██        | 1774/8564 [00:06<00:26, 256.55 examples/s]Tokenizing train dataset:  21%|██        | 1780/8564 [00:06<00:24, 280.36 examples/s]Tokenizing train dataset:  21%|██        | 1793/8564 [00:06<00:22, 300.51 examples/s]Tokenizing train dataset:  21%|██        | 1816/8564 [00:06<00:25, 263.60 examples/s]Tokenizing train dataset:  21%|██        | 1809/8564 [00:06<00:24, 281.44 examples/s]Tokenizing train dataset:  22%|██▏       | 1848/8564 [00:06<00:24, 274.15 examples/s]Tokenizing train dataset:  21%|██▏       | 1840/8564 [00:06<00:22, 299.01 examples/s]Tokenizing train dataset:  21%|██▏       | 1841/8564 [00:06<00:23, 288.32 examples/s]Tokenizing train dataset:  22%|██▏       | 1887/8564 [00:06<00:25, 264.07 examples/s]Tokenizing train dataset:  22%|██▏       | 1877/8564 [00:06<00:24, 278.32 examples/s]Tokenizing train dataset:  22%|██▏       | 1882/8564 [00:06<00:23, 280.40 examples/s]Tokenizing train dataset:  22%|██▏       | 1920/8564 [00:07<00:24, 275.44 examples/s]Tokenizing train dataset:  22%|██▏       | 1914/8564 [00:06<00:23, 286.69 examples/s]Tokenizing train dataset:  22%|██▏       | 1920/8564 [00:06<00:23, 279.82 examples/s]Tokenizing train dataset:  23%|██▎       | 1956/8564 [00:07<00:22, 294.38 examples/s]Tokenizing train dataset:  23%|██▎       | 1948/8564 [00:07<00:22, 295.73 examples/s]Tokenizing train dataset:  23%|██▎       | 1956/8564 [00:07<00:22, 294.64 examples/s]Tokenizing train dataset:  23%|██▎       | 1978/8564 [00:07<00:22, 296.36 examples/s]Tokenizing train dataset:  23%|██▎       | 2003/8564 [00:07<00:21, 298.42 examples/s]Tokenizing train dataset:  23%|██▎       | 1989/8564 [00:07<00:21, 298.96 examples/s]Tokenizing train dataset:  24%|██▍       | 2040/8564 [00:07<00:20, 312.19 examples/s]Tokenizing train dataset:  24%|██▎       | 2020/8564 [00:07<00:21, 298.72 examples/s]Tokenizing train dataset:  24%|██▎       | 2026/8564 [00:07<00:21, 301.58 examples/s]Tokenizing train dataset:  24%|██▍       | 2073/8564 [00:07<00:20, 313.39 examples/s]Tokenizing train dataset:  24%|██▍       | 2056/8564 [00:07<00:20, 313.10 examples/s]Tokenizing train dataset:  24%|██▍       | 2064/8564 [00:07<00:20, 318.24 examples/s]Tokenizing train dataset:  25%|██▍       | 2110/8564 [00:07<00:19, 324.74 examples/s]Tokenizing train dataset:  24%|██▍       | 2096/8564 [00:07<00:22, 292.64 examples/s]Tokenizing train dataset:  25%|██▍       | 2110/8564 [00:07<00:20, 310.93 examples/s]Tokenizing train dataset:  25%|██▍       | 2130/8564 [00:07<00:21, 303.01 examples/s]Tokenizing train dataset:  25%|██▌       | 2160/8564 [00:07<00:19, 322.62 examples/s]Tokenizing train dataset:  25%|██▌       | 2148/8564 [00:07<00:19, 325.26 examples/s]Tokenizing train dataset:  25%|██▌       | 2166/8564 [00:07<00:20, 313.94 examples/s]Tokenizing train dataset:  26%|██▌       | 2196/8564 [00:07<00:19, 330.07 examples/s]Tokenizing train dataset:  25%|██▌       | 2182/8564 [00:07<00:19, 325.88 examples/s]Tokenizing train dataset:  26%|██▌       | 2199/8564 [00:07<00:20, 313.52 examples/s]Tokenizing train dataset:  26%|██▌       | 2217/8564 [00:07<00:19, 328.39 examples/s]Tokenizing train dataset:  26%|██▌       | 2244/8564 [00:08<00:19, 324.20 examples/s]Tokenizing train dataset:  26%|██▌       | 2235/8564 [00:07<00:19, 324.32 examples/s]Tokenizing train dataset:  26%|██▋       | 2253/8564 [00:08<00:18, 334.20 examples/s]Tokenizing train dataset:  27%|██▋       | 2284/8564 [00:08<00:18, 342.63 examples/s]Tokenizing train dataset:  27%|██▋       | 2274/8564 [00:08<00:18, 341.00 examples/s]Tokenizing train dataset:  27%|██▋       | 2290/8564 [00:08<00:18, 339.00 examples/s]Tokenizing train dataset:  27%|██▋       | 2327/8564 [00:08<00:19, 317.48 examples/s]Tokenizing train dataset:  27%|██▋       | 2309/8564 [00:08<00:18, 337.49 examples/s]Tokenizing train dataset:  27%|██▋       | 2340/8564 [00:08<00:18, 331.67 examples/s]Tokenizing train dataset:  28%|██▊       | 2375/8564 [00:08<00:19, 316.29 examples/s]Tokenizing train dataset:  27%|██▋       | 2346/8564 [00:08<00:20, 300.14 examples/s]Tokenizing train dataset:  28%|██▊       | 2374/8564 [00:08<00:18, 330.45 examples/s]Tokenizing train dataset:  28%|██▊       | 2413/8564 [00:08<00:18, 326.58 examples/s]Tokenizing train dataset:  28%|██▊       | 2410/8564 [00:08<00:18, 331.87 examples/s]Tokenizing train dataset:  28%|██▊       | 2396/8564 [00:08<00:19, 310.10 examples/s]Tokenizing train dataset:  29%|██▊       | 2451/8564 [00:08<00:18, 337.04 examples/s]Tokenizing train dataset:  29%|██▊       | 2449/8564 [00:08<00:17, 341.44 examples/s]Tokenizing train dataset:  28%|██▊       | 2432/8564 [00:08<00:19, 319.80 examples/s]Tokenizing train dataset:  29%|██▉       | 2485/8564 [00:08<00:17, 343.99 examples/s]Tokenizing train dataset:  29%|██▉       | 2470/8564 [00:08<00:18, 330.44 examples/s]Tokenizing train dataset:  29%|██▉       | 2497/8564 [00:08<00:18, 320.66 examples/s]Tokenizing train dataset:  30%|██▉       | 2532/8564 [00:08<00:18, 325.12 examples/s]Tokenizing train dataset:  30%|██▉       | 2532/8564 [00:08<00:18, 329.69 examples/s]Tokenizing train dataset:  29%|██▉       | 2521/8564 [00:08<00:18, 328.50 examples/s]Tokenizing train dataset:  30%|███       | 2571/8564 [00:09<00:17, 340.73 examples/s]Tokenizing train dataset:  30%|███       | 2570/8564 [00:08<00:17, 336.25 examples/s]Tokenizing train dataset:  30%|██▉       | 2562/8564 [00:08<00:17, 344.97 examples/s]Tokenizing train dataset:  30%|███       | 2606/8564 [00:09<00:17, 340.01 examples/s]Tokenizing train dataset:  31%|███       | 2618/8564 [00:09<00:18, 329.24 examples/s]Tokenizing train dataset:  31%|███       | 2613/8564 [00:09<00:17, 340.58 examples/s]Tokenizing train dataset:  31%|███       | 2662/8564 [00:09<00:18, 314.02 examples/s]Tokenizing train dataset:  31%|███       | 2652/8564 [00:09<00:18, 313.27 examples/s]Tokenizing train dataset:  31%|███       | 2655/8564 [00:09<00:18, 316.54 examples/s]Tokenizing train dataset:  31%|███▏      | 2697/8564 [00:09<00:21, 278.89 examples/s]Tokenizing train dataset:  31%|███▏      | 2686/8564 [00:09<00:22, 264.94 examples/s]Tokenizing train dataset:  31%|███▏      | 2693/8564 [00:09<00:20, 291.90 examples/s]Tokenizing train dataset:  32%|███▏      | 2735/8564 [00:09<00:22, 257.72 examples/s]Tokenizing train dataset:  32%|███▏      | 2725/8564 [00:09<00:22, 254.32 examples/s]Tokenizing train dataset:  32%|███▏      | 2733/8564 [00:09<00:20, 277.72 examples/s]Tokenizing train dataset:  32%|███▏      | 2768/8564 [00:09<00:21, 272.10 examples/s]Tokenizing train dataset:  32%|███▏      | 2756/8564 [00:09<00:21, 265.57 examples/s]Tokenizing train dataset:  32%|███▏      | 2779/8564 [00:09<00:20, 283.43 examples/s]Tokenizing train dataset:  33%|███▎      | 2806/8564 [00:09<00:19, 296.66 examples/s]Tokenizing train dataset:  33%|███▎      | 2790/8564 [00:09<00:20, 280.64 examples/s]Tokenizing train dataset:  33%|███▎      | 2820/8564 [00:09<00:18, 308.00 examples/s]Tokenizing train dataset:  33%|███▎      | 2840/8564 [00:10<00:18, 304.32 examples/s]Tokenizing train dataset:  33%|███▎      | 2822/8564 [00:09<00:19, 288.31 examples/s]Tokenizing train dataset:  33%|███▎      | 2855/8564 [00:09<00:20, 275.67 examples/s]Tokenizing train dataset:  34%|███▎      | 2873/8564 [00:10<00:21, 264.43 examples/s]Tokenizing train dataset:  33%|███▎      | 2857/8564 [00:10<00:22, 258.10 examples/s]Tokenizing train dataset:  34%|███▎      | 2890/8564 [00:10<00:19, 289.82 examples/s]Tokenizing train dataset:  34%|███▍      | 2914/8564 [00:10<00:18, 297.58 examples/s]Tokenizing train dataset:  34%|███▍      | 2893/8564 [00:10<00:20, 281.52 examples/s]Tokenizing train dataset:  34%|███▍      | 2927/8564 [00:10<00:18, 307.13 examples/s]Tokenizing train dataset:  35%|███▍      | 2963/8564 [00:10<00:18, 304.65 examples/s]Tokenizing train dataset:  34%|███▍      | 2944/8564 [00:10<00:19, 294.74 examples/s]Tokenizing train dataset:  35%|███▍      | 2965/8564 [00:10<00:17, 322.69 examples/s]Tokenizing train dataset:  35%|███▌      | 3004/8564 [00:10<00:16, 328.99 examples/s]Tokenizing train dataset:  35%|███▍      | 2987/8564 [00:10<00:17, 321.39 examples/s]Tokenizing train dataset:  35%|███▌      | 3000/8564 [00:10<00:16, 327.34 examples/s]Tokenizing train dataset:  35%|███▌      | 3021/8564 [00:10<00:17, 322.21 examples/s]Tokenizing train dataset:  36%|███▌      | 3053/8564 [00:10<00:16, 325.52 examples/s]Tokenizing train dataset:  36%|███▌      | 3049/8564 [00:10<00:17, 323.08 examples/s]Tokenizing train dataset:  36%|███▌      | 3069/8564 [00:10<00:17, 316.36 examples/s]Tokenizing train dataset:  36%|███▌      | 3084/8564 [00:10<00:16, 327.48 examples/s]Tokenizing train dataset:  36%|███▌      | 3100/8564 [00:10<00:17, 316.22 examples/s]Tokenizing train dataset:  36%|███▌      | 3104/8564 [00:10<00:16, 321.95 examples/s]Tokenizing train dataset:  36%|███▋      | 3118/8564 [00:10<00:16, 326.53 examples/s]Tokenizing train dataset:  37%|███▋      | 3144/8564 [00:11<00:17, 306.52 examples/s]Tokenizing train dataset:  37%|███▋      | 3150/8564 [00:10<00:17, 314.27 examples/s]Tokenizing train dataset:  37%|███▋      | 3167/8564 [00:10<00:16, 321.72 examples/s]Tokenizing train dataset:  37%|███▋      | 3178/8564 [00:11<00:17, 312.74 examples/s]Tokenizing train dataset:  37%|███▋      | 3199/8564 [00:11<00:16, 316.35 examples/s]Tokenizing train dataset:  38%|███▊      | 3220/8564 [00:11<00:16, 329.16 examples/s]Tokenizing train dataset:  38%|███▊      | 3228/8564 [00:11<00:16, 318.00 examples/s]Tokenizing train dataset:  38%|███▊      | 3236/8564 [00:11<00:16, 325.97 examples/s]Tokenizing train dataset:  38%|███▊      | 3259/8564 [00:11<00:15, 341.58 examples/s]Tokenizing train dataset:  38%|███▊      | 3265/8564 [00:11<00:16, 328.26 examples/s]Tokenizing train dataset:  38%|███▊      | 3274/8564 [00:11<00:15, 336.39 examples/s]Tokenizing train dataset:  39%|███▊      | 3307/8564 [00:11<00:15, 331.00 examples/s]Tokenizing train dataset:  39%|███▊      | 3314/8564 [00:11<00:16, 321.33 examples/s]Tokenizing train dataset:  39%|███▊      | 3311/8564 [00:11<00:17, 301.11 examples/s]Tokenizing train dataset:  39%|███▉      | 3351/8564 [00:11<00:16, 316.94 examples/s]Tokenizing train dataset:  39%|███▉      | 3361/8564 [00:11<00:16, 313.08 examples/s]Tokenizing train dataset:  39%|███▉      | 3363/8564 [00:11<00:16, 312.27 examples/s]Tokenizing train dataset:  40%|███▉      | 3394/8564 [00:11<00:15, 336.32 examples/s]Tokenizing train dataset:  40%|███▉      | 3396/8564 [00:11<00:16, 319.22 examples/s]Tokenizing train dataset:  40%|███▉      | 3400/8564 [00:11<00:16, 322.33 examples/s]Tokenizing train dataset:  40%|████      | 3446/8564 [00:11<00:15, 336.38 examples/s]Tokenizing train dataset:  40%|████      | 3439/8564 [00:11<00:16, 305.31 examples/s]Tokenizing train dataset:  40%|████      | 3438/8564 [00:11<00:15, 334.15 examples/s]Tokenizing train dataset:  41%|████      | 3470/8564 [00:12<00:16, 303.61 examples/s]Tokenizing train dataset:  41%|████      | 3473/8564 [00:11<00:15, 336.60 examples/s]Tokenizing train dataset:  41%|████      | 3500/8564 [00:11<00:15, 336.08 examples/s]Tokenizing train dataset:  41%|████      | 3506/8564 [00:12<00:16, 315.04 examples/s]Tokenizing train dataset:  41%|████      | 3516/8564 [00:12<00:15, 316.27 examples/s]Tokenizing train dataset:  41%|████▏     | 3541/8564 [00:12<00:15, 314.09 examples/s]Tokenizing train dataset:  41%|████▏     | 3542/8564 [00:12<00:15, 325.34 examples/s]Tokenizing train dataset:  42%|████▏     | 3557/8564 [00:12<00:16, 298.09 examples/s]Tokenizing train dataset:  42%|████▏     | 3578/8564 [00:12<00:17, 291.86 examples/s]Tokenizing train dataset:  42%|████▏     | 3579/8564 [00:12<00:17, 290.92 examples/s]Tokenizing train dataset:  42%|████▏     | 3595/8564 [00:12<00:15, 316.45 examples/s]Tokenizing train dataset:  42%|████▏     | 3613/8564 [00:12<00:16, 302.53 examples/s]Tokenizing train dataset:  42%|████▏     | 3615/8564 [00:12<00:16, 305.44 examples/s]Tokenizing train dataset:  42%|████▏     | 3630/8564 [00:12<00:15, 320.64 examples/s]Tokenizing train dataset:  43%|████▎     | 3646/8564 [00:12<00:16, 306.17 examples/s]Tokenizing train dataset:  43%|████▎     | 3649/8564 [00:12<00:15, 311.60 examples/s]Tokenizing train dataset:  43%|████▎     | 3663/8564 [00:12<00:15, 317.47 examples/s]Tokenizing train dataset:  43%|████▎     | 3693/8564 [00:12<00:16, 303.47 examples/s]Tokenizing train dataset:  43%|████▎     | 3695/8564 [00:12<00:15, 307.73 examples/s]Tokenizing train dataset:  43%|████▎     | 3700/8564 [00:12<00:17, 280.16 examples/s]Tokenizing train dataset:  44%|████▎     | 3730/8564 [00:12<00:15, 318.10 examples/s]Tokenizing train dataset:  44%|████▎     | 3732/8564 [00:12<00:15, 320.89 examples/s]Tokenizing train dataset:  44%|████▎     | 3739/8564 [00:12<00:15, 304.13 examples/s]Tokenizing train dataset:  44%|████▍     | 3767/8564 [00:12<00:14, 327.48 examples/s]Tokenizing train dataset:  44%|████▍     | 3784/8564 [00:13<00:14, 325.58 examples/s]Tokenizing train dataset:  44%|████▍     | 3776/8564 [00:12<00:14, 319.36 examples/s]Tokenizing train dataset:  44%|████▍     | 3804/8564 [00:12<00:14, 333.37 examples/s]Tokenizing train dataset:  45%|████▍     | 3831/8564 [00:13<00:14, 319.11 examples/s]Tokenizing train dataset:  45%|████▍     | 3823/8564 [00:13<00:15, 313.84 examples/s]Tokenizing train dataset:  45%|████▍     | 3848/8564 [00:13<00:14, 315.87 examples/s]Tokenizing train dataset:  45%|████▌     | 3883/8564 [00:13<00:14, 322.83 examples/s]Tokenizing train dataset:  45%|████▌     | 3880/8564 [00:13<00:14, 318.76 examples/s]Tokenizing train dataset:  45%|████▌     | 3875/8564 [00:13<00:14, 323.60 examples/s]Tokenizing train dataset:  46%|████▌     | 3928/8564 [00:13<00:14, 312.97 examples/s]Tokenizing train dataset:  46%|████▌     | 3924/8564 [00:13<00:15, 307.76 examples/s]Tokenizing train dataset:  46%|████▌     | 3924/8564 [00:13<00:14, 321.56 examples/s]Tokenizing train dataset:  46%|████▋     | 3962/8564 [00:13<00:14, 316.89 examples/s]Tokenizing train dataset:  46%|████▌     | 3959/8564 [00:13<00:14, 313.46 examples/s]Tokenizing train dataset:  46%|████▌     | 3960/8564 [00:13<00:14, 326.53 examples/s]Tokenizing train dataset:  47%|████▋     | 4013/8564 [00:13<00:14, 321.16 examples/s]Tokenizing train dataset:  47%|████▋     | 4004/8564 [00:13<00:15, 303.95 examples/s]Tokenizing train dataset:  47%|████▋     | 4006/8564 [00:13<00:14, 318.05 examples/s]Tokenizing train dataset:  47%|████▋     | 4061/8564 [00:13<00:14, 317.62 examples/s]Tokenizing train dataset:  47%|████▋     | 4053/8564 [00:13<00:14, 306.79 examples/s]Tokenizing train dataset:  47%|████▋     | 4054/8564 [00:13<00:14, 315.72 examples/s]Tokenizing train dataset:  48%|████▊     | 4086/8564 [00:13<00:14, 313.59 examples/s]Tokenizing train dataset:  48%|████▊     | 4103/8564 [00:13<00:14, 303.09 examples/s]Tokenizing train dataset:  48%|████▊     | 4092/8564 [00:14<00:15, 291.04 examples/s]Tokenizing train dataset:  48%|████▊     | 4121/8564 [00:14<00:13, 321.25 examples/s]Tokenizing train dataset:  48%|████▊     | 4137/8564 [00:13<00:14, 307.53 examples/s]Tokenizing train dataset:  48%|████▊     | 4128/8564 [00:14<00:14, 304.15 examples/s]Tokenizing train dataset:  49%|████▊     | 4170/8564 [00:14<00:14, 310.74 examples/s]Tokenizing train dataset:  49%|████▊     | 4161/8564 [00:14<00:14, 307.35 examples/s]Tokenizing train dataset:  49%|████▊     | 4171/8564 [00:14<00:13, 320.45 examples/s]Tokenizing train dataset:  49%|████▉     | 4207/8564 [00:14<00:13, 321.74 examples/s]Tokenizing train dataset:  49%|████▉     | 4205/8564 [00:14<00:13, 324.86 examples/s]Tokenizing train dataset:  49%|████▉     | 4210/8564 [00:14<00:14, 310.93 examples/s]Tokenizing train dataset:  50%|████▉     | 4240/8564 [00:14<00:13, 315.82 examples/s]Tokenizing train dataset:  49%|████▉     | 4239/8564 [00:14<00:13, 323.40 examples/s]Tokenizing train dataset:  50%|████▉     | 4243/8564 [00:14<00:13, 312.04 examples/s]Tokenizing train dataset:  50%|████▉     | 4274/8564 [00:14<00:13, 320.65 examples/s]Tokenizing train dataset:  50%|█████     | 4283/8564 [00:14<00:13, 309.33 examples/s]Tokenizing train dataset:  50%|█████     | 4309/8564 [00:14<00:13, 326.30 examples/s]Tokenizing train dataset:  50%|█████     | 4291/8564 [00:14<00:13, 311.21 examples/s]Tokenizing train dataset:  50%|█████     | 4316/8564 [00:14<00:13, 312.14 examples/s]Tokenizing train dataset:  51%|█████     | 4325/8564 [00:14<00:13, 315.13 examples/s]Tokenizing train dataset:  51%|█████     | 4361/8564 [00:14<00:12, 325.70 examples/s]Tokenizing train dataset:  51%|█████     | 4359/8564 [00:14<00:13, 318.39 examples/s]Tokenizing train dataset:  51%|█████     | 4364/8564 [00:14<00:13, 310.60 examples/s]Tokenizing train dataset:  51%|█████▏    | 4404/8564 [00:14<00:14, 293.32 examples/s]Tokenizing train dataset:  51%|█████▏    | 4403/8564 [00:14<00:12, 326.50 examples/s]Tokenizing train dataset:  51%|█████▏    | 4409/8564 [00:15<00:13, 317.93 examples/s]Tokenizing train dataset:  52%|█████▏    | 4440/8564 [00:14<00:13, 305.18 examples/s]Tokenizing train dataset:  52%|█████▏    | 4439/8564 [00:14<00:12, 331.50 examples/s]Tokenizing train dataset:  52%|█████▏    | 4446/8564 [00:15<00:12, 327.69 examples/s]Tokenizing train dataset:  52%|█████▏    | 4476/8564 [00:15<00:12, 315.25 examples/s]Tokenizing train dataset:  52%|█████▏    | 4480/8564 [00:15<00:12, 328.95 examples/s]Tokenizing train dataset:  52%|█████▏    | 4491/8564 [00:15<00:12, 332.11 examples/s]Tokenizing train dataset:  53%|█████▎    | 4510/8564 [00:15<00:15, 268.16 examples/s]Tokenizing train dataset:  53%|█████▎    | 4515/8564 [00:15<00:14, 274.27 examples/s]Tokenizing train dataset:  53%|█████▎    | 4527/8564 [00:15<00:13, 291.59 examples/s]Tokenizing train dataset:  53%|█████▎    | 4540/8564 [00:15<00:14, 272.86 examples/s]Tokenizing train dataset:  53%|█████▎    | 4547/8564 [00:15<00:14, 284.07 examples/s]Tokenizing train dataset:  53%|█████▎    | 4562/8564 [00:15<00:13, 303.45 examples/s]Tokenizing train dataset:  53%|█████▎    | 4572/8564 [00:15<00:14, 279.99 examples/s]Tokenizing train dataset:  53%|█████▎    | 4580/8564 [00:15<00:13, 291.73 examples/s]Tokenizing train dataset:  54%|█████▍    | 4610/8564 [00:15<00:12, 305.79 examples/s]Tokenizing train dataset:  54%|█████▍    | 4610/8564 [00:15<00:14, 268.72 examples/s]Tokenizing train dataset:  54%|█████▍    | 4620/8564 [00:15<00:14, 279.99 examples/s]Tokenizing train dataset:  54%|█████▍    | 4657/8564 [00:15<00:12, 305.80 examples/s]Tokenizing train dataset:  54%|█████▍    | 4640/8564 [00:15<00:14, 273.31 examples/s]Tokenizing train dataset:  54%|█████▍    | 4649/8564 [00:15<00:13, 280.40 examples/s]Tokenizing train dataset:  55%|█████▍    | 4701/8564 [00:15<00:12, 298.74 examples/s]Tokenizing train dataset:  55%|█████▍    | 4682/8564 [00:15<00:14, 273.56 examples/s]Tokenizing train dataset:  55%|█████▍    | 4682/8564 [00:16<00:15, 253.57 examples/s]Tokenizing train dataset:  55%|█████▌    | 4732/8564 [00:15<00:12, 300.84 examples/s]Tokenizing train dataset:  55%|█████▌    | 4711/8564 [00:15<00:14, 273.65 examples/s]Tokenizing train dataset:  55%|█████▍    | 4709/8564 [00:16<00:15, 254.51 examples/s]Tokenizing train dataset:  56%|█████▌    | 4764/8564 [00:16<00:14, 253.89 examples/s]Tokenizing train dataset:  55%|█████▌    | 4747/8564 [00:16<00:15, 246.61 examples/s]Tokenizing train dataset:  55%|█████▌    | 4747/8564 [00:16<00:15, 249.37 examples/s]Tokenizing train dataset:  56%|█████▌    | 4810/8564 [00:16<00:13, 269.54 examples/s]Tokenizing train dataset:  56%|█████▌    | 4788/8564 [00:16<00:14, 254.00 examples/s]Tokenizing train dataset:  56%|█████▌    | 4785/8564 [00:16<00:15, 248.70 examples/s]Tokenizing train dataset:  57%|█████▋    | 4858/8564 [00:16<00:11, 315.15 examples/s]Tokenizing train dataset:  56%|█████▋    | 4829/8564 [00:16<00:12, 288.84 examples/s]Tokenizing train dataset:  56%|█████▋    | 4830/8564 [00:16<00:12, 295.29 examples/s]Tokenizing train dataset:  57%|█████▋    | 4911/8564 [00:16<00:10, 364.45 examples/s]Tokenizing train dataset:  57%|█████▋    | 4863/8564 [00:16<00:12, 298.92 examples/s]Tokenizing train dataset:  57%|█████▋    | 4869/8564 [00:16<00:11, 318.36 examples/s]Tokenizing train dataset:  58%|█████▊    | 4966/8564 [00:16<00:08, 408.59 examples/s]Tokenizing train dataset:  57%|█████▋    | 4912/8564 [00:16<00:10, 347.66 examples/s]Tokenizing train dataset:  58%|█████▊    | 4930/8564 [00:16<00:09, 389.16 examples/s]Tokenizing train dataset:  59%|█████▊    | 5011/8564 [00:16<00:08, 416.15 examples/s]Tokenizing train dataset:  58%|█████▊    | 4958/8564 [00:16<00:09, 377.38 examples/s]Tokenizing train dataset:  58%|█████▊    | 4975/8564 [00:16<00:08, 404.87 examples/s]Tokenizing train dataset:  59%|█████▉    | 5068/8564 [00:16<00:07, 457.97 examples/s]Tokenizing train dataset:  59%|█████▊    | 5015/8564 [00:16<00:08, 424.42 examples/s]Tokenizing train dataset:  59%|█████▊    | 5031/8564 [00:16<00:07, 445.14 examples/s]Tokenizing train dataset:  60%|█████▉    | 5126/8564 [00:16<00:07, 490.55 examples/s]Tokenizing train dataset:  59%|█████▉    | 5072/8564 [00:16<00:07, 459.43 examples/s]Tokenizing train dataset:  60%|█████▉    | 5107/8564 [00:17<00:07, 463.20 examples/s]Tokenizing train dataset:  61%|██████    | 5194/8564 [00:17<00:06, 541.16 examples/s]Tokenizing train dataset:  60%|█████▉    | 5126/8564 [00:16<00:07, 475.36 examples/s]Tokenizing train dataset:  60%|██████    | 5174/8564 [00:17<00:06, 514.16 examples/s]Tokenizing train dataset:  61%|██████▏   | 5261/8564 [00:17<00:05, 575.22 examples/s]Tokenizing train dataset:  61%|██████    | 5189/8564 [00:17<00:06, 518.56 examples/s]Tokenizing train dataset:  61%|██████▏   | 5251/8564 [00:17<00:06, 546.52 examples/s]Tokenizing train dataset:  61%|██████▏   | 5249/8564 [00:17<00:06, 507.05 examples/s]Tokenizing train dataset:  62%|██████▏   | 5344/8564 [00:17<00:05, 562.48 examples/s]Tokenizing train dataset:  62%|██████▏   | 5312/8564 [00:17<00:05, 560.04 examples/s]Tokenizing train dataset:  62%|██████▏   | 5312/8564 [00:17<00:06, 532.90 examples/s]Tokenizing train dataset:  63%|██████▎   | 5432/8564 [00:17<00:05, 567.36 examples/s]Tokenizing train dataset:  63%|██████▎   | 5392/8564 [00:17<00:05, 545.69 examples/s]Tokenizing train dataset:  63%|██████▎   | 5390/8564 [00:17<00:06, 525.21 examples/s]Tokenizing train dataset:  64%|██████▍   | 5516/8564 [00:17<00:05, 554.60 examples/s]Tokenizing train dataset:  64%|██████▎   | 5454/8564 [00:17<00:05, 560.73 examples/s]Tokenizing train dataset:  64%|██████▎   | 5451/8564 [00:17<00:05, 542.37 examples/s]Tokenizing train dataset:  65%|██████▌   | 5602/8564 [00:17<00:05, 556.47 examples/s]Tokenizing train dataset:  65%|██████▍   | 5538/8564 [00:17<00:05, 556.07 examples/s]Tokenizing train dataset:  65%|██████▍   | 5534/8564 [00:17<00:05, 544.35 examples/s]Tokenizing train dataset:  66%|██████▌   | 5668/8564 [00:17<00:05, 578.14 examples/s]Tokenizing train dataset:  65%|██████▌   | 5603/8564 [00:17<00:06, 430.68 examples/s]Tokenizing train dataset:  65%|██████▌   | 5596/8564 [00:18<00:07, 387.43 examples/s]Tokenizing train dataset:  67%|██████▋   | 5731/8564 [00:18<00:06, 460.11 examples/s]Tokenizing train dataset:  66%|██████▌   | 5663/8564 [00:18<00:06, 462.73 examples/s]Tokenizing train dataset:  66%|██████▌   | 5650/8564 [00:18<00:07, 414.56 examples/s]Tokenizing train dataset:  68%|██████▊   | 5787/8564 [00:18<00:05, 480.61 examples/s]Tokenizing train dataset:  67%|██████▋   | 5726/8564 [00:18<00:05, 494.67 examples/s]Tokenizing train dataset:  67%|██████▋   | 5698/8564 [00:18<00:06, 426.81 examples/s]Tokenizing train dataset:  68%|██████▊   | 5793/8564 [00:18<00:05, 535.97 examples/s]Tokenizing train dataset:  68%|██████▊   | 5863/8564 [00:18<00:05, 485.00 examples/s]Tokenizing train dataset:  67%|██████▋   | 5761/8564 [00:18<00:05, 472.17 examples/s]Tokenizing train dataset:  69%|██████▉   | 5918/8564 [00:18<00:05, 495.30 examples/s]Tokenizing train dataset:  69%|██████▊   | 5870/8564 [00:18<00:05, 525.78 examples/s]Tokenizing train dataset:  68%|██████▊   | 5827/8564 [00:18<00:05, 512.17 examples/s]Tokenizing train dataset:  70%|██████▉   | 5977/8564 [00:18<00:05, 512.91 examples/s]Tokenizing train dataset:  69%|██████▉   | 5934/8564 [00:18<00:04, 549.37 examples/s]Tokenizing train dataset:  69%|██████▉   | 5907/8564 [00:18<00:05, 518.43 examples/s]Tokenizing train dataset:  71%|███████   | 6055/8564 [00:18<00:04, 513.00 examples/s]Tokenizing train dataset:  70%|██████▉   | 5970/8564 [00:18<00:04, 542.17 examples/s]Tokenizing train dataset:  70%|███████   | 6014/8564 [00:18<00:04, 516.30 examples/s]Tokenizing train dataset:  72%|███████▏  | 6141/8564 [00:18<00:04, 530.14 examples/s]Tokenizing train dataset:  71%|███████   | 6045/8564 [00:19<00:04, 524.95 examples/s]Tokenizing train dataset:  71%|███████   | 6091/8564 [00:18<00:04, 509.11 examples/s]Tokenizing train dataset:  73%|███████▎  | 6225/8564 [00:18<00:04, 534.28 examples/s]Tokenizing train dataset:  72%|███████▏  | 6155/8564 [00:18<00:04, 538.51 examples/s]Tokenizing train dataset:  72%|███████▏  | 6133/8564 [00:19<00:04, 539.56 examples/s]Tokenizing train dataset:  72%|███████▏  | 6200/8564 [00:19<00:04, 567.39 examples/s]Tokenizing train dataset:  74%|███████▎  | 6307/8564 [00:19<00:04, 533.85 examples/s]Tokenizing train dataset:  73%|███████▎  | 6237/8564 [00:19<00:04, 534.65 examples/s]Tokenizing train dataset:  73%|███████▎  | 6268/8564 [00:19<00:03, 589.39 examples/s]Tokenizing train dataset:  74%|███████▍  | 6377/8564 [00:19<00:03, 566.97 examples/s]Tokenizing train dataset:  74%|███████▍  | 6316/8564 [00:19<00:04, 527.33 examples/s]Tokenizing train dataset:  74%|███████▍  | 6347/8564 [00:19<00:03, 558.90 examples/s]Tokenizing train dataset:  75%|███████▌  | 6455/8564 [00:19<00:03, 548.34 examples/s]Tokenizing train dataset:  75%|███████▍  | 6382/8564 [00:19<00:03, 555.91 examples/s]Tokenizing train dataset:  75%|███████▍  | 6409/8564 [00:19<00:03, 567.67 examples/s]Tokenizing train dataset:  75%|███████▌  | 6441/8564 [00:19<00:03, 559.85 examples/s]Tokenizing train dataset:  76%|███████▋  | 6539/8564 [00:19<00:03, 548.50 examples/s]Tokenizing train dataset:  76%|███████▌  | 6490/8564 [00:19<00:03, 555.13 examples/s]Tokenizing train dataset:  76%|███████▌  | 6516/8564 [00:19<00:03, 536.54 examples/s]Tokenizing train dataset:  77%|███████▋  | 6602/8564 [00:19<00:03, 505.81 examples/s]Tokenizing train dataset:  76%|███████▋  | 6551/8564 [00:19<00:04, 484.96 examples/s]Tokenizing train dataset:  77%|███████▋  | 6577/8564 [00:19<00:04, 474.26 examples/s]Tokenizing train dataset:  78%|███████▊  | 6672/8564 [00:19<00:03, 487.82 examples/s]Tokenizing train dataset:  79%|███████▊  | 6739/8564 [00:19<00:03, 526.75 examples/s]Tokenizing train dataset:  77%|███████▋  | 6628/8564 [00:20<00:03, 490.99 examples/s]Tokenizing train dataset:  78%|███████▊  | 6646/8564 [00:19<00:04, 460.49 examples/s]Tokenizing train dataset:  79%|███████▉  | 6800/8564 [00:20<00:03, 544.37 examples/s]Tokenizing train dataset:  78%|███████▊  | 6690/8564 [00:20<00:03, 516.59 examples/s]Tokenizing train dataset:  78%|███████▊  | 6715/8564 [00:20<00:03, 511.01 examples/s]Tokenizing train dataset:  79%|███████▉  | 6752/8564 [00:20<00:03, 540.81 examples/s]Tokenizing train dataset:  79%|███████▉  | 6776/8564 [00:20<00:03, 531.89 examples/s]Tokenizing train dataset:  80%|████████  | 6885/8564 [00:20<00:03, 544.91 examples/s]Tokenizing train dataset:  80%|███████▉  | 6819/8564 [00:20<00:03, 502.42 examples/s]Tokenizing train dataset:  80%|███████▉  | 6834/8564 [00:20<00:03, 476.36 examples/s]Tokenizing train dataset:  81%|████████  | 6958/8564 [00:20<00:03, 523.32 examples/s]Tokenizing train dataset:  80%|████████  | 6885/8564 [00:20<00:03, 474.88 examples/s]Tokenizing train dataset:  81%|████████  | 6900/8564 [00:20<00:03, 462.66 examples/s]Tokenizing train dataset:  82%|████████▏ | 7019/8564 [00:20<00:03, 483.24 examples/s]Tokenizing train dataset:  81%|████████  | 6950/8564 [00:20<00:03, 507.17 examples/s]Tokenizing train dataset:  81%|████████  | 6954/8564 [00:20<00:03, 479.90 examples/s]Tokenizing train dataset:  83%|████████▎ | 7086/8564 [00:20<00:02, 524.12 examples/s]Tokenizing train dataset:  82%|████████▏ | 7026/8564 [00:20<00:03, 505.77 examples/s]Tokenizing train dataset:  82%|████████▏ | 7016/8564 [00:20<00:03, 455.65 examples/s]Tokenizing train dataset:  84%|████████▎ | 7170/8564 [00:20<00:02, 533.42 examples/s]Tokenizing train dataset:  83%|████████▎ | 7097/8564 [00:20<00:02, 543.50 examples/s]Tokenizing train dataset:  83%|████████▎ | 7071/8564 [00:20<00:03, 477.48 examples/s]Tokenizing train dataset:  84%|████████▍ | 7225/8564 [00:20<00:02, 533.84 examples/s]Tokenizing train dataset:  84%|████████▎ | 7154/8564 [00:21<00:02, 548.61 examples/s]Tokenizing train dataset:  83%|████████▎ | 7124/8564 [00:20<00:02, 487.05 examples/s]Tokenizing train dataset:  85%|████████▌ | 7285/8564 [00:20<00:02, 548.02 examples/s]Tokenizing train dataset:  84%|████████▍ | 7213/8564 [00:21<00:02, 557.81 examples/s]Tokenizing train dataset:  84%|████████▍ | 7177/8564 [00:20<00:02, 496.90 examples/s]Tokenizing train dataset:  86%|████████▌ | 7373/8564 [00:21<00:02, 559.05 examples/s]Tokenizing train dataset:  85%|████████▌ | 7292/8564 [00:21<00:02, 541.94 examples/s]Tokenizing train dataset:  85%|████████▍ | 7258/8564 [00:21<00:02, 509.74 examples/s]Tokenizing train dataset:  87%|████████▋ | 7434/8564 [00:21<00:01, 567.99 examples/s]Tokenizing train dataset:  86%|████████▌ | 7351/8564 [00:21<00:02, 552.37 examples/s]Tokenizing train dataset:  85%|████████▌ | 7313/8564 [00:21<00:02, 515.21 examples/s]Tokenizing train dataset:  88%|████████▊ | 7495/8564 [00:21<00:01, 573.72 examples/s]Tokenizing train dataset:  87%|████████▋ | 7417/8564 [00:21<00:02, 510.28 examples/s]Tokenizing train dataset:  86%|████████▌ | 7382/8564 [00:21<00:02, 470.75 examples/s]Tokenizing train dataset:  88%|████████▊ | 7562/8564 [00:21<00:01, 526.89 examples/s]Tokenizing train dataset:  87%|████████▋ | 7440/8564 [00:21<00:02, 493.96 examples/s]Tokenizing train dataset:  89%|████████▉ | 7623/8564 [00:21<00:01, 545.77 examples/s]Tokenizing train dataset:  88%|████████▊ | 7497/8564 [00:21<00:02, 516.17 examples/s]Tokenizing train dataset:  88%|████████▊ | 7513/8564 [00:21<00:02, 489.14 examples/s]Tokenizing train dataset:  88%|████████▊ | 7570/8564 [00:21<00:01, 501.19 examples/s]Tokenizing train dataset:  90%|████████▉ | 7707/8564 [00:21<00:01, 478.61 examples/s]Tokenizing train dataset:  89%|████████▊ | 7582/8564 [00:21<00:01, 534.79 examples/s]Tokenizing train dataset:  89%|████████▉ | 7632/8564 [00:22<00:01, 527.93 examples/s]Tokenizing train dataset:  89%|████████▉ | 7640/8564 [00:21<00:01, 540.48 examples/s]Tokenizing train dataset:  91%|█████████ | 7781/8564 [00:21<00:01, 480.30 examples/s]Tokenizing train dataset:  90%|████████▉ | 7707/8564 [00:22<00:01, 515.49 examples/s]Tokenizing train dataset:  90%|█████████ | 7710/8564 [00:22<00:01, 512.09 examples/s]Tokenizing train dataset:  92%|█████████▏| 7859/8564 [00:22<00:01, 484.85 examples/s]Tokenizing train dataset:  91%|█████████ | 7760/8564 [00:22<00:01, 515.70 examples/s]Tokenizing train dataset:  92%|█████████▏| 7915/8564 [00:22<00:01, 495.78 examples/s]Tokenizing train dataset:  91%|█████████ | 7783/8564 [00:22<00:01, 499.47 examples/s]Tokenizing train dataset:  91%|█████████▏| 7825/8564 [00:22<00:01, 481.98 examples/s]Tokenizing train dataset:  92%|█████████▏| 7840/8564 [00:22<00:01, 512.65 examples/s]Tokenizing train dataset:  93%|█████████▎| 8000/8564 [00:22<00:01, 517.54 examples/s]Tokenizing train dataset:  92%|█████████▏| 7885/8564 [00:22<00:01, 508.19 examples/s]Tokenizing train dataset:  92%|█████████▏| 7905/8564 [00:22<00:01, 331.65 examples/s]Tokenizing train dataset:  94%|█████████▍| 8063/8564 [00:22<00:01, 347.37 examples/s]Tokenizing train dataset:  93%|█████████▎| 7947/8564 [00:22<00:01, 329.37 examples/s]Tokenizing train dataset:  93%|█████████▎| 7963/8564 [00:22<00:01, 373.98 examples/s]Tokenizing train dataset:  95%|█████████▍| 8110/8564 [00:22<00:01, 366.03 examples/s]Tokenizing train dataset:  94%|█████████▎| 8013/8564 [00:22<00:01, 387.61 examples/s]Tokenizing train dataset:  94%|█████████▍| 8032/8564 [00:22<00:01, 394.60 examples/s]Tokenizing train dataset:  95%|█████████▌| 8174/8564 [00:22<00:01, 369.23 examples/s]Tokenizing train dataset:  94%|█████████▍| 8074/8564 [00:23<00:01, 381.26 examples/s]Tokenizing train dataset:  94%|█████████▍| 8091/8564 [00:23<00:01, 391.18 examples/s]Tokenizing train dataset:  96%|█████████▋| 8246/8564 [00:23<00:00, 397.51 examples/s]Tokenizing train dataset:  95%|█████████▌| 8143/8564 [00:23<00:01, 400.18 examples/s]Tokenizing train dataset:  95%|█████████▌| 8148/8564 [00:23<00:00, 425.98 examples/s]Tokenizing train dataset:  97%|█████████▋| 8315/8564 [00:23<00:00, 456.84 examples/s]Tokenizing train dataset:  96%|█████████▌| 8204/8564 [00:23<00:00, 438.93 examples/s]Tokenizing train dataset:  96%|█████████▌| 8199/8564 [00:23<00:00, 441.20 examples/s]Tokenizing train dataset:  96%|█████████▋| 8259/8564 [00:23<00:00, 463.20 examples/s]Tokenizing train dataset:  98%|█████████▊| 8386/8564 [00:23<00:00, 458.98 examples/s]Tokenizing train dataset:  96%|█████████▋| 8251/8564 [00:23<00:00, 457.41 examples/s]Tokenizing train dataset:  97%|█████████▋| 8329/8564 [00:23<00:00, 517.45 examples/s]Tokenizing train dataset:  97%|█████████▋| 8324/8564 [00:23<00:00, 525.41 examples/s]Tokenizing train dataset:  99%|█████████▉| 8458/8564 [00:23<00:00, 459.93 examples/s]Tokenizing train dataset:  98%|█████████▊| 8386/8564 [00:23<00:00, 465.48 examples/s]Tokenizing train dataset:  99%|█████████▉| 8520/8564 [00:23<00:00, 492.02 examples/s]Tokenizing train dataset:  98%|█████████▊| 8397/8564 [00:23<00:00, 509.09 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:23<00:00, 360.64 examples/s]
Tokenizing train dataset:  99%|█████████▉| 8470/8564 [00:23<00:00, 492.05 examples/s]Tokenizing train dataset:  99%|█████████▊| 8452/8564 [00:23<00:00, 517.71 examples/s]Tokenizing train dataset: 100%|█████████▉| 8530/8564 [00:24<00:00, 513.98 examples/s]Tokenizing train dataset:  99%|█████████▉| 8516/8564 [00:23<00:00, 546.08 examples/s]Tokenizing train dataset: 100%|██████████| 8564/8564 [00:24<00:00, 355.30 examples/s]
Tokenizing train dataset: 100%|██████████| 8564/8564 [00:23<00:00, 357.72 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 10979.16 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 10802.85 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 7248.62 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 13281.98 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 11220.04 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 10713.75 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 325.63 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 325.52 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 324.55 examples/s]Tokenizing eval dataset:   7%|▋         | 70/953 [00:00<00:03, 263.72 examples/s]Tokenizing eval dataset:   7%|▋         | 69/953 [00:00<00:03, 258.57 examples/s]Tokenizing eval dataset:   8%|▊         | 73/953 [00:00<00:03, 270.35 examples/s]Tokenizing eval dataset:  11%|█▏        | 108/953 [00:00<00:03, 255.02 examples/s]Tokenizing eval dataset:  11%|█         | 101/953 [00:00<00:03, 234.17 examples/s]Tokenizing eval dataset:  12%|█▏        | 110/953 [00:00<00:03, 253.16 examples/s]Tokenizing eval dataset:  15%|█▌        | 147/953 [00:00<00:03, 251.33 examples/s]Tokenizing eval dataset:  15%|█▍        | 140/953 [00:00<00:03, 235.95 examples/s]Tokenizing eval dataset:  15%|█▌        | 147/953 [00:00<00:03, 246.43 examples/s]Tokenizing eval dataset:  19%|█▉        | 182/953 [00:00<00:03, 240.46 examples/s]Tokenizing eval dataset:  18%|█▊        | 170/953 [00:00<00:03, 219.13 examples/s]Tokenizing eval dataset:  19%|█▉        | 180/953 [00:00<00:03, 229.10 examples/s]Tokenizing eval dataset:  23%|██▎       | 223/953 [00:00<00:02, 248.85 examples/s]Tokenizing eval dataset:  21%|██        | 201/953 [00:00<00:03, 212.35 examples/s]Tokenizing eval dataset:  23%|██▎       | 215/953 [00:00<00:03, 226.62 examples/s]Tokenizing eval dataset:  30%|███       | 290/953 [00:00<00:01, 346.11 examples/s]Tokenizing eval dataset:  27%|██▋       | 258/953 [00:01<00:02, 274.25 examples/s]Tokenizing eval dataset:  26%|██▌       | 245/953 [00:01<00:02, 236.34 examples/s]Tokenizing eval dataset:  36%|███▌      | 339/953 [00:01<00:01, 381.16 examples/s]Tokenizing eval dataset:  33%|███▎      | 319/953 [00:01<00:01, 356.85 examples/s]Tokenizing eval dataset:  32%|███▏      | 304/953 [00:01<00:02, 317.68 examples/s]Tokenizing eval dataset:  41%|████      | 392/953 [00:01<00:01, 415.75 examples/s]Tokenizing eval dataset:  39%|███▉      | 376/953 [00:01<00:01, 411.90 examples/s]Tokenizing eval dataset:  38%|███▊      | 358/953 [00:01<00:01, 370.03 examples/s]Tokenizing eval dataset:  48%|████▊     | 459/953 [00:01<00:01, 483.59 examples/s]Tokenizing eval dataset:  46%|████▌     | 439/953 [00:01<00:01, 466.61 examples/s]Tokenizing eval dataset:  43%|████▎     | 414/953 [00:01<00:01, 418.94 examples/s]Tokenizing eval dataset:  53%|█████▎    | 504/953 [00:01<00:00, 514.85 examples/s]Tokenizing eval dataset:  57%|█████▋    | 541/953 [00:01<00:00, 504.92 examples/s]Tokenizing eval dataset:  50%|████▉     | 474/953 [00:01<00:01, 466.29 examples/s]Tokenizing eval dataset:  59%|█████▉    | 564/953 [00:01<00:00, 537.76 examples/s]Tokenizing eval dataset:  64%|██████▍   | 609/953 [00:01<00:00, 546.44 examples/s]Tokenizing eval dataset:  56%|█████▌    | 536/953 [00:01<00:00, 506.55 examples/s]Tokenizing eval dataset:  66%|██████▌   | 626/953 [00:01<00:00, 559.44 examples/s]Tokenizing eval dataset:  70%|██████▉   | 667/953 [00:01<00:00, 552.40 examples/s]Tokenizing eval dataset:  64%|██████▎   | 607/953 [00:01<00:00, 562.08 examples/s]Tokenizing eval dataset:  73%|███████▎  | 700/953 [00:01<00:00, 531.15 examples/s]Tokenizing eval dataset:  78%|███████▊  | 742/953 [00:01<00:00, 527.85 examples/s]Tokenizing eval dataset:  72%|███████▏  | 685/953 [00:01<00:00, 544.76 examples/s]Tokenizing eval dataset:  79%|███████▉  | 756/953 [00:02<00:00, 380.26 examples/s]Tokenizing eval dataset:  86%|████████▋ | 822/953 [00:02<00:00, 388.34 examples/s]Tokenizing eval dataset:  81%|████████  | 769/953 [00:02<00:00, 393.71 examples/s]Tokenizing eval dataset:  85%|████████▍ | 810/953 [00:02<00:00, 370.43 examples/s]Tokenizing eval dataset:  94%|█████████▍| 894/953 [00:02<00:00, 410.90 examples/s]Tokenizing eval dataset:  87%|████████▋ | 833/953 [00:02<00:00, 400.99 examples/s]Tokenizing eval dataset:  90%|█████████ | 860/953 [00:02<00:00, 392.87 examples/s]Tokenizing eval dataset: 100%|█████████▉| 950/953 [00:02<00:00, 395.80 examples/s]Tokenizing eval dataset:  93%|█████████▎| 889/953 [00:02<00:00, 390.04 examples/s]Tokenizing eval dataset:  96%|█████████▋| 918/953 [00:02<00:00, 389.45 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 386.54 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 378.86 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 400.30 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 368.17 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Parameter Offload: Total persistent parameters: 605696 in 169 params
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
wandb: Currently logged in as: vajdadario (slolama) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.7
wandb: Run data is saved locally in /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/wandb/run-20250530_234740-jf0h847o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DPO_r-64_lr-3e-07_e-3_b-0.2
wandb: ⭐️ View project at https://wandb.ai/slolama/GaMS-9B-Translation-DPO
wandb: 🚀 View run at https://wandb.ai/slolama/GaMS-9B-Translation-DPO/runs/jf0h847o
  0%|          | 0/1605 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  0%|          | 1/1605 [00:17<7:40:30, 17.23s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 0.0, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4416.0, 'logps/rejected': -3968.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.0}
  0%|          | 1/1605 [00:17<7:40:30, 17.23s/it]  0%|          | 2/1605 [00:24<5:01:30, 11.29s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.006071929005137e-11, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4624.0, 'logps/rejected': -4800.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.0}
  0%|          | 2/1605 [00:24<5:01:30, 11.29s/it]  0%|          | 3/1605 [00:31<4:15:19,  9.56s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4012143858010274e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5760.0, 'logps/rejected': -5488.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 3/1605 [00:31<4:15:19,  9.56s/it]  0%|          | 4/1605 [00:39<3:57:05,  8.89s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1018215787015413e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5120.0, 'logps/rejected': -5120.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 4/1605 [00:39<3:57:05,  8.89s/it]  0%|          | 5/1605 [00:47<3:42:57,  8.36s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.802428771602055e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4112.0, 'logps/rejected': -3936.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 5/1605 [00:47<3:42:57,  8.36s/it]  0%|          | 6/1605 [00:55<3:39:14,  8.23s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.5030359645025685e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -8368.0, 'logps/rejected': -6080.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 6/1605 [00:55<3:39:14,  8.23s/it]  0%|          | 7/1605 [01:02<3:34:11,  8.04s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.2036431574030827e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6624.0, 'logps/rejected': -4920.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 7/1605 [01:02<3:34:11,  8.04s/it]  0%|          | 8/1605 [01:10<3:31:24,  7.94s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.904250350303596e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6224.0, 'logps/rejected': -4712.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.01}
  0%|          | 8/1605 [01:10<3:31:24,  7.94s/it]  1%|          | 9/1605 [01:18<3:33:02,  8.01s/it]                                                  {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.60485754320411e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6544.0, 'logps/rejected': -5280.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.02}
  1%|          | 9/1605 [01:18<3:33:02,  8.01s/it]  1%|          | 10/1605 [01:26<3:32:39,  8.00s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.305464736104624e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6768.0, 'logps/rejected': -6112.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.02}
  1%|          | 10/1605 [01:26<3:32:39,  8.00s/it]  1%|          | 11/1605 [01:34<3:28:42,  7.86s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.006071929005137e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6096.0, 'logps/rejected': -5408.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.02}
  1%|          | 11/1605 [01:34<3:28:42,  7.86s/it]  1%|          | 12/1605 [01:41<3:26:37,  7.78s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.706679121905651e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5808.0, 'logps/rejected': -5872.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.02}
  1%|          | 12/1605 [01:41<3:26:37,  7.78s/it]  1%|          | 13/1605 [01:49<3:28:45,  7.87s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.407286314806165e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6256.0, 'logps/rejected': -6032.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.02}
  1%|          | 13/1605 [01:49<3:28:45,  7.87s/it]  1%|          | 14/1605 [01:57<3:28:18,  7.86s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.107893507706679e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5904.0, 'logps/rejected': -5904.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.03}
  1%|          | 14/1605 [01:57<3:28:18,  7.86s/it]  1%|          | 15/1605 [02:05<3:24:51,  7.73s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.808500700607193e-10, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5648.0, 'logps/rejected': -5840.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.03}
  1%|          | 15/1605 [02:05<3:24:51,  7.73s/it]  1%|          | 16/1605 [02:12<3:20:13,  7.56s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0509107893507706e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4464.0, 'logps/rejected': -3208.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.03}
  1%|          | 16/1605 [02:12<3:20:13,  7.56s/it]  1%|          | 17/1605 [02:19<3:16:17,  7.42s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.120971508640822e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5024.0, 'logps/rejected': -5024.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.03}
  1%|          | 17/1605 [02:19<3:16:17,  7.42s/it]  1%|          | 18/1605 [02:27<3:19:55,  7.56s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1910322279308732e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7072.0, 'logps/rejected': -5856.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.03}
  1%|          | 18/1605 [02:27<3:19:55,  7.56s/it]  1%|          | 19/1605 [02:34<3:20:54,  7.60s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2610929472209248e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5232.0, 'logps/rejected': -5024.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|          | 19/1605 [02:34<3:20:54,  7.60s/it]  1%|          | 20/1605 [02:41<3:16:07,  7.42s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3311536665109763e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4800.0, 'logps/rejected': -4960.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|          | 20/1605 [02:41<3:16:07,  7.42s/it]  1%|▏         | 21/1605 [02:50<3:23:25,  7.71s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4012143858010274e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6832.0, 'logps/rejected': -5952.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|▏         | 21/1605 [02:50<3:23:25,  7.71s/it]  1%|▏         | 22/1605 [02:58<3:23:14,  7.70s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.471275105091079e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6464.0, 'logps/rejected': -6368.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|▏         | 22/1605 [02:58<3:23:14,  7.70s/it]  1%|▏         | 23/1605 [03:06<3:27:29,  7.87s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.5413358243811302e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6608.0, 'logps/rejected': -6160.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|▏         | 23/1605 [03:06<3:27:29,  7.87s/it]  1%|▏         | 24/1605 [03:13<3:24:17,  7.75s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.6113965436711816e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5280.0, 'logps/rejected': -3824.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.04}
  1%|▏         | 24/1605 [03:13<3:24:17,  7.75s/it]  2%|▏         | 25/1605 [03:21<3:20:33,  7.62s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.681457262961233e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4472.0, 'logps/rejected': -3800.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.05}
  2%|▏         | 25/1605 [03:21<3:20:33,  7.62s/it]  2%|▏         | 26/1605 [03:28<3:19:29,  7.58s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.7515179822512844e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5024.0, 'logps/rejected': -4480.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.05}
  2%|▏         | 26/1605 [03:28<3:19:29,  7.58s/it]  2%|▏         | 27/1605 [03:35<3:17:01,  7.49s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8215787015413357e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5504.0, 'logps/rejected': -5168.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.05}
  2%|▏         | 27/1605 [03:35<3:17:01,  7.49s/it]  2%|▏         | 28/1605 [03:43<3:18:19,  7.55s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.891639420831387e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5536.0, 'logps/rejected': -4320.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.05}
  2%|▏         | 28/1605 [03:43<3:18:19,  7.55s/it]  2%|▏         | 29/1605 [03:51<3:18:00,  7.54s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.9617001401214386e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5920.0, 'logps/rejected': -4968.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.05}
  2%|▏         | 29/1605 [03:51<3:18:00,  7.54s/it]  2%|▏         | 30/1605 [03:58<3:18:24,  7.56s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.03176085941149e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5856.0, 'logps/rejected': -5232.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.06}
  2%|▏         | 30/1605 [03:58<3:18:24,  7.56s/it]  2%|▏         | 31/1605 [04:06<3:17:07,  7.51s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.101821578701541e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5280.0, 'logps/rejected': -4768.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.06}
  2%|▏         | 31/1605 [04:06<3:17:07,  7.51s/it]  2%|▏         | 32/1605 [04:13<3:16:29,  7.50s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1718822979915927e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5168.0, 'logps/rejected': -4608.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.06}
  2%|▏         | 32/1605 [04:13<3:16:29,  7.50s/it]  2%|▏         | 33/1605 [04:20<3:14:54,  7.44s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.241943017281644e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7056.0, 'logps/rejected': -5840.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.06}
  2%|▏         | 33/1605 [04:20<3:14:54,  7.44s/it]  2%|▏         | 34/1605 [04:28<3:17:31,  7.54s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3120037365716953e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7856.0, 'logps/rejected': -6608.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.06}
  2%|▏         | 34/1605 [04:28<3:17:31,  7.54s/it]  2%|▏         | 35/1605 [04:36<3:16:40,  7.52s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3820644558617465e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6448.0, 'logps/rejected': -4736.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 35/1605 [04:36<3:16:40,  7.52s/it]  2%|▏         | 36/1605 [04:43<3:18:00,  7.57s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.4521251751517984e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4848.0, 'logps/rejected': -5056.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 36/1605 [04:43<3:18:00,  7.57s/it]  2%|▏         | 37/1605 [04:50<3:14:19,  7.44s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5221858944418495e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4112.0, 'logps/rejected': -3576.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 37/1605 [04:50<3:14:19,  7.44s/it]  2%|▏         | 38/1605 [04:58<3:16:28,  7.52s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5922466137319006e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6832.0, 'logps/rejected': -4768.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 38/1605 [04:58<3:16:28,  7.52s/it]  2%|▏         | 39/1605 [05:06<3:17:42,  7.58s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6623073330219526e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6320.0, 'logps/rejected': -6080.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 39/1605 [05:06<3:17:42,  7.58s/it]  2%|▏         | 40/1605 [05:13<3:16:55,  7.55s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.7323680523120037e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5120.0, 'logps/rejected': -4528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.07}
  2%|▏         | 40/1605 [05:13<3:16:55,  7.55s/it]  3%|▎         | 41/1605 [05:21<3:16:14,  7.53s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.8024287716020548e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5376.0, 'logps/rejected': -4568.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.08}
  3%|▎         | 41/1605 [05:21<3:16:14,  7.53s/it]  3%|▎         | 42/1605 [05:28<3:12:20,  7.38s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.8724894908921063e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5200.0, 'logps/rejected': -4544.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.08}
  3%|▎         | 42/1605 [05:28<3:12:20,  7.38s/it]  3%|▎         | 43/1605 [05:35<3:13:00,  7.41s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.942550210182158e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5840.0, 'logps/rejected': -4992.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.08}
  3%|▎         | 43/1605 [05:35<3:13:00,  7.41s/it]  3%|▎         | 44/1605 [05:43<3:13:48,  7.45s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.012610929472209e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5392.0, 'logps/rejected': -5008.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.08}
  3%|▎         | 44/1605 [05:43<3:13:48,  7.45s/it]  3%|▎         | 45/1605 [05:51<3:15:46,  7.53s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.0826716487622605e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5376.0, 'logps/rejected': -5128.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.08}
  3%|▎         | 45/1605 [05:51<3:15:46,  7.53s/it]  3%|▎         | 46/1605 [05:58<3:13:33,  7.45s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.152732368052312e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4384.0, 'logps/rejected': -3868.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.09}
  3%|▎         | 46/1605 [05:58<3:13:33,  7.45s/it]  3%|▎         | 47/1605 [06:06<3:22:20,  7.79s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.222793087342363e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -8112.0, 'logps/rejected': -6528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.09}
  3%|▎         | 47/1605 [06:07<3:22:20,  7.79s/it]  3%|▎         | 48/1605 [06:15<3:28:26,  8.03s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.2928538066324146e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5712.0, 'logps/rejected': -5488.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.09}
  3%|▎         | 48/1605 [06:15<3:28:26,  8.03s/it]  3%|▎         | 49/1605 [06:23<3:26:07,  7.95s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.362914525922466e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6256.0, 'logps/rejected': -5424.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.09}
  3%|▎         | 49/1605 [06:23<3:26:07,  7.95s/it]  3%|▎         | 50/1605 [06:30<3:20:18,  7.73s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.4329752452125173e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5088.0, 'logps/rejected': -4992.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.09}
  3%|▎         | 50/1605 [06:30<3:20:18,  7.73s/it]  3%|▎         | 51/1605 [06:37<3:17:22,  7.62s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.5030359645025688e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6368.0, 'logps/rejected': -4672.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 51/1605 [06:37<3:17:22,  7.62s/it]  3%|▎         | 52/1605 [06:45<3:17:03,  7.61s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.5730966837926203e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5632.0, 'logps/rejected': -5136.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 52/1605 [06:45<3:17:03,  7.61s/it]  3%|▎         | 53/1605 [06:52<3:15:08,  7.54s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.6431574030826714e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4728.0, 'logps/rejected': -4544.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 53/1605 [06:52<3:15:08,  7.54s/it]  3%|▎         | 54/1605 [07:00<3:14:29,  7.52s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.713218122372723e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6448.0, 'logps/rejected': -5872.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 54/1605 [07:00<3:14:29,  7.52s/it]  3%|▎         | 55/1605 [07:08<3:17:57,  7.66s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.783278841662774e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4736.0, 'logps/rejected': -4136.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 55/1605 [07:08<3:17:57,  7.66s/it]  3%|▎         | 56/1605 [07:15<3:16:24,  7.61s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.8533395609528256e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5264.0, 'logps/rejected': -5504.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.1}
  3%|▎         | 56/1605 [07:15<3:16:24,  7.61s/it]  4%|▎         | 57/1605 [07:23<3:19:03,  7.72s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.923400280242877e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5696.0, 'logps/rejected': -5104.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.11}
  4%|▎         | 57/1605 [07:23<3:19:03,  7.72s/it]  4%|▎         | 58/1605 [07:31<3:23:06,  7.88s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 3.993460999532928e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6832.0, 'logps/rejected': -5312.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.11}
  4%|▎         | 58/1605 [07:32<3:23:06,  7.88s/it]  4%|▎         | 59/1605 [07:39<3:23:34,  7.90s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.06352171882298e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7104.0, 'logps/rejected': -6576.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.11}
  4%|▎         | 59/1605 [07:39<3:23:34,  7.90s/it]  4%|▎         | 60/1605 [07:48<3:25:08,  7.97s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.133582438113031e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6336.0, 'logps/rejected': -4560.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.11}
  4%|▎         | 60/1605 [07:48<3:25:08,  7.97s/it]  4%|▍         | 61/1605 [07:55<3:23:22,  7.90s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.203643157403082e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5552.0, 'logps/rejected': -4992.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.11}
  4%|▍         | 61/1605 [07:55<3:23:22,  7.90s/it]  4%|▍         | 62/1605 [08:03<3:20:06,  7.78s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.273703876693134e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5968.0, 'logps/rejected': -5856.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.12}
  4%|▍         | 62/1605 [08:03<3:20:06,  7.78s/it]  4%|▍         | 63/1605 [08:11<3:22:07,  7.86s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.3437645959831854e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6352.0, 'logps/rejected': -5872.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.12}
  4%|▍         | 63/1605 [08:11<3:22:07,  7.86s/it]  4%|▍         | 64/1605 [08:19<3:24:31,  7.96s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.413825315273236e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6192.0, 'logps/rejected': -5936.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.12}
  4%|▍         | 64/1605 [08:19<3:24:31,  7.96s/it]  4%|▍         | 65/1605 [08:27<3:21:01,  7.83s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.483886034563288e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4432.0, 'logps/rejected': -4320.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.12}
  4%|▍         | 65/1605 [08:27<3:21:01,  7.83s/it]  4%|▍         | 66/1605 [08:34<3:20:34,  7.82s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.553946753853339e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7888.0, 'logps/rejected': -6880.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.12}
  4%|▍         | 66/1605 [08:34<3:20:34,  7.82s/it]  4%|▍         | 67/1605 [08:42<3:21:30,  7.86s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.624007473143391e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7456.0, 'logps/rejected': -5896.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 67/1605 [08:42<3:21:30,  7.86s/it]  4%|▍         | 68/1605 [08:50<3:18:03,  7.73s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.694068192433442e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4432.0, 'logps/rejected': -3912.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 68/1605 [08:50<3:18:03,  7.73s/it]  4%|▍         | 69/1605 [08:57<3:16:00,  7.66s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.764128911723493e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4256.0, 'logps/rejected': -4416.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 69/1605 [08:57<3:16:00,  7.66s/it]  4%|▍         | 70/1605 [09:04<3:12:15,  7.52s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.834189631013545e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5360.0, 'logps/rejected': -4448.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 70/1605 [09:05<3:12:15,  7.52s/it]  4%|▍         | 71/1605 [09:12<3:13:18,  7.56s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.904250350303597e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5536.0, 'logps/rejected': -5376.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 71/1605 [09:12<3:13:18,  7.56s/it]  4%|▍         | 72/1605 [09:20<3:15:23,  7.65s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 4.9743110695936475e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5904.0, 'logps/rejected': -5248.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.13}
  4%|▍         | 72/1605 [09:20<3:15:23,  7.65s/it]  5%|▍         | 73/1605 [09:27<3:14:34,  7.62s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.044371788883699e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5440.0, 'logps/rejected': -5536.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.14}
  5%|▍         | 73/1605 [09:28<3:14:34,  7.62s/it]  5%|▍         | 74/1605 [09:35<3:14:46,  7.63s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.1144325081737505e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4784.0, 'logps/rejected': -4576.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.14}
  5%|▍         | 74/1605 [09:35<3:14:46,  7.63s/it]  5%|▍         | 75/1605 [09:43<3:15:51,  7.68s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.184493227463801e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6240.0, 'logps/rejected': -5536.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.14}
  5%|▍         | 75/1605 [09:43<3:15:51,  7.68s/it]  5%|▍         | 76/1605 [09:50<3:13:24,  7.59s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.254553946753854e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5576.0, 'logps/rejected': -5368.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.14}
  5%|▍         | 76/1605 [09:50<3:13:24,  7.59s/it]  5%|▍         | 77/1605 [09:57<3:09:52,  7.46s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.324614666043905e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4184.0, 'logps/rejected': -4232.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.14}
  5%|▍         | 77/1605 [09:58<3:09:52,  7.46s/it]  5%|▍         | 78/1605 [10:05<3:08:11,  7.39s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.394675385333956e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5664.0, 'logps/rejected': -5520.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▍         | 78/1605 [10:05<3:08:11,  7.39s/it]  5%|▍         | 79/1605 [10:12<3:07:12,  7.36s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.464736104624007e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4960.0, 'logps/rejected': -4528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▍         | 79/1605 [10:12<3:07:12,  7.36s/it]  5%|▍         | 80/1605 [10:20<3:11:38,  7.54s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.534796823914059e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4880.0, 'logps/rejected': -4344.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▍         | 80/1605 [10:20<3:11:38,  7.54s/it]  5%|▌         | 81/1605 [10:28<3:11:54,  7.56s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.6048575432041096e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6896.0, 'logps/rejected': -5824.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▌         | 81/1605 [10:28<3:11:54,  7.56s/it]  5%|▌         | 82/1605 [10:36<3:16:55,  7.76s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.674918262494162e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7312.0, 'logps/rejected': -5584.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▌         | 82/1605 [10:36<3:16:55,  7.76s/it]  5%|▌         | 83/1605 [10:43<3:14:10,  7.65s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.744978981784213e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5184.0, 'logps/rejected': -5056.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.15}
  5%|▌         | 83/1605 [10:43<3:14:10,  7.65s/it]  5%|▌         | 84/1605 [10:51<3:11:42,  7.56s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.815039701074264e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4144.0, 'logps/rejected': -3936.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.16}
  5%|▌         | 84/1605 [10:51<3:11:42,  7.56s/it]  5%|▌         | 85/1605 [10:59<3:16:14,  7.75s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.885100420364316e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5264.0, 'logps/rejected': -5312.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.16}
  5%|▌         | 85/1605 [10:59<3:16:14,  7.75s/it]  5%|▌         | 86/1605 [11:07<3:16:53,  7.78s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 5.955161139654366e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5888.0, 'logps/rejected': -3680.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.16}
  5%|▌         | 86/1605 [11:07<3:16:53,  7.78s/it]  5%|▌         | 87/1605 [11:14<3:12:45,  7.62s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.025221858944418e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4752.0, 'logps/rejected': -4608.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.16}
  5%|▌         | 87/1605 [11:14<3:12:45,  7.62s/it]  5%|▌         | 88/1605 [11:22<3:14:34,  7.70s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.09528257823447e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5072.0, 'logps/rejected': -3728.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.16}
  5%|▌         | 88/1605 [11:22<3:14:34,  7.70s/it]  6%|▌         | 89/1605 [11:30<3:15:38,  7.74s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.165343297524521e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6128.0, 'logps/rejected': -6320.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.17}
  6%|▌         | 89/1605 [11:30<3:15:38,  7.74s/it]  6%|▌         | 90/1605 [11:37<3:15:12,  7.73s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.2354040168145725e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5552.0, 'logps/rejected': -5224.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.17}
  6%|▌         | 90/1605 [11:37<3:15:12,  7.73s/it]  6%|▌         | 91/1605 [11:45<3:11:57,  7.61s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.305464736104624e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5520.0, 'logps/rejected': -5360.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.17}
  6%|▌         | 91/1605 [11:45<3:11:57,  7.61s/it]  6%|▌         | 92/1605 [11:52<3:12:26,  7.63s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.375525455394675e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5984.0, 'logps/rejected': -5408.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.17}
  6%|▌         | 92/1605 [11:52<3:12:26,  7.63s/it]  6%|▌         | 93/1605 [12:00<3:09:41,  7.53s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.445586174684726e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4448.0, 'logps/rejected': -4288.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.17}
  6%|▌         | 93/1605 [12:00<3:09:41,  7.53s/it]  6%|▌         | 94/1605 [12:07<3:07:02,  7.43s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.5156468939747786e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5128.0, 'logps/rejected': -4168.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 94/1605 [12:07<3:07:02,  7.43s/it]  6%|▌         | 95/1605 [12:15<3:09:52,  7.54s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.585707613264829e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6176.0, 'logps/rejected': -5248.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 95/1605 [12:15<3:09:52,  7.54s/it]  6%|▌         | 96/1605 [12:23<3:13:08,  7.68s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.655768332554881e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5280.0, 'logps/rejected': -5056.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 96/1605 [12:23<3:13:08,  7.68s/it]  6%|▌         | 97/1605 [12:30<3:13:33,  7.70s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.725829051844932e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6352.0, 'logps/rejected': -5744.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 97/1605 [12:30<3:13:33,  7.70s/it]  6%|▌         | 98/1605 [12:38<3:13:57,  7.72s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.795889771134983e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6048.0, 'logps/rejected': -4304.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 98/1605 [12:38<3:13:57,  7.72s/it]  6%|▌         | 99/1605 [12:45<3:11:18,  7.62s/it]                                                   {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.8659504904250345e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5736.0, 'logps/rejected': -5208.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.18}
  6%|▌         | 99/1605 [12:46<3:11:18,  7.62s/it]  6%|▌         | 100/1605 [12:53<3:09:50,  7.57s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 6.936011209715087e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3976.0, 'logps/rejected': -3608.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.19}
  6%|▌         | 100/1605 [12:53<3:09:50,  7.57s/it]  6%|▋         | 101/1605 [13:01<3:10:14,  7.59s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.0060719290051376e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5736.0, 'logps/rejected': -4768.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.19}
  6%|▋         | 101/1605 [13:01<3:10:14,  7.59s/it]  6%|▋         | 102/1605 [13:08<3:08:03,  7.51s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.076132648295189e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4416.0, 'logps/rejected': -4256.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.19}
  6%|▋         | 102/1605 [13:08<3:08:03,  7.51s/it]  6%|▋         | 103/1605 [13:15<3:07:12,  7.48s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.146193367585241e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6000.0, 'logps/rejected': -4784.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.19}
  6%|▋         | 103/1605 [13:15<3:07:12,  7.48s/it]  6%|▋         | 104/1605 [13:23<3:10:58,  7.63s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.216254086875291e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4448.0, 'logps/rejected': -4504.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.19}
  6%|▋         | 104/1605 [13:23<3:10:58,  7.63s/it]  7%|▋         | 105/1605 [13:31<3:14:18,  7.77s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.286314806165343e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7248.0, 'logps/rejected': -6128.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.2}
  7%|▋         | 105/1605 [13:31<3:14:18,  7.77s/it]  7%|▋         | 106/1605 [13:39<3:12:02,  7.69s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.3563755254553935e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5808.0, 'logps/rejected': -5968.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.2}
  7%|▋         | 106/1605 [13:39<3:12:02,  7.69s/it]  7%|▋         | 107/1605 [13:46<3:08:29,  7.55s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.426436244745446e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5232.0, 'logps/rejected': -4464.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.2}
  7%|▋         | 107/1605 [13:46<3:08:29,  7.55s/it]  7%|▋         | 108/1605 [13:54<3:11:57,  7.69s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.496496964035497e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5296.0, 'logps/rejected': -5248.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.2}
  7%|▋         | 108/1605 [13:54<3:11:57,  7.69s/it]  7%|▋         | 109/1605 [14:02<3:11:25,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.566557683325548e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4848.0, 'logps/rejected': -4040.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.2}
  7%|▋         | 109/1605 [14:02<3:11:25,  7.68s/it]  7%|▋         | 110/1605 [14:09<3:08:46,  7.58s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.6366184026156e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4256.0, 'logps/rejected': -3872.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.21}
  7%|▋         | 110/1605 [14:09<3:08:46,  7.58s/it]  7%|▋         | 111/1605 [14:16<3:06:22,  7.49s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.706679121905651e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6048.0, 'logps/rejected': -5360.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.21}
  7%|▋         | 111/1605 [14:16<3:06:22,  7.49s/it]  7%|▋         | 112/1605 [14:24<3:06:13,  7.48s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.776739841195703e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4848.0, 'logps/rejected': -4224.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.21}
  7%|▋         | 112/1605 [14:24<3:06:13,  7.48s/it]  7%|▋         | 113/1605 [14:31<3:07:17,  7.53s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.846800560485754e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5472.0, 'logps/rejected': -4416.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.21}
  7%|▋         | 113/1605 [14:32<3:07:17,  7.53s/it]  7%|▋         | 114/1605 [14:39<3:08:53,  7.60s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.916861279775806e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6256.0, 'logps/rejected': -4848.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.21}
  7%|▋         | 114/1605 [14:39<3:08:53,  7.60s/it]  7%|▋         | 115/1605 [14:47<3:08:40,  7.60s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 7.986921999065856e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5600.0, 'logps/rejected': -5056.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.21}
  7%|▋         | 115/1605 [14:47<3:08:40,  7.60s/it]  7%|▋         | 116/1605 [14:55<3:10:57,  7.69s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.056982718355909e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5568.0, 'logps/rejected': -4072.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.22}
  7%|▋         | 116/1605 [14:55<3:10:57,  7.69s/it]  7%|▋         | 117/1605 [15:03<3:13:34,  7.81s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.12704343764596e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6432.0, 'logps/rejected': -6384.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.22}
  7%|▋         | 117/1605 [15:03<3:13:34,  7.81s/it]  7%|▋         | 118/1605 [15:11<3:15:22,  7.88s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.19710415693601e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -8000.0, 'logps/rejected': -6272.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.22}
  7%|▋         | 118/1605 [15:11<3:15:22,  7.88s/it]  7%|▋         | 119/1605 [15:18<3:12:05,  7.76s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.267164876226062e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5344.0, 'logps/rejected': -4320.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.22}
  7%|▋         | 119/1605 [15:18<3:12:05,  7.76s/it]  7%|▋         | 120/1605 [15:26<3:11:21,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.337225595516113e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7152.0, 'logps/rejected': -7520.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.22}
  7%|▋         | 120/1605 [15:26<3:11:21,  7.73s/it]  8%|▊         | 121/1605 [15:33<3:07:18,  7.57s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.407286314806165e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4576.0, 'logps/rejected': -3912.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.23}
  8%|▊         | 121/1605 [15:33<3:07:18,  7.57s/it]  8%|▊         | 122/1605 [15:41<3:07:40,  7.59s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.477347034096216e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5472.0, 'logps/rejected': -4864.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.23}
  8%|▊         | 122/1605 [15:41<3:07:40,  7.59s/it]  8%|▊         | 123/1605 [15:49<3:10:53,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.547407753386268e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5856.0, 'logps/rejected': -5280.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.23}
  8%|▊         | 123/1605 [15:49<3:10:53,  7.73s/it]  8%|▊         | 124/1605 [15:56<3:08:11,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.61746847267632e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5056.0, 'logps/rejected': -5392.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.23}
  8%|▊         | 124/1605 [15:56<3:08:11,  7.62s/it]  8%|▊         | 125/1605 [16:05<3:15:39,  7.93s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.687529191966371e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7008.0, 'logps/rejected': -5728.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.23}
  8%|▊         | 125/1605 [16:05<3:15:39,  7.93s/it]  8%|▊         | 126/1605 [16:13<3:16:06,  7.96s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.757589911256422e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3904.0, 'logps/rejected': -3392.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.24}
  8%|▊         | 126/1605 [16:13<3:16:06,  7.96s/it]  8%|▊         | 127/1605 [16:21<3:18:00,  8.04s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.827650630546472e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7056.0, 'logps/rejected': -5024.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.24}
  8%|▊         | 127/1605 [16:21<3:18:00,  8.04s/it]  8%|▊         | 128/1605 [16:28<3:10:43,  7.75s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.897711349836525e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4936.0, 'logps/rejected': -4512.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.24}
  8%|▊         | 128/1605 [16:28<3:10:43,  7.75s/it]  8%|▊         | 129/1605 [16:36<3:10:11,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 8.967772069126575e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6304.0, 'logps/rejected': -5520.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.24}
  8%|▊         | 129/1605 [16:36<3:10:11,  7.73s/it]  8%|▊         | 130/1605 [16:43<3:08:22,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.037832788416627e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4624.0, 'logps/rejected': -4576.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.24}
  8%|▊         | 130/1605 [16:44<3:08:22,  7.66s/it]  8%|▊         | 131/1605 [16:51<3:07:36,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.107893507706678e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4576.0, 'logps/rejected': -4448.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.24}
  8%|▊         | 131/1605 [16:51<3:07:36,  7.64s/it]  8%|▊         | 132/1605 [16:58<3:05:52,  7.57s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.17795422699673e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3544.0, 'logps/rejected': -2844.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.25}
  8%|▊         | 132/1605 [16:58<3:05:52,  7.57s/it]  8%|▊         | 133/1605 [17:06<3:06:42,  7.61s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.248014946286781e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6048.0, 'logps/rejected': -5696.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.25}
  8%|▊         | 133/1605 [17:06<3:06:42,  7.61s/it]  8%|▊         | 134/1605 [17:14<3:10:58,  7.79s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.318075665576833e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7440.0, 'logps/rejected': -6640.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.25}
  8%|▊         | 134/1605 [17:14<3:10:58,  7.79s/it]  8%|▊         | 135/1605 [17:22<3:13:25,  7.89s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.388136384866884e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5664.0, 'logps/rejected': -5216.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.25}
  8%|▊         | 135/1605 [17:23<3:13:25,  7.89s/it]  8%|▊         | 136/1605 [17:30<3:13:11,  7.89s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.458197104156936e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6616.0, 'logps/rejected': -4704.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.25}
  8%|▊         | 136/1605 [17:30<3:13:11,  7.89s/it]  9%|▊         | 137/1605 [17:38<3:12:52,  7.88s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.528257823446986e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7600.0, 'logps/rejected': -6976.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.26}
  9%|▊         | 137/1605 [17:38<3:12:52,  7.88s/it]  9%|▊         | 138/1605 [17:46<3:09:28,  7.75s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.598318542737039e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5808.0, 'logps/rejected': -5344.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.26}
  9%|▊         | 138/1605 [17:46<3:09:28,  7.75s/it]  9%|▊         | 139/1605 [17:53<3:09:03,  7.74s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.66837926202709e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5696.0, 'logps/rejected': -4272.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.26}
  9%|▊         | 139/1605 [17:54<3:09:03,  7.74s/it]  9%|▊         | 140/1605 [18:01<3:10:44,  7.81s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.73843998131714e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4632.0, 'logps/rejected': -3992.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.26}
  9%|▊         | 140/1605 [18:02<3:10:44,  7.81s/it]  9%|▉         | 141/1605 [18:09<3:08:40,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.808500700607194e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5760.0, 'logps/rejected': -5520.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.26}
  9%|▉         | 141/1605 [18:09<3:08:40,  7.73s/it]  9%|▉         | 142/1605 [18:17<3:09:48,  7.78s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.878561419897243e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6048.0, 'logps/rejected': -4656.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.27}
  9%|▉         | 142/1605 [18:17<3:09:48,  7.78s/it]  9%|▉         | 143/1605 [18:24<3:07:57,  7.71s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 9.948622139187295e-09, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5520.0, 'logps/rejected': -5232.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.27}
  9%|▉         | 143/1605 [18:25<3:07:57,  7.71s/it]  9%|▉         | 144/1605 [18:32<3:08:19,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0018682858477348e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5568.0, 'logps/rejected': -5472.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.27}
  9%|▉         | 144/1605 [18:32<3:08:19,  7.73s/it]  9%|▉         | 145/1605 [18:40<3:06:38,  7.67s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0088743577767398e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4848.0, 'logps/rejected': -4752.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.27}
  9%|▉         | 145/1605 [18:40<3:06:38,  7.67s/it]  9%|▉         | 146/1605 [18:48<3:07:55,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.015880429705745e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5584.0, 'logps/rejected': -5024.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.27}
  9%|▉         | 146/1605 [18:48<3:07:55,  7.73s/it]  9%|▉         | 147/1605 [18:55<3:06:42,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0228865016347501e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4416.0, 'logps/rejected': -4320.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.27}
  9%|▉         | 147/1605 [18:55<3:06:42,  7.68s/it]  9%|▉         | 148/1605 [19:03<3:10:14,  7.83s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0298925735637553e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6064.0, 'logps/rejected': -4576.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.28}
  9%|▉         | 148/1605 [19:03<3:10:14,  7.83s/it]  9%|▉         | 149/1605 [19:11<3:10:53,  7.87s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0368986454927602e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6784.0, 'logps/rejected': -6288.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.28}
  9%|▉         | 149/1605 [19:11<3:10:53,  7.87s/it]  9%|▉         | 150/1605 [19:20<3:14:11,  8.01s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0439047174217656e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7072.0, 'logps/rejected': -6048.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.28}
  9%|▉         | 150/1605 [19:20<3:14:11,  8.01s/it]  9%|▉         | 151/1605 [19:27<3:11:22,  7.90s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0509107893507707e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5936.0, 'logps/rejected': -4480.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.28}
  9%|▉         | 151/1605 [19:27<3:11:22,  7.90s/it]  9%|▉         | 152/1605 [19:35<3:10:44,  7.88s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0579168612797757e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4160.0, 'logps/rejected': -3984.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.28}
  9%|▉         | 152/1605 [19:35<3:10:44,  7.88s/it] 10%|▉         | 153/1605 [19:43<3:09:46,  7.84s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.064922933208781e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5600.0, 'logps/rejected': -4400.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.29}
 10%|▉         | 153/1605 [19:43<3:09:46,  7.84s/it] 10%|▉         | 154/1605 [19:51<3:12:29,  7.96s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.071929005137786e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6448.0, 'logps/rejected': -5536.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.29}
 10%|▉         | 154/1605 [19:51<3:12:29,  7.96s/it] 10%|▉         | 155/1605 [19:59<3:11:03,  7.91s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0789350770667912e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3768.0, 'logps/rejected': -3808.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.29}
 10%|▉         | 155/1605 [19:59<3:11:03,  7.91s/it] 10%|▉         | 156/1605 [20:07<3:09:45,  7.86s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0859411489957965e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5104.0, 'logps/rejected': -4912.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.29}
 10%|▉         | 156/1605 [20:07<3:09:45,  7.86s/it] 10%|▉         | 157/1605 [20:15<3:10:30,  7.89s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0929472209248015e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5824.0, 'logps/rejected': -5176.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.29}
 10%|▉         | 157/1605 [20:15<3:10:30,  7.89s/it] 10%|▉         | 158/1605 [20:22<3:08:40,  7.82s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.0999532928538066e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5616.0, 'logps/rejected': -4384.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.3}
 10%|▉         | 158/1605 [20:22<3:08:40,  7.82s/it] 10%|▉         | 159/1605 [20:29<3:04:22,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1069593647828118e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4384.0, 'logps/rejected': -3528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.3}
 10%|▉         | 159/1605 [20:30<3:04:22,  7.65s/it] 10%|▉         | 160/1605 [20:37<3:01:02,  7.52s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.113965436711817e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4328.0, 'logps/rejected': -4216.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.3}
 10%|▉         | 160/1605 [20:37<3:01:02,  7.52s/it] 10%|█         | 161/1605 [20:44<3:01:59,  7.56s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1209715086408219e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5680.0, 'logps/rejected': -5280.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.3}
 10%|█         | 161/1605 [20:45<3:01:59,  7.56s/it] 10%|█         | 162/1605 [20:52<3:01:48,  7.56s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.127977580569827e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3808.0, 'logps/rejected': -3864.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.3}
 10%|█         | 162/1605 [20:52<3:01:48,  7.56s/it] 10%|█         | 163/1605 [21:00<3:03:17,  7.63s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1349836524988324e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6544.0, 'logps/rejected': -6240.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.3}
 10%|█         | 163/1605 [21:00<3:03:17,  7.63s/it] 10%|█         | 164/1605 [21:07<3:03:53,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1419897244278374e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5600.0, 'logps/rejected': -4464.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.31}
 10%|█         | 164/1605 [21:08<3:03:53,  7.66s/it] 10%|█         | 165/1605 [21:15<3:05:04,  7.71s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1489957963568425e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7968.0, 'logps/rejected': -7616.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.31}
 10%|█         | 165/1605 [21:15<3:05:04,  7.71s/it] 10%|█         | 166/1605 [21:23<3:06:34,  7.78s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1560018682858477e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -8192.0, 'logps/rejected': -6640.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.31}
 10%|█         | 166/1605 [21:23<3:06:34,  7.78s/it] 10%|█         | 167/1605 [21:31<3:07:04,  7.81s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1630079402148528e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6496.0, 'logps/rejected': -5584.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.31}
 10%|█         | 167/1605 [21:31<3:07:04,  7.81s/it] 10%|█         | 168/1605 [21:39<3:06:12,  7.77s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1700140121438578e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6480.0, 'logps/rejected': -5120.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.31}
 10%|█         | 168/1605 [21:39<3:06:12,  7.77s/it] 11%|█         | 169/1605 [21:46<3:03:37,  7.67s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1770200840728631e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5536.0, 'logps/rejected': -3984.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.32}
 11%|█         | 169/1605 [21:46<3:03:37,  7.67s/it] 11%|█         | 170/1605 [21:54<3:03:42,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1840261560018683e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6464.0, 'logps/rejected': -6256.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.32}
 11%|█         | 170/1605 [21:54<3:03:42,  7.68s/it] 11%|█         | 171/1605 [22:02<3:04:43,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1910322279308733e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7088.0, 'logps/rejected': -5920.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.32}
 11%|█         | 171/1605 [22:02<3:04:43,  7.73s/it] 11%|█         | 172/1605 [22:09<3:03:03,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.1980382998598786e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4840.0, 'logps/rejected': -4440.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.32}
 11%|█         | 172/1605 [22:09<3:03:03,  7.66s/it] 11%|█         | 173/1605 [22:17<3:03:46,  7.70s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2050443717888836e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7360.0, 'logps/rejected': -6816.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.32}
 11%|█         | 173/1605 [22:17<3:03:46,  7.70s/it] 11%|█         | 174/1605 [22:24<3:01:37,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2120504437178887e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4384.0, 'logps/rejected': -4216.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.32}
 11%|█         | 174/1605 [22:25<3:01:37,  7.62s/it] 11%|█         | 175/1605 [22:32<3:03:31,  7.70s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.219056515646894e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5872.0, 'logps/rejected': -5040.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.33}
 11%|█         | 175/1605 [22:32<3:03:31,  7.70s/it] 11%|█         | 176/1605 [22:40<3:04:40,  7.75s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.226062587575899e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5216.0, 'logps/rejected': -4816.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.33}
 11%|█         | 176/1605 [22:40<3:04:40,  7.75s/it] 11%|█         | 177/1605 [22:48<3:03:05,  7.69s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2330686595049042e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4568.0, 'logps/rejected': -4376.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.33}
 11%|█         | 177/1605 [22:48<3:03:05,  7.69s/it] 11%|█         | 178/1605 [22:55<3:03:10,  7.70s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2400747314339093e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5568.0, 'logps/rejected': -4896.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.33}
 11%|█         | 178/1605 [22:56<3:03:10,  7.70s/it] 11%|█         | 179/1605 [23:03<3:03:48,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2470808033629145e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7760.0, 'logps/rejected': -7120.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.33}
 11%|█         | 179/1605 [23:03<3:03:48,  7.73s/it] 11%|█         | 180/1605 [23:11<3:02:00,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2540868752919195e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4928.0, 'logps/rejected': -3592.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.34}
 11%|█         | 180/1605 [23:11<3:02:00,  7.66s/it] 11%|█▏        | 181/1605 [23:19<3:03:17,  7.72s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2610929472209248e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4896.0, 'logps/rejected': -3984.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.34}
 11%|█▏        | 181/1605 [23:19<3:03:17,  7.72s/it] 11%|█▏        | 182/1605 [23:26<3:01:37,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.26809901914993e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5824.0, 'logps/rejected': -5760.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.34}
 11%|█▏        | 182/1605 [23:26<3:01:37,  7.66s/it] 11%|█▏        | 183/1605 [23:34<3:00:16,  7.61s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.275105091078935e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3960.0, 'logps/rejected': -3752.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.34}
 11%|█▏        | 183/1605 [23:34<3:00:16,  7.61s/it] 11%|█▏        | 184/1605 [23:41<2:57:33,  7.50s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2821111630079403e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5008.0, 'logps/rejected': -4528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.34}
 11%|█▏        | 184/1605 [23:41<2:57:33,  7.50s/it] 12%|█▏        | 185/1605 [23:49<3:01:30,  7.67s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2891172349369452e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6160.0, 'logps/rejected': -5328.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.35}
 12%|█▏        | 185/1605 [23:49<3:01:30,  7.67s/it] 12%|█▏        | 186/1605 [23:56<2:58:19,  7.54s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.2961233068659504e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5192.0, 'logps/rejected': -4472.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.35}
 12%|█▏        | 186/1605 [23:56<2:58:19,  7.54s/it] 12%|█▏        | 187/1605 [24:04<2:56:42,  7.48s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3031293787949557e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6256.0, 'logps/rejected': -4704.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.35}
 12%|█▏        | 187/1605 [24:04<2:56:42,  7.48s/it] 12%|█▏        | 188/1605 [24:12<3:01:03,  7.67s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3101354507239607e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5456.0, 'logps/rejected': -5248.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.35}
 12%|█▏        | 188/1605 [24:12<3:01:03,  7.67s/it] 12%|█▏        | 189/1605 [24:19<3:00:42,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3171415226529658e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5568.0, 'logps/rejected': -4664.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.35}
 12%|█▏        | 189/1605 [24:19<3:00:42,  7.66s/it] 12%|█▏        | 190/1605 [24:27<2:59:44,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.324147594581971e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4064.0, 'logps/rejected': -4096.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.35}
 12%|█▏        | 190/1605 [24:27<2:59:44,  7.62s/it] 12%|█▏        | 191/1605 [24:35<3:02:45,  7.75s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3311536665109762e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3656.0, 'logps/rejected': -3512.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.36}
 12%|█▏        | 191/1605 [24:35<3:02:45,  7.75s/it] 12%|█▏        | 192/1605 [24:43<3:03:23,  7.79s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3381597384399811e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5264.0, 'logps/rejected': -5136.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.36}
 12%|█▏        | 192/1605 [24:43<3:03:23,  7.79s/it] 12%|█▏        | 193/1605 [24:50<3:03:03,  7.78s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3451658103689865e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5664.0, 'logps/rejected': -5296.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.36}
 12%|█▏        | 193/1605 [24:51<3:03:03,  7.78s/it] 12%|█▏        | 194/1605 [24:58<3:01:59,  7.74s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3521718822979916e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6656.0, 'logps/rejected': -4384.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.36}
 12%|█▏        | 194/1605 [24:58<3:01:59,  7.74s/it] 12%|█▏        | 195/1605 [25:06<3:00:04,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3591779542269966e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5504.0, 'logps/rejected': -4616.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.36}
 12%|█▏        | 195/1605 [25:06<3:00:04,  7.66s/it] 12%|█▏        | 196/1605 [25:13<3:00:43,  7.70s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3661840261560019e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6560.0, 'logps/rejected': -6112.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.37}
 12%|█▏        | 196/1605 [25:13<3:00:43,  7.70s/it] 12%|█▏        | 197/1605 [25:21<2:59:09,  7.63s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3731900980850069e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6896.0, 'logps/rejected': -5632.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.37}
 12%|█▏        | 197/1605 [25:21<2:59:09,  7.63s/it] 12%|█▏        | 198/1605 [25:28<2:57:13,  7.56s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.380196170014012e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4992.0, 'logps/rejected': -4632.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.37}
 12%|█▏        | 198/1605 [25:28<2:57:13,  7.56s/it] 12%|█▏        | 199/1605 [25:36<2:57:16,  7.57s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3872022419430174e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5136.0, 'logps/rejected': -4856.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.37}
 12%|█▏        | 199/1605 [25:36<2:57:16,  7.57s/it] 12%|█▏        | 200/1605 [25:43<2:52:58,  7.39s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.3942083138720224e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3608.0, 'logps/rejected': -3592.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.37}
 12%|█▏        | 200/1605 [25:43<2:52:58,  7.39s/it] 13%|█▎        | 201/1605 [25:51<2:57:19,  7.58s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4012143858010275e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7216.0, 'logps/rejected': -6192.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.38}
 13%|█▎        | 201/1605 [25:51<2:57:19,  7.58s/it] 13%|█▎        | 202/1605 [25:59<3:00:06,  7.70s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4082204577300327e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5760.0, 'logps/rejected': -4664.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.38}
 13%|█▎        | 202/1605 [25:59<3:00:06,  7.70s/it] 13%|█▎        | 203/1605 [26:07<3:05:20,  7.93s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4152265296590378e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7984.0, 'logps/rejected': -7472.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.38}
 13%|█▎        | 203/1605 [26:07<3:05:20,  7.93s/it] 13%|█▎        | 204/1605 [26:16<3:08:38,  8.08s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4222326015880428e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6400.0, 'logps/rejected': -5336.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.38}
 13%|█▎        | 204/1605 [26:16<3:08:38,  8.08s/it] 13%|█▎        | 205/1605 [26:24<3:06:53,  8.01s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4292386735170481e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6064.0, 'logps/rejected': -4720.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.38}
 13%|█▎        | 205/1605 [26:24<3:06:53,  8.01s/it] 13%|█▎        | 206/1605 [26:31<3:02:36,  7.83s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4362447454460533e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5552.0, 'logps/rejected': -5520.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.38}
 13%|█▎        | 206/1605 [26:31<3:02:36,  7.83s/it] 13%|█▎        | 207/1605 [26:39<3:00:28,  7.75s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4432508173750583e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5200.0, 'logps/rejected': -4944.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.39}
 13%|█▎        | 207/1605 [26:39<3:00:28,  7.75s/it] 13%|█▎        | 208/1605 [26:46<2:58:24,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4502568893040634e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4992.0, 'logps/rejected': -4528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.39}
 13%|█▎        | 208/1605 [26:46<2:58:24,  7.66s/it] 13%|█▎        | 209/1605 [26:54<3:00:47,  7.77s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4572629612330686e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7968.0, 'logps/rejected': -6432.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.39}
 13%|█▎        | 209/1605 [26:54<3:00:47,  7.77s/it] 13%|█▎        | 210/1605 [27:02<2:59:22,  7.71s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4642690331620737e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5760.0, 'logps/rejected': -4436.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.39}
 13%|█▎        | 210/1605 [27:02<2:59:22,  7.71s/it] 13%|█▎        | 211/1605 [27:09<2:57:08,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4712751050910787e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4784.0, 'logps/rejected': -3472.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.39}
 13%|█▎        | 211/1605 [27:09<2:57:08,  7.62s/it] 13%|█▎        | 212/1605 [27:16<2:54:58,  7.54s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.478281177020084e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5072.0, 'logps/rejected': -4568.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.4}
 13%|█▎        | 212/1605 [27:17<2:54:58,  7.54s/it] 13%|█▎        | 213/1605 [27:24<2:57:22,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4852872489490892e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6480.0, 'logps/rejected': -4384.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.4}
 13%|█▎        | 213/1605 [27:24<2:57:22,  7.65s/it] 13%|█▎        | 214/1605 [27:31<2:53:49,  7.50s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.492293320878094e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4128.0, 'logps/rejected': -3980.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.4}
 13%|█▎        | 214/1605 [27:32<2:53:49,  7.50s/it] 13%|█▎        | 215/1605 [27:39<2:52:28,  7.45s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.4992993928070993e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4528.0, 'logps/rejected': -3680.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.4}
 13%|█▎        | 215/1605 [27:39<2:52:28,  7.45s/it] 13%|█▎        | 216/1605 [27:46<2:51:52,  7.42s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.5063054647361045e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4392.0, 'logps/rejected': -4264.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.4}
 13%|█▎        | 216/1605 [27:46<2:51:52,  7.42s/it] 14%|█▎        | 217/1605 [27:53<2:50:43,  7.38s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.5133115366651096e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5648.0, 'logps/rejected': -5792.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.41}
 14%|█▎        | 217/1605 [27:53<2:50:43,  7.38s/it] 14%|█▎        | 218/1605 [28:01<2:50:29,  7.38s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.5203176085941148e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4592.0, 'logps/rejected': -4288.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.41}
 14%|█▎        | 218/1605 [28:01<2:50:29,  7.38s/it] 14%|█▎        | 219/1605 [28:09<2:55:01,  7.58s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.52732368052312e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5392.0, 'logps/rejected': -5440.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.41}
 14%|█▎        | 219/1605 [28:09<2:55:01,  7.58s/it] 14%|█▎        | 220/1605 [28:17<2:56:50,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.534329752452125e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4888.0, 'logps/rejected': -4352.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.41}
 14%|█▎        | 220/1605 [28:17<2:56:50,  7.66s/it] 14%|█▍        | 221/1605 [28:24<2:56:40,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.5413358243811302e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6224.0, 'logps/rejected': -5888.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.41}
 14%|█▍        | 221/1605 [28:24<2:56:40,  7.66s/it] 14%|█▍        | 222/1605 [28:32<2:57:35,  7.70s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.5483418963101354e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5488.0, 'logps/rejected': -5120.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.41}
 14%|█▍        | 222/1605 [28:32<2:57:35,  7.70s/it] 14%|█▍        | 223/1605 [28:40<2:58:03,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.5553479682391405e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4672.0, 'logps/rejected': -4288.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.42}
 14%|█▍        | 223/1605 [28:40<2:58:03,  7.73s/it] 14%|█▍        | 224/1605 [28:48<3:01:25,  7.88s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.5623540401681457e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5616.0, 'logps/rejected': -5696.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.42}
 14%|█▍        | 224/1605 [28:48<3:01:25,  7.88s/it] 14%|█▍        | 225/1605 [28:56<3:02:58,  7.96s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.569360112097151e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5680.0, 'logps/rejected': -5216.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.42}
 14%|█▍        | 225/1605 [28:56<3:02:58,  7.96s/it] 14%|█▍        | 226/1605 [29:04<2:59:57,  7.83s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.576366184026156e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5552.0, 'logps/rejected': -4672.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.42}
 14%|█▍        | 226/1605 [29:04<2:59:57,  7.83s/it] 14%|█▍        | 227/1605 [29:11<2:56:20,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.583372255955161e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5136.0, 'logps/rejected': -4736.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.42}
 14%|█▍        | 227/1605 [29:11<2:56:20,  7.68s/it] 14%|█▍        | 228/1605 [29:19<2:56:56,  7.71s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.5903783278841663e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6016.0, 'logps/rejected': -5008.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.43}
 14%|█▍        | 228/1605 [29:19<2:56:56,  7.71s/it] 14%|█▍        | 229/1605 [29:27<2:56:31,  7.70s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.597384399813171e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6048.0, 'logps/rejected': -5920.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.43}
 14%|█▍        | 229/1605 [29:27<2:56:31,  7.70s/it] 14%|█▍        | 230/1605 [29:34<2:55:55,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.6043904717421766e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4832.0, 'logps/rejected': -4576.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.43}
 14%|█▍        | 230/1605 [29:34<2:55:55,  7.68s/it] 14%|█▍        | 231/1605 [29:42<2:56:35,  7.71s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.6113965436711818e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5200.0, 'logps/rejected': -4848.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.43}
 14%|█▍        | 231/1605 [29:42<2:56:35,  7.71s/it] 14%|█▍        | 232/1605 [29:50<2:56:49,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.6184026156001866e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5808.0, 'logps/rejected': -4632.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.43}
 14%|█▍        | 232/1605 [29:50<2:56:49,  7.73s/it] 15%|█▍        | 233/1605 [29:58<2:57:45,  7.77s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.625408687529192e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7008.0, 'logps/rejected': -5120.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.44}
 15%|█▍        | 233/1605 [29:58<2:57:45,  7.77s/it] 15%|█▍        | 234/1605 [30:06<2:58:56,  7.83s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.632414759458197e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7632.0, 'logps/rejected': -6112.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.44}
 15%|█▍        | 234/1605 [30:06<2:58:56,  7.83s/it] 15%|█▍        | 235/1605 [30:13<2:54:07,  7.63s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.639420831387202e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4088.0, 'logps/rejected': -4192.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.44}
 15%|█▍        | 235/1605 [30:13<2:54:07,  7.63s/it] 15%|█▍        | 236/1605 [30:21<2:56:41,  7.74s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.6464269033162075e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6320.0, 'logps/rejected': -5776.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.44}
 15%|█▍        | 236/1605 [30:21<2:56:41,  7.74s/it] 15%|█▍        | 237/1605 [30:28<2:55:41,  7.71s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.6534329752452123e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5528.0, 'logps/rejected': -5472.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.44}
 15%|█▍        | 237/1605 [30:29<2:55:41,  7.71s/it] 15%|█▍        | 238/1605 [30:36<2:58:09,  7.82s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.6604390471742175e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6448.0, 'logps/rejected': -6464.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.44}
 15%|█▍        | 238/1605 [30:37<2:58:09,  7.82s/it] 15%|█▍        | 239/1605 [30:44<2:58:42,  7.85s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.6674451191032226e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5888.0, 'logps/rejected': -5264.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.45}
 15%|█▍        | 239/1605 [30:45<2:58:42,  7.85s/it] 15%|█▍        | 240/1605 [30:52<2:57:01,  7.78s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.6744511910322278e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5360.0, 'logps/rejected': -5664.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.45}
 15%|█▍        | 240/1605 [30:52<2:57:01,  7.78s/it] 15%|█▌        | 241/1605 [31:00<2:55:18,  7.71s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.681457262961233e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5024.0, 'logps/rejected': -4800.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.45}
 15%|█▌        | 241/1605 [31:00<2:55:18,  7.71s/it] 15%|█▌        | 242/1605 [31:08<2:57:19,  7.81s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.688463334890238e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6208.0, 'logps/rejected': -6272.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.45}
 15%|█▌        | 242/1605 [31:08<2:57:19,  7.81s/it] 15%|█▌        | 243/1605 [31:15<2:56:13,  7.76s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.6954694068192433e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6000.0, 'logps/rejected': -5424.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.45}
 15%|█▌        | 243/1605 [31:15<2:56:13,  7.76s/it] 15%|█▌        | 244/1605 [31:23<2:53:20,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.7024754787482484e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5392.0, 'logps/rejected': -5360.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.46}
 15%|█▌        | 244/1605 [31:23<2:53:20,  7.64s/it] 15%|█▌        | 245/1605 [31:30<2:54:03,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.7094815506772536e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4248.0, 'logps/rejected': -3680.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.46}
 15%|█▌        | 245/1605 [31:30<2:54:03,  7.68s/it] 15%|█▌        | 246/1605 [31:38<2:53:10,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.7164876226062587e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4952.0, 'logps/rejected': -4592.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.46}
 15%|█▌        | 246/1605 [31:38<2:53:10,  7.65s/it] 15%|█▌        | 247/1605 [31:46<2:53:51,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.723493694535264e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6432.0, 'logps/rejected': -6192.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.46}
 15%|█▌        | 247/1605 [31:46<2:53:51,  7.68s/it] 15%|█▌        | 248/1605 [31:53<2:52:48,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.730499766464269e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4480.0, 'logps/rejected': -3680.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.46}
 15%|█▌        | 248/1605 [31:53<2:52:48,  7.64s/it] 16%|█▌        | 249/1605 [32:01<2:53:39,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.7375058383932742e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7232.0, 'logps/rejected': -5632.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.46}
 16%|█▌        | 249/1605 [32:01<2:53:39,  7.68s/it] 16%|█▌        | 250/1605 [32:08<2:51:44,  7.61s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.7445119103222793e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4496.0, 'logps/rejected': -3976.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.47}
 16%|█▌        | 250/1605 [32:09<2:51:44,  7.61s/it] 16%|█▌        | 251/1605 [32:17<2:54:43,  7.74s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.7515179822512845e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6768.0, 'logps/rejected': -5152.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.47}
 16%|█▌        | 251/1605 [32:17<2:54:43,  7.74s/it] 16%|█▌        | 252/1605 [32:25<2:56:08,  7.81s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.7585240541802896e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5760.0, 'logps/rejected': -4672.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.47}
 16%|█▌        | 252/1605 [32:25<2:56:08,  7.81s/it] 16%|█▌        | 253/1605 [32:32<2:51:58,  7.63s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.7655301261092944e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4520.0, 'logps/rejected': -3888.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.47}
 16%|█▌        | 253/1605 [32:32<2:51:58,  7.63s/it] 16%|█▌        | 254/1605 [32:39<2:49:19,  7.52s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.7725361980382996e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5136.0, 'logps/rejected': -4720.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.47}
 16%|█▌        | 254/1605 [32:39<2:49:19,  7.52s/it] 16%|█▌        | 255/1605 [32:47<2:50:29,  7.58s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.779542269967305e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6624.0, 'logps/rejected': -5200.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.48}
 16%|█▌        | 255/1605 [32:47<2:50:29,  7.58s/it] 16%|█▌        | 256/1605 [32:54<2:51:02,  7.61s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.78654834189631e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6160.0, 'logps/rejected': -5248.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.48}
 16%|█▌        | 256/1605 [32:54<2:51:02,  7.61s/it] 16%|█▌        | 257/1605 [33:02<2:51:56,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.793554413825315e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7184.0, 'logps/rejected': -6928.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.48}
 16%|█▌        | 257/1605 [33:02<2:51:56,  7.65s/it] 16%|█▌        | 258/1605 [33:10<2:50:00,  7.57s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8005604857543202e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4528.0, 'logps/rejected': -3376.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.48}
 16%|█▌        | 258/1605 [33:10<2:50:00,  7.57s/it] 16%|█▌        | 259/1605 [33:17<2:48:20,  7.50s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8075665576833254e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3400.0, 'logps/rejected': -3376.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.48}
 16%|█▌        | 259/1605 [33:17<2:48:20,  7.50s/it] 16%|█▌        | 260/1605 [33:25<2:49:54,  7.58s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8145726296123305e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5296.0, 'logps/rejected': -4120.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.49}
 16%|█▌        | 260/1605 [33:25<2:49:54,  7.58s/it] 16%|█▋        | 261/1605 [33:32<2:50:50,  7.63s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8215787015413357e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4608.0, 'logps/rejected': -4232.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.49}
 16%|█▋        | 261/1605 [33:32<2:50:50,  7.63s/it] 16%|█▋        | 262/1605 [33:40<2:51:10,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8285847734703408e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6192.0, 'logps/rejected': -5632.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.49}
 16%|█▋        | 262/1605 [33:40<2:51:10,  7.65s/it] 16%|█▋        | 263/1605 [33:48<2:50:25,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.835590845399346e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4848.0, 'logps/rejected': -4864.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.49}
 16%|█▋        | 263/1605 [33:48<2:50:25,  7.62s/it] 16%|█▋        | 264/1605 [33:55<2:47:52,  7.51s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.842596917328351e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4096.0, 'logps/rejected': -4152.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.49}
 16%|█▋        | 264/1605 [33:55<2:47:52,  7.51s/it] 17%|█▋        | 265/1605 [34:02<2:47:58,  7.52s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8496029892573563e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4592.0, 'logps/rejected': -4624.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.49}
 17%|█▋        | 265/1605 [34:02<2:47:58,  7.52s/it] 17%|█▋        | 266/1605 [34:10<2:48:59,  7.57s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8566090611863614e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5504.0, 'logps/rejected': -4888.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.5}
 17%|█▋        | 266/1605 [34:10<2:48:59,  7.57s/it] 17%|█▋        | 267/1605 [34:17<2:46:43,  7.48s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8636151331153666e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4920.0, 'logps/rejected': -4176.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.5}
 17%|█▋        | 267/1605 [34:17<2:46:43,  7.48s/it] 17%|█▋        | 268/1605 [34:25<2:48:06,  7.54s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8706212050443717e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4480.0, 'logps/rejected': -3656.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.5}
 17%|█▋        | 268/1605 [34:25<2:48:06,  7.54s/it] 17%|█▋        | 269/1605 [34:33<2:48:43,  7.58s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.877627276973377e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4928.0, 'logps/rejected': -4528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.5}
 17%|█▋        | 269/1605 [34:33<2:48:43,  7.58s/it] 17%|█▋        | 270/1605 [34:40<2:47:45,  7.54s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.884633348902382e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4976.0, 'logps/rejected': -4648.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.5}
 17%|█▋        | 270/1605 [34:40<2:47:45,  7.54s/it] 17%|█▋        | 271/1605 [34:48<2:46:32,  7.49s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8916394208313872e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6272.0, 'logps/rejected': -5952.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.51}
 17%|█▋        | 271/1605 [34:48<2:46:32,  7.49s/it] 17%|█▋        | 272/1605 [34:55<2:49:15,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.8986454927603923e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6128.0, 'logps/rejected': -5520.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.51}
 17%|█▋        | 272/1605 [34:56<2:49:15,  7.62s/it] 17%|█▋        | 273/1605 [35:03<2:51:03,  7.71s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.9056515646893972e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6064.0, 'logps/rejected': -3952.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.51}
 17%|█▋        | 273/1605 [35:04<2:51:03,  7.71s/it] 17%|█▋        | 274/1605 [35:11<2:47:56,  7.57s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.9126576366184027e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3984.0, 'logps/rejected': -3712.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.51}
 17%|█▋        | 274/1605 [35:11<2:47:56,  7.57s/it] 17%|█▋        | 275/1605 [35:18<2:45:59,  7.49s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.9196637085474078e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5104.0, 'logps/rejected': -3896.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.51}
 17%|█▋        | 275/1605 [35:18<2:45:59,  7.49s/it] 17%|█▋        | 276/1605 [35:25<2:45:06,  7.45s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.926669780476413e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4928.0, 'logps/rejected': -4832.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.52}
 17%|█▋        | 276/1605 [35:25<2:45:06,  7.45s/it] 17%|█▋        | 277/1605 [35:33<2:46:22,  7.52s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.933675852405418e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5760.0, 'logps/rejected': -5184.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.52}
 17%|█▋        | 277/1605 [35:33<2:46:22,  7.52s/it] 17%|█▋        | 278/1605 [35:41<2:48:01,  7.60s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.940681924334423e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5904.0, 'logps/rejected': -5160.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.52}
 17%|█▋        | 278/1605 [35:41<2:48:01,  7.60s/it] 17%|█▋        | 279/1605 [35:48<2:47:05,  7.56s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.947687996263428e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5792.0, 'logps/rejected': -5760.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.52}
 17%|█▋        | 279/1605 [35:48<2:47:05,  7.56s/it] 17%|█▋        | 280/1605 [35:56<2:49:24,  7.67s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.9546940681924332e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7264.0, 'logps/rejected': -5504.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.52}
 17%|█▋        | 280/1605 [35:56<2:49:24,  7.67s/it] 18%|█▊        | 281/1605 [36:04<2:53:16,  7.85s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.9617001401214387e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6400.0, 'logps/rejected': -5408.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.52}
 18%|█▊        | 281/1605 [36:04<2:53:16,  7.85s/it] 18%|█▊        | 282/1605 [36:13<2:57:57,  8.07s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.968706212050444e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7472.0, 'logps/rejected': -7584.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.53}
 18%|█▊        | 282/1605 [36:13<2:57:57,  8.07s/it] 18%|█▊        | 283/1605 [36:21<2:54:04,  7.90s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.9757122839794487e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3400.0, 'logps/rejected': -2944.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.53}
 18%|█▊        | 283/1605 [36:21<2:54:04,  7.90s/it] 18%|█▊        | 284/1605 [36:28<2:50:15,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.982718355908454e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3784.0, 'logps/rejected': -3864.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.53}
 18%|█▊        | 284/1605 [36:28<2:50:15,  7.73s/it] 18%|█▊        | 285/1605 [36:35<2:48:01,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.989724427837459e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4888.0, 'logps/rejected': -4488.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.53}
 18%|█▊        | 285/1605 [36:35<2:48:01,  7.64s/it] 18%|█▊        | 286/1605 [36:43<2:48:04,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 1.996730499766464e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5728.0, 'logps/rejected': -5264.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.53}
 18%|█▊        | 286/1605 [36:43<2:48:04,  7.65s/it] 18%|█▊        | 287/1605 [36:50<2:45:44,  7.55s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0037365716954696e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3768.0, 'logps/rejected': -3336.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.54}
 18%|█▊        | 287/1605 [36:50<2:45:44,  7.55s/it] 18%|█▊        | 288/1605 [36:58<2:45:13,  7.53s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0107426436244745e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3912.0, 'logps/rejected': -3408.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.54}
 18%|█▊        | 288/1605 [36:58<2:45:13,  7.53s/it] 18%|█▊        | 289/1605 [37:06<2:47:39,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0177487155534796e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -8160.0, 'logps/rejected': -7024.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.54}
 18%|█▊        | 289/1605 [37:06<2:47:39,  7.64s/it] 18%|█▊        | 290/1605 [37:13<2:48:12,  7.67s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0247547874824848e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6560.0, 'logps/rejected': -5752.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.54}
 18%|█▊        | 290/1605 [37:13<2:48:12,  7.67s/it] 18%|█▊        | 291/1605 [37:21<2:49:15,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.03176085941149e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5400.0, 'logps/rejected': -5216.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.54}
 18%|█▊        | 291/1605 [37:21<2:49:15,  7.73s/it] 18%|█▊        | 292/1605 [37:29<2:49:09,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0387669313404947e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5888.0, 'logps/rejected': -5152.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.55}
 18%|█▊        | 292/1605 [37:29<2:49:09,  7.73s/it] 18%|█▊        | 293/1605 [37:37<2:50:48,  7.81s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0457730032695002e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6368.0, 'logps/rejected': -5760.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.55}
 18%|█▊        | 293/1605 [37:37<2:50:48,  7.81s/it] 18%|█▊        | 294/1605 [37:45<2:48:54,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0527790751985054e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4856.0, 'logps/rejected': -4408.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.55}
 18%|█▊        | 294/1605 [37:45<2:48:54,  7.73s/it] 18%|█▊        | 295/1605 [37:53<2:51:16,  7.84s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0597851471275105e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6288.0, 'logps/rejected': -6144.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.55}
 18%|█▊        | 295/1605 [37:53<2:51:16,  7.84s/it] 18%|█▊        | 296/1605 [38:00<2:48:01,  7.70s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0667912190565157e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4384.0, 'logps/rejected': -4488.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.55}
 18%|█▊        | 296/1605 [38:00<2:48:01,  7.70s/it] 19%|█▊        | 297/1605 [38:08<2:47:18,  7.67s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0737972909855205e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5424.0, 'logps/rejected': -5152.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.55}
 19%|█▊        | 297/1605 [38:08<2:47:18,  7.67s/it] 19%|█▊        | 298/1605 [38:16<2:49:49,  7.80s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0808033629145256e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5336.0, 'logps/rejected': -4240.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.56}
 19%|█▊        | 298/1605 [38:16<2:49:49,  7.80s/it] 19%|█▊        | 299/1605 [38:23<2:48:26,  7.74s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.087809434843531e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6048.0, 'logps/rejected': -5744.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.56}
 19%|█▊        | 299/1605 [38:23<2:48:26,  7.74s/it] 19%|█▊        | 300/1605 [38:31<2:45:39,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.0948155067725363e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5848.0, 'logps/rejected': -5192.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.56}
 19%|█▊        | 300/1605 [38:31<2:45:39,  7.62s/it] 19%|█▉        | 301/1605 [38:38<2:45:44,  7.63s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1018215787015414e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5504.0, 'logps/rejected': -5296.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.56}
 19%|█▉        | 301/1605 [38:38<2:45:44,  7.63s/it] 19%|█▉        | 302/1605 [38:46<2:45:22,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1088276506305463e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5280.0, 'logps/rejected': -4832.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.56}
 19%|█▉        | 302/1605 [38:46<2:45:22,  7.62s/it] 19%|█▉        | 303/1605 [38:54<2:45:51,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1158337225595514e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4008.0, 'logps/rejected': -3024.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.57}
 19%|█▉        | 303/1605 [38:54<2:45:51,  7.64s/it] 19%|█▉        | 304/1605 [39:01<2:46:08,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1228397944885566e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6416.0, 'logps/rejected': -5640.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.57}
 19%|█▉        | 304/1605 [39:01<2:46:08,  7.66s/it] 19%|█▉        | 305/1605 [39:09<2:44:39,  7.60s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.129845866417562e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4640.0, 'logps/rejected': -4576.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.57}
 19%|█▉        | 305/1605 [39:09<2:44:39,  7.60s/it] 19%|█▉        | 306/1605 [39:17<2:47:32,  7.74s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1368519383465672e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6320.0, 'logps/rejected': -5904.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.57}
 19%|█▉        | 306/1605 [39:17<2:47:32,  7.74s/it] 19%|█▉        | 307/1605 [39:25<2:49:07,  7.82s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.143858010275572e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6752.0, 'logps/rejected': -6112.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.57}
 19%|█▉        | 307/1605 [39:25<2:49:07,  7.82s/it] 19%|█▉        | 308/1605 [39:32<2:44:43,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1508640822045772e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4512.0, 'logps/rejected': -4056.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.58}
 19%|█▉        | 308/1605 [39:32<2:44:43,  7.62s/it] 19%|█▉        | 309/1605 [39:40<2:47:20,  7.75s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1578701541335823e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6512.0, 'logps/rejected': -6080.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.58}
 19%|█▉        | 309/1605 [39:40<2:47:20,  7.75s/it] 19%|█▉        | 310/1605 [39:48<2:46:57,  7.74s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1648762260625875e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5872.0, 'logps/rejected': -5392.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.58}
 19%|█▉        | 310/1605 [39:48<2:46:57,  7.74s/it] 19%|█▉        | 311/1605 [39:56<2:49:48,  7.87s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.171882297991593e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6208.0, 'logps/rejected': -5984.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.58}
 19%|█▉        | 311/1605 [39:56<2:49:48,  7.87s/it] 19%|█▉        | 312/1605 [40:04<2:49:18,  7.86s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1788883699205978e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5008.0, 'logps/rejected': -4680.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.58}
 19%|█▉        | 312/1605 [40:04<2:49:18,  7.86s/it] 20%|█▉        | 313/1605 [40:12<2:49:37,  7.88s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.185894441849603e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5600.0, 'logps/rejected': -4968.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.58}
 20%|█▉        | 313/1605 [40:12<2:49:37,  7.88s/it] 20%|█▉        | 314/1605 [40:19<2:48:41,  7.84s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.192900513778608e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6160.0, 'logps/rejected': -4632.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.59}
 20%|█▉        | 314/1605 [40:20<2:48:41,  7.84s/it] 20%|█▉        | 315/1605 [40:27<2:47:48,  7.81s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.1999065857076132e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6368.0, 'logps/rejected': -6208.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.59}
 20%|█▉        | 315/1605 [40:27<2:47:48,  7.81s/it] 20%|█▉        | 316/1605 [40:35<2:46:06,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.206912657636618e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4376.0, 'logps/rejected': -4056.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.59}
 20%|█▉        | 316/1605 [40:35<2:46:06,  7.73s/it] 20%|█▉        | 317/1605 [40:42<2:43:47,  7.63s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.2139187295656235e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5312.0, 'logps/rejected': -4672.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.59}
 20%|█▉        | 317/1605 [40:42<2:43:47,  7.63s/it] 20%|█▉        | 318/1605 [40:49<2:41:03,  7.51s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.2209248014946287e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5136.0, 'logps/rejected': -4928.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.59}
 20%|█▉        | 318/1605 [40:49<2:41:03,  7.51s/it] 20%|█▉        | 319/1605 [40:57<2:40:15,  7.48s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.227930873423634e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4696.0, 'logps/rejected': -4360.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.6}
 20%|█▉        | 319/1605 [40:57<2:40:15,  7.48s/it] 20%|█▉        | 320/1605 [41:04<2:39:22,  7.44s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.234936945352639e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5120.0, 'logps/rejected': -4512.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.6}
 20%|█▉        | 320/1605 [41:04<2:39:22,  7.44s/it] 20%|██        | 321/1605 [41:11<2:39:14,  7.44s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.2419430172816438e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5632.0, 'logps/rejected': -3952.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.6}
 20%|██        | 321/1605 [41:12<2:39:14,  7.44s/it] 20%|██        | 322/1605 [41:19<2:40:12,  7.49s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.248949089210649e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4976.0, 'logps/rejected': -5200.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.6}
 20%|██        | 322/1605 [41:19<2:40:12,  7.49s/it] 20%|██        | 323/1605 [41:27<2:44:00,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.255955161139654e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5968.0, 'logps/rejected': -5152.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.6}
 20%|██        | 323/1605 [41:27<2:44:00,  7.68s/it] 20%|██        | 324/1605 [41:35<2:43:14,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.2629612330686596e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5392.0, 'logps/rejected': -4392.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.61}
 20%|██        | 324/1605 [41:35<2:43:14,  7.65s/it] 20%|██        | 325/1605 [41:43<2:44:05,  7.69s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.2699673049976648e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7136.0, 'logps/rejected': -6304.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.61}
 20%|██        | 325/1605 [41:43<2:44:05,  7.69s/it] 20%|██        | 326/1605 [41:50<2:45:11,  7.75s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.2769733769266696e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7040.0, 'logps/rejected': -5696.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.61}
 20%|██        | 326/1605 [41:51<2:45:11,  7.75s/it] 20%|██        | 327/1605 [41:58<2:41:08,  7.57s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.2839794488556747e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5168.0, 'logps/rejected': -5152.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.61}
 20%|██        | 327/1605 [41:58<2:41:08,  7.57s/it] 20%|██        | 328/1605 [42:06<2:43:07,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.29098552078468e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6272.0, 'logps/rejected': -5888.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.61}
 20%|██        | 328/1605 [42:06<2:43:07,  7.66s/it] 20%|██        | 329/1605 [42:13<2:42:31,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.297991592713685e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7104.0, 'logps/rejected': -6528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.61}
 20%|██        | 329/1605 [42:13<2:42:31,  7.64s/it] 21%|██        | 330/1605 [42:21<2:41:06,  7.58s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3049976646426905e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4840.0, 'logps/rejected': -4320.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.62}
 21%|██        | 330/1605 [42:21<2:41:06,  7.58s/it] 21%|██        | 331/1605 [42:29<2:43:29,  7.70s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3120037365716953e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6416.0, 'logps/rejected': -5920.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.62}
 21%|██        | 331/1605 [42:29<2:43:29,  7.70s/it] 21%|██        | 332/1605 [42:36<2:44:37,  7.76s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3190098085007005e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6016.0, 'logps/rejected': -5384.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.62}
 21%|██        | 332/1605 [42:36<2:44:37,  7.76s/it] 21%|██        | 333/1605 [42:44<2:43:57,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3260158804297057e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5360.0, 'logps/rejected': -4944.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.62}
 21%|██        | 333/1605 [42:44<2:43:57,  7.73s/it] 21%|██        | 334/1605 [42:51<2:40:29,  7.58s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3330219523587108e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5168.0, 'logps/rejected': -4704.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.62}
 21%|██        | 334/1605 [42:51<2:40:29,  7.58s/it] 21%|██        | 335/1605 [42:59<2:38:36,  7.49s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3400280242877156e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4352.0, 'logps/rejected': -4336.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.63}
 21%|██        | 335/1605 [42:59<2:38:36,  7.49s/it] 21%|██        | 336/1605 [43:07<2:43:18,  7.72s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.347034096216721e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6560.0, 'logps/rejected': -5888.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.63}
 21%|██        | 336/1605 [43:07<2:43:18,  7.72s/it] 21%|██        | 337/1605 [43:14<2:39:54,  7.57s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3540401681457263e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4024.0, 'logps/rejected': -3560.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.63}
 21%|██        | 337/1605 [43:14<2:39:54,  7.57s/it] 21%|██        | 338/1605 [43:22<2:41:43,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3610462400747314e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6320.0, 'logps/rejected': -5584.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.63}
 21%|██        | 338/1605 [43:22<2:41:43,  7.66s/it] 21%|██        | 339/1605 [43:30<2:41:14,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3680523120037366e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4992.0, 'logps/rejected': -4592.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.63}
 21%|██        | 339/1605 [43:30<2:41:14,  7.64s/it] 21%|██        | 340/1605 [43:37<2:42:09,  7.69s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3750583839327414e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5760.0, 'logps/rejected': -5744.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.63}
 21%|██        | 340/1605 [43:37<2:42:09,  7.69s/it] 21%|██        | 341/1605 [43:45<2:39:31,  7.57s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3820644558617465e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3760.0, 'logps/rejected': -3416.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.64}
 21%|██        | 341/1605 [43:45<2:39:31,  7.57s/it] 21%|██▏       | 342/1605 [43:52<2:41:01,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.389070527790752e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6272.0, 'logps/rejected': -5648.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.64}
 21%|██▏       | 342/1605 [43:53<2:41:01,  7.65s/it] 21%|██▏       | 343/1605 [44:00<2:40:15,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.3960765997197572e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6240.0, 'logps/rejected': -5184.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.64}
 21%|██▏       | 343/1605 [44:00<2:40:15,  7.62s/it] 21%|██▏       | 344/1605 [44:08<2:40:19,  7.63s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.4030826716487623e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5280.0, 'logps/rejected': -5360.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.64}
 21%|██▏       | 344/1605 [44:08<2:40:19,  7.63s/it] 21%|██▏       | 345/1605 [44:16<2:42:55,  7.76s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.410088743577767e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7632.0, 'logps/rejected': -6800.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.64}
 21%|██▏       | 345/1605 [44:16<2:42:55,  7.76s/it] 22%|██▏       | 346/1605 [44:24<2:43:12,  7.78s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.4170948155067723e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6528.0, 'logps/rejected': -5456.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.65}
 22%|██▏       | 346/1605 [44:24<2:43:12,  7.78s/it] 22%|██▏       | 347/1605 [44:31<2:39:27,  7.61s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.4241008874357775e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4344.0, 'logps/rejected': -4280.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.65}
 22%|██▏       | 347/1605 [44:31<2:39:27,  7.61s/it] 22%|██▏       | 348/1605 [44:38<2:40:11,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.431106959364783e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5856.0, 'logps/rejected': -5520.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.65}
 22%|██▏       | 348/1605 [44:39<2:40:11,  7.65s/it] 22%|██▏       | 349/1605 [44:46<2:39:17,  7.61s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.438113031293788e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4864.0, 'logps/rejected': -4592.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.65}
 22%|██▏       | 349/1605 [44:46<2:39:17,  7.61s/it] 22%|██▏       | 350/1605 [44:53<2:37:41,  7.54s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.445119103222793e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6192.0, 'logps/rejected': -5152.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.65}
 22%|██▏       | 350/1605 [44:53<2:37:41,  7.54s/it] 22%|██▏       | 351/1605 [45:01<2:40:46,  7.69s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.452125175151798e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5104.0, 'logps/rejected': -4328.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.66}
 22%|██▏       | 351/1605 [45:02<2:40:46,  7.69s/it] 22%|██▏       | 352/1605 [45:08<2:36:30,  7.49s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.4591312470808032e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3760.0, 'logps/rejected': -3760.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.66}
 22%|██▏       | 352/1605 [45:09<2:36:30,  7.49s/it] 22%|██▏       | 353/1605 [45:16<2:38:34,  7.60s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.4661373190098084e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6016.0, 'logps/rejected': -5840.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.66}
 22%|██▏       | 353/1605 [45:16<2:38:34,  7.60s/it] 22%|██▏       | 354/1605 [45:24<2:36:40,  7.51s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.473143390938814e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4232.0, 'logps/rejected': -3992.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.66}
 22%|██▏       | 354/1605 [45:24<2:36:40,  7.51s/it] 22%|██▏       | 355/1605 [45:31<2:37:23,  7.55s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.4801494628678187e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5024.0, 'logps/rejected': -4432.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.66}
 22%|██▏       | 355/1605 [45:31<2:37:23,  7.55s/it] 22%|██▏       | 356/1605 [45:39<2:35:37,  7.48s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.4871555347968238e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6720.0, 'logps/rejected': -5936.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.66}
 22%|██▏       | 356/1605 [45:39<2:35:37,  7.48s/it] 22%|██▏       | 357/1605 [45:46<2:38:07,  7.60s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.494161606725829e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6832.0, 'logps/rejected': -5056.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.67}
 22%|██▏       | 357/1605 [45:47<2:38:07,  7.60s/it] 22%|██▏       | 358/1605 [45:55<2:41:05,  7.75s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.501167678654834e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6176.0, 'logps/rejected': -4536.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.67}
 22%|██▏       | 358/1605 [45:55<2:41:05,  7.75s/it] 22%|██▏       | 359/1605 [46:03<2:43:34,  7.88s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.508173750583839e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7024.0, 'logps/rejected': -5472.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.67}
 22%|██▏       | 359/1605 [46:03<2:43:34,  7.88s/it] 22%|██▏       | 360/1605 [46:10<2:38:08,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5151798225128444e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4328.0, 'logps/rejected': -4136.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.67}
 22%|██▏       | 360/1605 [46:10<2:38:08,  7.62s/it] 22%|██▏       | 361/1605 [46:18<2:39:17,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5221858944418496e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7392.0, 'logps/rejected': -6816.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.67}
 22%|██▏       | 361/1605 [46:18<2:39:17,  7.68s/it] 23%|██▎       | 362/1605 [46:26<2:41:05,  7.78s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5291919663708547e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6040.0, 'logps/rejected': -3496.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.68}
 23%|██▎       | 362/1605 [46:26<2:41:05,  7.78s/it] 23%|██▎       | 363/1605 [46:33<2:41:39,  7.81s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.53619803829986e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4848.0, 'logps/rejected': -3856.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.68}
 23%|██▎       | 363/1605 [46:34<2:41:39,  7.81s/it] 23%|██▎       | 364/1605 [46:41<2:42:31,  7.86s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5432041102288647e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4560.0, 'logps/rejected': -3688.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.68}
 23%|██▎       | 364/1605 [46:42<2:42:31,  7.86s/it] 23%|██▎       | 365/1605 [46:50<2:44:23,  7.95s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.55021018215787e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5656.0, 'logps/rejected': -5304.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.68}
 23%|██▎       | 365/1605 [46:50<2:44:23,  7.95s/it] 23%|██▎       | 366/1605 [46:58<2:45:36,  8.02s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.557216254086875e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5104.0, 'logps/rejected': -4656.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.68}
 23%|██▎       | 366/1605 [46:58<2:45:36,  8.02s/it] 23%|██▎       | 367/1605 [47:06<2:44:46,  7.99s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5642223260158805e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4832.0, 'logps/rejected': -5144.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.69}
 23%|██▎       | 367/1605 [47:06<2:44:46,  7.99s/it] 23%|██▎       | 368/1605 [47:13<2:42:03,  7.86s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5712283979448857e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4864.0, 'logps/rejected': -3816.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.69}
 23%|██▎       | 368/1605 [47:13<2:42:03,  7.86s/it] 23%|██▎       | 369/1605 [47:21<2:40:45,  7.80s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5782344698738905e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5936.0, 'logps/rejected': -4992.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.69}
 23%|██▎       | 369/1605 [47:21<2:40:45,  7.80s/it] 23%|██▎       | 370/1605 [47:29<2:41:09,  7.83s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5852405418028956e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6848.0, 'logps/rejected': -6544.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.69}
 23%|██▎       | 370/1605 [47:29<2:41:09,  7.83s/it] 23%|██▎       | 371/1605 [47:36<2:39:57,  7.78s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.5922466137319008e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5088.0, 'logps/rejected': -4440.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.69}
 23%|██▎       | 371/1605 [47:37<2:39:57,  7.78s/it] 23%|██▎       | 372/1605 [47:44<2:37:07,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.599252685660906e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4576.0, 'logps/rejected': -4416.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.69}
 23%|██▎       | 372/1605 [47:44<2:37:07,  7.65s/it] 23%|██▎       | 373/1605 [47:52<2:37:12,  7.66s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6062587575899114e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5792.0, 'logps/rejected': -5264.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.7}
 23%|██▎       | 373/1605 [47:52<2:37:12,  7.66s/it] 23%|██▎       | 374/1605 [47:59<2:38:30,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6132648295189162e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6528.0, 'logps/rejected': -6816.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.7}
 23%|██▎       | 374/1605 [48:00<2:38:30,  7.73s/it] 23%|██▎       | 375/1605 [48:07<2:38:09,  7.72s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6202709014479214e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5104.0, 'logps/rejected': -5056.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.7}
 23%|██▎       | 375/1605 [48:07<2:38:09,  7.72s/it] 23%|██▎       | 376/1605 [48:15<2:37:02,  7.67s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6272769733769265e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5376.0, 'logps/rejected': -4896.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.7}
 23%|██▎       | 376/1605 [48:15<2:37:02,  7.67s/it] 23%|██▎       | 377/1605 [48:22<2:36:37,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6342830453059317e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -3992.0, 'logps/rejected': -3552.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.7}
 23%|██▎       | 377/1605 [48:23<2:36:37,  7.65s/it] 24%|██▎       | 378/1605 [48:30<2:37:59,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6412891172349365e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5968.0, 'logps/rejected': -5552.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.71}
 24%|██▎       | 378/1605 [48:30<2:37:59,  7.73s/it] 24%|██▎       | 379/1605 [48:38<2:36:52,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.648295189163942e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4544.0, 'logps/rejected': -4192.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.71}
 24%|██▎       | 379/1605 [48:38<2:36:52,  7.68s/it] 24%|██▎       | 380/1605 [48:46<2:39:55,  7.83s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.655301261092947e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -7760.0, 'logps/rejected': -7088.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.71}
 24%|██▎       | 380/1605 [48:46<2:39:55,  7.83s/it] 24%|██▎       | 381/1605 [48:53<2:35:55,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6623073330219523e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4056.0, 'logps/rejected': -3528.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.71}
 24%|██▎       | 381/1605 [48:53<2:35:55,  7.64s/it] 24%|██▍       | 382/1605 [49:01<2:35:14,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6693134049509575e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6080.0, 'logps/rejected': -5600.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.71}
 24%|██▍       | 382/1605 [49:01<2:35:14,  7.62s/it] 24%|██▍       | 383/1605 [49:08<2:34:24,  7.58s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6763194768799623e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5744.0, 'logps/rejected': -5728.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.72}
 24%|██▍       | 383/1605 [49:08<2:34:24,  7.58s/it] 24%|██▍       | 384/1605 [49:16<2:35:27,  7.64s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.6833255488089674e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5552.0, 'logps/rejected': -5376.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.72}
 24%|██▍       | 384/1605 [49:16<2:35:27,  7.64s/it] 24%|██▍       | 385/1605 [49:25<2:42:18,  7.98s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.690331620737973e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6480.0, 'logps/rejected': -5488.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.72}
 24%|██▍       | 385/1605 [49:25<2:42:18,  7.98s/it] 24%|██▍       | 386/1605 [49:33<2:42:06,  7.98s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.697337692666978e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5568.0, 'logps/rejected': -4664.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.72}
 24%|██▍       | 386/1605 [49:33<2:42:06,  7.98s/it] 24%|██▍       | 387/1605 [49:41<2:42:12,  7.99s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.7043437645959832e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6704.0, 'logps/rejected': -5440.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.72}
 24%|██▍       | 387/1605 [49:41<2:42:12,  7.99s/it] 24%|██▍       | 388/1605 [49:48<2:39:59,  7.89s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.711349836524988e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5952.0, 'logps/rejected': -5536.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.72}
 24%|██▍       | 388/1605 [49:48<2:39:59,  7.89s/it] 24%|██▍       | 389/1605 [49:56<2:36:24,  7.72s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.7183559084539932e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4416.0, 'logps/rejected': -4432.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.73}
 24%|██▍       | 389/1605 [49:56<2:36:24,  7.72s/it] 24%|██▍       | 390/1605 [50:03<2:35:34,  7.68s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.7253619803829984e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5872.0, 'logps/rejected': -5552.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.73}
 24%|██▍       | 390/1605 [50:03<2:35:34,  7.68s/it] 24%|██▍       | 391/1605 [50:11<2:37:12,  7.77s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.7323680523120038e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5056.0, 'logps/rejected': -5240.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.73}
 24%|██▍       | 391/1605 [50:11<2:37:12,  7.77s/it] 24%|██▍       | 392/1605 [50:19<2:34:38,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.739374124241009e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5936.0, 'logps/rejected': -5568.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.73}
 24%|██▍       | 392/1605 [50:19<2:34:38,  7.65s/it] 24%|██▍       | 393/1605 [50:27<2:36:04,  7.73s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.7463801961700138e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5240.0, 'logps/rejected': -4768.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.73}
 24%|██▍       | 393/1605 [50:27<2:36:04,  7.73s/it] 25%|██▍       | 394/1605 [50:34<2:34:24,  7.65s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.753386268099019e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5888.0, 'logps/rejected': -5568.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.74}
 25%|██▍       | 394/1605 [50:34<2:34:24,  7.65s/it] 25%|██▍       | 395/1605 [50:42<2:37:29,  7.81s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.760392340028024e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -6192.0, 'logps/rejected': -5472.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.74}
 25%|██▍       | 395/1605 [50:42<2:37:29,  7.81s/it] 25%|██▍       | 396/1605 [50:50<2:36:56,  7.79s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.7673984119570293e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5936.0, 'logps/rejected': -5424.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.74}
 25%|██▍       | 396/1605 [50:50<2:36:56,  7.79s/it] 25%|██▍       | 397/1605 [50:57<2:35:04,  7.70s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.7744044838860347e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4672.0, 'logps/rejected': -3592.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.74}
 25%|██▍       | 397/1605 [50:57<2:35:04,  7.70s/it] 25%|██▍       | 398/1605 [51:05<2:33:11,  7.62s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.7814105558150396e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5056.0, 'logps/rejected': -4448.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.74}
 25%|██▍       | 398/1605 [51:05<2:33:11,  7.62s/it] 25%|██▍       | 399/1605 [51:12<2:30:33,  7.49s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.7884166277440447e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -4312.0, 'logps/rejected': -4432.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.75}
 25%|██▍       | 399/1605 [51:12<2:30:33,  7.49s/it] 25%|██▍       | 400/1605 [51:20<2:30:34,  7.50s/it]                                                    {'loss': 0.6914, 'grad_norm': 0.0, 'learning_rate': 2.79542269967305e-08, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/chosen': -5472.0, 'logps/rejected': -5824.0, 'logits/chosen': 0.0, 'logits/rejected': 0.0, 'epoch': 0.75}
 25%|██▍       | 400/1605 [51:20<2:30:34,  7.50s/it]slurmstepd: error: *** STEP 62080426.0 ON gn11 CANCELLED AT 2025-05-31T00:39:07 ***
