cpu-bind=MASK - gn04, task  2  0 [2120045]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 2 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 2     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63118224     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=1e-6 --total_epochs=3 --beta=0.1 --curriculum_stage=2
-------------------------------------------
[2025-06-12 21:15:49,964] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 21:15:51.346000 2120095 torch/distributed/run.py:792] 
W0612 21:15:51.346000 2120095 torch/distributed/run.py:792] *****************************************
W0612 21:15:51.346000 2120095 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 21:15:51.346000 2120095 torch/distributed/run.py:792] *****************************************
[2025-06-12 21:15:56,179] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:15:56,222] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:15:56,238] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:15:56,246] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
World size: 12
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Created datasets
Train dataset size: 6690
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-12 21:16:00,736] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 21:16:00,754] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 21:16:00,769] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curriculum-1
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][2025-06-12 21:16:01,145] [INFO] [comm.py:658:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:25,  8.66s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:26,  8.69s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:26,  8.69s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:25,  8.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:08,  8.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:08,  8.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:08,  8.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.10s/it]
Loaded model
Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.73s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.06s/it]
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   8%|▊         | 533/6690 [00:00<00:01, 5292.92 examples/s]Extracting prompt in train dataset:  16%|█▌        | 1080/6690 [00:00<00:01, 5377.38 examples/s][rank9]:[W612 21:16:36.561987777 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  24%|██▍       | 1630/6690 [00:00<00:00, 5430.54 examples/s][rank10]:[W612 21:16:36.601425689 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank11]:[W612 21:16:36.609591895 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  33%|███▎      | 2190/6690 [00:00<00:00, 5478.88 examples/s]Extracting prompt in train dataset:  45%|████▍     | 3010/6690 [00:00<00:00, 5442.19 examples/s]Extracting prompt in train dataset:  53%|█████▎    | 3570/6690 [00:00<00:00, 5457.61 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 4130/6690 [00:00<00:00, 5473.96 examples/s]Extracting prompt in train dataset:  74%|███████▍  | 4940/6690 [00:00<00:00, 5403.02 examples/s]Extracting prompt in train dataset:  82%|████████▏ | 5500/6690 [00:01<00:00, 5427.19 examples/s]Extracting prompt in train dataset:  91%|█████████ | 6060/6690 [00:01<00:00, 5450.20 examples/s]Extracting prompt in train dataset:  99%|█████████▉| 6621/6690 [00:01<00:00, 5494.00 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5388.74 examples/s]
Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 278/6690 [00:00<00:02, 2754.12 examples/s]Applying chat template to train dataset:   9%|▊         | 583/6690 [00:00<00:02, 2914.44 examples/s]Applying chat template to train dataset:  13%|█▎        | 888/6690 [00:00<00:01, 2973.64 examples/s]Applying chat template to train dataset:  18%|█▊        | 1191/6690 [00:00<00:01, 2991.64 examples/s]Applying chat template to train dataset:  22%|██▏       | 1496/6690 [00:00<00:01, 3010.21 examples/s]Applying chat template to train dataset:  29%|██▉       | 1937/6690 [00:00<00:01, 2976.21 examples/s]Applying chat template to train dataset:  36%|███▌      | 2375/6690 [00:00<00:01, 2951.11 examples/s]Applying chat template to train dataset:  40%|████      | 2680/6690 [00:00<00:01, 2972.91 examples/s]Applying chat template to train dataset:  45%|████▍     | 2985/6690 [00:01<00:01, 2991.59 examples/s]Applying chat template to train dataset:  49%|████▉     | 3290/6690 [00:01<00:01, 3005.64 examples/s]Applying chat template to train dataset:  54%|█████▎    | 3594/6690 [00:01<00:01, 3012.22 examples/s]Applying chat template to train dataset:  58%|█████▊    | 3899/6690 [00:01<00:00, 3021.48 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4203/6690 [00:01<00:00, 3025.29 examples/s]Applying chat template to train dataset:  69%|██████▉   | 4643/6690 [00:01<00:00, 2982.46 examples/s]Applying chat template to train dataset:  74%|███████▍  | 4946/6690 [00:01<00:00, 2993.05 examples/s]Applying chat template to train dataset:  81%|████████  | 5395/6690 [00:01<00:00, 2991.15 examples/s]Applying chat template to train dataset:  85%|████████▌ | 5699/6690 [00:01<00:00, 3001.90 examples/s]Applying chat template to train dataset:  90%|████████▉ | 6003/6690 [00:02<00:00, 3007.47 examples/s]Applying chat template to train dataset:  94%|█████████▍| 6308/6690 [00:02<00:00, 3017.42 examples/s]Applying chat template to train dataset:  99%|█████████▉| 6612/6690 [00:02<00:00, 3017.54 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 2987.53 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 37/6690 [00:00<00:18, 367.19 examples/s]Tokenizing train dataset:   1%|▏         | 88/6690 [00:00<00:15, 432.87 examples/s]Tokenizing train dataset:   2%|▏         | 152/6690 [00:00<00:15, 422.44 examples/s]Tokenizing train dataset:   3%|▎         | 202/6690 [00:00<00:14, 443.58 examples/s]Tokenizing train dataset:   4%|▎         | 250/6690 [00:00<00:14, 448.69 examples/s]Tokenizing train dataset:   5%|▍         | 316/6690 [00:00<00:14, 440.49 examples/s]Tokenizing train dataset:   5%|▌         | 364/6690 [00:00<00:14, 444.25 examples/s]Tokenizing train dataset:   6%|▌         | 410/6690 [00:00<00:14, 445.26 examples/s]Tokenizing train dataset:   7%|▋         | 460/6690 [00:01<00:13, 456.25 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:13, 460.96 examples/s]Tokenizing train dataset:   8%|▊         | 559/6690 [00:01<00:13, 464.05 examples/s]Tokenizing train dataset:   9%|▉         | 607/6690 [00:01<00:13, 463.82 examples/s]Tokenizing train dataset:  10%|▉         | 655/6690 [00:01<00:13, 463.89 examples/s]Tokenizing train dataset:  11%|█         | 703/6690 [00:01<00:12, 466.89 examples/s]Tokenizing train dataset:  11%|█▏        | 756/6690 [00:01<00:12, 477.29 examples/s]Tokenizing train dataset:  12%|█▏        | 807/6690 [00:01<00:12, 483.06 examples/s]Tokenizing train dataset:  13%|█▎        | 876/6690 [00:01<00:12, 466.33 examples/s]Tokenizing train dataset:  14%|█▍        | 937/6690 [00:02<00:13, 438.69 examples/s]Tokenizing train dataset:  15%|█▌        | 1006/6690 [00:02<00:12, 442.22 examples/s]Tokenizing train dataset:  16%|█▌        | 1052/6690 [00:02<00:12, 443.99 examples/s]Tokenizing train dataset:  16%|█▋        | 1100/6690 [00:02<00:12, 450.16 examples/s]Tokenizing train dataset:  17%|█▋        | 1168/6690 [00:02<00:12, 444.62 examples/s]Tokenizing train dataset:  18%|█▊        | 1233/6690 [00:02<00:12, 434.81 examples/s]Tokenizing train dataset:  19%|█▉        | 1289/6690 [00:02<00:13, 411.39 examples/s]Tokenizing train dataset:  20%|██        | 1338/6690 [00:03<00:12, 427.93 examples/s]Tokenizing train dataset:  21%|██        | 1384/6690 [00:03<00:12, 432.02 examples/s]Tokenizing train dataset:  22%|██▏       | 1445/6690 [00:03<00:12, 421.25 examples/s]Tokenizing train dataset:  23%|██▎       | 1510/6690 [00:03<00:12, 422.54 examples/s]Tokenizing train dataset:  23%|██▎       | 1560/6690 [00:03<00:11, 433.93 examples/s]Tokenizing train dataset:  24%|██▍       | 1604/6690 [00:03<00:11, 432.79 examples/s]Tokenizing train dataset:  25%|██▍       | 1649/6690 [00:03<00:11, 431.77 examples/s]Tokenizing train dataset:  25%|██▌       | 1695/6690 [00:03<00:11, 433.98 examples/s]Tokenizing train dataset:  26%|██▌       | 1741/6690 [00:03<00:11, 441.08 examples/s]Tokenizing train dataset:  27%|██▋       | 1807/6690 [00:04<00:11, 439.33 examples/s]Tokenizing train dataset:  28%|██▊       | 1876/6690 [00:04<00:10, 442.43 examples/s]Tokenizing train dataset:  29%|██▉       | 1946/6690 [00:04<00:10, 439.80 examples/s]Tokenizing train dataset:  30%|██▉       | 2003/6690 [00:04<00:11, 418.63 examples/s]Tokenizing train dataset:  31%|███       | 2046/6690 [00:04<00:11, 420.16 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 422.75 examples/s]Tokenizing train dataset:  32%|███▏      | 2139/6690 [00:04<00:10, 437.40 examples/s]Tokenizing train dataset:  33%|███▎      | 2188/6690 [00:04<00:09, 450.64 examples/s]Tokenizing train dataset:  33%|███▎      | 2238/6690 [00:05<00:09, 462.94 examples/s]Tokenizing train dataset:  34%|███▍      | 2300/6690 [00:05<00:09, 440.16 examples/s]Tokenizing train dataset:  35%|███▌      | 2347/6690 [00:05<00:09, 444.18 examples/s]Tokenizing train dataset:  36%|███▌      | 2395/6690 [00:05<00:09, 448.82 examples/s]Tokenizing train dataset:  37%|███▋      | 2465/6690 [00:05<00:09, 448.49 examples/s]Tokenizing train dataset:  38%|███▊      | 2536/6690 [00:05<00:09, 452.11 examples/s]Tokenizing train dataset:  39%|███▊      | 2587/6690 [00:05<00:08, 465.32 examples/s]Tokenizing train dataset:  40%|███▉      | 2651/6690 [00:05<00:09, 448.33 examples/s]Tokenizing train dataset:  41%|████      | 2715/6690 [00:06<00:09, 435.60 examples/s]Tokenizing train dataset:  41%|████▏     | 2775/6690 [00:06<00:09, 421.89 examples/s]Tokenizing train dataset:  42%|████▏     | 2838/6690 [00:06<00:09, 418.72 examples/s]Tokenizing train dataset:  43%|████▎     | 2882/6690 [00:06<00:09, 422.41 examples/s]Tokenizing train dataset:  44%|████▍     | 2933/6690 [00:06<00:08, 438.99 examples/s]Tokenizing train dataset:  45%|████▍     | 2986/6690 [00:06<00:08, 455.33 examples/s]Tokenizing train dataset:  45%|████▌     | 3033/6690 [00:06<00:07, 458.60 examples/s]Tokenizing train dataset:  46%|████▌     | 3088/6690 [00:06<00:07, 482.06 examples/s]Tokenizing train dataset:  47%|████▋     | 3152/6690 [00:07<00:07, 453.99 examples/s]Tokenizing train dataset:  48%|████▊     | 3199/6690 [00:07<00:07, 456.48 examples/s]Tokenizing train dataset:  49%|████▊     | 3248/6690 [00:07<00:07, 460.91 examples/s]Tokenizing train dataset:  49%|████▉     | 3306/6690 [00:07<00:07, 431.51 examples/s]Tokenizing train dataset:  50%|█████     | 3368/6690 [00:07<00:07, 422.48 examples/s]Tokenizing train dataset:  51%|█████▏    | 3429/6690 [00:07<00:07, 413.02 examples/s]Tokenizing train dataset:  52%|█████▏    | 3494/6690 [00:07<00:07, 415.38 examples/s]Tokenizing train dataset:  53%|█████▎    | 3549/6690 [00:08<00:07, 396.04 examples/s]Tokenizing train dataset:  54%|█████▍    | 3607/6690 [00:08<00:07, 389.52 examples/s]Tokenizing train dataset:  55%|█████▍    | 3653/6690 [00:08<00:07, 398.97 examples/s]Tokenizing train dataset:  55%|█████▌    | 3699/6690 [00:08<00:07, 408.58 examples/s]Tokenizing train dataset:  56%|█████▌    | 3752/6690 [00:08<00:07, 386.31 examples/s]Tokenizing train dataset:  57%|█████▋    | 3797/6690 [00:08<00:07, 398.47 examples/s]Tokenizing train dataset:  57%|█████▋    | 3843/6690 [00:08<00:06, 411.30 examples/s]Tokenizing train dataset:  58%|█████▊    | 3891/6690 [00:08<00:06, 426.35 examples/s]Tokenizing train dataset:  59%|█████▉    | 3940/6690 [00:09<00:06, 439.58 examples/s]Tokenizing train dataset:  60%|█████▉    | 3988/6690 [00:09<00:06, 446.05 examples/s]Tokenizing train dataset:  61%|██████    | 4055/6690 [00:09<00:05, 440.79 examples/s]Tokenizing train dataset:  62%|██████▏   | 4120/6690 [00:09<00:05, 434.44 examples/s]Tokenizing train dataset:  62%|██████▏   | 4170/6690 [00:09<00:05, 446.65 examples/s]Tokenizing train dataset:  63%|██████▎   | 4216/6690 [00:09<00:05, 444.09 examples/s]Tokenizing train dataset:  64%|██████▍   | 4281/6690 [00:09<00:05, 436.29 examples/s]Tokenizing train dataset:  65%|██████▍   | 4329/6690 [00:09<00:05, 446.18 examples/s]Tokenizing train dataset:  66%|██████▌   | 4386/6690 [00:10<00:05, 419.70 examples/s]Tokenizing train dataset:  66%|██████▌   | 4431/6690 [00:10<00:05, 425.93 examples/s]Tokenizing train dataset:  67%|██████▋   | 4495/6690 [00:10<00:05, 422.44 examples/s]Tokenizing train dataset:  68%|██████▊   | 4541/6690 [00:10<00:04, 430.35 examples/s]Tokenizing train dataset:  69%|██████▊   | 4586/6690 [00:10<00:04, 431.05 examples/s]Tokenizing train dataset:  69%|██████▉   | 4636/6690 [00:10<00:04, 447.57 examples/s]Tokenizing train dataset:  70%|███████   | 4684/6690 [00:10<00:04, 451.74 examples/s]Tokenizing train dataset:  71%|███████   | 4731/6690 [00:10<00:04, 451.21 examples/s]Tokenizing train dataset:  72%|███████▏  | 4790/6690 [00:10<00:04, 423.57 examples/s]Tokenizing train dataset:  72%|███████▏  | 4840/6690 [00:11<00:04, 436.80 examples/s]Tokenizing train dataset:  73%|███████▎  | 4904/6690 [00:11<00:04, 427.73 examples/s]Tokenizing train dataset:  74%|███████▍  | 4948/6690 [00:11<00:04, 430.61 examples/s]Tokenizing train dataset:  75%|███████▍  | 5010/6690 [00:11<00:03, 421.27 examples/s]Tokenizing train dataset:  76%|███████▌  | 5074/6690 [00:11<00:03, 419.81 examples/s]Tokenizing train dataset:  77%|███████▋  | 5133/6690 [00:11<00:03, 408.87 examples/s]Tokenizing train dataset:  77%|███████▋  | 5182/6690 [00:11<00:03, 425.56 examples/s]Tokenizing train dataset:  78%|███████▊  | 5226/6690 [00:12<00:03, 426.98 examples/s]Tokenizing train dataset:  79%|███████▉  | 5271/6690 [00:12<00:03, 426.46 examples/s]Tokenizing train dataset:  80%|███████▉  | 5333/6690 [00:12<00:03, 416.41 examples/s]Tokenizing train dataset:  80%|████████  | 5385/6690 [00:12<00:02, 441.04 examples/s]Tokenizing train dataset:  81%|████████▏ | 5450/6690 [00:12<00:02, 432.68 examples/s]Tokenizing train dataset:  82%|████████▏ | 5497/6690 [00:12<00:02, 436.41 examples/s]Tokenizing train dataset:  83%|████████▎ | 5560/6690 [00:12<00:02, 421.34 examples/s]Tokenizing train dataset:  84%|████████▍ | 5624/6690 [00:12<00:02, 417.84 examples/s]Tokenizing train dataset:  85%|████████▍ | 5669/6690 [00:13<00:02, 423.22 examples/s]Tokenizing train dataset:  85%|████████▌ | 5712/6690 [00:13<00:02, 422.12 examples/s]Tokenizing train dataset:  86%|████████▌ | 5762/6690 [00:13<00:02, 435.14 examples/s]Tokenizing train dataset:  87%|████████▋ | 5808/6690 [00:13<00:02, 440.48 examples/s]Tokenizing train dataset:  88%|████████▊ | 5867/6690 [00:13<00:01, 417.48 examples/s]Tokenizing train dataset:  88%|████████▊ | 5916/6690 [00:13<00:01, 434.56 examples/s]Tokenizing train dataset:  89%|████████▉ | 5980/6690 [00:13<00:01, 425.16 examples/s]Tokenizing train dataset:  90%|█████████ | 6025/6690 [00:13<00:01, 429.39 examples/s]Tokenizing train dataset:  91%|█████████ | 6073/6690 [00:13<00:01, 439.95 examples/s]Tokenizing train dataset:  92%|█████████▏| 6126/6690 [00:14<00:01, 462.34 examples/s]Tokenizing train dataset:  93%|█████████▎| 6191/6690 [00:14<00:01, 447.73 examples/s]Tokenizing train dataset:  93%|█████████▎| 6244/6690 [00:14<00:01, 414.19 examples/s]Tokenizing train dataset:  94%|█████████▍| 6289/6690 [00:14<00:00, 420.16 examples/s]Tokenizing train dataset:  95%|█████████▍| 6350/6690 [00:14<00:00, 407.90 examples/s]Tokenizing train dataset:  96%|█████████▌| 6418/6690 [00:14<00:00, 418.91 examples/s]Tokenizing train dataset:  97%|█████████▋| 6461/6690 [00:14<00:00, 416.46 examples/s]Tokenizing train dataset:  97%|█████████▋| 6507/6690 [00:15<00:00, 424.75 examples/s]Tokenizing train dataset:  98%|█████████▊| 6568/6690 [00:15<00:00, 416.39 examples/s]Tokenizing train dataset:  99%|█████████▉| 6610/6690 [00:15<00:00, 413.81 examples/s]Tokenizing train dataset: 100%|█████████▉| 6672/6690 [00:15<00:00, 407.05 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 432.70 examples/s]
[rank8]:[W612 21:16:56.206504145 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5644.69 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5586.20 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5566.96 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5568.88 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5561.84 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1153/6690 [00:00<00:00, 5734.91 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1140/6690 [00:00<00:00, 5629.52 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1150/6690 [00:00<00:00, 5662.90 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1740/6690 [00:00<00:00, 5778.85 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1720/6690 [00:00<00:00, 5686.37 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1730/6690 [00:00<00:00, 5695.88 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2610/6690 [00:00<00:00, 5772.95 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2574/6690 [00:00<00:00, 5681.89 examples/s]Extracting prompt in train dataset:  39%|███▊      | 2580/6690 [00:00<00:00, 5667.77 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  48%|████▊     | 3197/6690 [00:00<00:00, 5802.89 examples/s]Extracting prompt in train dataset:  47%|████▋     | 3150/6690 [00:00<00:00, 5695.02 examples/s]Extracting prompt in train dataset:  47%|████▋     | 3152/6690 [00:00<00:00, 5682.44 examples/s]Applying chat template to eval dataset:  32%|███▏      | 302/953 [00:00<00:00, 2993.36 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 3780/6690 [00:00<00:00, 5795.80 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3730/6690 [00:00<00:00, 5709.61 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3730/6690 [00:00<00:00, 5685.24 examples/s]Applying chat template to eval dataset:  65%|██████▍   | 617/953 [00:00<00:00, 3076.96 examples/s]Extracting prompt in train dataset:  65%|██████▌   | 4370/6690 [00:00<00:00, 5809.90 examples/s]Extracting prompt in train dataset:  64%|██████▍   | 4310/6690 [00:00<00:00, 5724.31 examples/s]Extracting prompt in train dataset:  64%|██████▍   | 4310/6690 [00:00<00:00, 5696.01 examples/s]Applying chat template to eval dataset:  98%|█████████▊| 931/953 [00:00<00:00, 3101.04 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3070.09 examples/s]
Extracting prompt in train dataset:  78%|███████▊  | 5220/6690 [00:00<00:00, 5727.16 examples/s]Extracting prompt in train dataset:  77%|███████▋  | 5140/6690 [00:00<00:00, 5638.73 examples/s]Extracting prompt in train dataset:  77%|███████▋  | 5140/6690 [00:00<00:00, 5614.74 examples/s]Extracting prompt in train dataset:  87%|████████▋ | 5808/6690 [00:01<00:00, 5768.10 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 5720/6690 [00:01<00:00, 5667.38 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 5720/6690 [00:01<00:00, 5643.86 examples/s]Extracting prompt in train dataset:  96%|█████████▌| 6393/6690 [00:01<00:00, 5775.04 examples/s]Extracting prompt in train dataset:  94%|█████████▍| 6300/6690 [00:01<00:00, 5687.27 examples/s]Extracting prompt in train dataset:  94%|█████████▍| 6300/6690 [00:01<00:00, 5663.48 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5734.19 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5645.09 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5627.06 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 318.98 examples/s]Tokenizing eval dataset:   8%|▊         | 78/953 [00:00<00:03, 289.58 examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 280/6690 [00:00<00:02, 2760.57 examples/s]Applying chat template to train dataset:   4%|▍         | 287/6690 [00:00<00:02, 2842.13 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 273.27 examples/s]Applying chat template to train dataset:   4%|▍         | 292/6690 [00:00<00:02, 2890.38 examples/s]Applying chat template to train dataset:   9%|▉         | 586/6690 [00:00<00:02, 2930.50 examples/s]Applying chat template to train dataset:   9%|▉         | 600/6690 [00:00<00:02, 2999.38 examples/s]Applying chat template to train dataset:   9%|▉         | 612/6690 [00:00<00:01, 3066.01 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 265.07 examples/s]Applying chat template to train dataset:  13%|█▎        | 894/6690 [00:00<00:01, 2987.92 examples/s]Applying chat template to train dataset:  14%|█▎        | 915/6690 [00:00<00:01, 3060.98 examples/s]Applying chat template to train dataset:  14%|█▍        | 933/6690 [00:00<00:01, 3127.51 examples/s]Applying chat template to train dataset:  18%|█▊        | 1199/6690 [00:00<00:01, 3011.05 examples/s]Applying chat template to train dataset:  18%|█▊        | 1228/6690 [00:00<00:01, 3083.59 examples/s]Applying chat template to train dataset:  19%|█▊        | 1251/6690 [00:00<00:01, 3144.26 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 250.81 examples/s]Applying chat template to train dataset:  22%|██▏       | 1504/6690 [00:00<00:01, 3022.63 examples/s]Applying chat template to train dataset:  23%|██▎       | 1541/6690 [00:00<00:01, 3096.88 examples/s]Applying chat template to train dataset:  24%|██▎       | 1573/6690 [00:00<00:01, 3166.06 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 267.33 examples/s]Applying chat template to train dataset:  27%|██▋       | 1811/6690 [00:00<00:01, 3033.62 examples/s]Applying chat template to train dataset:  28%|██▊       | 1856/6690 [00:00<00:01, 3110.57 examples/s]Applying chat template to train dataset:  28%|██▊       | 1893/6690 [00:00<00:01, 3175.89 examples/s]Tokenizing eval dataset:  31%|███       | 291/953 [00:00<00:01, 366.60 examples/s]Applying chat template to train dataset:  32%|███▏      | 2119/6690 [00:00<00:01, 3043.57 examples/s]Applying chat template to train dataset:  33%|███▎      | 2211/6690 [00:00<00:01, 3172.27 examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 436.63 examples/s]Applying chat template to train dataset:  34%|███▍      | 2275/6690 [00:00<00:01, 2973.82 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 485.19 examples/s]Applying chat template to train dataset:  38%|███▊      | 2575/6690 [00:00<00:01, 3034.53 examples/s]Applying chat template to train dataset:  39%|███▊      | 2589/6690 [00:00<00:01, 3019.20 examples/s]Applying chat template to train dataset:  40%|████      | 2689/6690 [00:00<00:01, 3174.03 examples/s]Applying chat template to train dataset:  43%|████▎     | 2880/6690 [00:00<00:01, 3034.70 examples/s]Tokenizing eval dataset:  51%|█████     | 484/953 [00:01<00:00, 533.56 examples/s]Applying chat template to train dataset:  43%|████▎     | 2900/6690 [00:00<00:01, 3044.71 examples/s]Applying chat template to train dataset:  45%|████▍     | 3009/6690 [00:00<00:01, 3178.46 examples/s]Applying chat template to train dataset:  48%|████▊     | 3187/6690 [00:01<00:01, 3042.22 examples/s]Tokenizing eval dataset:  58%|█████▊    | 551/953 [00:01<00:00, 569.33 examples/s]Applying chat template to train dataset:  48%|████▊     | 3215/6690 [00:01<00:01, 3074.45 examples/s]Applying chat template to train dataset:  50%|████▉     | 3329/6690 [00:01<00:01, 3183.25 examples/s]Applying chat template to train dataset:  52%|█████▏    | 3493/6690 [00:01<00:01, 3039.88 examples/s]Tokenizing eval dataset:  65%|██████▍   | 615/953 [00:01<00:00, 584.94 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3529/6690 [00:01<00:01, 3090.52 examples/s]Applying chat template to train dataset:  55%|█████▍    | 3648/6690 [00:01<00:00, 3183.00 examples/s]Applying chat template to train dataset:  57%|█████▋    | 3799/6690 [00:01<00:00, 3043.33 examples/s]Tokenizing eval dataset:  71%|███████   | 677/953 [00:01<00:00, 589.26 examples/s]Applying chat template to train dataset:  57%|█████▋    | 3841/6690 [00:01<00:00, 3097.12 examples/s]Applying chat template to train dataset:  59%|█████▉    | 3969/6690 [00:01<00:00, 3187.08 examples/s]Applying chat template to train dataset:  61%|██████▏   | 4104/6690 [00:01<00:00, 3043.14 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4156/6690 [00:01<00:00, 3109.85 examples/s]Applying chat template to train dataset:  64%|██████▍   | 4289/6690 [00:01<00:00, 3189.71 examples/s]Tokenizing eval dataset:  80%|███████▉  | 759/953 [00:01<00:00, 567.96 examples/s]Applying chat template to train dataset:  68%|██████▊   | 4544/6690 [00:01<00:00, 2996.67 examples/s]Applying chat template to train dataset:  69%|██████▉   | 4607/6690 [00:01<00:00, 3066.58 examples/s]Applying chat template to train dataset:  71%|███████   | 4749/6690 [00:01<00:00, 3140.35 examples/s]Tokenizing eval dataset:  87%|████████▋ | 832/953 [00:01<00:00, 535.74 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4850/6690 [00:01<00:00, 3007.08 examples/s]Applying chat template to train dataset:  74%|███████▎  | 4920/6690 [00:01<00:00, 3078.94 examples/s]Applying chat template to train dataset:  76%|███████▌  | 5067/6690 [00:01<00:00, 3148.80 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5155/6690 [00:01<00:00, 3014.40 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5233/6690 [00:01<00:00, 3089.97 examples/s]Tokenizing eval dataset:  95%|█████████▌| 907/953 [00:02<00:00, 519.94 examples/s]Applying chat template to train dataset:  81%|████████  | 5386/6690 [00:01<00:00, 3157.53 examples/s]Applying chat template to train dataset:  82%|████████▏ | 5460/6690 [00:01<00:00, 3020.18 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 450.37 examples/s]
Applying chat template to train dataset:  83%|████████▎ | 5547/6690 [00:01<00:00, 3100.55 examples/s]Applying chat template to train dataset:  85%|████████▌ | 5706/6690 [00:01<00:00, 3165.90 examples/s]Applying chat template to train dataset:  86%|████████▌ | 5766/6690 [00:01<00:00, 3026.92 examples/s]Applying chat template to train dataset:  88%|████████▊ | 5860/6690 [00:01<00:00, 3102.86 examples/s]Applying chat template to train dataset:  90%|█████████ | 6025/6690 [00:01<00:00, 3171.25 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6174/6690 [00:02<00:00, 3111.14 examples/s]Applying chat template to train dataset:  95%|█████████▍| 6344/6690 [00:02<00:00, 3174.76 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6204/6690 [00:02<00:00, 2982.33 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6487/6690 [00:02<00:00, 3112.92 examples/s]Applying chat template to train dataset: 100%|█████████▉| 6662/6690 [00:02<00:00, 3175.24 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3156.01 examples/s]
Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3071.44 examples/s]
Applying chat template to train dataset:  99%|█████████▉| 6639/6690 [00:02<00:00, 2950.12 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 2995.46 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 40/6690 [00:00<00:17, 387.26 examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:15, 421.15 examples/s]Tokenizing train dataset:   1%|          | 40/6690 [00:00<00:17, 387.50 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:15, 439.69 examples/s]Tokenizing train dataset:   1%|▏         | 91/6690 [00:00<00:14, 441.43 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:15, 439.37 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 441.77 examples/s]Tokenizing train dataset:   2%|▏         | 137/6690 [00:00<00:14, 447.45 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 440.56 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 474.12 examples/s]Tokenizing train dataset:   3%|▎         | 190/6690 [00:00<00:13, 473.89 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 472.19 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 474.27 examples/s]Tokenizing train dataset:   4%|▎         | 238/6690 [00:00<00:13, 475.21 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 471.90 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 471.97 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 469.49 examples/s]Tokenizing train dataset:   5%|▍         | 307/6690 [00:00<00:13, 461.71 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 462.30 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 464.29 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 459.41 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 476.10 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 477.78 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 473.24 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 484.12 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 485.19 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 481.09 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 485.81 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 486.99 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 483.37 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 486.34 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 487.52 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 483.93 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 486.78 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 487.77 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 484.25 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 487.17 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 488.03 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 485.01 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 494.44 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 495.11 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 492.20 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 505.69 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 506.32 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 502.78 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 510.70 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 511.18 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 508.19 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:11, 483.84 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:12, 483.17 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:12, 480.73 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 464.44 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 463.60 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 461.95 examples/s]Tokenizing train dataset:  15%|█▌        | 1025/6690 [00:02<00:12, 460.98 examples/s]Tokenizing train dataset:  15%|█▌        | 1026/6690 [00:02<00:12, 461.62 examples/s]Tokenizing train dataset:  15%|█▌        | 1025/6690 [00:02<00:12, 458.45 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 470.72 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 470.18 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 468.55 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:12, 461.91 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:12, 461.53 examples/s]Tokenizing train dataset:  17%|█▋        | 1145/6690 [00:02<00:12, 459.78 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 460.96 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 460.78 examples/s]Tokenizing train dataset:  18%|█▊        | 1192/6690 [00:02<00:11, 460.18 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 453.77 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 453.51 examples/s]Tokenizing train dataset:  19%|█▉        | 1260/6690 [00:02<00:12, 450.90 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 453.81 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 453.77 examples/s]Tokenizing train dataset:  20%|█▉        | 1307/6690 [00:02<00:11, 451.39 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 467.95 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 468.14 examples/s]Tokenizing train dataset:  20%|██        | 1360/6690 [00:02<00:11, 468.01 examples/s]Tokenizing train dataset:  21%|██        | 1420/6690 [00:03<00:12, 437.42 examples/s]Tokenizing train dataset:  21%|██        | 1421/6690 [00:03<00:11, 439.13 examples/s]Tokenizing train dataset:  21%|██        | 1420/6690 [00:03<00:12, 435.95 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 438.46 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 438.62 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 437.23 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 453.23 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 453.24 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:14, 352.46 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:15, 324.74 examples/s]Tokenizing train dataset:  23%|██▎       | 1517/6690 [00:03<00:16, 307.13 examples/s]Tokenizing train dataset:  24%|██▍       | 1606/6690 [00:03<00:13, 369.62 examples/s]Tokenizing train dataset:  24%|██▍       | 1606/6690 [00:03<00:14, 346.82 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:15, 336.31 examples/s]Tokenizing train dataset:  24%|██▍       | 1606/6690 [00:03<00:14, 356.11 examples/s]Tokenizing train dataset:  25%|██▍       | 1652/6690 [00:03<00:13, 370.26 examples/s]Tokenizing train dataset:  25%|██▍       | 1652/6690 [00:03<00:12, 388.21 examples/s]Tokenizing train dataset:  25%|██▍       | 1652/6690 [00:03<00:13, 377.66 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:12, 393.95 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:12, 407.69 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:11, 422.34 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:11, 432.94 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:12, 398.92 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:11, 425.78 examples/s]Tokenizing train dataset:  27%|██▋       | 1797/6690 [00:04<00:11, 425.69 examples/s]Tokenizing train dataset:  27%|██▋       | 1797/6690 [00:03<00:11, 433.60 examples/s]Tokenizing train dataset:  28%|██▊       | 1844/6690 [00:04<00:11, 433.26 examples/s]Tokenizing train dataset:  27%|██▋       | 1797/6690 [00:04<00:11, 427.77 examples/s]Tokenizing train dataset:  28%|██▊       | 1844/6690 [00:04<00:11, 438.97 examples/s]Tokenizing train dataset:  28%|██▊       | 1892/6690 [00:04<00:10, 444.58 examples/s]Tokenizing train dataset:  28%|██▊       | 1892/6690 [00:04<00:10, 448.90 examples/s]Tokenizing train dataset:  28%|██▊       | 1844/6690 [00:04<00:11, 434.56 examples/s]Tokenizing train dataset:  28%|██▊       | 1892/6690 [00:04<00:10, 444.85 examples/s]Tokenizing train dataset:  29%|██▉       | 1958/6690 [00:04<00:10, 440.85 examples/s]Tokenizing train dataset:  29%|██▉       | 1958/6690 [00:04<00:10, 443.59 examples/s]Tokenizing train dataset:  29%|██▉       | 1960/6690 [00:04<00:10, 442.18 examples/s]Tokenizing train dataset:  30%|███       | 2018/6690 [00:04<00:11, 422.95 examples/s]Tokenizing train dataset:  30%|███       | 2018/6690 [00:04<00:11, 424.34 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:11, 423.82 examples/s]Tokenizing train dataset:  31%|███       | 2086/6690 [00:04<00:10, 430.26 examples/s]Tokenizing train dataset:  31%|███       | 2086/6690 [00:04<00:10, 431.26 examples/s]Tokenizing train dataset:  32%|███▏      | 2135/6690 [00:04<00:10, 442.22 examples/s]Tokenizing train dataset:  32%|███▏      | 2135/6690 [00:04<00:10, 443.12 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 426.63 examples/s]Tokenizing train dataset:  33%|███▎      | 2186/6690 [00:04<00:09, 455.72 examples/s]Tokenizing train dataset:  33%|███▎      | 2186/6690 [00:04<00:09, 456.46 examples/s]Tokenizing train dataset:  32%|███▏      | 2140/6690 [00:04<00:10, 440.70 examples/s]Tokenizing train dataset:  33%|███▎      | 2237/6690 [00:04<00:09, 468.30 examples/s]Tokenizing train dataset:  33%|███▎      | 2237/6690 [00:04<00:09, 468.76 examples/s]Tokenizing train dataset:  33%|███▎      | 2190/6690 [00:04<00:09, 454.00 examples/s]Tokenizing train dataset:  34%|███▎      | 2245/6690 [00:05<00:09, 471.70 examples/s]Tokenizing train dataset:  34%|███▍      | 2301/6690 [00:05<00:09, 448.66 examples/s]Tokenizing train dataset:  34%|███▍      | 2304/6690 [00:05<00:09, 450.27 examples/s]Tokenizing train dataset:  35%|███▌      | 2349/6690 [00:05<00:09, 453.71 examples/s]Tokenizing train dataset:  35%|███▌      | 2354/6690 [00:05<00:09, 455.62 examples/s]Tokenizing train dataset:  35%|███▍      | 2309/6690 [00:05<00:09, 447.23 examples/s]Tokenizing train dataset:  36%|███▌      | 2398/6690 [00:05<00:09, 458.19 examples/s]Tokenizing train dataset:  36%|███▌      | 2401/6690 [00:05<00:09, 450.85 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:05<00:09, 456.77 examples/s]Tokenizing train dataset:  37%|███▋      | 2449/6690 [00:05<00:09, 456.26 examples/s]Tokenizing train dataset:  37%|███▋      | 2469/6690 [00:05<00:09, 460.13 examples/s]Tokenizing train dataset:  36%|███▋      | 2430/6690 [00:05<00:09, 452.65 examples/s]Tokenizing train dataset:  37%|███▋      | 2497/6690 [00:05<00:09, 460.50 examples/s]Tokenizing train dataset:  38%|███▊      | 2516/6690 [00:05<00:09, 457.92 examples/s]Tokenizing train dataset:  37%|███▋      | 2478/6690 [00:05<00:09, 455.92 examples/s]Tokenizing train dataset:  38%|███▊      | 2550/6690 [00:05<00:08, 479.38 examples/s]Tokenizing train dataset:  38%|███▊      | 2570/6690 [00:05<00:08, 476.80 examples/s]Tokenizing train dataset:  38%|███▊      | 2526/6690 [00:05<00:09, 458.75 examples/s]Tokenizing train dataset:  39%|███▉      | 2601/6690 [00:05<00:08, 482.75 examples/s]Tokenizing train dataset:  39%|███▉      | 2620/6690 [00:05<00:08, 476.80 examples/s]Tokenizing train dataset:  39%|███▊      | 2580/6690 [00:05<00:08, 478.98 examples/s]Tokenizing train dataset:  40%|███▉      | 2664/6690 [00:05<00:08, 453.48 examples/s]Tokenizing train dataset:  40%|████      | 2683/6690 [00:05<00:08, 452.69 examples/s]Tokenizing train dataset:  40%|███▉      | 2646/6690 [00:05<00:08, 463.08 examples/s]Tokenizing train dataset:  41%|████      | 2730/6690 [00:06<00:08, 444.70 examples/s]Tokenizing train dataset:  41%|████      | 2746/6690 [00:06<00:08, 438.41 examples/s]Tokenizing train dataset:  41%|████      | 2712/6690 [00:06<00:08, 449.66 examples/s]Tokenizing train dataset:  42%|████▏     | 2794/6690 [00:06<00:09, 432.74 examples/s]Tokenizing train dataset:  42%|████▏     | 2809/6690 [00:06<00:09, 426.69 examples/s]Tokenizing train dataset:  41%|████▏     | 2774/6690 [00:06<00:09, 429.82 examples/s]Tokenizing train dataset:  43%|████▎     | 2856/6690 [00:06<00:08, 434.65 examples/s]Tokenizing train dataset:  43%|████▎     | 2860/6690 [00:06<00:08, 433.10 examples/s]Tokenizing train dataset:  42%|████▏     | 2839/6690 [00:06<00:08, 429.82 examples/s]Tokenizing train dataset:  43%|████▎     | 2903/6690 [00:06<00:08, 440.46 examples/s]Tokenizing train dataset:  43%|████▎     | 2910/6690 [00:06<00:08, 445.46 examples/s]Tokenizing train dataset:  43%|████▎     | 2885/6690 [00:06<00:08, 434.47 examples/s]Tokenizing train dataset:  44%|████▍     | 2957/6690 [00:06<00:08, 459.29 examples/s]Tokenizing train dataset:  44%|████▍     | 2960/6690 [00:06<00:08, 455.44 examples/s]Tokenizing train dataset:  44%|████▍     | 2935/6690 [00:06<00:08, 450.23 examples/s]Tokenizing train dataset:  45%|████▍     | 3010/6690 [00:06<00:07, 472.31 examples/s]Tokenizing train dataset:  45%|████▌     | 3012/6690 [00:06<00:07, 470.12 examples/s]Tokenizing train dataset:  45%|████▍     | 2986/6690 [00:06<00:07, 464.66 examples/s]Tokenizing train dataset:  46%|████▌     | 3064/6690 [00:06<00:07, 482.49 examples/s]Tokenizing train dataset:  46%|████▌     | 3061/6690 [00:06<00:07, 478.55 examples/s]Tokenizing train dataset:  45%|████▌     | 3035/6690 [00:06<00:07, 468.93 examples/s]Tokenizing train dataset:  47%|████▋     | 3114/6690 [00:06<00:07, 486.20 examples/s]Tokenizing train dataset:  47%|████▋     | 3113/6690 [00:06<00:07, 486.49 examples/s]Tokenizing train dataset:  46%|████▌     | 3092/6690 [00:06<00:07, 495.38 examples/s]Tokenizing train dataset:  48%|████▊     | 3179/6690 [00:07<00:07, 463.84 examples/s]Tokenizing train dataset:  48%|████▊     | 3179/6690 [00:06<00:07, 464.87 examples/s]Tokenizing train dataset:  47%|████▋     | 3156/6690 [00:07<00:07, 467.48 examples/s]Tokenizing train dataset:  48%|████▊     | 3228/6690 [00:07<00:07, 467.87 examples/s]Tokenizing train dataset:  48%|████▊     | 3228/6690 [00:07<00:07, 468.36 examples/s]Tokenizing train dataset:  49%|████▉     | 3276/6690 [00:07<00:07, 466.69 examples/s]Tokenizing train dataset:  49%|████▉     | 3276/6690 [00:07<00:07, 466.97 examples/s]Tokenizing train dataset:  48%|████▊     | 3228/6690 [00:07<00:07, 467.11 examples/s]Tokenizing train dataset:  49%|████▉     | 3276/6690 [00:07<00:07, 466.09 examples/s]Tokenizing train dataset:  50%|████▉     | 3333/6690 [00:07<00:07, 431.50 examples/s]Tokenizing train dataset:  50%|████▉     | 3333/6690 [00:07<00:07, 431.57 examples/s]Tokenizing train dataset:  50%|█████     | 3378/6690 [00:07<00:07, 434.23 examples/s]Tokenizing train dataset:  50%|█████     | 3378/6690 [00:07<00:07, 434.24 examples/s]Tokenizing train dataset:  50%|████▉     | 3333/6690 [00:07<00:07, 432.35 examples/s]Tokenizing train dataset:  50%|█████     | 3378/6690 [00:07<00:07, 433.81 examples/s]Tokenizing train dataset:  51%|█████▏    | 3438/6690 [00:07<00:07, 419.46 examples/s]Tokenizing train dataset:  51%|█████▏    | 3438/6690 [00:07<00:07, 419.02 examples/s]Tokenizing train dataset:  52%|█████▏    | 3483/6690 [00:07<00:07, 425.52 examples/s]Tokenizing train dataset:  52%|█████▏    | 3483/6690 [00:07<00:07, 425.20 examples/s]Tokenizing train dataset:  51%|█████▏    | 3437/6690 [00:07<00:07, 418.24 examples/s]Tokenizing train dataset:  52%|█████▏    | 3483/6690 [00:07<00:07, 424.92 examples/s]Tokenizing train dataset:  53%|█████▎    | 3543/6690 [00:07<00:07, 411.44 examples/s]Tokenizing train dataset:  53%|█████▎    | 3543/6690 [00:07<00:07, 411.13 examples/s]Tokenizing train dataset:  53%|█████▎    | 3543/6690 [00:07<00:07, 410.98 examples/s]Tokenizing train dataset:  54%|█████▍    | 3602/6690 [00:08<00:07, 399.72 examples/s]Tokenizing train dataset:  54%|█████▍    | 3600/6690 [00:08<00:07, 398.48 examples/s]Tokenizing train dataset:  54%|█████▍    | 3646/6690 [00:08<00:07, 412.29 examples/s]Tokenizing train dataset:  55%|█████▍    | 3648/6690 [00:08<00:07, 410.73 examples/s]Tokenizing train dataset:  54%|█████▍    | 3600/6690 [00:08<00:07, 398.26 examples/s]Tokenizing train dataset:  55%|█████▌    | 3690/6690 [00:08<00:07, 414.17 examples/s]Tokenizing train dataset:  55%|█████▌    | 3694/6690 [00:08<00:07, 417.13 examples/s]Tokenizing train dataset:  54%|█████▍    | 3646/6690 [00:08<00:07, 411.58 examples/s]Tokenizing train dataset:  55%|█████▌    | 3690/6690 [00:08<00:07, 413.41 examples/s]Tokenizing train dataset:  56%|█████▌    | 3749/6690 [00:08<00:07, 397.99 examples/s]Tokenizing train dataset:  56%|█████▌    | 3750/6690 [00:08<00:07, 394.77 examples/s]Tokenizing train dataset:  57%|█████▋    | 3795/6690 [00:08<00:07, 410.33 examples/s]Tokenizing train dataset:  57%|█████▋    | 3797/6690 [00:08<00:07, 408.75 examples/s]Tokenizing train dataset:  56%|█████▌    | 3740/6690 [00:08<00:07, 373.76 examples/s]Tokenizing train dataset:  57%|█████▋    | 3844/6690 [00:08<00:06, 422.76 examples/s]Tokenizing train dataset:  57%|█████▋    | 3842/6690 [00:08<00:06, 420.99 examples/s]Tokenizing train dataset:  57%|█████▋    | 3787/6690 [00:08<00:07, 395.87 examples/s]Tokenizing train dataset:  58%|█████▊    | 3893/6690 [00:08<00:06, 436.64 examples/s]Tokenizing train dataset:  58%|█████▊    | 3891/6690 [00:08<00:06, 434.88 examples/s]Tokenizing train dataset:  57%|█████▋    | 3831/6690 [00:08<00:07, 402.88 examples/s]Tokenizing train dataset:  59%|█████▉    | 3941/6690 [00:08<00:06, 449.44 examples/s]Tokenizing train dataset:  59%|█████▉    | 3944/6690 [00:08<00:06, 452.78 examples/s]Tokenizing train dataset:  58%|█████▊    | 3876/6690 [00:08<00:06, 414.55 examples/s]Tokenizing train dataset:  60%|█████▉    | 3990/6690 [00:08<00:05, 457.69 examples/s]Tokenizing train dataset:  60%|█████▉    | 3993/6690 [00:08<00:05, 459.76 examples/s]Tokenizing train dataset:  59%|█████▊    | 3926/6690 [00:08<00:06, 436.61 examples/s]Tokenizing train dataset:  59%|█████▉    | 3976/6690 [00:09<00:06, 447.76 examples/s]Tokenizing train dataset:  61%|██████    | 4058/6690 [00:09<00:05, 452.45 examples/s]Tokenizing train dataset:  61%|██████    | 4060/6690 [00:09<00:05, 451.31 examples/s]Tokenizing train dataset:  60%|██████    | 4026/6690 [00:09<00:05, 457.01 examples/s]Tokenizing train dataset:  62%|██████▏   | 4127/6690 [00:09<00:05, 447.18 examples/s]Tokenizing train dataset:  62%|██████▏   | 4126/6690 [00:09<00:05, 445.51 examples/s]Tokenizing train dataset:  61%|██████    | 4091/6690 [00:09<00:05, 440.85 examples/s]Tokenizing train dataset:  62%|██████▏   | 4179/6690 [00:09<00:05, 461.08 examples/s]Tokenizing train dataset:  62%|██████▏   | 4176/6690 [00:09<00:05, 457.99 examples/s]Tokenizing train dataset:  62%|██████▏   | 4139/6690 [00:09<00:05, 449.23 examples/s]Tokenizing train dataset:  63%|██████▎   | 4248/6690 [00:09<00:05, 456.11 examples/s]Tokenizing train dataset:  63%|██████▎   | 4247/6690 [00:09<00:05, 457.90 examples/s]Tokenizing train dataset:  63%|██████▎   | 4191/6690 [00:09<00:05, 463.49 examples/s]Tokenizing train dataset:  64%|██████▍   | 4314/6690 [00:09<00:05, 452.08 examples/s]Tokenizing train dataset:  65%|██████▍   | 4317/6690 [00:09<00:05, 451.54 examples/s]Tokenizing train dataset:  64%|██████▎   | 4256/6690 [00:09<00:05, 446.00 examples/s]Tokenizing train dataset:  64%|██████▍   | 4302/6690 [00:09<00:05, 448.67 examples/s]Tokenizing train dataset:  65%|██████▌   | 4376/6690 [00:09<00:05, 434.79 examples/s]Tokenizing train dataset:  65%|██████▌   | 4379/6690 [00:09<00:05, 435.41 examples/s]Tokenizing train dataset:  66%|██████▌   | 4423/6690 [00:09<00:05, 438.36 examples/s]Tokenizing train dataset:  66%|██████▌   | 4425/6690 [00:09<00:05, 436.57 examples/s]Tokenizing train dataset:  65%|██████▌   | 4366/6690 [00:09<00:05, 435.83 examples/s]Tokenizing train dataset:  67%|██████▋   | 4469/6690 [00:10<00:05, 432.16 examples/s]Tokenizing train dataset:  67%|██████▋   | 4489/6690 [00:10<00:05, 436.85 examples/s]Tokenizing train dataset:  66%|██████▋   | 4437/6690 [00:10<00:05, 439.93 examples/s]Tokenizing train dataset:  68%|██████▊   | 4517/6690 [00:10<00:04, 439.62 examples/s]Tokenizing train dataset:  68%|██████▊   | 4535/6690 [00:10<00:04, 438.53 examples/s]Tokenizing train dataset:  68%|██████▊   | 4563/6690 [00:10<00:04, 440.36 examples/s]Tokenizing train dataset:  67%|██████▋   | 4500/6690 [00:10<00:05, 427.78 examples/s]Tokenizing train dataset:  69%|██████▊   | 4584/6690 [00:10<00:04, 446.46 examples/s]Tokenizing train dataset:  69%|██████▉   | 4613/6690 [00:10<00:04, 454.23 examples/s]Tokenizing train dataset:  68%|██████▊   | 4552/6690 [00:10<00:04, 444.16 examples/s]Tokenizing train dataset:  69%|██████▉   | 4635/6690 [00:10<00:04, 454.89 examples/s]Tokenizing train dataset:  70%|██████▉   | 4663/6690 [00:10<00:04, 466.50 examples/s]Tokenizing train dataset:  70%|██████▉   | 4682/6690 [00:10<00:04, 458.28 examples/s]Tokenizing train dataset:  69%|██████▉   | 4624/6690 [00:10<00:04, 451.81 examples/s]Tokenizing train dataset:  70%|███████   | 4711/6690 [00:10<00:04, 460.24 examples/s]Tokenizing train dataset:  71%|███████   | 4731/6690 [00:10<00:04, 461.42 examples/s]Tokenizing train dataset:  70%|██████▉   | 4672/6690 [00:10<00:04, 457.01 examples/s]Tokenizing train dataset:  71%|███████   | 4758/6690 [00:10<00:04, 461.77 examples/s]Tokenizing train dataset:  72%|███████▏  | 4791/6690 [00:10<00:04, 433.03 examples/s]Tokenizing train dataset:  71%|███████   | 4745/6690 [00:10<00:04, 458.31 examples/s]Tokenizing train dataset:  72%|███████▏  | 4819/6690 [00:10<00:04, 429.79 examples/s]Tokenizing train dataset:  72%|███████▏  | 4842/6690 [00:10<00:04, 448.16 examples/s]Tokenizing train dataset:  73%|███████▎  | 4867/6690 [00:10<00:04, 442.10 examples/s]Tokenizing train dataset:  72%|███████▏  | 4806/6690 [00:10<00:04, 437.04 examples/s]Tokenizing train dataset:  73%|███████▎  | 4906/6690 [00:10<00:04, 435.95 examples/s]Tokenizing train dataset:  73%|███████▎  | 4852/6690 [00:10<00:04, 442.28 examples/s]Tokenizing train dataset:  74%|███████▍  | 4934/6690 [00:11<00:04, 433.22 examples/s]Tokenizing train dataset:  74%|███████▍  | 4952/6690 [00:11<00:03, 437.83 examples/s]Tokenizing train dataset:  73%|███████▎  | 4914/6690 [00:11<00:04, 429.49 examples/s]Tokenizing train dataset:  75%|███████▍  | 4997/6690 [00:11<00:03, 436.97 examples/s]Tokenizing train dataset:  75%|███████▍  | 5000/6690 [00:11<00:03, 432.48 examples/s]Tokenizing train dataset:  74%|███████▍  | 4962/6690 [00:11<00:03, 438.82 examples/s]Tokenizing train dataset:  76%|███████▌  | 5060/6690 [00:11<00:03, 427.07 examples/s]Tokenizing train dataset:  76%|███████▌  | 5065/6690 [00:11<00:03, 429.23 examples/s]Tokenizing train dataset:  75%|███████▌  | 5026/6690 [00:11<00:03, 429.19 examples/s]Tokenizing train dataset:  76%|███████▋  | 5104/6690 [00:11<00:03, 428.92 examples/s]Tokenizing train dataset:  77%|███████▋  | 5129/6690 [00:11<00:03, 424.95 examples/s]Tokenizing train dataset:  76%|███████▌  | 5070/6690 [00:11<00:03, 425.31 examples/s]Tokenizing train dataset:  77%|███████▋  | 5169/6690 [00:11<00:03, 428.58 examples/s]Tokenizing train dataset:  77%|███████▋  | 5178/6690 [00:11<00:03, 436.41 examples/s]Tokenizing train dataset:  76%|███████▋  | 5115/6690 [00:11<00:03, 425.16 examples/s]Tokenizing train dataset:  78%|███████▊  | 5217/6690 [00:11<00:03, 439.70 examples/s]Tokenizing train dataset:  78%|███████▊  | 5223/6690 [00:11<00:03, 436.96 examples/s]Tokenizing train dataset:  77%|███████▋  | 5160/6690 [00:11<00:03, 426.83 examples/s]Tokenizing train dataset:  79%|███████▉  | 5269/6690 [00:11<00:03, 440.45 examples/s]Tokenizing train dataset:  78%|███████▊  | 5210/6690 [00:11<00:03, 437.98 examples/s]Tokenizing train dataset:  79%|███████▉  | 5283/6690 [00:11<00:03, 437.73 examples/s]Tokenizing train dataset:  79%|███████▊  | 5255/6690 [00:11<00:03, 438.42 examples/s]Tokenizing train dataset:  80%|███████▉  | 5333/6690 [00:12<00:03, 424.72 examples/s]Tokenizing train dataset:  80%|███████▉  | 5346/6690 [00:11<00:03, 428.33 examples/s]Tokenizing train dataset:  81%|████████  | 5388/6690 [00:12<00:02, 453.70 examples/s]Tokenizing train dataset:  79%|███████▉  | 5316/6690 [00:12<00:03, 425.08 examples/s]Tokenizing train dataset:  81%|████████  | 5399/6690 [00:12<00:02, 451.69 examples/s]Tokenizing train dataset:  80%|████████  | 5366/6690 [00:12<00:02, 442.06 examples/s]Tokenizing train dataset:  82%|████████▏ | 5454/6690 [00:12<00:02, 442.57 examples/s]Tokenizing train dataset:  82%|████████▏ | 5465/6690 [00:12<00:02, 441.55 examples/s]Tokenizing train dataset:  81%|████████  | 5415/6690 [00:12<00:02, 450.39 examples/s]Tokenizing train dataset:  82%|████████▏ | 5500/6690 [00:12<00:02, 442.46 examples/s]Tokenizing train dataset:  82%|████████▏ | 5512/6690 [00:12<00:02, 443.39 examples/s]Tokenizing train dataset:  82%|████████▏ | 5483/6690 [00:12<00:02, 445.68 examples/s]Tokenizing train dataset:  83%|████████▎ | 5566/6690 [00:12<00:02, 436.16 examples/s]Tokenizing train dataset:  83%|████████▎ | 5575/6690 [00:12<00:02, 432.00 examples/s]Tokenizing train dataset:  83%|████████▎ | 5547/6690 [00:12<00:02, 435.24 examples/s]Tokenizing train dataset:  84%|████████▍ | 5632/6690 [00:12<00:02, 433.16 examples/s]Tokenizing train dataset:  84%|████████▍ | 5642/6690 [00:12<00:02, 433.84 examples/s]Tokenizing train dataset:  85%|████████▍ | 5678/6690 [00:12<00:02, 437.03 examples/s]Tokenizing train dataset:  84%|████████▍ | 5613/6690 [00:12<00:02, 430.50 examples/s]Tokenizing train dataset:  85%|████████▌ | 5707/6690 [00:12<00:02, 431.69 examples/s]Tokenizing train dataset:  86%|████████▌ | 5723/6690 [00:12<00:02, 433.31 examples/s]Tokenizing train dataset:  85%|████████▍ | 5681/6690 [00:12<00:02, 432.57 examples/s]Tokenizing train dataset:  86%|████████▌ | 5757/6690 [00:12<00:02, 446.07 examples/s]Tokenizing train dataset:  86%|████████▌ | 5768/6690 [00:12<00:02, 434.36 examples/s]Tokenizing train dataset:  86%|████████▌ | 5727/6690 [00:13<00:02, 436.85 examples/s]Tokenizing train dataset:  87%|████████▋ | 5817/6690 [00:13<00:01, 447.98 examples/s]Tokenizing train dataset:  87%|████████▋ | 5821/6690 [00:13<00:01, 437.65 examples/s]Tokenizing train dataset:  87%|████████▋ | 5798/6690 [00:13<00:02, 442.56 examples/s]Tokenizing train dataset:  88%|████████▊ | 5876/6690 [00:13<00:01, 424.80 examples/s]Tokenizing train dataset:  88%|████████▊ | 5881/6690 [00:13<00:01, 422.74 examples/s]Tokenizing train dataset:  89%|████████▊ | 5926/6690 [00:13<00:01, 443.22 examples/s]Tokenizing train dataset:  88%|████████▊ | 5859/6690 [00:13<00:01, 427.70 examples/s]Tokenizing train dataset:  89%|████████▊ | 5935/6690 [00:13<00:01, 444.73 examples/s]Tokenizing train dataset:  89%|████████▉ | 5973/6690 [00:13<00:01, 444.97 examples/s]Tokenizing train dataset:  88%|████████▊ | 5911/6690 [00:13<00:01, 446.50 examples/s]Tokenizing train dataset:  90%|████████▉ | 6003/6690 [00:13<00:01, 443.72 examples/s]Tokenizing train dataset:  90%|████████▉ | 6020/6690 [00:13<00:01, 365.99 examples/s]Tokenizing train dataset:  91%|█████████ | 6072/6690 [00:13<00:01, 442.67 examples/s]Tokenizing train dataset:  89%|████████▉ | 5976/6690 [00:13<00:01, 385.46 examples/s]Tokenizing train dataset:  91%|█████████ | 6067/6690 [00:13<00:01, 389.08 examples/s]Tokenizing train dataset:  92%|█████████▏| 6127/6690 [00:13<00:01, 462.12 examples/s]Tokenizing train dataset:  90%|█████████ | 6023/6690 [00:13<00:01, 397.98 examples/s]Tokenizing train dataset:  92%|█████████▏| 6122/6690 [00:13<00:01, 425.35 examples/s]Tokenizing train dataset:  91%|█████████ | 6072/6690 [00:13<00:01, 416.66 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6690 [00:13<00:01, 452.51 examples/s]Tokenizing train dataset:  92%|█████████▏| 6168/6690 [00:13<00:01, 432.58 examples/s]Tokenizing train dataset:  92%|█████████▏| 6127/6690 [00:13<00:01, 443.54 examples/s]Tokenizing train dataset:  93%|█████████▎| 6228/6690 [00:14<00:01, 415.35 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:14<00:01, 416.41 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6690 [00:14<00:01, 439.61 examples/s]Tokenizing train dataset:  94%|█████████▍| 6299/6690 [00:14<00:00, 431.54 examples/s]Tokenizing train dataset:  94%|█████████▍| 6290/6690 [00:14<00:00, 407.92 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:14<00:01, 406.69 examples/s]Tokenizing train dataset:  95%|█████████▍| 6333/6690 [00:14<00:00, 411.33 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 416.91 examples/s]Tokenizing train dataset:  94%|█████████▍| 6298/6690 [00:14<00:00, 423.17 examples/s]Tokenizing train dataset:  96%|█████████▌| 6404/6690 [00:14<00:00, 419.74 examples/s]Tokenizing train dataset:  95%|█████████▌| 6375/6690 [00:14<00:00, 406.13 examples/s]Tokenizing train dataset:  96%|█████████▋| 6452/6690 [00:14<00:00, 431.45 examples/s]Tokenizing train dataset:  96%|█████████▌| 6425/6690 [00:14<00:00, 425.84 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 411.33 examples/s]Tokenizing train dataset:  97%|█████████▋| 6497/6690 [00:14<00:00, 434.34 examples/s]Tokenizing train dataset:  97%|█████████▋| 6469/6690 [00:14<00:00, 426.11 examples/s]Tokenizing train dataset:  96%|█████████▌| 6403/6690 [00:14<00:00, 413.24 examples/s]Tokenizing train dataset:  98%|█████████▊| 6541/6690 [00:14<00:00, 432.96 examples/s]Tokenizing train dataset:  96%|█████████▋| 6451/6690 [00:14<00:00, 427.54 examples/s]Tokenizing train dataset:  98%|█████████▊| 6535/6690 [00:14<00:00, 429.31 examples/s]Tokenizing train dataset:  98%|█████████▊| 6588/6690 [00:14<00:00, 438.36 examples/s]Tokenizing train dataset:  97%|█████████▋| 6497/6690 [00:14<00:00, 431.19 examples/s]Tokenizing train dataset:  99%|█████████▊| 6600/6690 [00:14<00:00, 426.81 examples/s]Tokenizing train dataset:  98%|█████████▊| 6541/6690 [00:14<00:00, 430.08 examples/s]Tokenizing train dataset:  99%|█████████▉| 6648/6690 [00:14<00:00, 418.42 examples/s]Tokenizing train dataset:  98%|█████████▊| 6587/6690 [00:15<00:00, 434.95 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 443.38 examples/s]
Tokenizing train dataset: 100%|█████████▉| 6660/6690 [00:15<00:00, 410.90 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 440.09 examples/s]
Tokenizing train dataset:  99%|█████████▉| 6648/6690 [00:15<00:00, 417.37 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 437.25 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5533.64 examples/s]Extracting prompt in eval dataset:  58%|█████▊    | 554/953 [00:00<00:00, 5480.49 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5593.52 examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:17:22] Error running `scontrol show job $SLURM_JOB_ID` to count SLURM-available cpus. Using the machine's cpu count.
[codecarbon INFO @ 21:17:22] [setup] RAM Tracking...
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:17:22] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:17:22] [setup] CPU Tracking...
[codecarbon WARNING @ 21:17:22] No CPU tracking mode found. Falling back on CPU constant mode. 
 Linux OS detected: Please ensure RAPL files exist at \sys\class\powercap\intel-rapl to measure CPU

Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5584.34 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5521.08 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5456.83 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  31%|███       | 296/953 [00:00<00:00, 2927.36 examples/s]Applying chat template to eval dataset:  32%|███▏      | 302/953 [00:00<00:00, 2993.30 examples/s]Applying chat template to eval dataset:  33%|███▎      | 310/953 [00:00<00:00, 3057.71 examples/s]Applying chat template to eval dataset:  63%|██████▎   | 604/953 [00:00<00:00, 3012.72 examples/s]Applying chat template to eval dataset:  65%|██████▍   | 617/953 [00:00<00:00, 3077.66 examples/s]Applying chat template to eval dataset:  66%|██████▋   | 632/953 [00:00<00:00, 3143.67 examples/s]Applying chat template to eval dataset:  96%|█████████▌| 912/953 [00:00<00:00, 3039.28 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3007.15 examples/s]
Applying chat template to eval dataset:  98%|█████████▊| 935/953 [00:00<00:00, 3119.75 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3169.39 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3134.74 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3080.95 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.54 examples/s][codecarbon INFO @ 21:17:23] CPU Model on constant consumption mode: AMD EPYC 7H12 64-Core Processor
[codecarbon INFO @ 21:17:23] [setup] GPU Tracking...
[codecarbon INFO @ 21:17:23] Tracking Nvidia GPU via pynvml
Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 318.98 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.77 examples/s][codecarbon INFO @ 21:17:23] >>> Tracker's metadata:
[codecarbon INFO @ 21:17:23]   Platform system: Linux-5.15.112-1.el8.vega.x86_64-x86_64-with-glibc2.35
[codecarbon INFO @ 21:17:23]   Python version: 3.10.12
[codecarbon INFO @ 21:17:23]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 21:17:23]   Available RAM : 503.683 GB
[codecarbon INFO @ 21:17:23]   CPU count: 256
[codecarbon INFO @ 21:17:23]   CPU model: AMD EPYC 7H12 64-Core Processor
[codecarbon INFO @ 21:17:23]   GPU count: 4
[codecarbon INFO @ 21:17:23]   GPU model: 4 x NVIDIA A100-SXM4-40GB BUT only tracking these GPU ids : [0, 1, 2, 3]
Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 286.74 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 286.23 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 286.27 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 276.62 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 276.29 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 274.81 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 264.94 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 264.41 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 265.77 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 251.66 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 250.94 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 251.10 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 268.01 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 267.61 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 267.65 examples/s]Tokenizing eval dataset:  31%|███       | 291/953 [00:00<00:01, 366.88 examples/s]Tokenizing eval dataset:  31%|███       | 291/953 [00:00<00:01, 366.93 examples/s]Tokenizing eval dataset:  31%|███       | 291/953 [00:00<00:01, 366.97 examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 436.95 examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 437.46 examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 437.43 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 485.89 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 486.58 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 486.18 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 538.93 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 539.70 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 539.16 examples/s]Tokenizing eval dataset:  58%|█████▊    | 553/953 [00:01<00:00, 568.77 examples/s]Tokenizing eval dataset:  58%|█████▊    | 554/953 [00:01<00:00, 572.50 examples/s]Tokenizing eval dataset:  58%|█████▊    | 554/953 [00:01<00:00, 571.97 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 582.70 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 583.26 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 582.34 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 594.29 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 594.74 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 594.23 examples/s]Tokenizing eval dataset:  80%|████████  | 766/953 [00:01<00:00, 574.14 examples/s]Tokenizing eval dataset:  80%|████████  | 766/953 [00:01<00:00, 574.91 examples/s]Tokenizing eval dataset:  80%|████████  | 766/953 [00:01<00:00, 574.44 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 540.60 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 540.78 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 540.89 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 515.91 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 516.25 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 515.75 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 451.73 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 451.97 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 451.67 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[codecarbon INFO @ 21:17:26] Saving emissions data to file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/training_run/curriculum-2_r-64_lr-1e-06_b-0.1/emissions.csv
Set up DPO trainer
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4405736923217773 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3563480377197266 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3544809818267822 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3467137813568115 seconds
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:18:01] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:18:01] Energy consumed for RAM : 0.000788 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:18:01] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:18:01] Energy consumed for all CPUs : 0.000584 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:18:01] Energy consumed for all GPUs : 0.001077 kWh. Total GPU Power : 258.1987743394508 W
[codecarbon INFO @ 21:18:01] 0.002448 kWh of electricity used since the beginning.
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:18:16] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:18:16] Energy consumed for RAM : 0.001574 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:18:16] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:18:16] Energy consumed for all CPUs : 0.001167 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:18:16] Energy consumed for all GPUs : 0.002585 kWh. Total GPU Power : 362.13701471719514 W
[codecarbon INFO @ 21:18:16] 0.005325 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:18:31] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:18:31] Energy consumed for RAM : 0.002360 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:18:31] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:18:31] Energy consumed for all CPUs : 0.001750 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:18:31] Energy consumed for all GPUs : 0.004088 kWh. Total GPU Power : 360.9508250032607 W
[codecarbon INFO @ 21:18:31] 0.008198 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:18:46] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:18:46] Energy consumed for RAM : 0.003147 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:18:46] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:18:46] Energy consumed for all CPUs : 0.002333 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:18:46] Energy consumed for all GPUs : 0.005591 kWh. Total GPU Power : 360.9406310567121 W
[codecarbon INFO @ 21:18:46] 0.011070 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:19:01] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:19:01] Energy consumed for RAM : 0.003933 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:19:01] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:19:01] Energy consumed for all CPUs : 0.002916 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:19:01] Energy consumed for all GPUs : 0.007089 kWh. Total GPU Power : 359.80407135468784 W
[codecarbon INFO @ 21:19:01] 0.013937 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:19:16] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:19:16] Energy consumed for RAM : 0.004719 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:19:16] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:19:16] Energy consumed for all CPUs : 0.003499 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:19:16] Energy consumed for all GPUs : 0.008592 kWh. Total GPU Power : 361.0092034564754 W
[codecarbon INFO @ 21:19:16] 0.016810 kWh of electricity used since the beginning.
[rank11]:[E612 21:19:31.138836528 ProcessGroupNCCL.cpp:552] [Rank 11] Collective WorkNCCL(SeqNum=3, OpType=ALLGATHER, NumelIn=1, NumelOut=12, Timeout(ms)=1800000) raised the following async exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgressOpt: Call to recv from 10.210.3.85<35113> failed : Broken pipe
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6f8d16c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f6f3b4211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f6f3b42964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f6f3b42b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f6f3b42c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f6f8d8575c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f6f8f6ffac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f6f8f791a40 in /lib/x86_64-linux-gnu/libc.so.6)

[rank11]:[E612 21:19:31.140381339 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 11]  failure detected by watchdog at work sequence id: 3 PG status: last enqueued work: 4, last completed work: 2
[rank11]:[E612 21:19:31.140389179 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank11]: Traceback (most recent call last):
[rank11]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 235, in <module>
[rank11]:     main(get_train_data(CURRICULUM_STAGE), val_data, args.rank, args.learning_rate, args.total_epochs, args.beta, CURRICULUM_STAGE)
[rank11]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 211, in main
[rank11]:     dpo_trainer.train()
[rank11]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank11]:     return inner_training_loop(
[rank11]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank11]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank11]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3736, in training_step
[rank11]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1356, in compute_loss
[rank11]:     loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1315, in get_batch_loss_metrics
[rank11]:     metrics[f"{prefix}rewards/chosen"] = self.accelerator.gather_for_metrics(chosen_rewards).mean().item()
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2583, in gather_for_metrics
[rank11]:     data = self.gather(input_data)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2539, in gather
[rank11]:     return gather(tensor)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 384, in wrapper
[rank11]:     output = gather_object([shapes])
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 459, in gather_object
[rank11]:     return _gpu_gather_object(object)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 440, in _gpu_gather_object
[rank11]:     torch.distributed.all_gather_object(output_objects, object)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank11]:     return func(*args, **kwargs)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3052, in all_gather_object
[rank11]:     object_list[i] = _tensor_to_object(tensor, tensor_size, group)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2961, in _tensor_to_object
[rank11]:     return _unpickler(io.BytesIO(buf)).load()
[rank11]: EOFError: Ran out of input
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:19:31] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:19:31] Energy consumed for RAM : 0.005506 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:19:31] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:19:31] Energy consumed for all CPUs : 0.004082 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:19:31] Energy consumed for all GPUs : 0.010092 kWh. Total GPU Power : 360.1480566430572 W
[codecarbon INFO @ 21:19:31] 0.019679 kWh of electricity used since the beginning.
[rank11]:[E612 21:19:32.767578796 ProcessGroupNCCL.cpp:681] [Rank 11] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank11]:[E612 21:19:32.767600476 ProcessGroupNCCL.cpp:695] [Rank 11] To avoid data inconsistency, we are taking the entire process down.
[rank11]:[E612 21:19:32.767656387 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 11] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgressOpt: Call to recv from 10.210.3.85<35113> failed : Broken pipe
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6f8d16c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f6f3b4211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f6f3b42964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f6f3b42b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f6f3b42c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f6f8d8575c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f6f8f6ffac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f6f8f791a40 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 11] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgressOpt: Call to recv from 10.210.3.85<35113> failed : Broken pipe
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6f8d16c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f6f3b4211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f6f3b42964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f6f3b42b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f6f3b42c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f6f8d8575c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f6f8f6ffac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f6f8f791a40 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f6f8d16c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f6f3b0876fc in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f6f8d8575c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f6f8f6ffac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126a40 (0x7f6f8f791a40 in /lib/x86_64-linux-gnu/libc.so.6)

[rank8]:[E612 21:19:32.882712627 ProcessGroupNCCL.cpp:552] [Rank 8] Collective WorkNCCL(SeqNum=4, OpType=ALLGATHER, NumelIn=20, NumelOut=240, Timeout(ms)=1800000) raised the following async exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn02.vega.pri<44684>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f9a1e96c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f99ccc211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f99ccc2964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f99ccc2b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f99ccc2c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f9a1f0415c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f9a20ee9ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f9a20f7ba40 in /lib/x86_64-linux-gnu/libc.so.6)

[rank8]:[E612 21:19:32.884230257 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 8]  failure detected by watchdog at work sequence id: 4 PG status: last enqueued work: 4, last completed work: 3
[rank8]:[E612 21:19:32.884236947 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank9]: Traceback (most recent call last):
[rank9]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 235, in <module>
[rank9]:     main(get_train_data(CURRICULUM_STAGE), val_data, args.rank, args.learning_rate, args.total_epochs, args.beta, CURRICULUM_STAGE)
[rank9]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 211, in main
[rank9]:     dpo_trainer.train()
[rank9]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank9]:     return inner_training_loop(
[rank9]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank9]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank9]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3736, in training_step
[rank9]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1356, in compute_loss
[rank9]:     loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1315, in get_batch_loss_metrics
[rank9]:     metrics[f"{prefix}rewards/chosen"] = self.accelerator.gather_for_metrics(chosen_rewards).mean().item()
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2583, in gather_for_metrics
[rank9]:     data = self.gather(input_data)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2539, in gather
[rank9]:     return gather(tensor)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 384, in wrapper
[rank9]:     output = gather_object([shapes])
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 459, in gather_object
[rank9]:     return _gpu_gather_object(object)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 440, in _gpu_gather_object
[rank9]:     torch.distributed.all_gather_object(output_objects, object)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank9]:     return func(*args, **kwargs)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3052, in all_gather_object
[rank9]:     object_list[i] = _tensor_to_object(tensor, tensor_size, group)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2961, in _tensor_to_object
[rank9]:     return _unpickler(io.BytesIO(buf)).load()
[rank9]: EOFError: Ran out of input
[rank8]: Traceback (most recent call last):
[rank8]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 235, in <module>
[rank8]:     main(get_train_data(CURRICULUM_STAGE), val_data, args.rank, args.learning_rate, args.total_epochs, args.beta, CURRICULUM_STAGE)
[rank8]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 211, in main
[rank8]:     dpo_trainer.train()
[rank8]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank8]:     return inner_training_loop(
[rank8]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank8]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank8]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3736, in training_step
[rank8]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1356, in compute_loss
[rank8]:     loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1315, in get_batch_loss_metrics
[rank8]:     metrics[f"{prefix}rewards/chosen"] = self.accelerator.gather_for_metrics(chosen_rewards).mean().item()
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2583, in gather_for_metrics
[rank8]:     data = self.gather(input_data)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2539, in gather
[rank8]:     return gather(tensor)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 384, in wrapper
[rank8]:     output = gather_object([shapes])
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 459, in gather_object
[rank8]:     return _gpu_gather_object(object)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 440, in _gpu_gather_object
[rank8]:     torch.distributed.all_gather_object(output_objects, object)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank8]:     return func(*args, **kwargs)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3052, in all_gather_object
[rank8]:     object_list[i] = _tensor_to_object(tensor, tensor_size, group)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2961, in _tensor_to_object
[rank8]:     return _unpickler(io.BytesIO(buf)).load()
[rank8]: EOFError: Ran out of input
[rank10]: Traceback (most recent call last):
[rank10]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 235, in <module>
[rank10]:     main(get_train_data(CURRICULUM_STAGE), val_data, args.rank, args.learning_rate, args.total_epochs, args.beta, CURRICULUM_STAGE)
[rank10]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 211, in main
[rank10]:     dpo_trainer.train()
[rank10]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank10]:     return inner_training_loop(
[rank10]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank10]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank10]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3736, in training_step
[rank10]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1356, in compute_loss
[rank10]:     loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1315, in get_batch_loss_metrics
[rank10]:     metrics[f"{prefix}rewards/chosen"] = self.accelerator.gather_for_metrics(chosen_rewards).mean().item()
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2583, in gather_for_metrics
[rank10]:     data = self.gather(input_data)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2539, in gather
[rank10]:     return gather(tensor)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 384, in wrapper
[rank10]:     output = gather_object([shapes])
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 459, in gather_object
[rank10]:     return _gpu_gather_object(object)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 440, in _gpu_gather_object
[rank10]:     torch.distributed.all_gather_object(output_objects, object)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank10]:     return func(*args, **kwargs)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3052, in all_gather_object
[rank10]:     object_list[i] = _tensor_to_object(tensor, tensor_size, group)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2961, in _tensor_to_object
[rank10]:     return _unpickler(io.BytesIO(buf)).load()
[rank10]: EOFError: Ran out of input
[rank8]:[E612 21:19:32.263084979 ProcessGroupNCCL.cpp:681] [Rank 8] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank8]:[E612 21:19:32.263103309 ProcessGroupNCCL.cpp:695] [Rank 8] To avoid data inconsistency, we are taking the entire process down.
[rank8]:[E612 21:19:32.263144769 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 8] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn02.vega.pri<44684>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f9a1e96c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f99ccc211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f99ccc2964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f99ccc2b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f99ccc2c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f9a1f0415c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f9a20ee9ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f9a20f7ba40 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 8] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn02.vega.pri<44684>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f9a1e96c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f99ccc211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f99ccc2964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f99ccc2b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f99ccc2c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f9a1f0415c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f9a20ee9ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f9a20f7ba40 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f9a1e96c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f99cc8876fc in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f9a1f0415c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f9a20ee9ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126a40 (0x7f9a20f7ba40 in /lib/x86_64-linux-gnu/libc.so.6)

W0612 21:19:32.572000 2120095 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2120273 closing signal SIGTERM
W0612 21:19:32.572000 2120095 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2120274 closing signal SIGTERM
W0612 21:19:32.572000 2120095 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2120275 closing signal SIGTERM
E0612 21:19:33.405000 2120095 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 3 (pid: 2120276) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1182, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 861, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train_curriculum.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-12_21:19:32
  host      : pm5-nod09.vega.pri
  rank      : 11 (local_rank: 3)
  exitcode  : -6 (pid: 2120276)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2120276
========================================================
