cpu-bind=MASK - gn04, task  2  0 [2118227]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 2 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 2     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63118025     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=1e-6 --total_epochs=3 --beta=0.1 --curriculum_stage=2
-------------------------------------------
[2025-06-12 21:08:50,042] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 21:08:51.419000 2118279 torch/distributed/run.py:792] 
W0612 21:08:51.419000 2118279 torch/distributed/run.py:792] *****************************************
W0612 21:08:51.419000 2118279 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 21:08:51.419000 2118279 torch/distributed/run.py:792] *****************************************
[2025-06-12 21:08:56,292] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:08:56,307] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:08:56,318] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:08:56,329] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
World size: 12
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Created datasets
Train dataset size: 6690
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-12 21:09:00,882] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 21:09:00,912] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 21:09:00,915] [INFO] [comm.py:658:init_distributed] cdb=None
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curriculum-1
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][2025-06-12 21:09:01,167] [INFO] [comm.py:658:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:26,  8.68s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:26,  8.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:25,  8.64s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:26,  8.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.00s/it]
Loaded model
Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.03s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.04s/it]
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   8%|▊         | 550/6690 [00:00<00:01, 5372.94 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1112/6690 [00:00<00:01, 5499.40 examples/s]Extracting prompt in train dataset:  25%|██▌       | 1680/6690 [00:00<00:00, 5566.25 examples/s][rank9]:[W612 21:09:36.302738175 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  34%|███▎      | 2250/6690 [00:00<00:00, 5566.45 examples/s][rank10]:[W612 21:09:36.478289642 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  42%|████▏     | 2820/6690 [00:00<00:00, 5593.99 examples/s][rank11]:[W612 21:09:36.558131462 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  51%|█████     | 3400/6690 [00:00<00:00, 5626.20 examples/s]Extracting prompt in train dataset:  59%|█████▉    | 3970/6690 [00:00<00:00, 5638.43 examples/s]Extracting prompt in train dataset:  72%|███████▏  | 4790/6690 [00:00<00:00, 5559.20 examples/s]Extracting prompt in train dataset:  80%|████████  | 5360/6690 [00:00<00:00, 5593.33 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 5940/6690 [00:01<00:00, 5621.13 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 6510/6690 [00:01<00:00, 5636.73 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5532.73 examples/s]
Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 296/6690 [00:00<00:02, 2929.31 examples/s]Applying chat template to train dataset:   9%|▉         | 619/6690 [00:00<00:01, 3099.51 examples/s]Applying chat template to train dataset:  14%|█▍        | 941/6690 [00:00<00:01, 3149.74 examples/s]Applying chat template to train dataset:  19%|█▉        | 1263/6690 [00:00<00:01, 3172.62 examples/s]Applying chat template to train dataset:  24%|██▎       | 1586/6690 [00:00<00:01, 3191.54 examples/s]Applying chat template to train dataset:  29%|██▊       | 1909/6690 [00:00<00:01, 3203.93 examples/s]Applying chat template to train dataset:  33%|███▎      | 2232/6690 [00:00<00:01, 3207.16 examples/s]Applying chat template to train dataset:  41%|████      | 2711/6690 [00:00<00:01, 3195.08 examples/s]Applying chat template to train dataset:  45%|████▌     | 3035/6690 [00:00<00:01, 3204.76 examples/s]Applying chat template to train dataset:  50%|█████     | 3359/6690 [00:01<00:01, 3211.63 examples/s]Applying chat template to train dataset:  55%|█████▌    | 3681/6690 [00:01<00:00, 3208.60 examples/s]Applying chat template to train dataset:  60%|█████▉    | 4005/6690 [00:01<00:00, 3215.50 examples/s]Applying chat template to train dataset:  65%|██████▍   | 4328/6690 [00:01<00:00, 3218.85 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4791/6690 [00:01<00:00, 3163.37 examples/s]Applying chat template to train dataset:  76%|███████▋  | 5114/6690 [00:01<00:00, 3178.73 examples/s]Applying chat template to train dataset:  81%|████████▏ | 5438/6690 [00:01<00:00, 3192.02 examples/s]Applying chat template to train dataset:  86%|████████▌ | 5760/6690 [00:01<00:00, 3195.72 examples/s]Applying chat template to train dataset:  91%|█████████ | 6083/6690 [00:01<00:00, 3201.98 examples/s]Applying chat template to train dataset:  96%|█████████▌| 6405/6690 [00:02<00:00, 3205.19 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3184.26 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 40/6690 [00:00<00:17, 384.02 examples/s]Tokenizing train dataset:   1%|▏         | 89/6690 [00:00<00:15, 439.54 examples/s]Tokenizing train dataset:   2%|▏         | 157/6690 [00:00<00:14, 442.46 examples/s]Tokenizing train dataset:   3%|▎         | 204/6690 [00:00<00:14, 450.60 examples/s]Tokenizing train dataset:   4%|▍         | 254/6690 [00:00<00:13, 461.82 examples/s]Tokenizing train dataset:   5%|▍         | 319/6690 [00:00<00:14, 445.72 examples/s]Tokenizing train dataset:   6%|▌         | 370/6690 [00:00<00:13, 460.76 examples/s]Tokenizing train dataset:   7%|▋         | 442/6690 [00:00<00:13, 467.04 examples/s]Tokenizing train dataset:   7%|▋         | 491/6690 [00:01<00:13, 471.93 examples/s]Tokenizing train dataset:   8%|▊         | 540/6690 [00:01<00:13, 470.85 examples/s]Tokenizing train dataset:   9%|▉         | 589/6690 [00:01<00:12, 472.86 examples/s]Tokenizing train dataset:  10%|▉         | 641/6690 [00:01<00:12, 479.48 examples/s]Tokenizing train dataset:  10%|█         | 690/6690 [00:01<00:12, 476.84 examples/s]Tokenizing train dataset:  11%|█         | 743/6690 [00:01<00:12, 489.01 examples/s]Tokenizing train dataset:  12%|█▏        | 793/6690 [00:01<00:12, 487.69 examples/s]Tokenizing train dataset:  13%|█▎        | 866/6690 [00:01<00:12, 477.83 examples/s]Tokenizing train dataset:  14%|█▍        | 926/6690 [00:02<00:12, 449.48 examples/s]Tokenizing train dataset:  15%|█▍        | 973/6690 [00:02<00:12, 454.11 examples/s]Tokenizing train dataset:  16%|█▌        | 1040/6690 [00:02<00:12, 448.55 examples/s]Tokenizing train dataset:  16%|█▋        | 1088/6690 [00:02<00:12, 454.14 examples/s]Tokenizing train dataset:  17%|█▋        | 1155/6690 [00:02<00:12, 446.48 examples/s]Tokenizing train dataset:  18%|█▊        | 1203/6690 [00:02<00:12, 448.73 examples/s]Tokenizing train dataset:  19%|█▉        | 1268/6690 [00:02<00:12, 439.86 examples/s]Tokenizing train dataset:  20%|█▉        | 1317/6690 [00:02<00:11, 450.27 examples/s]Tokenizing train dataset:  20%|██        | 1363/6690 [00:02<00:11, 449.25 examples/s]Tokenizing train dataset:  21%|██▏       | 1422/6690 [00:03<00:12, 427.93 examples/s]Tokenizing train dataset:  22%|██▏       | 1485/6690 [00:03<00:12, 424.18 examples/s]Tokenizing train dataset:  23%|██▎       | 1540/6690 [00:03<00:11, 450.24 examples/s]Tokenizing train dataset:  24%|██▍       | 1604/6690 [00:03<00:11, 437.16 examples/s]Tokenizing train dataset:  25%|██▍       | 1649/6690 [00:03<00:11, 435.86 examples/s]Tokenizing train dataset:  25%|██▌       | 1695/6690 [00:03<00:11, 437.45 examples/s]Tokenizing train dataset:  26%|██▌       | 1742/6690 [00:03<00:11, 442.12 examples/s]Tokenizing train dataset:  27%|██▋       | 1807/6690 [00:04<00:11, 434.57 examples/s]Tokenizing train dataset:  28%|██▊       | 1876/6690 [00:04<00:10, 439.94 examples/s]Tokenizing train dataset:  29%|██▉       | 1946/6690 [00:04<00:10, 438.41 examples/s]Tokenizing train dataset:  30%|██▉       | 2003/6690 [00:04<00:11, 418.60 examples/s]Tokenizing train dataset:  31%|███       | 2047/6690 [00:04<00:11, 421.81 examples/s]Tokenizing train dataset:  31%|███▏      | 2091/6690 [00:04<00:10, 424.11 examples/s]Tokenizing train dataset:  32%|███▏      | 2139/6690 [00:04<00:10, 437.68 examples/s]Tokenizing train dataset:  33%|███▎      | 2190/6690 [00:04<00:10, 449.34 examples/s]Tokenizing train dataset:  33%|███▎      | 2241/6690 [00:04<00:09, 463.50 examples/s]Tokenizing train dataset:  34%|███▍      | 2304/6690 [00:05<00:09, 441.88 examples/s]Tokenizing train dataset:  35%|███▌      | 2350/6690 [00:05<00:09, 444.10 examples/s]Tokenizing train dataset:  36%|███▌      | 2397/6690 [00:05<00:09, 448.08 examples/s]Tokenizing train dataset:  37%|███▋      | 2468/6690 [00:05<00:09, 452.15 examples/s]Tokenizing train dataset:  38%|███▊      | 2540/6690 [00:05<00:09, 458.34 examples/s]Tokenizing train dataset:  39%|███▊      | 2591/6690 [00:05<00:08, 467.28 examples/s]Tokenizing train dataset:  40%|███▉      | 2654/6690 [00:05<00:09, 446.97 examples/s]Tokenizing train dataset:  40%|████      | 2700/6690 [00:06<00:08, 444.66 examples/s]Tokenizing train dataset:  41%|████▏     | 2761/6690 [00:06<00:09, 425.75 examples/s]Tokenizing train dataset:  42%|████▏     | 2824/6690 [00:06<00:09, 419.30 examples/s]Tokenizing train dataset:  43%|████▎     | 2871/6690 [00:06<00:08, 427.27 examples/s]Tokenizing train dataset:  44%|████▎     | 2921/6690 [00:06<00:08, 441.43 examples/s]Tokenizing train dataset:  44%|████▍     | 2970/6690 [00:06<00:08, 451.88 examples/s]Tokenizing train dataset:  45%|████▌     | 3020/6690 [00:06<00:08, 458.56 examples/s]Tokenizing train dataset:  46%|████▌     | 3072/6690 [00:06<00:07, 474.65 examples/s]Tokenizing train dataset:  47%|████▋     | 3120/6690 [00:06<00:07, 468.96 examples/s]Tokenizing train dataset:  48%|████▊     | 3184/6690 [00:07<00:07, 449.62 examples/s]Tokenizing train dataset:  48%|████▊     | 3234/6690 [00:07<00:07, 461.16 examples/s]Tokenizing train dataset:  49%|████▉     | 3300/6690 [00:07<00:07, 441.92 examples/s]Tokenizing train dataset:  50%|█████     | 3363/6690 [00:07<00:07, 426.77 examples/s]Tokenizing train dataset:  51%|█████     | 3422/6690 [00:07<00:07, 413.19 examples/s]Tokenizing train dataset:  52%|█████▏    | 3490/6690 [00:07<00:07, 418.95 examples/s]Tokenizing train dataset:  53%|█████▎    | 3549/6690 [00:07<00:07, 405.54 examples/s]Tokenizing train dataset:  54%|█████▍    | 3608/6690 [00:08<00:07, 396.46 examples/s]Tokenizing train dataset:  55%|█████▍    | 3653/6690 [00:08<00:07, 405.16 examples/s]Tokenizing train dataset:  55%|█████▌    | 3699/6690 [00:08<00:07, 413.66 examples/s]Tokenizing train dataset:  56%|█████▌    | 3754/6690 [00:08<00:07, 391.60 examples/s]Tokenizing train dataset:  57%|█████▋    | 3799/6690 [00:08<00:07, 402.44 examples/s]Tokenizing train dataset:  57%|█████▋    | 3845/6690 [00:08<00:06, 414.14 examples/s]Tokenizing train dataset:  58%|█████▊    | 3893/6690 [00:08<00:06, 428.51 examples/s]Tokenizing train dataset:  59%|█████▉    | 3942/6690 [00:08<00:06, 440.45 examples/s]Tokenizing train dataset:  60%|█████▉    | 3991/6690 [00:09<00:06, 448.95 examples/s]Tokenizing train dataset:  61%|██████    | 4058/6690 [00:09<00:05, 444.67 examples/s]Tokenizing train dataset:  61%|██████▏   | 4103/6690 [00:09<00:05, 439.60 examples/s]Tokenizing train dataset:  62%|██████▏   | 4149/6690 [00:09<00:05, 443.76 examples/s]Tokenizing train dataset:  63%|██████▎   | 4199/6690 [00:09<00:05, 454.84 examples/s]Tokenizing train dataset:  64%|██████▍   | 4265/6690 [00:09<00:05, 438.27 examples/s]Tokenizing train dataset:  64%|██████▍   | 4312/6690 [00:09<00:05, 444.14 examples/s]Tokenizing train dataset:  65%|██████▌   | 4373/6690 [00:09<00:05, 424.90 examples/s]Tokenizing train dataset:  66%|██████▋   | 4441/6690 [00:10<00:05, 431.35 examples/s]Tokenizing train dataset:  67%|██████▋   | 4485/6690 [00:10<00:05, 428.43 examples/s]Tokenizing train dataset:  68%|██████▊   | 4529/6690 [00:10<00:05, 430.72 examples/s]Tokenizing train dataset:  68%|██████▊   | 4573/6690 [00:10<00:04, 432.90 examples/s]Tokenizing train dataset:  69%|██████▉   | 4623/6690 [00:10<00:04, 446.80 examples/s]Tokenizing train dataset:  70%|██████▉   | 4670/6690 [00:10<00:04, 450.43 examples/s]Tokenizing train dataset:  70%|███████   | 4716/6690 [00:10<00:04, 448.44 examples/s]Tokenizing train dataset:  71%|███████▏  | 4778/6690 [00:10<00:04, 432.00 examples/s]Tokenizing train dataset:  72%|███████▏  | 4848/6690 [00:10<00:04, 438.55 examples/s]Tokenizing train dataset:  73%|███████▎  | 4910/6690 [00:11<00:04, 422.41 examples/s]Tokenizing train dataset:  74%|███████▍  | 4956/6690 [00:11<00:04, 428.11 examples/s]Tokenizing train dataset:  75%|███████▍  | 5000/6690 [00:11<00:03, 425.93 examples/s]Tokenizing train dataset:  76%|███████▌  | 5064/6690 [00:11<00:03, 421.47 examples/s]Tokenizing train dataset:  77%|███████▋  | 5125/6690 [00:11<00:03, 412.63 examples/s]Tokenizing train dataset:  77%|███████▋  | 5173/6690 [00:11<00:03, 427.44 examples/s]Tokenizing train dataset:  78%|███████▊  | 5219/6690 [00:11<00:03, 431.25 examples/s]Tokenizing train dataset:  79%|███████▊  | 5263/6690 [00:11<00:03, 431.02 examples/s]Tokenizing train dataset:  80%|███████▉  | 5326/6690 [00:12<00:03, 420.41 examples/s]Tokenizing train dataset:  80%|████████  | 5378/6690 [00:12<00:02, 444.32 examples/s]Tokenizing train dataset:  81%|████████▏ | 5440/6690 [00:12<00:02, 430.57 examples/s]Tokenizing train dataset:  82%|████████▏ | 5489/6690 [00:12<00:02, 444.51 examples/s]Tokenizing train dataset:  83%|████████▎ | 5551/6690 [00:12<00:02, 429.53 examples/s]Tokenizing train dataset:  84%|████████▍ | 5616/6690 [00:12<00:02, 421.12 examples/s]Tokenizing train dataset:  85%|████████▍ | 5659/6690 [00:12<00:02, 422.85 examples/s]Tokenizing train dataset:  85%|████████▌ | 5705/6690 [00:12<00:02, 428.28 examples/s]Tokenizing train dataset:  86%|████████▌ | 5754/6690 [00:13<00:02, 443.63 examples/s]Tokenizing train dataset:  87%|████████▋ | 5819/6690 [00:13<00:02, 433.56 examples/s]Tokenizing train dataset:  88%|████████▊ | 5878/6690 [00:13<00:01, 417.73 examples/s]Tokenizing train dataset:  89%|████████▊ | 5928/6690 [00:13<00:01, 434.30 examples/s]Tokenizing train dataset:  90%|████████▉ | 5993/6690 [00:13<00:01, 428.99 examples/s]Tokenizing train dataset:  90%|█████████ | 6037/6690 [00:13<00:01, 430.82 examples/s]Tokenizing train dataset:  91%|█████████ | 6086/6690 [00:13<00:01, 443.34 examples/s]Tokenizing train dataset:  92%|█████████▏| 6139/6690 [00:13<00:01, 461.65 examples/s]Tokenizing train dataset:  93%|█████████▎| 6201/6690 [00:14<00:01, 439.15 examples/s]Tokenizing train dataset:  94%|█████████▎| 6260/6690 [00:14<00:01, 373.95 examples/s]Tokenizing train dataset:  94%|█████████▍| 6307/6690 [00:14<00:00, 393.49 examples/s]Tokenizing train dataset:  95%|█████████▌| 6369/6690 [00:14<00:00, 392.27 examples/s]Tokenizing train dataset:  96%|█████████▌| 6412/6690 [00:14<00:00, 399.12 examples/s]Tokenizing train dataset:  97%|█████████▋| 6457/6690 [00:14<00:00, 411.10 examples/s]Tokenizing train dataset:  97%|█████████▋| 6500/6690 [00:14<00:00, 412.25 examples/s]Tokenizing train dataset:  98%|█████████▊| 6546/6690 [00:15<00:00, 420.35 examples/s]Tokenizing train dataset:  98%|█████████▊| 6589/6690 [00:15<00:00, 421.39 examples/s]Tokenizing train dataset:  99%|█████████▉| 6647/6690 [00:15<00:00, 406.11 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 408.65 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 435.26 examples/s]
[rank8]:[W612 21:09:55.653606661 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 571/953 [00:00<00:00, 5655.94 examples/s]Extracting prompt in train dataset:   8%|▊         | 565/6690 [00:00<00:01, 5570.02 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5595.86 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5558.81 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5637.72 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1145/6690 [00:00<00:00, 5668.15 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1140/6690 [00:00<00:00, 5624.69 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1150/6690 [00:00<00:00, 5674.60 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1720/6690 [00:00<00:00, 5700.68 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1733/6690 [00:00<00:00, 5729.75 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1720/6690 [00:00<00:00, 5679.03 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2574/6690 [00:00<00:00, 5694.31 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2574/6690 [00:00<00:00, 5679.46 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2594/6690 [00:00<00:00, 5721.12 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  47%|████▋     | 3150/6690 [00:00<00:00, 5711.63 examples/s]Extracting prompt in train dataset:  47%|████▋     | 3170/6690 [00:00<00:00, 5729.37 examples/s]Extracting prompt in train dataset:  47%|████▋     | 3150/6690 [00:00<00:00, 5687.70 examples/s]Applying chat template to eval dataset:  33%|███▎      | 316/953 [00:00<00:00, 3128.09 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3730/6690 [00:00<00:00, 5722.43 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3750/6690 [00:00<00:00, 5735.86 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3730/6690 [00:00<00:00, 5697.82 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 643/953 [00:00<00:00, 3207.81 examples/s]Extracting prompt in train dataset:  64%|██████▍   | 4310/6690 [00:00<00:00, 5731.96 examples/s]Extracting prompt in train dataset:  65%|██████▍   | 4330/6690 [00:00<00:00, 5750.57 examples/s]Extracting prompt in train dataset:  64%|██████▍   | 4310/6690 [00:00<00:00, 5711.93 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3198.50 examples/s]
Extracting prompt in train dataset:  77%|███████▋  | 5140/6690 [00:00<00:00, 5646.19 examples/s]Extracting prompt in train dataset:  77%|███████▋  | 5160/6690 [00:00<00:00, 5661.99 examples/s]Extracting prompt in train dataset:  77%|███████▋  | 5140/6690 [00:00<00:00, 5623.99 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 5740/6690 [00:01<00:00, 5689.25 examples/s]Extracting prompt in train dataset:  85%|████████▌ | 5710/6690 [00:01<00:00, 5632.88 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 5720/6690 [00:01<00:00, 5653.28 examples/s]Extracting prompt in train dataset:  94%|█████████▍| 6320/6690 [00:01<00:00, 5711.81 examples/s]Extracting prompt in train dataset:  94%|█████████▍| 6300/6690 [00:01<00:00, 5676.46 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 6550/6690 [00:01<00:00, 5584.22 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5673.64 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5634.28 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5605.43 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 321.39 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 290.28 examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 277.34 examples/s]Applying chat template to train dataset:   4%|▍         | 292/6690 [00:00<00:02, 2892.37 examples/s]Applying chat template to train dataset:   4%|▍         | 296/6690 [00:00<00:02, 2931.76 examples/s]Applying chat template to train dataset:   4%|▍         | 290/6690 [00:00<00:02, 2872.95 examples/s]Applying chat template to train dataset:   9%|▉         | 612/6690 [00:00<00:01, 3065.71 examples/s]Applying chat template to train dataset:   9%|▉         | 620/6690 [00:00<00:01, 3097.76 examples/s]Applying chat template to train dataset:   9%|▉         | 610/6690 [00:00<00:01, 3048.81 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.97 examples/s]Applying chat template to train dataset:  14%|█▍        | 934/6690 [00:00<00:01, 3133.83 examples/s]Applying chat template to train dataset:  14%|█▍        | 945/6690 [00:00<00:01, 3162.41 examples/s]Applying chat template to train dataset:  14%|█▍        | 930/6690 [00:00<00:01, 3111.03 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.83 examples/s]Applying chat template to train dataset:  19%|█▊        | 1253/6690 [00:00<00:01, 3151.91 examples/s]Applying chat template to train dataset:  19%|█▉        | 1267/6690 [00:00<00:01, 3181.75 examples/s]Applying chat template to train dataset:  19%|█▊        | 1249/6690 [00:00<00:01, 3136.68 examples/s]Tokenizing eval dataset:  24%|██▍       | 230/953 [00:00<00:02, 276.56 examples/s]Applying chat template to train dataset:  24%|██▍       | 1590/6690 [00:00<00:01, 3193.91 examples/s]Applying chat template to train dataset:  23%|██▎       | 1568/6690 [00:00<00:01, 3152.35 examples/s]Applying chat template to train dataset:  26%|██▌       | 1714/6690 [00:00<00:01, 3112.28 examples/s]Tokenizing eval dataset:  31%|███       | 295/953 [00:00<00:01, 372.97 examples/s]Applying chat template to train dataset:  29%|██▊       | 1914/6690 [00:00<00:01, 3208.75 examples/s]Applying chat template to train dataset:  28%|██▊       | 1888/6690 [00:00<00:01, 3165.92 examples/s]Applying chat template to train dataset:  32%|███▏      | 2173/6690 [00:00<00:01, 3087.94 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 441.18 examples/s]Applying chat template to train dataset:  33%|███▎      | 2239/6690 [00:00<00:01, 3218.82 examples/s]Applying chat template to train dataset:  33%|███▎      | 2207/6690 [00:00<00:01, 3171.62 examples/s]Applying chat template to train dataset:  37%|███▋      | 2483/6690 [00:00<00:01, 3089.85 examples/s]Tokenizing eval dataset:  44%|████▍     | 424/953 [00:01<00:01, 495.48 examples/s]Applying chat template to train dataset:  41%|████      | 2720/6690 [00:00<00:01, 3208.81 examples/s]Applying chat template to train dataset:  40%|████      | 2681/6690 [00:00<00:01, 3163.53 examples/s]Applying chat template to train dataset:  42%|████▏     | 2804/6690 [00:00<00:01, 2543.84 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:01, 446.40 examples/s]Applying chat template to train dataset:  47%|████▋     | 3127/6690 [00:01<00:01, 3023.34 examples/s]Applying chat template to train dataset:  46%|████▌     | 3094/6690 [00:01<00:01, 3007.29 examples/s]Applying chat template to train dataset:  47%|████▋     | 3125/6690 [00:01<00:01, 2706.82 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 499.06 examples/s]Applying chat template to train dataset:  52%|█████▏    | 3450/6690 [00:01<00:01, 3072.36 examples/s]Applying chat template to train dataset:  51%|█████     | 3412/6690 [00:01<00:01, 3051.22 examples/s]Applying chat template to train dataset:  51%|█████▏    | 3445/6690 [00:01<00:01, 2831.67 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 537.89 examples/s]Applying chat template to train dataset:  56%|█████▋    | 3773/6690 [00:01<00:00, 3113.03 examples/s]Applying chat template to train dataset:  56%|█████▌    | 3731/6690 [00:01<00:00, 3084.78 examples/s]Applying chat template to train dataset:  56%|█████▋    | 3764/6690 [00:01<00:00, 2926.53 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 556.23 examples/s]Applying chat template to train dataset:  61%|██████    | 4097/6690 [00:01<00:00, 3146.56 examples/s]Applying chat template to train dataset:  61%|██████    | 4051/6690 [00:01<00:00, 3113.85 examples/s]Applying chat template to train dataset:  61%|██████    | 4082/6690 [00:01<00:00, 2995.05 examples/s]Applying chat template to train dataset:  65%|██████▌   | 4371/6690 [00:01<00:00, 3134.49 examples/s]Tokenizing eval dataset:  81%|████████  | 774/953 [00:01<00:00, 551.42 examples/s]Applying chat template to train dataset:  68%|██████▊   | 4562/6690 [00:01<00:00, 3124.56 examples/s]Applying chat template to train dataset:  68%|██████▊   | 4529/6690 [00:01<00:00, 2987.78 examples/s]Applying chat template to train dataset:  73%|███████▎  | 4885/6690 [00:01<00:00, 3149.96 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4830/6690 [00:01<00:00, 3103.05 examples/s]Tokenizing eval dataset:  89%|████████▉ | 848/953 [00:01<00:00, 526.11 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4847/6690 [00:01<00:00, 3035.57 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5208/6690 [00:01<00:00, 3168.66 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5150/6690 [00:01<00:00, 3122.08 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5165/6690 [00:01<00:00, 3074.02 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5530/6690 [00:01<00:00, 3180.95 examples/s]Tokenizing eval dataset:  96%|█████████▋| 918/953 [00:02<00:00, 502.20 examples/s]Applying chat template to train dataset:  82%|████████▏ | 5470/6690 [00:01<00:00, 3137.67 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 439.56 examples/s]
Applying chat template to train dataset:  82%|████████▏ | 5484/6690 [00:01<00:00, 3103.70 examples/s]Applying chat template to train dataset:  87%|████████▋ | 5852/6690 [00:01<00:00, 3191.40 examples/s]Applying chat template to train dataset:  87%|████████▋ | 5790/6690 [00:01<00:00, 3149.24 examples/s]Applying chat template to train dataset:  87%|████████▋ | 5807/6690 [00:01<00:00, 3136.90 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6176/6690 [00:01<00:00, 3202.86 examples/s]Applying chat template to train dataset:  91%|█████████▏| 6110/6690 [00:01<00:00, 3158.69 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6130/6690 [00:02<00:00, 3158.39 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6499/6690 [00:02<00:00, 3206.57 examples/s]Applying chat template to train dataset:  96%|█████████▌| 6430/6690 [00:02<00:00, 3164.12 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3153.20 examples/s]
Applying chat template to train dataset:  96%|█████████▋| 6453/6690 [00:02<00:00, 3175.26 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3117.20 examples/s]
Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3019.28 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:15, 420.86 examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:15, 422.92 examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:16, 414.30 examples/s]Tokenizing train dataset:   1%|▏         | 91/6690 [00:00<00:14, 441.90 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:14, 442.50 examples/s]Tokenizing train dataset:   1%|▏         | 91/6690 [00:00<00:14, 440.53 examples/s]Tokenizing train dataset:   2%|▏         | 137/6690 [00:00<00:14, 448.08 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 443.11 examples/s]Tokenizing train dataset:   2%|▏         | 137/6690 [00:00<00:14, 448.26 examples/s]Tokenizing train dataset:   3%|▎         | 190/6690 [00:00<00:13, 474.70 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 474.70 examples/s]Tokenizing train dataset:   3%|▎         | 190/6690 [00:00<00:13, 475.28 examples/s]Tokenizing train dataset:   4%|▎         | 239/6690 [00:00<00:13, 475.38 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 474.00 examples/s]Tokenizing train dataset:   4%|▎         | 239/6690 [00:00<00:13, 477.04 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 471.87 examples/s]Tokenizing train dataset:   5%|▍         | 307/6690 [00:00<00:13, 461.46 examples/s]Tokenizing train dataset:   5%|▍         | 307/6690 [00:00<00:13, 463.91 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 463.70 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 461.30 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 466.35 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 477.42 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 475.30 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 480.22 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 485.17 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 483.20 examples/s]Tokenizing train dataset:   7%|▋         | 460/6690 [00:00<00:12, 485.36 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 486.97 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 484.94 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 489.14 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 486.88 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 485.65 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 488.86 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 486.76 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 486.84 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 488.83 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 486.85 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 487.40 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 488.37 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 493.77 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 494.27 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 495.51 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 505.03 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 504.95 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 506.96 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 509.87 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 509.69 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 512.15 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:12, 481.97 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:12, 482.41 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:11, 484.67 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 462.52 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 462.91 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:01<00:12, 465.08 examples/s]Tokenizing train dataset:  15%|█▌        | 1025/6690 [00:02<00:12, 459.20 examples/s]Tokenizing train dataset:  15%|█▌        | 1025/6690 [00:02<00:12, 459.23 examples/s]Tokenizing train dataset:  15%|█▌        | 1026/6690 [00:02<00:12, 462.59 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 469.14 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 468.97 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 471.06 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:12, 460.62 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:12, 460.49 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:11, 462.45 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 459.99 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 459.62 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 461.95 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 452.61 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 452.61 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 454.92 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 452.86 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 452.66 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 455.27 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 467.34 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 467.03 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 469.17 examples/s]Tokenizing train dataset:  21%|██        | 1420/6690 [00:03<00:12, 437.00 examples/s]Tokenizing train dataset:  21%|██        | 1420/6690 [00:03<00:12, 437.09 examples/s]Tokenizing train dataset:  21%|██        | 1420/6690 [00:03<00:12, 438.37 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 437.97 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 438.15 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 439.78 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 452.36 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 452.93 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 454.44 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 452.57 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 453.19 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 454.48 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 447.18 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 447.20 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 448.44 examples/s]Tokenizing train dataset:  25%|██▌       | 1690/6690 [00:03<00:11, 426.21 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 448.07 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 449.63 examples/s]Tokenizing train dataset:  26%|██▌       | 1740/6690 [00:03<00:11, 440.18 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 460.85 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 462.27 examples/s]Tokenizing train dataset:  27%|██▋       | 1786/6690 [00:03<00:11, 440.29 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 450.15 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 451.77 examples/s]Tokenizing train dataset:  27%|██▋       | 1836/6690 [00:03<00:10, 450.61 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:04<00:10, 451.04 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:04<00:10, 453.25 examples/s]Tokenizing train dataset:  28%|██▊       | 1883/6690 [00:04<00:10, 451.01 examples/s]Tokenizing train dataset:  29%|██▊       | 1912/6690 [00:04<00:10, 451.27 examples/s]Tokenizing train dataset:  29%|██▉       | 1932/6690 [00:04<00:10, 445.83 examples/s]Tokenizing train dataset:  29%|██▉       | 1952/6690 [00:04<00:10, 447.85 examples/s]Tokenizing train dataset:  29%|██▉       | 1960/6690 [00:04<00:10, 456.40 examples/s]Tokenizing train dataset:  30%|██▉       | 1977/6690 [00:04<00:10, 446.24 examples/s]Tokenizing train dataset:  30%|███       | 2011/6690 [00:04<00:11, 423.25 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:10, 432.91 examples/s]Tokenizing train dataset:  31%|███       | 2041/6690 [00:04<00:10, 432.92 examples/s]Tokenizing train dataset:  31%|███       | 2057/6690 [00:04<00:10, 428.98 examples/s]Tokenizing train dataset:  31%|███       | 2067/6690 [00:04<00:10, 433.63 examples/s]Tokenizing train dataset:  31%|███       | 2086/6690 [00:04<00:10, 434.19 examples/s]Tokenizing train dataset:  31%|███▏      | 2105/6690 [00:04<00:10, 439.27 examples/s]Tokenizing train dataset:  32%|███▏      | 2119/6690 [00:04<00:10, 450.21 examples/s]Tokenizing train dataset:  32%|███▏      | 2135/6690 [00:04<00:10, 445.74 examples/s]Tokenizing train dataset:  32%|███▏      | 2155/6690 [00:04<00:10, 452.25 examples/s]Tokenizing train dataset:  32%|███▏      | 2166/6690 [00:04<00:09, 453.24 examples/s]Tokenizing train dataset:  33%|███▎      | 2185/6690 [00:04<00:09, 457.22 examples/s]Tokenizing train dataset:  33%|███▎      | 2207/6690 [00:04<00:09, 469.12 examples/s]Tokenizing train dataset:  33%|███▎      | 2218/6690 [00:04<00:09, 469.48 examples/s]Tokenizing train dataset:  33%|███▎      | 2236/6690 [00:04<00:09, 469.44 examples/s]Tokenizing train dataset:  34%|███▍      | 2266/6690 [00:04<00:09, 467.73 examples/s]Tokenizing train dataset:  34%|███▍      | 2279/6690 [00:04<00:09, 467.82 examples/s]Tokenizing train dataset:  34%|███▍      | 2300/6690 [00:04<00:09, 450.79 examples/s]Tokenizing train dataset:  35%|███▌      | 2345/6690 [00:05<00:09, 456.65 examples/s]Tokenizing train dataset:  35%|███▍      | 2333/6690 [00:05<00:09, 451.26 examples/s]Tokenizing train dataset:  35%|███▌      | 2347/6690 [00:05<00:09, 454.72 examples/s]Tokenizing train dataset:  36%|███▌      | 2383/6690 [00:05<00:09, 461.67 examples/s]Tokenizing train dataset:  36%|███▌      | 2395/6690 [00:05<00:09, 457.84 examples/s]Tokenizing train dataset:  36%|███▌      | 2395/6690 [00:05<00:09, 459.77 examples/s]Tokenizing train dataset:  36%|███▋      | 2430/6690 [00:05<00:09, 457.21 examples/s]Tokenizing train dataset:  37%|███▋      | 2469/6690 [00:05<00:09, 457.46 examples/s]Tokenizing train dataset:  37%|███▋      | 2469/6690 [00:05<00:09, 458.36 examples/s]Tokenizing train dataset:  37%|███▋      | 2478/6690 [00:05<00:09, 460.36 examples/s]Tokenizing train dataset:  38%|███▊      | 2515/6690 [00:05<00:09, 456.00 examples/s]Tokenizing train dataset:  38%|███▊      | 2526/6690 [00:05<00:08, 463.13 examples/s]Tokenizing train dataset:  38%|███▊      | 2544/6690 [00:05<00:08, 469.53 examples/s]Tokenizing train dataset:  38%|███▊      | 2568/6690 [00:05<00:08, 474.44 examples/s]Tokenizing train dataset:  39%|███▊      | 2581/6690 [00:05<00:08, 486.10 examples/s]Tokenizing train dataset:  39%|███▉      | 2595/6690 [00:05<00:08, 476.88 examples/s]Tokenizing train dataset:  39%|███▉      | 2618/6690 [00:05<00:08, 477.60 examples/s]Tokenizing train dataset:  40%|███▉      | 2649/6690 [00:05<00:08, 467.67 examples/s]Tokenizing train dataset:  40%|███▉      | 2658/6690 [00:05<00:08, 455.21 examples/s]Tokenizing train dataset:  40%|████      | 2680/6690 [00:05<00:08, 449.48 examples/s]Tokenizing train dataset:  41%|████      | 2713/6690 [00:05<00:08, 451.32 examples/s]Tokenizing train dataset:  41%|████      | 2724/6690 [00:05<00:08, 445.03 examples/s]Tokenizing train dataset:  41%|████      | 2744/6690 [00:05<00:09, 436.32 examples/s]Tokenizing train dataset:  41%|████▏     | 2774/6690 [00:06<00:09, 429.89 examples/s]Tokenizing train dataset:  42%|████▏     | 2787/6690 [00:06<00:09, 428.30 examples/s]Tokenizing train dataset:  42%|████▏     | 2809/6690 [00:06<00:09, 427.20 examples/s]Tokenizing train dataset:  42%|████▏     | 2833/6690 [00:06<00:08, 430.94 examples/s]Tokenizing train dataset:  42%|████▏     | 2841/6690 [00:06<00:08, 428.13 examples/s]Tokenizing train dataset:  43%|████▎     | 2856/6690 [00:06<00:08, 434.99 examples/s]Tokenizing train dataset:  43%|████▎     | 2878/6690 [00:06<00:08, 434.17 examples/s]Tokenizing train dataset:  43%|████▎     | 2889/6690 [00:06<00:08, 437.88 examples/s]Tokenizing train dataset:  43%|████▎     | 2903/6690 [00:06<00:08, 440.85 examples/s]Tokenizing train dataset:  44%|████▍     | 2930/6690 [00:06<00:08, 448.52 examples/s]Tokenizing train dataset:  44%|████▍     | 2941/6690 [00:06<00:08, 456.59 examples/s]Tokenizing train dataset:  44%|████▍     | 2957/6690 [00:06<00:08, 459.35 examples/s]Tokenizing train dataset:  45%|████▍     | 2981/6690 [00:06<00:08, 461.12 examples/s]Tokenizing train dataset:  45%|████▍     | 2992/6690 [00:06<00:07, 467.19 examples/s]Tokenizing train dataset:  45%|████▍     | 3010/6690 [00:06<00:07, 472.31 examples/s]Tokenizing train dataset:  45%|████▌     | 3033/6690 [00:06<00:07, 469.55 examples/s]Tokenizing train dataset:  45%|████▌     | 3040/6690 [00:06<00:07, 468.86 examples/s]Tokenizing train dataset:  46%|████▌     | 3061/6690 [00:06<00:07, 478.67 examples/s]Tokenizing train dataset:  46%|████▌     | 3090/6690 [00:06<00:07, 492.59 examples/s]Tokenizing train dataset:  46%|████▋     | 3099/6690 [00:06<00:07, 501.33 examples/s]Tokenizing train dataset:  47%|████▋     | 3113/6690 [00:06<00:07, 486.29 examples/s]Tokenizing train dataset:  47%|████▋     | 3155/6690 [00:06<00:07, 468.63 examples/s]Tokenizing train dataset:  47%|████▋     | 3163/6690 [00:06<00:07, 468.96 examples/s]Tokenizing train dataset:  48%|████▊     | 3179/6690 [00:06<00:07, 464.95 examples/s]Tokenizing train dataset:  48%|████▊     | 3211/6690 [00:06<00:07, 468.87 examples/s]Tokenizing train dataset:  48%|████▊     | 3228/6690 [00:07<00:07, 468.59 examples/s]Tokenizing train dataset:  48%|████▊     | 3228/6690 [00:07<00:07, 468.15 examples/s]Tokenizing train dataset:  49%|████▉     | 3276/6690 [00:07<00:07, 466.83 examples/s]Tokenizing train dataset:  49%|████▉     | 3276/6690 [00:07<00:07, 466.62 examples/s]Tokenizing train dataset:  49%|████▉     | 3280/6690 [00:07<00:07, 464.40 examples/s]Tokenizing train dataset:  50%|████▉     | 3333/6690 [00:07<00:07, 431.11 examples/s]Tokenizing train dataset:  50%|████▉     | 3333/6690 [00:07<00:07, 433.08 examples/s]Tokenizing train dataset:  50%|████▉     | 3340/6690 [00:07<00:07, 434.39 examples/s]Tokenizing train dataset:  50%|█████     | 3378/6690 [00:07<00:07, 433.46 examples/s]Tokenizing train dataset:  50%|█████     | 3378/6690 [00:07<00:07, 434.78 examples/s]Tokenizing train dataset:  51%|█████     | 3408/6690 [00:07<00:07, 433.46 examples/s]Tokenizing train dataset:  51%|█████▏    | 3437/6690 [00:07<00:07, 417.40 examples/s]Tokenizing train dataset:  51%|█████▏    | 3437/6690 [00:07<00:07, 418.59 examples/s]Tokenizing train dataset:  52%|█████▏    | 3465/6690 [00:07<00:07, 413.47 examples/s]Tokenizing train dataset:  52%|█████▏    | 3483/6690 [00:07<00:07, 425.09 examples/s]Tokenizing train dataset:  52%|█████▏    | 3483/6690 [00:07<00:07, 425.23 examples/s]Tokenizing train dataset:  53%|█████▎    | 3514/6690 [00:07<00:07, 425.42 examples/s]Tokenizing train dataset:  53%|█████▎    | 3543/6690 [00:07<00:07, 410.99 examples/s]Tokenizing train dataset:  53%|█████▎    | 3543/6690 [00:07<00:07, 410.98 examples/s]Tokenizing train dataset:  53%|█████▎    | 3572/6690 [00:07<00:07, 408.37 examples/s]Tokenizing train dataset:  54%|█████▍    | 3600/6690 [00:07<00:07, 397.97 examples/s]Tokenizing train dataset:  54%|█████▍    | 3600/6690 [00:07<00:07, 397.99 examples/s]Tokenizing train dataset:  54%|█████▍    | 3640/6690 [00:07<00:07, 418.07 examples/s]Tokenizing train dataset:  54%|█████▍    | 3646/6690 [00:08<00:07, 411.45 examples/s]Tokenizing train dataset:  54%|█████▍    | 3646/6690 [00:08<00:07, 411.31 examples/s]Tokenizing train dataset:  55%|█████▌    | 3685/6690 [00:08<00:07, 421.95 examples/s]Tokenizing train dataset:  55%|█████▌    | 3690/6690 [00:08<00:07, 413.30 examples/s]Tokenizing train dataset:  55%|█████▌    | 3690/6690 [00:08<00:07, 413.12 examples/s]Tokenizing train dataset:  56%|█████▌    | 3743/6690 [00:08<00:07, 403.38 examples/s]Tokenizing train dataset:  56%|█████▌    | 3749/6690 [00:08<00:07, 399.88 examples/s]Tokenizing train dataset:  56%|█████▌    | 3749/6690 [00:08<00:07, 398.51 examples/s]Tokenizing train dataset:  57%|█████▋    | 3790/6690 [00:08<00:06, 416.30 examples/s]Tokenizing train dataset:  57%|█████▋    | 3795/6690 [00:08<00:07, 411.71 examples/s]Tokenizing train dataset:  57%|█████▋    | 3795/6690 [00:08<00:07, 410.66 examples/s]Tokenizing train dataset:  57%|█████▋    | 3835/6690 [00:08<00:06, 422.13 examples/s]Tokenizing train dataset:  57%|█████▋    | 3841/6690 [00:08<00:06, 422.92 examples/s]Tokenizing train dataset:  57%|█████▋    | 3841/6690 [00:08<00:06, 421.88 examples/s]Tokenizing train dataset:  58%|█████▊    | 3883/6690 [00:08<00:06, 433.78 examples/s]Tokenizing train dataset:  58%|█████▊    | 3891/6690 [00:08<00:06, 434.61 examples/s]Tokenizing train dataset:  58%|█████▊    | 3891/6690 [00:08<00:07, 382.65 examples/s]Tokenizing train dataset:  59%|█████▉    | 3931/6690 [00:08<00:06, 443.08 examples/s]Tokenizing train dataset:  59%|█████▉    | 3941/6690 [00:08<00:06, 449.00 examples/s]Tokenizing train dataset:  59%|█████▉    | 3941/6690 [00:08<00:06, 408.63 examples/s]Tokenizing train dataset:  59%|█████▉    | 3980/6690 [00:08<00:05, 453.93 examples/s]Tokenizing train dataset:  60%|█████▉    | 3990/6690 [00:08<00:05, 457.17 examples/s]Tokenizing train dataset:  60%|█████▉    | 3990/6690 [00:08<00:06, 426.51 examples/s]Tokenizing train dataset:  60%|██████    | 4030/6690 [00:08<00:05, 462.63 examples/s]Tokenizing train dataset:  61%|██████    | 4058/6690 [00:08<00:05, 451.84 examples/s]Tokenizing train dataset:  60%|██████    | 4036/6690 [00:08<00:06, 431.90 examples/s]Tokenizing train dataset:  61%|██████    | 4094/6690 [00:09<00:05, 446.43 examples/s]Tokenizing train dataset:  62%|██████▏   | 4126/6690 [00:09<00:05, 445.04 examples/s]Tokenizing train dataset:  61%|██████▏   | 4103/6690 [00:09<00:05, 432.90 examples/s]Tokenizing train dataset:  62%|██████▏   | 4141/6690 [00:09<00:05, 452.31 examples/s]Tokenizing train dataset:  62%|██████▏   | 4176/6690 [00:09<00:05, 457.35 examples/s]Tokenizing train dataset:  62%|██████▏   | 4151/6690 [00:09<00:05, 439.67 examples/s]Tokenizing train dataset:  63%|██████▎   | 4192/6690 [00:09<00:05, 466.56 examples/s]Tokenizing train dataset:  63%|██████▎   | 4200/6690 [00:09<00:05, 449.78 examples/s]Tokenizing train dataset:  63%|██████▎   | 4247/6690 [00:09<00:05, 457.39 examples/s]Tokenizing train dataset:  64%|██████▎   | 4257/6690 [00:09<00:05, 450.62 examples/s]Tokenizing train dataset:  64%|██████▍   | 4265/6690 [00:09<00:05, 441.70 examples/s]Tokenizing train dataset:  64%|██████▍   | 4305/6690 [00:09<00:05, 452.24 examples/s]Tokenizing train dataset:  64%|██████▍   | 4314/6690 [00:09<00:05, 451.45 examples/s]Tokenizing train dataset:  64%|██████▍   | 4312/6690 [00:09<00:05, 448.45 examples/s]Tokenizing train dataset:  65%|██████▌   | 4370/6690 [00:09<00:05, 437.15 examples/s]Tokenizing train dataset:  65%|██████▌   | 4376/6690 [00:09<00:05, 433.93 examples/s]Tokenizing train dataset:  65%|██████▌   | 4374/6690 [00:09<00:05, 429.97 examples/s]Tokenizing train dataset:  66%|██████▌   | 4415/6690 [00:09<00:05, 435.03 examples/s]Tokenizing train dataset:  66%|██████▌   | 4423/6690 [00:09<00:05, 437.79 examples/s]Tokenizing train dataset:  66%|██████▌   | 4420/6690 [00:09<00:05, 431.30 examples/s]Tokenizing train dataset:  67%|██████▋   | 4461/6690 [00:09<00:05, 438.78 examples/s]Tokenizing train dataset:  67%|██████▋   | 4489/6690 [00:09<00:05, 436.62 examples/s]Tokenizing train dataset:  67%|██████▋   | 4464/6690 [00:09<00:05, 432.84 examples/s]Tokenizing train dataset:  68%|██████▊   | 4530/6690 [00:09<00:04, 441.89 examples/s]Tokenizing train dataset:  68%|██████▊   | 4535/6690 [00:10<00:04, 438.53 examples/s]Tokenizing train dataset:  67%|██████▋   | 4508/6690 [00:10<00:05, 430.64 examples/s]Tokenizing train dataset:  68%|██████▊   | 4578/6690 [00:10<00:04, 449.18 examples/s]Tokenizing train dataset:  69%|██████▊   | 4584/6690 [00:10<00:04, 446.34 examples/s]Tokenizing train dataset:  68%|██████▊   | 4557/6690 [00:10<00:04, 443.39 examples/s]Tokenizing train dataset:  69%|██████▉   | 4630/6690 [00:10<00:04, 459.01 examples/s]Tokenizing train dataset:  69%|██████▉   | 4635/6690 [00:10<00:04, 454.93 examples/s]Tokenizing train dataset:  69%|██████▉   | 4603/6690 [00:10<00:04, 444.99 examples/s]Tokenizing train dataset:  70%|██████▉   | 4677/6690 [00:10<00:04, 461.27 examples/s]Tokenizing train dataset:  70%|██████▉   | 4682/6690 [00:10<00:04, 458.12 examples/s]Tokenizing train dataset:  70%|██████▉   | 4653/6690 [00:10<00:04, 459.57 examples/s]Tokenizing train dataset:  71%|███████   | 4730/6690 [00:10<00:04, 463.31 examples/s]Tokenizing train dataset:  71%|███████   | 4748/6690 [00:10<00:04, 462.09 examples/s]Tokenizing train dataset:  71%|███████   | 4722/6690 [00:10<00:04, 457.79 examples/s]Tokenizing train dataset:  72%|███████▏  | 4790/6690 [00:10<00:04, 432.95 examples/s]Tokenizing train dataset:  72%|███████▏  | 4809/6690 [00:10<00:04, 441.06 examples/s]Tokenizing train dataset:  72%|███████▏  | 4784/6690 [00:10<00:04, 439.33 examples/s]Tokenizing train dataset:  72%|███████▏  | 4840/6690 [00:10<00:04, 447.17 examples/s]Tokenizing train dataset:  73%|███████▎  | 4855/6690 [00:10<00:04, 444.11 examples/s]Tokenizing train dataset:  72%|███████▏  | 4830/6690 [00:10<00:04, 439.13 examples/s]Tokenizing train dataset:  73%|███████▎  | 4905/6690 [00:10<00:04, 438.04 examples/s]Tokenizing train dataset:  73%|███████▎  | 4879/6690 [00:10<00:04, 449.49 examples/s]Tokenizing train dataset:  74%|███████▎  | 4919/6690 [00:10<00:04, 433.54 examples/s]Tokenizing train dataset:  74%|███████▍  | 4951/6690 [00:10<00:03, 441.40 examples/s]Tokenizing train dataset:  74%|███████▍  | 4966/6690 [00:10<00:03, 441.14 examples/s]Tokenizing train dataset:  74%|███████▍  | 4940/6690 [00:11<00:04, 430.68 examples/s]Tokenizing train dataset:  75%|███████▍  | 5017/6690 [00:11<00:03, 432.48 examples/s]Tokenizing train dataset:  74%|███████▍  | 4984/6690 [00:11<00:03, 429.51 examples/s]Tokenizing train dataset:  75%|███████▌  | 5029/6690 [00:11<00:03, 430.39 examples/s]Tokenizing train dataset:  75%|███████▌  | 5028/6690 [00:11<00:03, 430.56 examples/s]Tokenizing train dataset:  76%|███████▌  | 5073/6690 [00:11<00:03, 431.57 examples/s]Tokenizing train dataset:  76%|███████▌  | 5082/6690 [00:11<00:03, 424.56 examples/s]Tokenizing train dataset:  76%|███████▌  | 5092/6690 [00:11<00:03, 424.27 examples/s]Tokenizing train dataset:  77%|███████▋  | 5134/6690 [00:11<00:03, 419.53 examples/s]Tokenizing train dataset:  77%|███████▋  | 5147/6690 [00:11<00:03, 423.94 examples/s]Tokenizing train dataset:  78%|███████▊  | 5185/6690 [00:11<00:03, 437.88 examples/s]Tokenizing train dataset:  78%|███████▊  | 5197/6690 [00:11<00:03, 438.93 examples/s]Tokenizing train dataset:  77%|███████▋  | 5160/6690 [00:11<00:03, 427.25 examples/s]Tokenizing train dataset:  78%|███████▊  | 5230/6690 [00:11<00:03, 434.97 examples/s]Tokenizing train dataset:  78%|███████▊  | 5209/6690 [00:11<00:03, 441.60 examples/s]Tokenizing train dataset:  79%|███████▊  | 5264/6690 [00:11<00:03, 436.46 examples/s]Tokenizing train dataset:  79%|███████▉  | 5277/6690 [00:11<00:03, 442.58 examples/s]Tokenizing train dataset:  79%|███████▉  | 5275/6690 [00:11<00:03, 436.37 examples/s]Tokenizing train dataset:  80%|███████▉  | 5328/6690 [00:11<00:03, 430.06 examples/s]Tokenizing train dataset:  80%|███████▉  | 5339/6690 [00:11<00:03, 427.68 examples/s]Tokenizing train dataset:  80%|████████  | 5380/6690 [00:11<00:02, 446.75 examples/s]Tokenizing train dataset:  80%|███████▉  | 5338/6690 [00:11<00:03, 423.86 examples/s]Tokenizing train dataset:  81%|████████  | 5396/6690 [00:11<00:02, 461.09 examples/s]Tokenizing train dataset:  81%|████████  | 5392/6690 [00:12<00:02, 450.14 examples/s]Tokenizing train dataset:  81%|████████▏ | 5445/6690 [00:12<00:02, 440.15 examples/s]Tokenizing train dataset:  82%|████████▏ | 5461/6690 [00:12<00:02, 448.55 examples/s]Tokenizing train dataset:  82%|████████▏ | 5494/6690 [00:12<00:02, 447.45 examples/s]Tokenizing train dataset:  82%|████████▏ | 5460/6690 [00:12<00:02, 443.71 examples/s]Tokenizing train dataset:  82%|████████▏ | 5507/6690 [00:12<00:02, 447.83 examples/s]Tokenizing train dataset:  83%|████████▎ | 5559/6690 [00:12<00:02, 438.15 examples/s]Tokenizing train dataset:  83%|████████▎ | 5526/6690 [00:12<00:02, 440.28 examples/s]Tokenizing train dataset:  83%|████████▎ | 5571/6690 [00:12<00:02, 436.30 examples/s]Tokenizing train dataset:  84%|████████▍ | 5623/6690 [00:12<00:02, 429.41 examples/s]Tokenizing train dataset:  84%|████████▎ | 5590/6690 [00:12<00:02, 429.61 examples/s]Tokenizing train dataset:  84%|████████▍ | 5640/6690 [00:12<00:02, 438.22 examples/s]Tokenizing train dataset:  85%|████████▍ | 5669/6690 [00:12<00:02, 433.25 examples/s]Tokenizing train dataset:  84%|████████▍ | 5639/6690 [00:12<00:02, 438.98 examples/s]Tokenizing train dataset:  85%|████████▌ | 5705/6690 [00:12<00:02, 434.89 examples/s]Tokenizing train dataset:  85%|████████▌ | 5713/6690 [00:12<00:02, 429.01 examples/s]Tokenizing train dataset:  85%|████████▌ | 5703/6690 [00:12<00:02, 430.52 examples/s]Tokenizing train dataset:  86%|████████▌ | 5757/6690 [00:12<00:02, 448.50 examples/s]Tokenizing train dataset:  86%|████████▌ | 5763/6690 [00:12<00:02, 442.52 examples/s]Tokenizing train dataset:  86%|████████▌ | 5753/6690 [00:12<00:02, 445.87 examples/s]Tokenizing train dataset:  87%|████████▋ | 5811/6690 [00:12<00:01, 451.18 examples/s]Tokenizing train dataset:  87%|████████▋ | 5821/6690 [00:12<00:01, 439.84 examples/s]Tokenizing train dataset:  87%|████████▋ | 5820/6690 [00:13<00:01, 438.08 examples/s]Tokenizing train dataset:  88%|████████▊ | 5869/6690 [00:13<00:01, 424.76 examples/s]Tokenizing train dataset:  88%|████████▊ | 5881/6690 [00:13<00:01, 425.07 examples/s]Tokenizing train dataset:  88%|████████▊ | 5920/6690 [00:13<00:01, 440.61 examples/s]Tokenizing train dataset:  88%|████████▊ | 5880/6690 [00:13<00:01, 422.80 examples/s]Tokenizing train dataset:  89%|████████▊ | 5935/6690 [00:13<00:01, 447.19 examples/s]Tokenizing train dataset:  89%|████████▉ | 5965/6690 [00:13<00:01, 441.89 examples/s]Tokenizing train dataset:  89%|████████▊ | 5932/6690 [00:13<00:01, 440.80 examples/s]Tokenizing train dataset:  90%|████████▉ | 6003/6690 [00:13<00:01, 445.73 examples/s]Tokenizing train dataset:  90%|████████▉ | 6010/6690 [00:13<00:01, 442.78 examples/s]Tokenizing train dataset:  90%|████████▉ | 6001/6690 [00:13<00:01, 441.04 examples/s]Tokenizing train dataset:  91%|█████████ | 6055/6690 [00:13<00:01, 442.56 examples/s]Tokenizing train dataset:  91%|█████████ | 6072/6690 [00:13<00:01, 448.66 examples/s]Tokenizing train dataset:  90%|█████████ | 6046/6690 [00:13<00:01, 438.87 examples/s]Tokenizing train dataset:  91%|█████████▏| 6108/6690 [00:13<00:01, 465.90 examples/s]Tokenizing train dataset:  92%|█████████▏| 6127/6690 [00:13<00:01, 467.19 examples/s]Tokenizing train dataset:  91%|█████████ | 6100/6690 [00:13<00:01, 460.74 examples/s]Tokenizing train dataset:  92%|█████████▏| 6155/6690 [00:13<00:01, 464.61 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6690 [00:13<00:01, 456.28 examples/s]Tokenizing train dataset:  92%|█████████▏| 6155/6690 [00:13<00:01, 425.47 examples/s]Tokenizing train dataset:  93%|█████████▎| 6219/6690 [00:13<00:01, 448.10 examples/s]Tokenizing train dataset:  93%|█████████▎| 6199/6690 [00:13<00:01, 427.20 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:13<00:01, 419.94 examples/s]Tokenizing train dataset:  94%|█████████▍| 6277/6690 [00:14<00:00, 422.66 examples/s]Tokenizing train dataset:  94%|█████████▍| 6299/6690 [00:13<00:00, 434.99 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:14<00:01, 395.06 examples/s]Tokenizing train dataset:  94%|█████████▍| 6321/6690 [00:14<00:00, 423.25 examples/s]Tokenizing train dataset:  94%|█████████▍| 6298/6690 [00:14<00:00, 415.42 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 419.50 examples/s]Tokenizing train dataset:  95%|█████████▌| 6381/6690 [00:14<00:00, 409.62 examples/s]Tokenizing train dataset:  96%|█████████▌| 6404/6690 [00:14<00:00, 422.47 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 405.91 examples/s]Tokenizing train dataset:  96%|█████████▌| 6432/6690 [00:14<00:00, 430.33 examples/s]Tokenizing train dataset:  96%|█████████▋| 6452/6690 [00:14<00:00, 433.94 examples/s]Tokenizing train dataset:  96%|█████████▌| 6404/6690 [00:14<00:00, 411.54 examples/s]Tokenizing train dataset:  97%|█████████▋| 6477/6690 [00:14<00:00, 432.74 examples/s]Tokenizing train dataset:  97%|█████████▋| 6497/6690 [00:14<00:00, 436.76 examples/s]Tokenizing train dataset:  96%|█████████▋| 6451/6690 [00:14<00:00, 424.79 examples/s]Tokenizing train dataset:  97%|█████████▋| 6521/6690 [00:14<00:00, 432.64 examples/s]Tokenizing train dataset:  98%|█████████▊| 6542/6690 [00:14<00:00, 435.77 examples/s]Tokenizing train dataset:  97%|█████████▋| 6497/6690 [00:14<00:00, 429.50 examples/s]Tokenizing train dataset:  98%|█████████▊| 6588/6690 [00:14<00:00, 440.60 examples/s]Tokenizing train dataset:  98%|█████████▊| 6587/6690 [00:14<00:00, 432.72 examples/s]Tokenizing train dataset:  98%|█████████▊| 6541/6690 [00:14<00:00, 428.54 examples/s]Tokenizing train dataset:  98%|█████████▊| 6588/6690 [00:14<00:00, 435.09 examples/s]Tokenizing train dataset:  99%|█████████▉| 6648/6690 [00:14<00:00, 420.79 examples/s]Tokenizing train dataset:  99%|█████████▉| 6647/6690 [00:14<00:00, 417.11 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:14<00:00, 448.39 examples/s]
Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 419.49 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 445.70 examples/s]
Tokenizing train dataset:  99%|█████████▉| 6648/6690 [00:15<00:00, 416.34 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 442.90 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:  59%|█████▊    | 558/953 [00:00<00:00, 5540.82 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5556.08 examples/s]Extracting prompt in eval dataset:  58%|█████▊    | 555/953 [00:00<00:00, 5504.08 examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:10:28] Error running `scontrol show job $SLURM_JOB_ID` to count SLURM-available cpus. Using the machine's cpu count.
[codecarbon INFO @ 21:10:28] [setup] RAM Tracking...
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:10:28] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:10:28] [setup] CPU Tracking...
[codecarbon WARNING @ 21:10:28] No CPU tracking mode found. Falling back on CPU constant mode. 
 Linux OS detected: Please ensure RAPL files exist at \sys\class\powercap\intel-rapl to measure CPU

Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5541.82 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5498.31 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5476.83 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  33%|███▎      | 310/953 [00:00<00:00, 3060.51 examples/s]Applying chat template to eval dataset:  33%|███▎      | 315/953 [00:00<00:00, 3121.90 examples/s]Applying chat template to eval dataset:  33%|███▎      | 310/953 [00:00<00:00, 3056.93 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 634/953 [00:00<00:00, 3159.47 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 641/953 [00:00<00:00, 3197.23 examples/s]Applying chat template to eval dataset:  66%|██████▋   | 632/953 [00:00<00:00, 3141.46 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3147.46 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3191.08 examples/s]
Applying chat template to eval dataset: 100%|█████████▉| 952/953 [00:00<00:00, 3166.60 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3132.64 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.61 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 318.94 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.01 examples/s][codecarbon INFO @ 21:10:29] CPU Model on constant consumption mode: AMD EPYC 7H12 64-Core Processor
[codecarbon INFO @ 21:10:29] [setup] GPU Tracking...
[codecarbon INFO @ 21:10:29] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 21:10:29] >>> Tracker's metadata:
[codecarbon INFO @ 21:10:29]   Platform system: Linux-5.15.112-1.el8.vega.x86_64-x86_64-with-glibc2.35
[codecarbon INFO @ 21:10:29]   Python version: 3.10.12
[codecarbon INFO @ 21:10:29]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 21:10:29]   Available RAM : 503.683 GB
[codecarbon INFO @ 21:10:29]   CPU count: 256
[codecarbon INFO @ 21:10:29]   CPU model: AMD EPYC 7H12 64-Core Processor
[codecarbon INFO @ 21:10:29]   GPU count: 4
[codecarbon INFO @ 21:10:29]   GPU model: 4 x NVIDIA A100-SXM4-40GB BUT only tracking these GPU ids : [0, 1, 2, 3]
Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 286.03 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 285.59 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 285.17 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 275.85 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 276.13 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 275.51 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 264.34 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 264.70 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 263.94 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 251.06 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 251.38 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 250.87 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 267.70 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 267.89 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 267.30 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:00<00:01, 367.68 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:00<00:01, 368.49 examples/s]Tokenizing eval dataset:  31%|███       | 291/953 [00:00<00:01, 365.85 examples/s]Tokenizing eval dataset:  37%|███▋      | 357/953 [00:01<00:01, 439.47 examples/s]Tokenizing eval dataset:  37%|███▋      | 357/953 [00:01<00:01, 440.96 examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 436.06 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 485.18 examples/s]Tokenizing eval dataset:  44%|████▍     | 419/953 [00:01<00:01, 488.00 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 484.41 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 538.62 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 541.13 examples/s]Tokenizing eval dataset:  51%|█████     | 484/953 [00:01<00:00, 533.20 examples/s]Tokenizing eval dataset:  58%|█████▊    | 554/953 [00:01<00:00, 572.16 examples/s]Tokenizing eval dataset:  58%|█████▊    | 554/953 [00:01<00:00, 574.03 examples/s]Tokenizing eval dataset:  58%|█████▊    | 551/953 [00:01<00:00, 569.50 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 581.81 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 584.65 examples/s]Tokenizing eval dataset:  65%|██████▍   | 615/953 [00:01<00:00, 585.64 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 593.73 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 596.01 examples/s]Tokenizing eval dataset:  71%|███████   | 676/953 [00:01<00:00, 588.83 examples/s]Tokenizing eval dataset:  80%|███████▉  | 762/953 [00:01<00:00, 570.54 examples/s]Tokenizing eval dataset:  80%|████████  | 766/953 [00:01<00:00, 575.91 examples/s]Tokenizing eval dataset:  80%|███████▉  | 759/953 [00:01<00:00, 569.27 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 540.33 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 541.59 examples/s]Tokenizing eval dataset:  87%|████████▋ | 832/953 [00:01<00:00, 537.42 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 516.32 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 516.92 examples/s]Tokenizing eval dataset:  95%|█████████▌| 907/953 [00:02<00:00, 521.87 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 451.49 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 452.71 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 450.81 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[codecarbon INFO @ 21:10:33] Saving emissions data to file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/training_run/curriculum-2_r-64_lr-1e-06_b-0.1/emissions.csv
Set up DPO trainer
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.500772714614868 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.393871784210205 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3581295013427734 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3461246490478516 seconds
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:25] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:25] Energy consumed for RAM : 0.000788 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:25] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:25] Energy consumed for all CPUs : 0.000584 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:11:25] Energy consumed for all GPUs : 0.001076 kWh. Total GPU Power : 257.98829154607495 W
[codecarbon INFO @ 21:11:25] 0.002447 kWh of electricity used since the beginning.
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:40] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:40] Energy consumed for RAM : 0.001574 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:40] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:40] Energy consumed for all CPUs : 0.001167 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:11:40] Energy consumed for all GPUs : 0.002576 kWh. Total GPU Power : 360.308825473361 W
[codecarbon INFO @ 21:11:40] 0.005317 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:55] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:55] Energy consumed for RAM : 0.002360 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:55] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:55] Energy consumed for all CPUs : 0.001750 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:11:55] Energy consumed for all GPUs : 0.004079 kWh. Total GPU Power : 361.01720038469745 W
[codecarbon INFO @ 21:11:55] 0.008189 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:10] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:10] Energy consumed for RAM : 0.003146 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:10] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:10] Energy consumed for all CPUs : 0.002333 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:12:10] Energy consumed for all GPUs : 0.005580 kWh. Total GPU Power : 360.43556754780815 W
[codecarbon INFO @ 21:12:10] 0.011059 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:25] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:25] Energy consumed for RAM : 0.003933 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:25] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:25] Energy consumed for all CPUs : 0.002916 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:12:25] Energy consumed for all GPUs : 0.007083 kWh. Total GPU Power : 360.9647598294581 W
[codecarbon INFO @ 21:12:25] 0.013931 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:40] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:40] Energy consumed for RAM : 0.004719 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:40] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:40] Energy consumed for all CPUs : 0.003498 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:12:40] Energy consumed for all GPUs : 0.008584 kWh. Total GPU Power : 360.4109913335146 W
[codecarbon INFO @ 21:12:40] 0.016801 kWh of electricity used since the beginning.
[rank11]:[E612 21:12:54.660097250 ProcessGroupNCCL.cpp:552] [Rank 11] Collective WorkNCCL(SeqNum=3, OpType=ALLGATHER, NumelIn=1, NumelOut=12, Timeout(ms)=1800000) raised the following async exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgressOpt: Call to recv from 10.210.3.85<35745> failed : Broken pipe
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f3ab1b6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f3a5fe211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f3a5fe2964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f3a5fe2b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f3a5fe2c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f3ab1fee5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f3ab40adac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f3ab413fa40 in /lib/x86_64-linux-gnu/libc.so.6)

[rank11]:[E612 21:12:54.661600971 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 11]  failure detected by watchdog at work sequence id: 3 PG status: last enqueued work: 4, last completed work: 2
[rank11]:[E612 21:12:54.661607471 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank11]: Traceback (most recent call last):
[rank11]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 235, in <module>
[rank11]:     main(get_train_data(CURRICULUM_STAGE), val_data, args.rank, args.learning_rate, args.total_epochs, args.beta, CURRICULUM_STAGE)
[rank11]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 211, in main
[rank11]:     dpo_trainer.train()
[rank11]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank11]:     return inner_training_loop(
[rank11]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank11]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank11]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3736, in training_step
[rank11]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1356, in compute_loss
[rank11]:     loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1315, in get_batch_loss_metrics
[rank11]:     metrics[f"{prefix}rewards/chosen"] = self.accelerator.gather_for_metrics(chosen_rewards).mean().item()
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2583, in gather_for_metrics
[rank11]:     data = self.gather(input_data)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2539, in gather
[rank11]:     return gather(tensor)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 384, in wrapper
[rank11]:     output = gather_object([shapes])
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 459, in gather_object
[rank11]:     return _gpu_gather_object(object)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 440, in _gpu_gather_object
[rank11]:     torch.distributed.all_gather_object(output_objects, object)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank11]:     return func(*args, **kwargs)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3052, in all_gather_object
[rank11]:     object_list[i] = _tensor_to_object(tensor, tensor_size, group)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2961, in _tensor_to_object
[rank11]:     return _unpickler(io.BytesIO(buf)).load()
[rank11]: EOFError: Ran out of input
[rank11]:[E612 21:12:55.254710316 ProcessGroupNCCL.cpp:681] [Rank 11] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank11]:[E612 21:12:55.254750386 ProcessGroupNCCL.cpp:695] [Rank 11] To avoid data inconsistency, we are taking the entire process down.
[rank11]:[E612 21:12:55.254804546 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 11] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgressOpt: Call to recv from 10.210.3.85<35745> failed : Broken pipe
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f3ab1b6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f3a5fe211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f3a5fe2964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f3a5fe2b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f3a5fe2c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f3ab1fee5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f3ab40adac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f3ab413fa40 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 11] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgressOpt: Call to recv from 10.210.3.85<35745> failed : Broken pipe
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f3ab1b6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f3a5fe211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f3a5fe2964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f3a5fe2b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f3a5fe2c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f3ab1fee5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f3ab40adac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f3ab413fa40 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f3ab1b6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f3a5fa876fc in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f3ab1fee5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f3ab40adac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126a40 (0x7f3ab413fa40 in /lib/x86_64-linux-gnu/libc.so.6)

/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:55] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:55] Energy consumed for RAM : 0.005505 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:55] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:55] Energy consumed for all CPUs : 0.004081 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:12:55] Energy consumed for all GPUs : 0.010082 kWh. Total GPU Power : 359.92684232104193 W
[codecarbon INFO @ 21:12:55] 0.019669 kWh of electricity used since the beginning.
W0612 21:12:56.016000 2118279 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2118454 closing signal SIGTERM
W0612 21:12:56.016000 2118279 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2118455 closing signal SIGTERM
W0612 21:12:56.017000 2118279 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2118456 closing signal SIGTERM
[rank8]:[E612 21:12:57.513261662 ProcessGroupNCCL.cpp:552] [Rank 8] Collective WorkNCCL(SeqNum=4, OpType=ALLGATHER, NumelIn=20, NumelOut=240, Timeout(ms)=1800000) raised the following async exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn02.vega.pri<48616>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f1fa156c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f1f4f8211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f1f4f82964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f1f4f82b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f1f4f82c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f1fa1cd65c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f1fa3b7eac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f1fa3c10a40 in /lib/x86_64-linux-gnu/libc.so.6)

[rank8]:[E612 21:12:57.515277856 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 8]  failure detected by watchdog at work sequence id: 4 PG status: last enqueued work: 4, last completed work: 3
[rank8]:[E612 21:12:57.515285396 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
E0612 21:12:59.238000 2118279 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 3 (pid: 2118457) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1182, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 861, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train_curriculum.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-12_21:12:56
  host      : pm5-nod09.vega.pri
  rank      : 11 (local_rank: 3)
  exitcode  : -6 (pid: 2118457)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2118457
========================================================
