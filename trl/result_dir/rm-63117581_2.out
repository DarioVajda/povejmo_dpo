cpu-bind=MASK - gn04, task  2  0 [2116367]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 2 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 2     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63117581     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=1e-6 --total_epochs=3 --beta=0.1 --curriculum_stage=2
-------------------------------------------
[2025-06-12 20:55:10,326] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 20:55:12.109000 2116420 torch/distributed/run.py:792] 
W0612 20:55:12.109000 2116420 torch/distributed/run.py:792] *****************************************
W0612 20:55:12.109000 2116420 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 20:55:12.109000 2116420 torch/distributed/run.py:792] *****************************************
[2025-06-12 20:55:17,117] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 20:55:17,164] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 20:55:17,176] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 20:55:17,184] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
World size: 12
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
[2025-06-12 20:55:23,142] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 6690
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-12 20:55:23,151] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 20:55:23,298] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curriculum-1
[2025-06-12 20:55:23,318] [INFO] [comm.py:658:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:10, 23.42s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.26s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:47, 23.53s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:46, 23.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:46, 23.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.19s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 21.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 21.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 21.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.26s/it]
Loaded model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank10]:[W612 20:56:56.080043596 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   8%|▊         | 550/6690 [00:00<00:01, 5458.63 examples/s][rank9]:[W612 20:56:56.268906862 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank11]:[W612 20:56:56.305335390 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  17%|█▋        | 1116/6690 [00:00<00:01, 5560.70 examples/s]Extracting prompt in train dataset:  25%|██▌       | 1680/6690 [00:00<00:00, 5582.02 examples/s]Extracting prompt in train dataset:  34%|███▎      | 2250/6690 [00:00<00:00, 5571.56 examples/s]Extracting prompt in train dataset:  42%|████▏     | 2820/6690 [00:00<00:00, 5606.29 examples/s]Extracting prompt in train dataset:  51%|█████     | 3400/6690 [00:00<00:00, 5638.59 examples/s]Extracting prompt in train dataset:  59%|█████▉    | 3970/6690 [00:00<00:00, 5652.65 examples/s]Extracting prompt in train dataset:  72%|███████▏  | 4790/6690 [00:00<00:00, 5571.14 examples/s]Extracting prompt in train dataset:  80%|████████  | 5360/6690 [00:00<00:00, 5600.69 examples/s]Extracting prompt in train dataset:  89%|████████▊ | 5930/6690 [00:01<00:00, 5623.35 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 6509/6690 [00:01<00:00, 5653.80 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5544.17 examples/s]
Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 295/6690 [00:00<00:02, 2921.82 examples/s]Applying chat template to train dataset:   9%|▉         | 614/6690 [00:00<00:01, 3077.32 examples/s]Applying chat template to train dataset:  14%|█▍        | 935/6690 [00:00<00:01, 3135.81 examples/s]Applying chat template to train dataset:  21%|██        | 1401/6690 [00:00<00:01, 3115.45 examples/s]Applying chat template to train dataset:  26%|██▌       | 1722/6690 [00:00<00:01, 3143.12 examples/s]Applying chat template to train dataset:  31%|███       | 2044/6690 [00:00<00:01, 3162.80 examples/s]Applying chat template to train dataset:  38%|███▊      | 2521/6690 [00:00<00:01, 3164.53 examples/s]Applying chat template to train dataset:  42%|████▏     | 2842/6690 [00:00<00:01, 3174.78 examples/s]Applying chat template to train dataset:  47%|████▋     | 3164/6690 [00:01<00:01, 3186.53 examples/s]Applying chat template to train dataset:  52%|█████▏    | 3485/6690 [00:01<00:01, 3189.81 examples/s]Applying chat template to train dataset:  57%|█████▋    | 3806/6690 [00:01<00:00, 3191.64 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4127/6690 [00:01<00:00, 3196.28 examples/s]Applying chat template to train dataset:  69%|██████▊   | 4590/6690 [00:01<00:00, 3147.51 examples/s]Applying chat template to train dataset:  73%|███████▎  | 4910/6690 [00:01<00:00, 3160.57 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5231/6690 [00:01<00:00, 3170.23 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5552/6690 [00:01<00:00, 3178.88 examples/s]Applying chat template to train dataset:  88%|████████▊ | 5872/6690 [00:01<00:00, 3181.69 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6194/6690 [00:01<00:00, 3190.61 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6514/6690 [00:02<00:00, 3189.91 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3160.02 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 40/6690 [00:00<00:17, 388.45 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:15, 432.93 examples/s]Tokenizing train dataset:   2%|▏         | 134/6690 [00:00<00:15, 432.99 examples/s]Tokenizing train dataset:   3%|▎         | 188/6690 [00:00<00:13, 466.80 examples/s]Tokenizing train dataset:   4%|▎         | 235/6690 [00:00<00:13, 466.07 examples/s]Tokenizing train dataset:   4%|▍         | 282/6690 [00:00<00:13, 461.77 examples/s]Tokenizing train dataset:   5%|▌         | 349/6690 [00:00<00:14, 450.38 examples/s]Tokenizing train dataset:   6%|▌         | 400/6690 [00:00<00:13, 462.36 examples/s]Tokenizing train dataset:   7%|▋         | 453/6690 [00:00<00:13, 475.50 examples/s]Tokenizing train dataset:   8%|▊         | 505/6690 [00:01<00:12, 480.05 examples/s]Tokenizing train dataset:   9%|▊         | 581/6690 [00:01<00:12, 483.11 examples/s]Tokenizing train dataset:   9%|▉         | 632/6690 [00:01<00:12, 482.25 examples/s]Tokenizing train dataset:  10%|█         | 702/6690 [00:01<00:12, 476.06 examples/s]Tokenizing train dataset:  11%|█▏        | 757/6690 [00:01<00:12, 488.00 examples/s]Tokenizing train dataset:  12%|█▏        | 810/6690 [00:01<00:11, 497.00 examples/s]Tokenizing train dataset:  13%|█▎        | 879/6690 [00:01<00:12, 478.33 examples/s]Tokenizing train dataset:  14%|█▍        | 944/6690 [00:02<00:12, 457.06 examples/s]Tokenizing train dataset:  15%|█▌        | 1012/6690 [00:02<00:12, 451.88 examples/s]Tokenizing train dataset:  16%|█▌        | 1062/6690 [00:02<00:12, 460.97 examples/s]Tokenizing train dataset:  17%|█▋        | 1129/6690 [00:02<00:12, 450.73 examples/s]Tokenizing train dataset:  18%|█▊        | 1178/6690 [00:02<00:12, 455.65 examples/s]Tokenizing train dataset:  19%|█▊        | 1243/6690 [00:02<00:12, 440.47 examples/s]Tokenizing train dataset:  19%|█▉        | 1288/6690 [00:02<00:12, 441.82 examples/s]Tokenizing train dataset:  20%|█▉        | 1337/6690 [00:02<00:11, 450.93 examples/s]Tokenizing train dataset:  21%|██        | 1396/6690 [00:03<00:12, 423.80 examples/s]Tokenizing train dataset:  22%|██▏       | 1458/6690 [00:03<00:12, 418.84 examples/s]Tokenizing train dataset:  22%|██▏       | 1502/6690 [00:03<00:12, 422.54 examples/s]Tokenizing train dataset:  23%|██▎       | 1553/6690 [00:03<00:11, 441.25 examples/s]Tokenizing train dataset:  24%|██▍       | 1598/6690 [00:03<00:11, 441.82 examples/s]Tokenizing train dataset:  25%|██▍       | 1643/6690 [00:03<00:11, 439.93 examples/s]Tokenizing train dataset:  25%|██▌       | 1688/6690 [00:03<00:11, 436.28 examples/s]Tokenizing train dataset:  26%|██▌       | 1737/6690 [00:03<00:11, 448.51 examples/s]Tokenizing train dataset:  27%|██▋       | 1784/6690 [00:03<00:10, 446.89 examples/s]Tokenizing train dataset:  27%|██▋       | 1833/6690 [00:04<00:10, 451.17 examples/s]Tokenizing train dataset:  28%|██▊       | 1900/6690 [00:04<00:10, 442.50 examples/s]Tokenizing train dataset:  29%|██▉       | 1946/6690 [00:04<00:10, 444.31 examples/s]Tokenizing train dataset:  30%|██▉       | 2003/6690 [00:04<00:11, 420.74 examples/s]Tokenizing train dataset:  31%|███       | 2047/6690 [00:04<00:10, 424.88 examples/s]Tokenizing train dataset:  31%|███▏      | 2091/6690 [00:04<00:10, 427.53 examples/s]Tokenizing train dataset:  32%|███▏      | 2140/6690 [00:04<00:10, 440.94 examples/s]Tokenizing train dataset:  33%|███▎      | 2190/6690 [00:04<00:09, 454.48 examples/s]Tokenizing train dataset:  33%|███▎      | 2241/6690 [00:04<00:09, 468.70 examples/s]Tokenizing train dataset:  34%|███▍      | 2304/6690 [00:05<00:09, 445.97 examples/s]Tokenizing train dataset:  35%|███▌      | 2350/6690 [00:05<00:09, 448.34 examples/s]Tokenizing train dataset:  36%|███▌      | 2397/6690 [00:05<00:09, 452.20 examples/s]Tokenizing train dataset:  37%|███▋      | 2469/6690 [00:05<00:09, 454.88 examples/s]Tokenizing train dataset:  38%|███▊      | 2515/6690 [00:05<00:09, 452.79 examples/s]Tokenizing train dataset:  38%|███▊      | 2567/6690 [00:05<00:08, 469.30 examples/s]Tokenizing train dataset:  39%|███▉      | 2615/6690 [00:05<00:08, 470.26 examples/s]Tokenizing train dataset:  40%|████      | 2679/6690 [00:05<00:08, 449.53 examples/s]Tokenizing train dataset:  41%|████      | 2740/6690 [00:06<00:09, 431.93 examples/s]Tokenizing train dataset:  42%|████▏     | 2803/6690 [00:06<00:09, 423.93 examples/s]Tokenizing train dataset:  43%|████▎     | 2847/6690 [00:06<00:09, 426.87 examples/s]Tokenizing train dataset:  43%|████▎     | 2894/6690 [00:06<00:08, 432.18 examples/s]Tokenizing train dataset:  44%|████▍     | 2947/6690 [00:06<00:08, 456.04 examples/s]Tokenizing train dataset:  45%|████▍     | 2996/6690 [00:06<00:07, 462.43 examples/s]Tokenizing train dataset:  46%|████▌     | 3045/6690 [00:06<00:07, 467.23 examples/s]Tokenizing train dataset:  46%|████▋     | 3100/6690 [00:06<00:07, 487.58 examples/s]Tokenizing train dataset:  47%|████▋     | 3165/6690 [00:07<00:07, 461.38 examples/s]Tokenizing train dataset:  48%|████▊     | 3212/6690 [00:07<00:07, 456.09 examples/s]Tokenizing train dataset:  49%|████▊     | 3260/6690 [00:07<00:07, 459.37 examples/s]Tokenizing train dataset:  50%|████▉     | 3319/6690 [00:07<00:07, 428.63 examples/s]Tokenizing train dataset:  50%|█████     | 3363/6690 [00:07<00:07, 430.93 examples/s]Tokenizing train dataset:  51%|█████     | 3422/6690 [00:07<00:07, 415.45 examples/s]Tokenizing train dataset:  52%|█████▏    | 3490/6690 [00:07<00:07, 421.98 examples/s]Tokenizing train dataset:  53%|█████▎    | 3549/6690 [00:07<00:07, 408.20 examples/s]Tokenizing train dataset:  54%|█████▍    | 3608/6690 [00:08<00:07, 398.81 examples/s]Tokenizing train dataset:  55%|█████▍    | 3653/6690 [00:08<00:07, 408.00 examples/s]Tokenizing train dataset:  55%|█████▌    | 3699/6690 [00:08<00:07, 416.81 examples/s]Tokenizing train dataset:  56%|█████▌    | 3754/6690 [00:08<00:07, 394.62 examples/s]Tokenizing train dataset:  57%|█████▋    | 3799/6690 [00:08<00:07, 405.85 examples/s]Tokenizing train dataset:  57%|█████▋    | 3845/6690 [00:08<00:06, 417.75 examples/s]Tokenizing train dataset:  58%|█████▊    | 3893/6690 [00:08<00:06, 432.14 examples/s]Tokenizing train dataset:  59%|█████▉    | 3942/6690 [00:08<00:06, 444.15 examples/s]Tokenizing train dataset:  60%|█████▉    | 3992/6690 [00:08<00:05, 455.24 examples/s]Tokenizing train dataset:  61%|██████    | 4058/6690 [00:09<00:05, 447.83 examples/s]Tokenizing train dataset:  62%|██████▏   | 4126/6690 [00:09<00:05, 440.83 examples/s]Tokenizing train dataset:  62%|██████▏   | 4176/6690 [00:09<00:05, 452.68 examples/s]Tokenizing train dataset:  63%|██████▎   | 4245/6690 [00:09<00:05, 451.64 examples/s]Tokenizing train dataset:  64%|██████▍   | 4312/6690 [00:09<00:05, 447.33 examples/s]Tokenizing train dataset:  65%|██████▌   | 4373/6690 [00:09<00:05, 430.20 examples/s]Tokenizing train dataset:  66%|██████▌   | 4417/6690 [00:09<00:05, 429.94 examples/s]Tokenizing train dataset:  67%|██████▋   | 4462/6690 [00:10<00:05, 432.90 examples/s]Tokenizing train dataset:  68%|██████▊   | 4530/6690 [00:10<00:04, 433.87 examples/s]Tokenizing train dataset:  68%|██████▊   | 4577/6690 [00:10<00:04, 440.05 examples/s]Tokenizing train dataset:  69%|██████▉   | 4628/6690 [00:10<00:04, 451.49 examples/s]Tokenizing train dataset:  70%|██████▉   | 4675/6690 [00:10<00:04, 454.26 examples/s]Tokenizing train dataset:  71%|███████   | 4746/6690 [00:10<00:04, 455.53 examples/s]Tokenizing train dataset:  72%|███████▏  | 4808/6690 [00:10<00:04, 433.79 examples/s]Tokenizing train dataset:  73%|███████▎  | 4855/6690 [00:10<00:04, 438.20 examples/s]Tokenizing train dataset:  74%|███████▎  | 4918/6690 [00:11<00:04, 428.43 examples/s]Tokenizing train dataset:  74%|███████▍  | 4964/6690 [00:11<00:03, 432.01 examples/s]Tokenizing train dataset:  75%|███████▍  | 5008/6690 [00:11<00:03, 430.78 examples/s]Tokenizing train dataset:  76%|███████▌  | 5070/6690 [00:11<00:03, 422.46 examples/s]Tokenizing train dataset:  76%|███████▋  | 5115/6690 [00:11<00:03, 422.24 examples/s]Tokenizing train dataset:  77%|███████▋  | 5158/6690 [00:11<00:03, 422.31 examples/s]Tokenizing train dataset:  78%|███████▊  | 5208/6690 [00:11<00:03, 437.80 examples/s]Tokenizing train dataset:  79%|███████▉  | 5274/6690 [00:11<00:03, 434.44 examples/s]Tokenizing train dataset:  80%|███████▉  | 5338/6690 [00:12<00:03, 421.90 examples/s]Tokenizing train dataset:  81%|████████  | 5391/6690 [00:12<00:02, 447.38 examples/s]Tokenizing train dataset:  82%|████████▏ | 5456/6690 [00:12<00:02, 438.24 examples/s]Tokenizing train dataset:  82%|████████▏ | 5502/6690 [00:12<00:02, 440.00 examples/s]Tokenizing train dataset:  83%|████████▎ | 5566/6690 [00:12<00:02, 432.33 examples/s]Tokenizing train dataset:  84%|████████▍ | 5631/6690 [00:12<00:02, 427.77 examples/s]Tokenizing train dataset:  85%|████████▍ | 5678/6690 [00:12<00:02, 433.61 examples/s]Tokenizing train dataset:  86%|████████▌ | 5722/6690 [00:12<00:02, 428.54 examples/s]Tokenizing train dataset:  86%|████████▌ | 5767/6690 [00:13<00:02, 430.90 examples/s]Tokenizing train dataset:  87%|████████▋ | 5814/6690 [00:13<00:01, 438.76 examples/s]Tokenizing train dataset:  88%|████████▊ | 5873/6690 [00:13<00:01, 417.99 examples/s]Tokenizing train dataset:  89%|████████▊ | 5925/6690 [00:13<00:01, 442.04 examples/s]Tokenizing train dataset:  89%|████████▉ | 5971/6690 [00:13<00:01, 439.61 examples/s]Tokenizing train dataset:  90%|████████▉ | 6017/6690 [00:13<00:01, 440.66 examples/s]Tokenizing train dataset:  91%|█████████ | 6090/6690 [00:13<00:01, 449.43 examples/s]Tokenizing train dataset:  92%|█████████▏| 6142/6690 [00:13<00:01, 464.21 examples/s]Tokenizing train dataset:  93%|█████████▎| 6206/6690 [00:14<00:01, 448.42 examples/s]Tokenizing train dataset:  94%|█████████▎| 6260/6690 [00:14<00:01, 410.78 examples/s]Tokenizing train dataset:  94%|█████████▍| 6307/6690 [00:14<00:00, 424.24 examples/s]Tokenizing train dataset:  95%|█████████▌| 6369/6690 [00:14<00:00, 413.74 examples/s]Tokenizing train dataset:  96%|█████████▌| 6413/6690 [00:14<00:00, 419.26 examples/s]Tokenizing train dataset:  97%|█████████▋| 6458/6690 [00:14<00:00, 426.01 examples/s]Tokenizing train dataset:  98%|█████████▊| 6524/6690 [00:14<00:00, 425.85 examples/s]Tokenizing train dataset:  99%|█████████▊| 6591/6690 [00:14<00:00, 430.27 examples/s]Tokenizing train dataset:  99%|█████████▉| 6650/6690 [00:15<00:00, 413.75 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 439.98 examples/s]
[rank8]:[W612 20:57:15.637575812 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5659.89 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5656.34 examples/s]Extracting prompt in train dataset:   9%|▊         | 569/6690 [00:00<00:01, 5604.55 examples/s]Extracting prompt in train dataset:   9%|▊         | 584/6690 [00:00<00:01, 5746.67 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5636.42 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1160/6690 [00:00<00:00, 5752.59 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1140/6690 [00:00<00:00, 5640.62 examples/s]Extracting prompt in train dataset:  18%|█▊        | 1180/6690 [00:00<00:00, 5811.90 examples/s]Extracting prompt in train dataset:  26%|██▋       | 1770/6690 [00:00<00:00, 5890.73 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1725/6690 [00:00<00:00, 5716.57 examples/s]Extracting prompt in train dataset:  27%|██▋       | 1788/6690 [00:00<00:00, 5931.86 examples/s]Extracting prompt in train dataset:  40%|███▉      | 2660/6690 [00:00<00:00, 5903.00 examples/s]Extracting prompt in train dataset:  39%|███▊      | 2586/6690 [00:00<00:00, 5718.25 examples/s]Extracting prompt in train dataset:  40%|███▉      | 2674/6690 [00:00<00:00, 5901.56 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  47%|████▋     | 3166/6690 [00:00<00:00, 5742.60 examples/s]Extracting prompt in train dataset:  49%|████▉     | 3270/6690 [00:00<00:00, 5945.84 examples/s]Extracting prompt in train dataset:  49%|████▉     | 3271/6690 [00:00<00:00, 5919.66 examples/s]Applying chat template to eval dataset:  33%|███▎      | 318/953 [00:00<00:00, 3151.43 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3747/6690 [00:00<00:00, 5762.64 examples/s]Extracting prompt in train dataset:  58%|█████▊    | 3871/6690 [00:00<00:00, 5938.26 examples/s]Extracting prompt in train dataset:  58%|█████▊    | 3880/6690 [00:00<00:00, 5975.21 examples/s]Applying chat template to eval dataset:  68%|██████▊   | 647/953 [00:00<00:00, 3226.53 examples/s]Extracting prompt in train dataset:  65%|██████▍   | 4330/6690 [00:00<00:00, 5765.76 examples/s]Extracting prompt in train dataset:  71%|███████   | 4736/6690 [00:00<00:00, 5867.83 examples/s]Extracting prompt in train dataset:  71%|███████   | 4740/6690 [00:00<00:00, 5876.54 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3212.51 examples/s]
Extracting prompt in train dataset:  80%|███████▉  | 5332/6690 [00:00<00:00, 5892.07 examples/s]Extracting prompt in train dataset:  80%|███████▉  | 5337/6690 [00:00<00:00, 5901.71 examples/s]Extracting prompt in train dataset:  77%|███████▋  | 5174/6690 [00:00<00:00, 5695.86 examples/s]Extracting prompt in train dataset:  89%|████████▊ | 5932/6690 [00:01<00:00, 5921.10 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 5757/6690 [00:01<00:00, 5731.57 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 5945/6690 [00:01<00:00, 5937.58 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 6531/6690 [00:01<00:00, 5938.64 examples/s]Extracting prompt in train dataset:  95%|█████████▍| 6341/6690 [00:01<00:00, 5760.56 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 6554/6690 [00:01<00:00, 5963.65 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5870.16 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5866.55 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5695.69 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 323.44 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 292.65 examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   5%|▍         | 304/6690 [00:00<00:02, 3008.44 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:03, 273.89 examples/s]Applying chat template to train dataset:   4%|▍         | 300/6690 [00:00<00:02, 2965.28 examples/s]Applying chat template to train dataset:   4%|▍         | 290/6690 [00:00<00:02, 2869.87 examples/s]Applying chat template to train dataset:  10%|▉         | 637/6690 [00:00<00:01, 3187.35 examples/s]Applying chat template to train dataset:  10%|▉         | 636/6690 [00:00<00:01, 3194.76 examples/s]Applying chat template to train dataset:   9%|▉         | 610/6690 [00:00<00:01, 3053.24 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 265.61 examples/s]Applying chat template to train dataset:  14%|█▍        | 970/6690 [00:00<00:01, 3241.18 examples/s]Applying chat template to train dataset:  15%|█▍        | 974/6690 [00:00<00:01, 3275.47 examples/s]Applying chat template to train dataset:  14%|█▍        | 930/6690 [00:00<00:01, 3116.00 examples/s]Applying chat template to train dataset:  19%|█▉        | 1300/6690 [00:00<00:01, 3261.62 examples/s]Tokenizing eval dataset:  21%|██        | 196/953 [00:00<00:02, 256.09 examples/s]Applying chat template to train dataset:  20%|█▉        | 1310/6690 [00:00<00:01, 3300.57 examples/s]Applying chat template to train dataset:  19%|█▊        | 1247/6690 [00:00<00:01, 3132.69 examples/s]Applying chat template to train dataset:  24%|██▍       | 1632/6690 [00:00<00:01, 3278.55 examples/s]Tokenizing eval dataset:  25%|██▍       | 234/953 [00:00<00:02, 285.01 examples/s]Applying chat template to train dataset:  25%|██▍       | 1646/6690 [00:00<00:01, 3319.02 examples/s]Applying chat template to train dataset:  23%|██▎       | 1565/6690 [00:00<00:01, 3147.65 examples/s]Applying chat template to train dataset:  29%|██▉       | 1964/6690 [00:00<00:01, 3292.29 examples/s]Applying chat template to train dataset:  30%|██▉       | 1984/6690 [00:00<00:01, 3336.11 examples/s]Tokenizing eval dataset:  32%|███▏      | 301/953 [00:00<00:01, 384.37 examples/s]Applying chat template to train dataset:  28%|██▊       | 1891/6690 [00:00<00:01, 3183.18 examples/s]Tokenizing eval dataset:  38%|███▊      | 363/953 [00:01<00:01, 441.72 examples/s]Applying chat template to train dataset:  33%|███▎      | 2219/6690 [00:00<00:01, 3211.57 examples/s]Applying chat template to train dataset:  37%|███▋      | 2460/6690 [00:00<00:01, 3291.44 examples/s]Applying chat template to train dataset:  37%|███▋      | 2480/6690 [00:00<00:01, 3316.88 examples/s]Tokenizing eval dataset:  45%|████▌     | 432/953 [00:01<00:01, 505.42 examples/s]Applying chat template to train dataset:  42%|████▏     | 2792/6690 [00:00<00:01, 3297.42 examples/s]Applying chat template to train dataset:  40%|████      | 2693/6690 [00:00<00:01, 3186.95 examples/s]Applying chat template to train dataset:  44%|████▍     | 2976/6690 [00:00<00:01, 3306.76 examples/s]Tokenizing eval dataset:  52%|█████▏    | 500/953 [00:01<00:00, 549.16 examples/s]Applying chat template to train dataset:  47%|████▋     | 3126/6690 [00:00<00:01, 3307.21 examples/s]Applying chat template to train dataset:  45%|████▌     | 3020/6690 [00:00<00:01, 3205.19 examples/s]Applying chat template to train dataset:  50%|████▉     | 3313/6690 [00:01<00:01, 3319.00 examples/s]Tokenizing eval dataset:  59%|█████▉    | 566/953 [00:01<00:00, 571.12 examples/s]Applying chat template to train dataset:  52%|█████▏    | 3457/6690 [00:01<00:00, 3305.46 examples/s]Applying chat template to train dataset:  50%|█████     | 3346/6690 [00:01<00:01, 3219.72 examples/s]Applying chat template to train dataset:  55%|█████▍    | 3650/6690 [00:01<00:00, 3326.86 examples/s]Tokenizing eval dataset:  66%|██████▋   | 633/953 [00:01<00:00, 598.63 examples/s]Applying chat template to train dataset:  57%|█████▋    | 3788/6690 [00:01<00:00, 3306.19 examples/s]Applying chat template to train dataset:  55%|█████▍    | 3670/6690 [00:01<00:00, 3222.60 examples/s]Applying chat template to train dataset:  60%|█████▉    | 3990/6690 [00:01<00:00, 3339.08 examples/s]Tokenizing eval dataset:  73%|███████▎  | 695/953 [00:01<00:00, 602.70 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4120/6690 [00:01<00:00, 3306.11 examples/s]Applying chat template to train dataset:  60%|█████▉    | 3997/6690 [00:01<00:00, 3235.18 examples/s]Applying chat template to train dataset:  65%|██████▍   | 4330/6690 [00:01<00:00, 3348.46 examples/s]Tokenizing eval dataset:  82%|████████▏ | 777/953 [00:01<00:00, 574.11 examples/s]Applying chat template to train dataset:  65%|██████▍   | 4324/6690 [00:01<00:00, 3243.04 examples/s]Applying chat template to train dataset:  69%|██████▊   | 4598/6690 [00:01<00:00, 3255.43 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4805/6690 [00:01<00:00, 3274.49 examples/s]Applying chat template to train dataset:  74%|███████▎  | 4929/6690 [00:01<00:00, 3268.79 examples/s]Applying chat template to train dataset:  71%|███████▏  | 4783/6690 [00:01<00:00, 3169.97 examples/s]Tokenizing eval dataset:  89%|████████▉ | 851/953 [00:01<00:00, 536.04 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5140/6690 [00:01<00:00, 3291.64 examples/s]Applying chat template to train dataset:  79%|███████▊  | 5260/6690 [00:01<00:00, 3275.18 examples/s]Applying chat template to train dataset:  76%|███████▋  | 5106/6690 [00:01<00:00, 3184.90 examples/s]Applying chat template to train dataset:  82%|████████▏ | 5477/6690 [00:01<00:00, 3311.48 examples/s]Applying chat template to train dataset:  84%|████████▎ | 5591/6690 [00:01<00:00, 3283.60 examples/s]Tokenizing eval dataset:  97%|█████████▋| 929/953 [00:02<00:00, 523.15 examples/s]Applying chat template to train dataset:  81%|████████  | 5431/6690 [00:01<00:00, 3199.92 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 457.16 examples/s]
Applying chat template to train dataset:  87%|████████▋ | 5814/6690 [00:01<00:00, 3325.33 examples/s]Applying chat template to train dataset:  89%|████████▊ | 5923/6690 [00:01<00:00, 3290.77 examples/s]Applying chat template to train dataset:  86%|████████▌ | 5758/6690 [00:01<00:00, 3216.94 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6151/6690 [00:01<00:00, 3335.09 examples/s]Applying chat template to train dataset:  94%|█████████▎| 6256/6690 [00:01<00:00, 3295.30 examples/s]Applying chat template to train dataset:  91%|█████████ | 6084/6690 [00:01<00:00, 3222.61 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6490/6690 [00:01<00:00, 3343.30 examples/s]Applying chat template to train dataset:  98%|█████████▊| 6588/6690 [00:02<00:00, 3299.10 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3305.44 examples/s]
Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3274.26 examples/s]
Applying chat template to train dataset:  96%|█████████▌| 6409/6690 [00:02<00:00, 3228.64 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3187.48 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:15, 424.13 examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:15, 418.18 examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|▏         | 91/6690 [00:00<00:14, 444.86 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:15, 439.62 examples/s]Tokenizing train dataset:   1%|          | 43/6690 [00:00<00:15, 416.68 examples/s]Tokenizing train dataset:   2%|▏         | 140/6690 [00:00<00:14, 450.74 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 441.37 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:15, 437.94 examples/s]Tokenizing train dataset:   3%|▎         | 195/6690 [00:00<00:13, 480.25 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 474.33 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 439.31 examples/s]Tokenizing train dataset:   4%|▎         | 245/6690 [00:00<00:13, 479.86 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 473.86 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 471.30 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 471.64 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 471.20 examples/s]Tokenizing train dataset:   5%|▍         | 317/6690 [00:00<00:13, 470.05 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 468.41 examples/s]Tokenizing train dataset:   6%|▌         | 369/6690 [00:00<00:13, 482.06 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 461.58 examples/s]Tokenizing train dataset:   6%|▌         | 418/6690 [00:00<00:13, 482.11 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 475.80 examples/s]Tokenizing train dataset:   5%|▌         | 353/6690 [00:00<00:13, 460.73 examples/s]Tokenizing train dataset:   7%|▋         | 472/6690 [00:00<00:12, 497.14 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 484.00 examples/s]Tokenizing train dataset:   6%|▌         | 404/6690 [00:00<00:13, 466.20 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 486.20 examples/s]Tokenizing train dataset:   7%|▋         | 457/6690 [00:00<00:12, 482.31 examples/s]Tokenizing train dataset:   8%|▊         | 546/6690 [00:01<00:12, 493.18 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 486.44 examples/s]Tokenizing train dataset:   8%|▊         | 509/6690 [00:01<00:12, 488.77 examples/s]Tokenizing train dataset:   9%|▉         | 597/6690 [00:01<00:12, 495.90 examples/s]Tokenizing train dataset:  10%|▉         | 647/6690 [00:01<00:12, 493.54 examples/s]Tokenizing train dataset:   9%|▉         | 616/6690 [00:01<00:13, 440.51 examples/s]Tokenizing train dataset:   9%|▊         | 569/6690 [00:01<00:13, 449.93 examples/s]Tokenizing train dataset:  10%|▉         | 664/6690 [00:01<00:13, 448.98 examples/s]Tokenizing train dataset:   9%|▉         | 619/6690 [00:01<00:13, 459.77 examples/s]Tokenizing train dataset:  11%|█         | 725/6690 [00:01<00:11, 498.02 examples/s]Tokenizing train dataset:  11%|█         | 717/6690 [00:01<00:12, 470.38 examples/s]Tokenizing train dataset:  10%|▉         | 668/6690 [00:01<00:12, 464.00 examples/s]Tokenizing train dataset:  12%|█▏        | 780/6690 [00:01<00:11, 501.47 examples/s]Tokenizing train dataset:  12%|█▏        | 771/6690 [00:01<00:12, 484.84 examples/s]Tokenizing train dataset:  11%|█         | 723/6690 [00:01<00:12, 484.48 examples/s]Tokenizing train dataset:  12%|█▏        | 831/6690 [00:01<00:11, 501.01 examples/s]Tokenizing train dataset:  12%|█▏        | 823/6690 [00:01<00:11, 489.56 examples/s]Tokenizing train dataset:  12%|█▏        | 776/6690 [00:01<00:11, 493.61 examples/s]Tokenizing train dataset:  13%|█▎        | 897/6690 [00:01<00:12, 476.58 examples/s]Tokenizing train dataset:  13%|█▎        | 891/6690 [00:01<00:12, 469.52 examples/s]Tokenizing train dataset:  13%|█▎        | 849/6690 [00:01<00:11, 487.58 examples/s]Tokenizing train dataset:  14%|█▍        | 946/6690 [00:01<00:12, 473.11 examples/s]Tokenizing train dataset:  14%|█▍        | 960/6690 [00:02<00:12, 459.70 examples/s]Tokenizing train dataset:  14%|█▎        | 912/6690 [00:01<00:12, 461.81 examples/s]Tokenizing train dataset:  15%|█▌        | 1016/6690 [00:02<00:12, 466.84 examples/s]Tokenizing train dataset:  15%|█▌        | 1009/6690 [00:02<00:12, 461.44 examples/s]Tokenizing train dataset:  14%|█▍        | 960/6690 [00:02<00:12, 460.77 examples/s]Tokenizing train dataset:  16%|█▌        | 1068/6690 [00:02<00:11, 475.92 examples/s]Tokenizing train dataset:  15%|█▌        | 1009/6690 [00:02<00:12, 461.70 examples/s]Tokenizing train dataset:  16%|█▌        | 1081/6690 [00:02<00:12, 464.43 examples/s]Tokenizing train dataset:  17%|█▋        | 1137/6690 [00:02<00:11, 467.47 examples/s]Tokenizing train dataset:  16%|█▌        | 1081/6690 [00:02<00:12, 463.96 examples/s]Tokenizing train dataset:  18%|█▊        | 1187/6690 [00:02<00:11, 471.78 examples/s]Tokenizing train dataset:  17%|█▋        | 1150/6690 [00:02<00:12, 456.76 examples/s]Tokenizing train dataset:  18%|█▊        | 1198/6690 [00:02<00:11, 461.57 examples/s]Tokenizing train dataset:  17%|█▋        | 1150/6690 [00:02<00:12, 455.63 examples/s]Tokenizing train dataset:  19%|█▊        | 1251/6690 [00:02<00:12, 452.49 examples/s]Tokenizing train dataset:  18%|█▊        | 1198/6690 [00:02<00:11, 460.29 examples/s]Tokenizing train dataset:  19%|█▉        | 1297/6690 [00:02<00:11, 450.70 examples/s]Tokenizing train dataset:  19%|█▉        | 1266/6690 [00:02<00:12, 450.33 examples/s]Tokenizing train dataset:  20%|██        | 1354/6690 [00:02<00:11, 476.85 examples/s]Tokenizing train dataset:  20%|█▉        | 1318/6690 [00:02<00:11, 463.14 examples/s]Tokenizing train dataset:  19%|█▉        | 1265/6690 [00:02<00:12, 448.98 examples/s]Tokenizing train dataset:  20%|██        | 1368/6690 [00:02<00:11, 465.60 examples/s]Tokenizing train dataset:  20%|█▉        | 1315/6690 [00:02<00:11, 459.51 examples/s]Tokenizing train dataset:  21%|██        | 1416/6690 [00:02<00:11, 447.87 examples/s]Tokenizing train dataset:  20%|██        | 1363/6690 [00:02<00:11, 461.46 examples/s]Tokenizing train dataset:  21%|██▏       | 1431/6690 [00:03<00:11, 444.73 examples/s]Tokenizing train dataset:  22%|██▏       | 1482/6690 [00:03<00:11, 439.30 examples/s]Tokenizing train dataset:  21%|██▏       | 1425/6690 [00:03<00:11, 440.24 examples/s]Tokenizing train dataset:  23%|██▎       | 1540/6690 [00:03<00:11, 467.93 examples/s]Tokenizing train dataset:  22%|██▏       | 1495/6690 [00:03<00:11, 434.33 examples/s]Tokenizing train dataset:  23%|██▎       | 1551/6690 [00:03<00:11, 458.21 examples/s]Tokenizing train dataset:  22%|██▏       | 1490/6690 [00:03<00:11, 435.44 examples/s]Tokenizing train dataset:  24%|██▍       | 1606/6690 [00:03<00:11, 451.64 examples/s]Tokenizing train dataset:  23%|██▎       | 1545/6690 [00:03<00:11, 460.40 examples/s]Tokenizing train dataset:  24%|██▍       | 1616/6690 [00:03<00:11, 448.24 examples/s]Tokenizing train dataset:  25%|██▌       | 1675/6690 [00:03<00:11, 451.44 examples/s]Tokenizing train dataset:  24%|██▍       | 1609/6690 [00:03<00:11, 443.32 examples/s]Tokenizing train dataset:  26%|██▌       | 1725/6690 [00:03<00:10, 458.10 examples/s]Tokenizing train dataset:  25%|██▌       | 1684/6690 [00:03<00:11, 447.59 examples/s]Tokenizing train dataset:  27%|██▋       | 1774/6690 [00:03<00:10, 461.39 examples/s]Tokenizing train dataset:  26%|██▌       | 1732/6690 [00:03<00:10, 454.81 examples/s]Tokenizing train dataset:  25%|██▌       | 1679/6690 [00:03<00:11, 447.38 examples/s]Tokenizing train dataset:  27%|██▋       | 1779/6690 [00:03<00:10, 455.32 examples/s]Tokenizing train dataset:  26%|██▌       | 1728/6690 [00:03<00:10, 451.90 examples/s]Tokenizing train dataset:  28%|██▊       | 1845/6690 [00:03<00:10, 457.24 examples/s]Tokenizing train dataset:  27%|██▋       | 1826/6690 [00:03<00:10, 457.40 examples/s]Tokenizing train dataset:  27%|██▋       | 1777/6690 [00:03<00:10, 456.51 examples/s]Tokenizing train dataset:  28%|██▊       | 1897/6690 [00:04<00:10, 466.62 examples/s]Tokenizing train dataset:  28%|██▊       | 1897/6690 [00:04<00:10, 459.81 examples/s]Tokenizing train dataset:  28%|██▊       | 1845/6690 [00:04<00:10, 450.13 examples/s]Tokenizing train dataset:  29%|██▉       | 1965/6690 [00:04<00:10, 457.50 examples/s]Tokenizing train dataset:  28%|██▊       | 1894/6690 [00:04<00:10, 458.82 examples/s]Tokenizing train dataset:  29%|██▉       | 1965/6690 [00:04<00:10, 451.43 examples/s]Tokenizing train dataset:  30%|███       | 2025/6690 [00:04<00:10, 435.43 examples/s]Tokenizing train dataset:  29%|██▉       | 1961/6690 [00:04<00:10, 452.04 examples/s]Tokenizing train dataset:  30%|███       | 2025/6690 [00:04<00:10, 430.09 examples/s]Tokenizing train dataset:  31%|███       | 2072/6690 [00:04<00:10, 441.00 examples/s]Tokenizing train dataset:  31%|███       | 2071/6690 [00:04<00:10, 436.24 examples/s]Tokenizing train dataset:  32%|███▏      | 2120/6690 [00:04<00:10, 447.55 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:10, 429.52 examples/s]Tokenizing train dataset:  32%|███▏      | 2171/6690 [00:04<00:09, 461.29 examples/s]Tokenizing train dataset:  32%|███▏      | 2120/6690 [00:04<00:10, 442.48 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 430.09 examples/s]Tokenizing train dataset:  33%|███▎      | 2221/6690 [00:04<00:09, 467.64 examples/s]Tokenizing train dataset:  32%|███▏      | 2171/6690 [00:04<00:09, 456.82 examples/s]Tokenizing train dataset:  32%|███▏      | 2140/6690 [00:04<00:10, 443.07 examples/s]Tokenizing train dataset:  33%|███▎      | 2221/6690 [00:04<00:09, 463.61 examples/s]Tokenizing train dataset:  34%|███▍      | 2290/6690 [00:04<00:09, 460.92 examples/s]Tokenizing train dataset:  33%|███▎      | 2190/6690 [00:04<00:09, 455.85 examples/s]Tokenizing train dataset:  34%|███▍      | 2268/6690 [00:04<00:09, 462.99 examples/s]Tokenizing train dataset:  35%|███▍      | 2337/6690 [00:05<00:09, 457.70 examples/s]Tokenizing train dataset:  34%|███▎      | 2245/6690 [00:04<00:09, 473.36 examples/s]Tokenizing train dataset:  36%|███▌      | 2386/6690 [00:05<00:09, 463.40 examples/s]Tokenizing train dataset:  35%|███▍      | 2337/6690 [00:05<00:09, 453.18 examples/s]Tokenizing train dataset:  35%|███▍      | 2309/6690 [00:05<00:09, 448.28 examples/s]Tokenizing train dataset:  36%|███▌      | 2386/6690 [00:05<00:09, 459.11 examples/s]Tokenizing train dataset:  37%|███▋      | 2456/6690 [00:05<00:09, 462.16 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:05<00:09, 457.69 examples/s]Tokenizing train dataset:  37%|███▋      | 2504/6690 [00:05<00:09, 463.67 examples/s]Tokenizing train dataset:  37%|███▋      | 2456/6690 [00:05<00:09, 458.97 examples/s]Tokenizing train dataset:  36%|███▋      | 2430/6690 [00:05<00:09, 453.37 examples/s]Tokenizing train dataset:  37%|███▋      | 2504/6690 [00:05<00:09, 460.69 examples/s]Tokenizing train dataset:  38%|███▊      | 2560/6690 [00:05<00:08, 476.10 examples/s]Tokenizing train dataset:  37%|███▋      | 2478/6690 [00:05<00:09, 456.70 examples/s]Tokenizing train dataset:  39%|███▉      | 2612/6690 [00:05<00:08, 484.42 examples/s]Tokenizing train dataset:  38%|███▊      | 2560/6690 [00:05<00:08, 472.97 examples/s]Tokenizing train dataset:  38%|███▊      | 2526/6690 [00:05<00:09, 459.35 examples/s]Tokenizing train dataset:  39%|███▉      | 2611/6690 [00:05<00:08, 481.48 examples/s]Tokenizing train dataset:  40%|████      | 2677/6690 [00:05<00:08, 459.10 examples/s]Tokenizing train dataset:  39%|███▊      | 2580/6690 [00:05<00:08, 479.69 examples/s]Tokenizing train dataset:  40%|███▉      | 2675/6690 [00:05<00:08, 455.34 examples/s]Tokenizing train dataset:  41%|████      | 2741/6690 [00:05<00:08, 442.68 examples/s]Tokenizing train dataset:  40%|███▉      | 2647/6690 [00:05<00:08, 466.01 examples/s]Tokenizing train dataset:  41%|████      | 2741/6690 [00:05<00:08, 440.27 examples/s]Tokenizing train dataset:  42%|████▏     | 2806/6690 [00:06<00:08, 432.54 examples/s]Tokenizing train dataset:  41%|████      | 2712/6690 [00:05<00:08, 449.57 examples/s]Tokenizing train dataset:  43%|████▎     | 2855/6690 [00:06<00:08, 442.29 examples/s]Tokenizing train dataset:  42%|████▏     | 2806/6690 [00:06<00:09, 429.80 examples/s]Tokenizing train dataset:  41%|████▏     | 2774/6690 [00:06<00:09, 429.85 examples/s]Tokenizing train dataset:  43%|████▎     | 2901/6690 [00:06<00:08, 445.90 examples/s]Tokenizing train dataset:  43%|████▎     | 2854/6690 [00:06<00:08, 439.24 examples/s]Tokenizing train dataset:  44%|████▍     | 2955/6690 [00:06<00:08, 465.20 examples/s]Tokenizing train dataset:  43%|████▎     | 2899/6690 [00:06<00:08, 440.78 examples/s]Tokenizing train dataset:  42%|████▏     | 2839/6690 [00:06<00:08, 429.79 examples/s]Tokenizing train dataset:  45%|████▍     | 3008/6690 [00:06<00:07, 478.61 examples/s]Tokenizing train dataset:  44%|████▍     | 2953/6690 [00:06<00:08, 462.88 examples/s]Tokenizing train dataset:  43%|████▎     | 2885/6690 [00:06<00:08, 434.12 examples/s]Tokenizing train dataset:  46%|████▌     | 3060/6690 [00:06<00:07, 482.92 examples/s]Tokenizing train dataset:  45%|████▍     | 3005/6690 [00:06<00:07, 473.93 examples/s]Tokenizing train dataset:  44%|████▍     | 2936/6690 [00:06<00:08, 451.30 examples/s]Tokenizing train dataset:  47%|████▋     | 3112/6690 [00:06<00:07, 489.65 examples/s]Tokenizing train dataset:  46%|████▌     | 3055/6690 [00:06<00:07, 478.83 examples/s]Tokenizing train dataset:  45%|████▍     | 2988/6690 [00:06<00:07, 465.55 examples/s]Tokenizing train dataset:  46%|████▋     | 3108/6690 [00:06<00:07, 492.08 examples/s]Tokenizing train dataset:  45%|████▌     | 3038/6690 [00:06<00:07, 469.16 examples/s]Tokenizing train dataset:  48%|████▊     | 3178/6690 [00:06<00:07, 469.16 examples/s]Tokenizing train dataset:  46%|████▋     | 3095/6690 [00:06<00:07, 494.87 examples/s]Tokenizing train dataset:  48%|████▊     | 3228/6690 [00:06<00:07, 472.69 examples/s]Tokenizing train dataset:  47%|████▋     | 3173/6690 [00:06<00:07, 462.49 examples/s]Tokenizing train dataset:  49%|████▉     | 3276/6690 [00:07<00:07, 471.25 examples/s]Tokenizing train dataset:  47%|████▋     | 3161/6690 [00:06<00:07, 468.61 examples/s]Tokenizing train dataset:  49%|████▊     | 3249/6690 [00:07<00:07, 470.92 examples/s]Tokenizing train dataset:  50%|████▉     | 3333/6690 [00:07<00:07, 435.77 examples/s]Tokenizing train dataset:  48%|████▊     | 3234/6690 [00:07<00:07, 468.16 examples/s]Tokenizing train dataset:  49%|████▉     | 3310/6690 [00:07<00:07, 443.65 examples/s]Tokenizing train dataset:  50%|█████     | 3378/6690 [00:07<00:07, 438.26 examples/s]Tokenizing train dataset:  49%|████▉     | 3300/6690 [00:07<00:07, 450.95 examples/s]Tokenizing train dataset:  50%|█████     | 3376/6690 [00:07<00:07, 439.87 examples/s]Tokenizing train dataset:  51%|█████▏    | 3438/6690 [00:07<00:07, 423.15 examples/s]Tokenizing train dataset:  50%|█████     | 3363/6690 [00:07<00:07, 435.58 examples/s]Tokenizing train dataset:  52%|█████▏    | 3485/6690 [00:07<00:07, 428.11 examples/s]Tokenizing train dataset:  51%|█████▏    | 3437/6690 [00:07<00:07, 423.83 examples/s]Tokenizing train dataset:  51%|█████     | 3408/6690 [00:07<00:07, 430.56 examples/s]Tokenizing train dataset:  53%|█████▎    | 3529/6690 [00:07<00:07, 421.75 examples/s]Tokenizing train dataset:  52%|█████▏    | 3483/6690 [00:07<00:07, 429.45 examples/s]Tokenizing train dataset:  52%|█████▏    | 3464/6690 [00:07<00:07, 408.17 examples/s]Tokenizing train dataset:  54%|█████▎    | 3586/6690 [00:07<00:07, 402.71 examples/s]Tokenizing train dataset:  53%|█████▎    | 3543/6690 [00:07<00:07, 415.51 examples/s]Tokenizing train dataset:  52%|█████▏    | 3511/6690 [00:07<00:07, 422.43 examples/s]Tokenizing train dataset:  54%|█████▍    | 3630/6690 [00:07<00:07, 407.69 examples/s]Tokenizing train dataset:  54%|█████▍    | 3602/6690 [00:07<00:07, 403.29 examples/s]Tokenizing train dataset:  55%|█████▍    | 3674/6690 [00:08<00:07, 414.32 examples/s]Tokenizing train dataset:  53%|█████▎    | 3570/6690 [00:07<00:07, 407.24 examples/s]Tokenizing train dataset:  55%|█████▍    | 3648/6690 [00:08<00:07, 413.09 examples/s]Tokenizing train dataset:  56%|█████▌    | 3734/6690 [00:08<00:07, 406.31 examples/s]Tokenizing train dataset:  55%|█████▌    | 3694/6690 [00:08<00:07, 419.16 examples/s]Tokenizing train dataset:  54%|█████▍    | 3637/6690 [00:08<00:07, 413.58 examples/s]Tokenizing train dataset:  56%|█████▋    | 3779/6690 [00:08<00:07, 415.40 examples/s]Tokenizing train dataset:  55%|█████▌    | 3680/6690 [00:08<00:07, 414.17 examples/s]Tokenizing train dataset:  56%|█████▌    | 3750/6690 [00:08<00:07, 397.95 examples/s]Tokenizing train dataset:  57%|█████▋    | 3826/6690 [00:08<00:06, 426.14 examples/s]Tokenizing train dataset:  57%|█████▋    | 3797/6690 [00:08<00:07, 411.11 examples/s]Tokenizing train dataset:  56%|█████▌    | 3740/6690 [00:08<00:07, 398.55 examples/s]Tokenizing train dataset:  58%|█████▊    | 3870/6690 [00:08<00:06, 427.04 examples/s]Tokenizing train dataset:  57%|█████▋    | 3844/6690 [00:08<00:06, 424.53 examples/s]Tokenizing train dataset:  57%|█████▋    | 3788/6690 [00:08<00:06, 415.30 examples/s]Tokenizing train dataset:  59%|█████▊    | 3921/6690 [00:08<00:06, 449.02 examples/s]Tokenizing train dataset:  58%|█████▊    | 3893/6690 [00:08<00:06, 438.33 examples/s]Tokenizing train dataset:  57%|█████▋    | 3831/6690 [00:08<00:06, 416.56 examples/s]Tokenizing train dataset:  59%|█████▉    | 3969/6690 [00:08<00:05, 454.20 examples/s]Tokenizing train dataset:  59%|█████▉    | 3944/6690 [00:08<00:06, 454.26 examples/s]Tokenizing train dataset:  58%|█████▊    | 3876/6690 [00:08<00:06, 424.34 examples/s]Tokenizing train dataset:  60%|██████    | 4020/6690 [00:08<00:05, 468.54 examples/s]Tokenizing train dataset:  60%|█████▉    | 3993/6690 [00:08<00:05, 461.54 examples/s]Tokenizing train dataset:  59%|█████▊    | 3926/6690 [00:08<00:06, 443.64 examples/s]Tokenizing train dataset:  61%|██████    | 4085/6690 [00:08<00:05, 447.37 examples/s]Tokenizing train dataset:  59%|█████▉    | 3976/6690 [00:08<00:05, 452.86 examples/s]Tokenizing train dataset:  61%|██████    | 4060/6690 [00:08<00:05, 452.73 examples/s]Tokenizing train dataset:  62%|██████▏   | 4137/6690 [00:09<00:05, 459.39 examples/s]Tokenizing train dataset:  60%|██████    | 4025/6690 [00:08<00:05, 461.06 examples/s]Tokenizing train dataset:  63%|██████▎   | 4188/6690 [00:09<00:05, 469.62 examples/s]Tokenizing train dataset:  62%|██████▏   | 4127/6690 [00:09<00:05, 448.48 examples/s]Tokenizing train dataset:  61%|██████    | 4087/6690 [00:09<00:05, 441.08 examples/s]Tokenizing train dataset:  62%|██████▏   | 4179/6690 [00:09<00:05, 462.30 examples/s]Tokenizing train dataset:  64%|██████▎   | 4255/6690 [00:09<00:05, 456.20 examples/s]Tokenizing train dataset:  62%|██████▏   | 4136/6690 [00:09<00:05, 452.09 examples/s]Tokenizing train dataset:  63%|██████▎   | 4248/6690 [00:09<00:05, 456.82 examples/s]Tokenizing train dataset:  63%|██████▎   | 4188/6690 [00:09<00:05, 464.36 examples/s]Tokenizing train dataset:  65%|██████▍   | 4327/6690 [00:09<00:05, 460.74 examples/s]Tokenizing train dataset:  65%|██████▍   | 4317/6690 [00:09<00:05, 452.59 examples/s]Tokenizing train dataset:  64%|██████▎   | 4255/6690 [00:09<00:05, 450.78 examples/s]Tokenizing train dataset:  66%|██████▌   | 4386/6690 [00:09<00:05, 434.99 examples/s]Tokenizing train dataset:  64%|██████▍   | 4301/6690 [00:09<00:05, 450.06 examples/s]Tokenizing train dataset:  66%|██████▋   | 4438/6690 [00:09<00:05, 448.37 examples/s]Tokenizing train dataset:  65%|██████▌   | 4379/6690 [00:09<00:05, 436.83 examples/s]Tokenizing train dataset:  65%|██████▍   | 4347/6690 [00:09<00:05, 446.75 examples/s]Tokenizing train dataset:  66%|██████▌   | 4425/6690 [00:09<00:05, 438.83 examples/s]Tokenizing train dataset:  67%|██████▋   | 4500/6690 [00:09<00:05, 429.37 examples/s]Tokenizing train dataset:  66%|██████▌   | 4407/6690 [00:09<00:05, 427.54 examples/s]Tokenizing train dataset:  68%|██████▊   | 4552/6690 [00:09<00:04, 445.50 examples/s]Tokenizing train dataset:  67%|██████▋   | 4490/6690 [00:09<00:05, 432.57 examples/s]Tokenizing train dataset:  67%|██████▋   | 4454/6690 [00:09<00:05, 435.46 examples/s]Tokenizing train dataset:  68%|██████▊   | 4537/6690 [00:10<00:04, 435.26 examples/s]Tokenizing train dataset:  69%|██████▉   | 4624/6690 [00:10<00:04, 454.44 examples/s]Tokenizing train dataset:  69%|██████▊   | 4585/6690 [00:10<00:04, 440.61 examples/s]Tokenizing train dataset:  68%|██████▊   | 4520/6690 [00:10<00:05, 432.68 examples/s]Tokenizing train dataset:  70%|██████▉   | 4673/6690 [00:10<00:04, 462.23 examples/s]Tokenizing train dataset:  69%|██████▉   | 4636/6690 [00:10<00:04, 455.84 examples/s]Tokenizing train dataset:  68%|██████▊   | 4565/6690 [00:10<00:04, 434.97 examples/s]Tokenizing train dataset:  71%|███████   | 4745/6690 [00:10<00:04, 462.69 examples/s]Tokenizing train dataset:  70%|███████   | 4684/6690 [00:10<00:04, 461.02 examples/s]Tokenizing train dataset:  69%|██████▉   | 4617/6690 [00:10<00:04, 454.82 examples/s]Tokenizing train dataset:  71%|███████   | 4733/6690 [00:10<00:04, 463.06 examples/s]Tokenizing train dataset:  70%|██████▉   | 4666/6690 [00:10<00:04, 459.60 examples/s]Tokenizing train dataset:  72%|███████▏  | 4806/6690 [00:10<00:04, 441.71 examples/s]Tokenizing train dataset:  73%|███████▎  | 4853/6690 [00:10<00:04, 447.22 examples/s]Tokenizing train dataset:  72%|███████▏  | 4792/6690 [00:10<00:04, 433.85 examples/s]Tokenizing train dataset:  71%|███████   | 4737/6690 [00:10<00:04, 462.55 examples/s]Tokenizing train dataset:  72%|███████▏  | 4842/6690 [00:10<00:04, 448.30 examples/s]Tokenizing train dataset:  73%|███████▎  | 4916/6690 [00:10<00:04, 435.12 examples/s]Tokenizing train dataset:  72%|███████▏  | 4795/6690 [00:10<00:04, 430.13 examples/s]Tokenizing train dataset:  74%|███████▍  | 4963/6690 [00:10<00:03, 441.33 examples/s]Tokenizing train dataset:  73%|███████▎  | 4906/6690 [00:10<00:04, 436.03 examples/s]Tokenizing train dataset:  72%|███████▏  | 4845/6690 [00:10<00:04, 442.81 examples/s]Tokenizing train dataset:  74%|███████▍  | 4952/6690 [00:10<00:03, 437.49 examples/s]Tokenizing train dataset:  73%|███████▎  | 4890/6690 [00:10<00:04, 436.28 examples/s]Tokenizing train dataset:  75%|███████▌  | 5027/6690 [00:11<00:03, 433.66 examples/s]Tokenizing train dataset:  75%|███████▍  | 4997/6690 [00:11<00:03, 437.20 examples/s]Tokenizing train dataset:  74%|███████▍  | 4956/6690 [00:11<00:04, 433.48 examples/s]Tokenizing train dataset:  76%|███████▌  | 5092/6690 [00:11<00:03, 429.49 examples/s]Tokenizing train dataset:  76%|███████▌  | 5060/6690 [00:11<00:03, 427.49 examples/s]Tokenizing train dataset:  75%|███████▍  | 5000/6690 [00:11<00:03, 431.28 examples/s]Tokenizing train dataset:  77%|███████▋  | 5160/6690 [00:11<00:03, 432.60 examples/s]Tokenizing train dataset:  76%|███████▋  | 5104/6690 [00:11<00:03, 429.46 examples/s]Tokenizing train dataset:  76%|███████▌  | 5065/6690 [00:11<00:03, 427.68 examples/s]Tokenizing train dataset:  78%|███████▊  | 5210/6690 [00:11<00:03, 441.91 examples/s]Tokenizing train dataset:  77%|███████▋  | 5169/6690 [00:11<00:03, 429.22 examples/s]Tokenizing train dataset:  76%|███████▋  | 5108/6690 [00:11<00:03, 424.93 examples/s]Tokenizing train dataset:  79%|███████▊  | 5256/6690 [00:11<00:03, 443.12 examples/s]Tokenizing train dataset:  78%|███████▊  | 5217/6690 [00:11<00:03, 440.62 examples/s]Tokenizing train dataset:  77%|███████▋  | 5178/6690 [00:11<00:03, 434.16 examples/s]Tokenizing train dataset:  80%|███████▉  | 5320/6690 [00:11<00:03, 432.59 examples/s]Tokenizing train dataset:  79%|███████▉  | 5284/6690 [00:11<00:03, 439.07 examples/s]Tokenizing train dataset:  78%|███████▊  | 5223/6690 [00:11<00:03, 434.98 examples/s]Tokenizing train dataset:  80%|████████  | 5372/6690 [00:11<00:02, 450.16 examples/s]Tokenizing train dataset:  79%|███████▉  | 5269/6690 [00:11<00:03, 439.10 examples/s]Tokenizing train dataset:  81%|████████  | 5419/6690 [00:11<00:02, 449.61 examples/s]Tokenizing train dataset:  80%|███████▉  | 5350/6690 [00:11<00:03, 428.29 examples/s]Tokenizing train dataset:  82%|████████▏ | 5466/6690 [00:12<00:02, 450.28 examples/s]Tokenizing train dataset:  81%|████████  | 5404/6690 [00:12<00:02, 452.26 examples/s]Tokenizing train dataset:  80%|███████▉  | 5330/6690 [00:11<00:03, 422.89 examples/s]Tokenizing train dataset:  82%|████████▏ | 5513/6690 [00:12<00:02, 450.87 examples/s]Tokenizing train dataset:  80%|████████  | 5384/6690 [00:12<00:02, 450.97 examples/s]Tokenizing train dataset:  82%|████████▏ | 5469/6690 [00:12<00:02, 442.30 examples/s]Tokenizing train dataset:  83%|████████▎ | 5578/6690 [00:12<00:02, 439.92 examples/s]Tokenizing train dataset:  82%|████████▏ | 5516/6690 [00:12<00:02, 445.14 examples/s]Tokenizing train dataset:  81%|████████▏ | 5451/6690 [00:12<00:02, 443.09 examples/s]Tokenizing train dataset:  82%|████████▏ | 5497/6690 [00:12<00:02, 445.57 examples/s]Tokenizing train dataset:  84%|████████▍ | 5645/6690 [00:12<00:02, 437.05 examples/s]Tokenizing train dataset:  83%|████████▎ | 5580/6690 [00:12<00:02, 433.37 examples/s]Tokenizing train dataset:  85%|████████▌ | 5690/6690 [00:12<00:02, 436.02 examples/s]Tokenizing train dataset:  83%|████████▎ | 5560/6690 [00:12<00:02, 431.12 examples/s]Tokenizing train dataset:  84%|████████▍ | 5647/6690 [00:12<00:02, 434.52 examples/s]Tokenizing train dataset:  86%|████████▌ | 5738/6690 [00:12<00:02, 443.45 examples/s]Tokenizing train dataset:  85%|████████▌ | 5692/6690 [00:12<00:02, 432.76 examples/s]Tokenizing train dataset:  84%|████████▍ | 5624/6690 [00:12<00:02, 427.69 examples/s]Tokenizing train dataset:  86%|████████▋ | 5784/6690 [00:12<00:02, 443.94 examples/s]Tokenizing train dataset:  86%|████████▌ | 5740/6690 [00:12<00:02, 441.64 examples/s]Tokenizing train dataset:  85%|████████▍ | 5671/6690 [00:12<00:02, 433.51 examples/s]Tokenizing train dataset:  87%|████████▋ | 5849/6690 [00:12<00:01, 434.05 examples/s]Tokenizing train dataset:  86%|████████▋ | 5786/6690 [00:12<00:02, 441.62 examples/s]Tokenizing train dataset:  85%|████████▌ | 5715/6690 [00:12<00:02, 427.90 examples/s]Tokenizing train dataset:  88%|████████▊ | 5897/6690 [00:13<00:01, 438.27 examples/s]Tokenizing train dataset:  86%|████████▌ | 5764/6690 [00:12<00:02, 440.98 examples/s]Tokenizing train dataset:  87%|████████▋ | 5848/6690 [00:13<00:01, 430.05 examples/s]Tokenizing train dataset:  89%|████████▉ | 5944/6690 [00:13<00:01, 442.97 examples/s]Tokenizing train dataset:  87%|████████▋ | 5812/6690 [00:13<00:01, 448.18 examples/s]Tokenizing train dataset:  88%|████████▊ | 5896/6690 [00:13<00:01, 439.93 examples/s]Tokenizing train dataset:  90%|████████▉ | 6013/6690 [00:13<00:01, 444.50 examples/s]Tokenizing train dataset:  89%|████████▉ | 5944/6690 [00:13<00:01, 444.66 examples/s]Tokenizing train dataset:  88%|████████▊ | 5871/6690 [00:13<00:01, 423.36 examples/s]Tokenizing train dataset:  91%|█████████ | 6060/6690 [00:13<00:01, 444.30 examples/s]Tokenizing train dataset:  89%|████████▊ | 5923/6690 [00:13<00:01, 443.34 examples/s]Tokenizing train dataset:  90%|████████▉ | 6013/6690 [00:13<00:01, 444.67 examples/s]Tokenizing train dataset:  91%|█████████▏| 6116/6690 [00:13<00:01, 472.97 examples/s]Tokenizing train dataset:  89%|████████▉ | 5970/6690 [00:13<00:01, 444.89 examples/s]Tokenizing train dataset:  91%|█████████ | 6060/6690 [00:13<00:01, 443.81 examples/s]Tokenizing train dataset:  90%|████████▉ | 6016/6690 [00:13<00:01, 446.72 examples/s]Tokenizing train dataset:  92%|█████████▏| 6184/6690 [00:13<00:01, 462.74 examples/s]Tokenizing train dataset:  91%|█████████▏| 6116/6690 [00:13<00:01, 471.93 examples/s]Tokenizing train dataset:  91%|█████████ | 6086/6690 [00:13<00:01, 451.28 examples/s]Tokenizing train dataset:  93%|█████████▎| 6240/6690 [00:13<00:01, 426.21 examples/s]Tokenizing train dataset:  92%|█████████▏| 6184/6690 [00:13<00:01, 461.59 examples/s]Tokenizing train dataset:  92%|█████████▏| 6140/6690 [00:13<00:01, 469.40 examples/s]Tokenizing train dataset:  94%|█████████▍| 6289/6690 [00:13<00:00, 434.12 examples/s]Tokenizing train dataset:  93%|█████████▎| 6240/6690 [00:13<00:01, 425.02 examples/s]Tokenizing train dataset:  93%|█████████▎| 6204/6690 [00:13<00:01, 452.74 examples/s]Tokenizing train dataset:  95%|█████████▍| 6350/6690 [00:14<00:00, 421.31 examples/s]Tokenizing train dataset:  94%|█████████▍| 6289/6690 [00:14<00:00, 432.88 examples/s]Tokenizing train dataset:  94%|█████████▎| 6260/6690 [00:14<00:01, 415.00 examples/s]Tokenizing train dataset:  96%|█████████▌| 6420/6690 [00:14<00:00, 433.06 examples/s]Tokenizing train dataset:  95%|█████████▍| 6350/6690 [00:14<00:00, 420.08 examples/s]Tokenizing train dataset:  94%|█████████▍| 6310/6690 [00:14<00:00, 431.74 examples/s]Tokenizing train dataset:  97%|█████████▋| 6465/6690 [00:14<00:00, 434.59 examples/s]Tokenizing train dataset:  96%|█████████▌| 6420/6690 [00:14<00:00, 431.31 examples/s]Tokenizing train dataset:  97%|█████████▋| 6510/6690 [00:14<00:00, 434.53 examples/s]Tokenizing train dataset:  95%|█████████▌| 6369/6690 [00:14<00:00, 417.33 examples/s]Tokenizing train dataset:  97%|█████████▋| 6465/6690 [00:14<00:00, 432.54 examples/s]Tokenizing train dataset:  98%|█████████▊| 6554/6690 [00:14<00:00, 431.36 examples/s]Tokenizing train dataset:  96%|█████████▌| 6414/6690 [00:14<00:00, 424.84 examples/s]Tokenizing train dataset:  97%|█████████▋| 6510/6690 [00:14<00:00, 432.19 examples/s]Tokenizing train dataset:  99%|█████████▊| 6599/6690 [00:14<00:00, 433.50 examples/s]Tokenizing train dataset:  97%|█████████▋| 6459/6690 [00:14<00:00, 429.33 examples/s]Tokenizing train dataset:  98%|█████████▊| 6554/6690 [00:14<00:00, 428.75 examples/s]Tokenizing train dataset:  97%|█████████▋| 6504/6690 [00:14<00:00, 431.72 examples/s]Tokenizing train dataset: 100%|█████████▉| 6658/6690 [00:14<00:00, 416.34 examples/s]Tokenizing train dataset:  99%|█████████▊| 6599/6690 [00:14<00:00, 430.61 examples/s]Tokenizing train dataset:  98%|█████████▊| 6549/6690 [00:14<00:00, 432.91 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:14<00:00, 450.83 examples/s]
Tokenizing train dataset: 100%|█████████▉| 6658/6690 [00:14<00:00, 413.67 examples/s]Tokenizing train dataset:  99%|█████████▊| 6593/6690 [00:14<00:00, 433.37 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:14<00:00, 446.23 examples/s]
Tokenizing train dataset:  99%|█████████▉| 6651/6690 [00:14<00:00, 412.12 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 444.43 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  61%|██████    | 579/953 [00:00<00:00, 5752.33 examples/s]Extracting prompt in eval dataset:  61%|██████    | 578/953 [00:00<00:00, 5709.68 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5533.83 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5683.07 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5673.96 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5568.85 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  29%|██▉       | 274/953 [00:00<00:00, 2708.15 examples/s]Applying chat template to eval dataset:  30%|███       | 287/953 [00:00<00:00, 2835.36 examples/s]Applying chat template to eval dataset:  29%|██▉       | 280/953 [00:00<00:00, 2758.76 examples/s]Applying chat template to eval dataset:  58%|█████▊    | 554/953 [00:00<00:00, 2756.92 examples/s]Applying chat template to eval dataset:  62%|██████▏   | 589/953 [00:00<00:00, 2936.84 examples/s]Applying chat template to eval dataset:  59%|█████▉    | 566/953 [00:00<00:00, 2813.98 examples/s]Applying chat template to eval dataset:  88%|████████▊ | 840/953 [00:00<00:00, 2799.03 examples/s]Applying chat template to eval dataset:  93%|█████████▎| 887/953 [00:00<00:00, 2953.21 examples/s]Applying chat template to eval dataset:  90%|█████████ | 858/953 [00:00<00:00, 2859.66 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2920.05 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2769.47 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2825.71 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 283.40 examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 260.14 examples/s]Tokenizing eval dataset:   3%|▎         | 29/953 [00:00<00:03, 282.29 examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 251.59 examples/s]Tokenizing eval dataset:   7%|▋         | 62/953 [00:00<00:03, 236.08 examples/s]Tokenizing eval dataset:   7%|▋         | 65/953 [00:00<00:03, 245.13 examples/s]Tokenizing eval dataset:  10%|▉         | 93/953 [00:00<00:03, 249.27 examples/s]Tokenizing eval dataset:   9%|▉         | 88/953 [00:00<00:03, 241.60 examples/s]Tokenizing eval dataset:  10%|█         | 100/953 [00:00<00:03, 231.88 examples/s]Tokenizing eval dataset:  13%|█▎        | 124/953 [00:00<00:03, 226.72 examples/s]Tokenizing eval dataset:  12%|█▏        | 119/953 [00:00<00:03, 220.80 examples/s]Tokenizing eval dataset:  14%|█▍        | 133/953 [00:00<00:03, 223.72 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:03, 222.37 examples/s]Tokenizing eval dataset:  16%|█▌        | 151/953 [00:00<00:03, 211.24 examples/s]Tokenizing eval dataset:  17%|█▋        | 165/953 [00:00<00:03, 217.66 examples/s]Tokenizing eval dataset:  18%|█▊        | 173/953 [00:00<00:03, 210.10 examples/s]Tokenizing eval dataset:  20%|█▉        | 189/953 [00:00<00:03, 213.10 examples/s]Tokenizing eval dataset:  21%|██        | 196/953 [00:00<00:03, 210.40 examples/s]Tokenizing eval dataset:  22%|██▏       | 213/953 [00:00<00:03, 217.82 examples/s]Tokenizing eval dataset:  22%|██▏       | 207/953 [00:00<00:03, 212.21 examples/s]Tokenizing eval dataset:  24%|██▎       | 225/953 [00:00<00:03, 225.28 examples/s]Tokenizing eval dataset:  28%|██▊       | 263/953 [00:01<00:02, 290.57 examples/s]Tokenizing eval dataset:  26%|██▌       | 246/953 [00:01<00:02, 254.82 examples/s]Tokenizing eval dataset:  30%|██▉       | 283/953 [00:01<00:02, 310.66 examples/s]Tokenizing eval dataset:  34%|███▎      | 320/953 [00:01<00:01, 365.07 examples/s]Tokenizing eval dataset:  32%|███▏      | 303/953 [00:01<00:01, 334.54 examples/s]Tokenizing eval dataset:  35%|███▌      | 334/953 [00:01<00:01, 361.05 examples/s]Tokenizing eval dataset:  39%|███▊      | 369/953 [00:01<00:01, 396.57 examples/s]Tokenizing eval dataset:  37%|███▋      | 354/953 [00:01<00:01, 380.95 examples/s]Tokenizing eval dataset:  40%|███▉      | 380/953 [00:01<00:01, 386.50 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 439.01 examples/s]Tokenizing eval dataset:  42%|████▏     | 397/953 [00:01<00:01, 392.94 examples/s]Tokenizing eval dataset:  45%|████▌     | 433/953 [00:01<00:01, 423.96 examples/s]Tokenizing eval dataset:  50%|█████     | 479/953 [00:01<00:01, 461.49 examples/s]Tokenizing eval dataset:  48%|████▊     | 461/953 [00:01<00:01, 458.44 examples/s]Tokenizing eval dataset:  51%|█████     | 482/953 [00:01<00:01, 441.18 examples/s]Tokenizing eval dataset:  56%|█████▌    | 532/953 [00:01<00:00, 474.02 examples/s]Tokenizing eval dataset:  56%|█████▌    | 530/953 [00:01<00:00, 452.52 examples/s]Tokenizing eval dataset:  56%|█████▌    | 530/953 [00:01<00:00, 450.90 examples/s]Tokenizing eval dataset:  61%|██████▏   | 585/953 [00:01<00:00, 488.67 examples/s]Tokenizing eval dataset:  61%|██████    | 583/953 [00:01<00:00, 468.66 examples/s]Tokenizing eval dataset:  61%|██████    | 583/953 [00:01<00:00, 469.19 examples/s]Tokenizing eval dataset:  67%|██████▋   | 638/953 [00:01<00:00, 497.74 examples/s]Tokenizing eval dataset:  66%|██████▋   | 632/953 [00:01<00:00, 472.05 examples/s]Tokenizing eval dataset:  67%|██████▋   | 638/953 [00:01<00:00, 485.05 examples/s]Tokenizing eval dataset:  74%|███████▍  | 709/953 [00:01<00:00, 484.17 examples/s]Tokenizing eval dataset:  72%|███████▏  | 688/953 [00:01<00:00, 485.48 examples/s]Tokenizing eval dataset:  73%|███████▎  | 697/953 [00:01<00:00, 452.59 examples/s]Tokenizing eval dataset:  82%|████████▏ | 777/953 [00:02<00:00, 465.42 examples/s]Tokenizing eval dataset:  78%|███████▊  | 747/953 [00:02<00:00, 445.53 examples/s]Tokenizing eval dataset:  80%|███████▉  | 758/953 [00:02<00:00, 434.53 examples/s]Tokenizing eval dataset:  87%|████████▋ | 832/953 [00:02<00:00, 430.23 examples/s]Tokenizing eval dataset:  85%|████████▍ | 806/953 [00:02<00:00, 424.47 examples/s]Tokenizing eval dataset:  86%|████████▌ | 816/953 [00:02<00:00, 411.92 examples/s]Tokenizing eval dataset:  94%|█████████▍| 894/953 [00:02<00:00, 420.19 examples/s]Tokenizing eval dataset:  91%|█████████ | 863/953 [00:02<00:00, 406.93 examples/s]Tokenizing eval dataset:  92%|█████████▏| 873/953 [00:02<00:00, 398.52 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 407.16 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 374.72 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  97%|█████████▋| 920/953 [00:02<00:00, 395.91 examples/s]Tokenizing eval dataset:  98%|█████████▊| 936/953 [00:02<00:00, 401.09 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 365.41 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 360.47 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4327292442321777 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3409929275512695 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.342754602432251 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.317810297012329 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[rank11]:[E612 20:59:57.320794271 ProcessGroupNCCL.cpp:552] [Rank 11] Collective WorkNCCL(SeqNum=3, OpType=ALLGATHER, NumelIn=1, NumelOut=12, Timeout(ms)=1800000) raised the following async exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgressOpt: Call to recv from 10.210.3.85<37259> failed : Broken pipe
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f794896c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f78f70211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f78f702964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f78f702b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f78f702c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f79494db5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f794b283ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f794b315a40 in /lib/x86_64-linux-gnu/libc.so.6)

[rank11]:[E612 20:59:57.324245595 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 11]  failure detected by watchdog at work sequence id: 3 PG status: last enqueued work: 4, last completed work: 2
[rank11]:[E612 20:59:57.324255335 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank11]: Traceback (most recent call last):
[rank11]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 235, in <module>
[rank11]:     main(get_train_data(CURRICULUM_STAGE), val_data, args.rank, args.learning_rate, args.total_epochs, args.beta, CURRICULUM_STAGE)
[rank11]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 211, in main
[rank11]:     dpo_trainer.train()
[rank11]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank11]:     return inner_training_loop(
[rank11]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank11]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank11]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3736, in training_step
[rank11]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1356, in compute_loss
[rank11]:     loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1315, in get_batch_loss_metrics
[rank11]:     metrics[f"{prefix}rewards/chosen"] = self.accelerator.gather_for_metrics(chosen_rewards).mean().item()
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2583, in gather_for_metrics
[rank11]:     data = self.gather(input_data)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2539, in gather
[rank11]:     return gather(tensor)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 384, in wrapper
[rank11]:     output = gather_object([shapes])
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 459, in gather_object
[rank11]:     return _gpu_gather_object(object)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 440, in _gpu_gather_object
[rank11]:     torch.distributed.all_gather_object(output_objects, object)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank11]:     return func(*args, **kwargs)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3052, in all_gather_object
[rank11]:     object_list[i] = _tensor_to_object(tensor, tensor_size, group)
[rank11]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2961, in _tensor_to_object
[rank11]:     return _unpickler(io.BytesIO(buf)).load()
[rank11]: EOFError: Ran out of input
[rank11]:[E612 20:59:58.964869103 ProcessGroupNCCL.cpp:681] [Rank 11] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank11]:[E612 20:59:58.964884043 ProcessGroupNCCL.cpp:695] [Rank 11] To avoid data inconsistency, we are taking the entire process down.
[rank11]:[E612 20:59:58.964913553 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 11] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgressOpt: Call to recv from 10.210.3.85<37259> failed : Broken pipe
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f794896c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f78f70211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f78f702964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f78f702b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f78f702c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f79494db5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f794b283ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f794b315a40 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 11] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgressOpt: Call to recv from 10.210.3.85<37259> failed : Broken pipe
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f794896c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f78f70211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f78f702964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f78f702b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f78f702c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f79494db5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f794b283ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f794b315a40 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f794896c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f78f6c876fc in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f79494db5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f794b283ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126a40 (0x7f794b315a40 in /lib/x86_64-linux-gnu/libc.so.6)

[rank8]:[E612 20:59:58.141438781 ProcessGroupNCCL.cpp:552] [Rank 8] Collective WorkNCCL(SeqNum=4, OpType=ALLGATHER, NumelIn=20, NumelOut=240, Timeout(ms)=1800000) raised the following async exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn02.vega.pri<60500>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f4cb6f6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f4c656211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f4c6562964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f4c6562b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f4c6562c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f4cb76e15c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f4cb9791ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f4cb9823a40 in /lib/x86_64-linux-gnu/libc.so.6)

[rank8]:[E612 20:59:58.142982101 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 8]  failure detected by watchdog at work sequence id: 4 PG status: last enqueued work: 4, last completed work: 3
[rank8]:[E612 20:59:58.142990281 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank8]: Traceback (most recent call last):
[rank8]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 235, in <module>
[rank8]:     main(get_train_data(CURRICULUM_STAGE), val_data, args.rank, args.learning_rate, args.total_epochs, args.beta, CURRICULUM_STAGE)
[rank8]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 211, in main
[rank8]:     dpo_trainer.train()
[rank8]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank8]:     return inner_training_loop(
[rank8]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank8]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank8]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3736, in training_step
[rank8]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1356, in compute_loss
[rank8]:     loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1315, in get_batch_loss_metrics
[rank8]:     metrics[f"{prefix}rewards/chosen"] = self.accelerator.gather_for_metrics(chosen_rewards).mean().item()
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2583, in gather_for_metrics
[rank8]:     data = self.gather(input_data)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2539, in gather
[rank8]:     return gather(tensor)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 384, in wrapper
[rank8]:     output = gather_object([shapes])
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 459, in gather_object
[rank8]:     return _gpu_gather_object(object)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 440, in _gpu_gather_object
[rank8]:     torch.distributed.all_gather_object(output_objects, object)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank8]:     return func(*args, **kwargs)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3052, in all_gather_object
[rank8]:     object_list[i] = _tensor_to_object(tensor, tensor_size, group)
[rank8]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2961, in _tensor_to_object
[rank8]:     return _unpickler(io.BytesIO(buf)).load()
[rank8]: EOFError: Ran out of input
[rank10]: Traceback (most recent call last):
[rank10]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 235, in <module>
[rank10]:     main(get_train_data(CURRICULUM_STAGE), val_data, args.rank, args.learning_rate, args.total_epochs, args.beta, CURRICULUM_STAGE)
[rank10]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 211, in main
[rank10]:     dpo_trainer.train()
[rank10]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank10]:     return inner_training_loop(
[rank10]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank10]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank10]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3736, in training_step
[rank10]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1356, in compute_loss
[rank10]:     loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1315, in get_batch_loss_metrics
[rank10]:     metrics[f"{prefix}rewards/chosen"] = self.accelerator.gather_for_metrics(chosen_rewards).mean().item()
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2583, in gather_for_metrics
[rank10]:     data = self.gather(input_data)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2539, in gather
[rank10]:     return gather(tensor)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 384, in wrapper
[rank10]:     output = gather_object([shapes])
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 459, in gather_object
[rank10]:     return _gpu_gather_object(object)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 440, in _gpu_gather_object
[rank10]:     torch.distributed.all_gather_object(output_objects, object)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank10]:     return func(*args, **kwargs)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3052, in all_gather_object
[rank10]:     object_list[i] = _tensor_to_object(tensor, tensor_size, group)
[rank10]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2961, in _tensor_to_object
[rank10]:     return _unpickler(io.BytesIO(buf)).load()
[rank10]: EOFError: Ran out of input
[rank9]: Traceback (most recent call last):
[rank9]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 235, in <module>
[rank9]:     main(get_train_data(CURRICULUM_STAGE), val_data, args.rank, args.learning_rate, args.total_epochs, args.beta, CURRICULUM_STAGE)
[rank9]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 211, in main
[rank9]:     dpo_trainer.train()
[rank9]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank9]:     return inner_training_loop(
[rank9]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank9]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank9]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 3736, in training_step
[rank9]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1356, in compute_loss
[rank9]:     loss, metrics = self.get_batch_loss_metrics(model, inputs, train_eval="train")
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/trl/trainer/dpo_trainer.py", line 1315, in get_batch_loss_metrics
[rank9]:     metrics[f"{prefix}rewards/chosen"] = self.accelerator.gather_for_metrics(chosen_rewards).mean().item()
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2583, in gather_for_metrics
[rank9]:     data = self.gather(input_data)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py", line 2539, in gather
[rank9]:     return gather(tensor)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 384, in wrapper
[rank9]:     output = gather_object([shapes])
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 459, in gather_object
[rank9]:     return _gpu_gather_object(object)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py", line 440, in _gpu_gather_object
[rank9]:     torch.distributed.all_gather_object(output_objects, object)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 81, in wrapper
[rank9]:     return func(*args, **kwargs)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3052, in all_gather_object
[rank9]:     object_list[i] = _tensor_to_object(tensor, tensor_size, group)
[rank9]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2961, in _tensor_to_object
[rank9]:     return _unpickler(io.BytesIO(buf)).load()
[rank9]: EOFError: Ran out of input
W0612 20:59:58.698000 2116420 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2116612 closing signal SIGTERM
W0612 20:59:58.699000 2116420 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2116613 closing signal SIGTERM
W0612 20:59:58.700000 2116420 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2116614 closing signal SIGTERM
E0612 20:59:59.435000 2116420 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 3 (pid: 2116615) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1182, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 861, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train_curriculum.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-12_20:59:58
  host      : pm5-nod09.vega.pri
  rank      : 11 (local_rank: 3)
  exitcode  : -6 (pid: 2116615)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2116615
========================================================
