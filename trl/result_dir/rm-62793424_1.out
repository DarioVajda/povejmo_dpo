cpu-bind=MASK - gn54, task  1  0 [1392811]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 1 ---
Total Nodes: 4
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn49
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 4     --machine_rank 1     --main_process_ip gn49     --main_process_port 29500     --num_processes 16     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62793424     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.2 --curriculum_stage=1
-------------------------------------------
[2025-06-09 01:29:43,234] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0609 01:29:45.077000 1392868 torch/distributed/run.py:792] 
W0609 01:29:45.077000 1392868 torch/distributed/run.py:792] *****************************************
W0609 01:29:45.077000 1392868 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0609 01:29:45.077000 1392868 torch/distributed/run.py:792] *****************************************
[2025-06-09 01:29:50,218] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-09 01:29:50,233] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-09 01:29:50,260] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-09 01:29:50,268] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Training data of type 'bad_lang_examples':    3489
Training data of type 'short_examples':       699
Training data of type 'choose_examples':      13379
Training data of type 'bad_format_examples':  3148
**************************************************
Training data of type 'bad_lang_examples':   Training data of type 'bad_lang_examples':     34893489

Training data of type 'short_examples':      Training data of type 'short_examples':        699699

Training data of type 'choose_examples':     Training data of type 'choose_examples':       1337913379

Training data of type 'bad_format_examples': Training data of type 'bad_format_examples':   31483148

****************************************************************************************************

Training data of type 'bad_lang_examples':    3489
Training data of type 'short_examples':       699
Training data of type 'choose_examples':      13379
Training data of type 'bad_format_examples':  3148
**************************************************
Evaluation data size: 953
Evaluation data size: 953Evaluation data size:
 953
Evaluation data size: 953
Curriculum stage 0 training data size: 7336
Curriculum stage 0 training data size: 7336
Curriculum stage 0 training data size: 7336
Curriculum stage 0 training data size: 7336
Curriculum stage 1 training data size: 6689
Curriculum stage 1 training data size: 6689
Curriculum stage 1 training data size: 6689
Curriculum stage 1 training data size: 6689
Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
4e-07
Curriculum stage 2 training data size: 6690
Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
4e-07
Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
4e-07
World size: 16
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
4e-07
[2025-06-09 01:29:54,273] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 6689
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-09 01:29:54,274] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-09 01:29:54,276] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-09 01:29:54,276] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curri-0
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:12, 24.25s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.62s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.62s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:47, 23.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:48<00:48, 24.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:48<00:47, 23.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:48<00:47, 23.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:10<00:23, 23.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:11<00:23, 23.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:11<00:23, 23.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:11<00:23, 23.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.18s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.18s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.18s/it]
Loaded model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
[rank5]:[W609 01:31:27.812330085 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W609 01:31:27.925827893 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W609 01:31:27.934279849 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6689 [00:00<00:01, 5610.15 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1153/6689 [00:00<00:00, 5717.61 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1732/6689 [00:00<00:00, 5731.62 examples/s]Extracting prompt in train dataset:  35%|███▍      | 2308/6689 [00:00<00:00, 5738.73 examples/s]Extracting prompt in train dataset:  47%|████▋     | 3170/6689 [00:00<00:00, 5728.42 examples/s]Extracting prompt in train dataset:  60%|██████    | 4020/6689 [00:00<00:00, 5687.05 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 4600/6689 [00:00<00:00, 5699.20 examples/s]Extracting prompt in train dataset:  77%|███████▋  | 5180/6689 [00:00<00:00, 5710.77 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 5760/6689 [00:01<00:00, 5719.07 examples/s]Extracting prompt in train dataset:  99%|█████████▊| 6590/6689 [00:01<00:00, 5629.56 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5621.08 examples/s]
Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 298/6689 [00:00<00:02, 2947.36 examples/s]Applying chat template to train dataset:   9%|▉         | 623/6689 [00:00<00:01, 3117.89 examples/s]Applying chat template to train dataset:  14%|█▍        | 949/6689 [00:00<00:01, 3179.38 examples/s]Applying chat template to train dataset:  19%|█▉        | 1273/6689 [00:00<00:01, 3199.11 examples/s]Applying chat template to train dataset:  24%|██▍       | 1595/6689 [00:00<00:01, 3204.45 examples/s]Applying chat template to train dataset:  29%|██▊       | 1917/6689 [00:00<00:01, 3207.40 examples/s]Applying chat template to train dataset:  33%|███▎      | 2240/6689 [00:00<00:01, 3209.64 examples/s]Applying chat template to train dataset:  41%|████      | 2720/6689 [00:00<00:01, 3202.93 examples/s]Applying chat template to train dataset:  45%|████▌     | 3042/6689 [00:00<00:01, 3203.81 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3522/6689 [00:01<00:00, 3200.09 examples/s]Applying chat template to train dataset:  60%|█████▉    | 3999/6689 [00:01<00:00, 3191.97 examples/s]Applying chat template to train dataset:  65%|██████▍   | 4320/6689 [00:01<00:00, 3193.74 examples/s]Applying chat template to train dataset:  69%|██████▉   | 4641/6689 [00:01<00:00, 3197.24 examples/s]Applying chat template to train dataset:  74%|███████▍  | 4963/6689 [00:01<00:00, 3199.84 examples/s]Applying chat template to train dataset:  79%|███████▉  | 5285/6689 [00:01<00:00, 3202.63 examples/s]Applying chat template to train dataset:  84%|████████▍ | 5607/6689 [00:01<00:00, 3205.02 examples/s]Applying chat template to train dataset:  89%|████████▊ | 5928/6689 [00:01<00:00, 3204.33 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6250/6689 [00:01<00:00, 3203.27 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3147.95 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3178.14 examples/s]
Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   2%|▏         | 105/6689 [00:00<00:06, 1036.80 examples/s]Tokenizing train dataset:   3%|▎         | 223/6689 [00:00<00:05, 1117.16 examples/s]Tokenizing train dataset:   5%|▌         | 340/6689 [00:00<00:05, 1131.07 examples/s]Tokenizing train dataset:   7%|▋         | 500/6689 [00:00<00:05, 1092.66 examples/s]Tokenizing train dataset:  10%|▉         | 657/6689 [00:00<00:05, 1068.55 examples/s]Tokenizing train dataset:  12%|█▏        | 799/6689 [00:00<00:05, 1018.37 examples/s]Tokenizing train dataset:  14%|█▍        | 930/6689 [00:00<00:05, 962.94 examples/s] Tokenizing train dataset:  16%|█▌        | 1065/6689 [00:01<00:05, 940.06 examples/s]Tokenizing train dataset:  18%|█▊        | 1185/6689 [00:01<00:06, 889.81 examples/s]Tokenizing train dataset:  20%|█▉        | 1307/6689 [00:01<00:06, 857.96 examples/s]Tokenizing train dataset:  21%|██        | 1421/6689 [00:01<00:06, 821.13 examples/s]Tokenizing train dataset:  23%|██▎       | 1535/6689 [00:01<00:06, 796.25 examples/s]Tokenizing train dataset:  24%|██▍       | 1635/6689 [00:01<00:06, 753.85 examples/s]Tokenizing train dataset:  26%|██▌       | 1740/6689 [00:01<00:06, 732.59 examples/s]Tokenizing train dataset:  28%|██▊       | 1844/6689 [00:02<00:06, 715.11 examples/s]Tokenizing train dataset:  29%|██▊       | 1919/6689 [00:02<00:06, 718.88 examples/s]Tokenizing train dataset:  30%|███       | 2018/6689 [00:02<00:06, 696.35 examples/s]Tokenizing train dataset:  31%|███▏      | 2093/6689 [00:02<00:06, 706.42 examples/s]Tokenizing train dataset:  33%|███▎      | 2197/6689 [00:02<00:06, 696.79 examples/s]Tokenizing train dataset:  34%|███▍      | 2292/6689 [00:02<00:06, 666.76 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6689 [00:02<00:06, 666.72 examples/s]Tokenizing train dataset:  37%|███▋      | 2453/6689 [00:03<00:06, 646.66 examples/s]Tokenizing train dataset:  38%|███▊      | 2549/6689 [00:03<00:06, 635.26 examples/s]Tokenizing train dataset:  39%|███▉      | 2621/6689 [00:03<00:06, 653.94 examples/s]Tokenizing train dataset:  41%|████      | 2710/6689 [00:03<00:06, 629.67 examples/s]Tokenizing train dataset:  42%|████▏     | 2793/6689 [00:03<00:06, 601.43 examples/s]Tokenizing train dataset:  43%|████▎     | 2855/6689 [00:03<00:06, 601.25 examples/s]Tokenizing train dataset:  44%|████▎     | 2917/6689 [00:03<00:06, 600.98 examples/s]Tokenizing train dataset:  45%|████▍     | 3001/6689 [00:03<00:06, 581.99 examples/s]Tokenizing train dataset:  46%|████▌     | 3082/6689 [00:04<00:06, 566.54 examples/s]Tokenizing train dataset:  47%|████▋     | 3144/6689 [00:04<00:06, 575.65 examples/s]Tokenizing train dataset:  48%|████▊     | 3230/6689 [00:04<00:06, 566.59 examples/s]Tokenizing train dataset:  49%|████▉     | 3308/6689 [00:04<00:06, 547.78 examples/s]Tokenizing train dataset:  50%|█████     | 3368/6689 [00:04<00:05, 557.10 examples/s]Tokenizing train dataset:  51%|█████▏    | 3431/6689 [00:04<00:05, 570.92 examples/s]Tokenizing train dataset:  52%|█████▏    | 3494/6689 [00:04<00:05, 581.92 examples/s]Tokenizing train dataset:  53%|█████▎    | 3574/6689 [00:05<00:05, 558.66 examples/s]Tokenizing train dataset:  55%|█████▍    | 3655/6689 [00:05<00:05, 550.62 examples/s]Tokenizing train dataset:  56%|█████▌    | 3733/6689 [00:05<00:05, 536.61 examples/s]Tokenizing train dataset:  57%|█████▋    | 3788/6689 [00:05<00:05, 534.86 examples/s]Tokenizing train dataset:  57%|█████▋    | 3843/6689 [00:05<00:05, 537.00 examples/s]Tokenizing train dataset:  59%|█████▊    | 3922/6689 [00:05<00:05, 527.32 examples/s]Tokenizing train dataset:  60%|█████▉    | 3990/6689 [00:05<00:05, 498.36 examples/s]Tokenizing train dataset:  61%|██████    | 4070/6689 [00:05<00:05, 503.06 examples/s]Tokenizing train dataset:  62%|██████▏   | 4130/6689 [00:06<00:04, 523.31 examples/s]Tokenizing train dataset:  63%|██████▎   | 4185/6689 [00:06<00:04, 527.73 examples/s]Tokenizing train dataset:  63%|██████▎   | 4243/6689 [00:06<00:04, 537.35 examples/s]Tokenizing train dataset:  65%|██████▍   | 4318/6689 [00:06<00:04, 517.91 examples/s]Tokenizing train dataset:  65%|██████▌   | 4376/6689 [00:06<00:04, 530.52 examples/s]Tokenizing train dataset:  66%|██████▌   | 4430/6689 [00:06<00:04, 530.56 examples/s]Tokenizing train dataset:  67%|██████▋   | 4485/6689 [00:06<00:04, 526.99 examples/s]Tokenizing train dataset:  68%|██████▊   | 4564/6689 [00:06<00:04, 520.25 examples/s]Tokenizing train dataset:  69%|██████▉   | 4634/6689 [00:07<00:04, 498.72 examples/s]Tokenizing train dataset:  70%|███████   | 4687/6689 [00:07<00:03, 502.42 examples/s]Tokenizing train dataset:  71%|███████   | 4745/6689 [00:07<00:03, 517.20 examples/s]Tokenizing train dataset:  72%|███████▏  | 4803/6689 [00:07<00:03, 532.43 examples/s]Tokenizing train dataset:  73%|███████▎  | 4880/6689 [00:07<00:03, 520.55 examples/s]Tokenizing train dataset:  74%|███████▍  | 4952/6689 [00:07<00:03, 501.88 examples/s]Tokenizing train dataset:  75%|███████▍  | 5012/6689 [00:07<00:03, 523.51 examples/s]Tokenizing train dataset:  76%|███████▌  | 5082/6689 [00:07<00:03, 503.38 examples/s]Tokenizing train dataset:  77%|███████▋  | 5159/6689 [00:08<00:03, 501.93 examples/s]Tokenizing train dataset:  78%|███████▊  | 5212/6689 [00:08<00:02, 507.51 examples/s]Tokenizing train dataset:  79%|███████▉  | 5270/6689 [00:08<00:02, 522.03 examples/s]Tokenizing train dataset:  80%|███████▉  | 5323/6689 [00:08<00:02, 519.36 examples/s]Tokenizing train dataset:  81%|████████  | 5398/6689 [00:08<00:02, 509.51 examples/s]Tokenizing train dataset:  82%|████████▏ | 5465/6689 [00:08<00:02, 483.90 examples/s]Tokenizing train dataset:  83%|████████▎ | 5520/6689 [00:08<00:02, 494.55 examples/s]Tokenizing train dataset:  84%|████████▎ | 5597/6689 [00:08<00:02, 497.52 examples/s]Tokenizing train dataset:  84%|████████▍ | 5650/6689 [00:09<00:02, 498.45 examples/s]Tokenizing train dataset:  85%|████████▌ | 5703/6689 [00:09<00:01, 504.19 examples/s]Tokenizing train dataset:  86%|████████▌ | 5754/6689 [00:09<00:01, 502.91 examples/s]Tokenizing train dataset:  87%|████████▋ | 5806/6689 [00:09<00:01, 500.12 examples/s]Tokenizing train dataset:  88%|████████▊ | 5875/6689 [00:09<00:01, 482.57 examples/s]Tokenizing train dataset:  89%|████████▉ | 5950/6689 [00:09<00:01, 486.77 examples/s]Tokenizing train dataset:  90%|████████▉ | 6000/6689 [00:09<00:01, 487.14 examples/s]Tokenizing train dataset:  90%|█████████ | 6049/6689 [00:09<00:01, 487.73 examples/s]Tokenizing train dataset:  91%|█████████ | 6100/6689 [00:09<00:01, 488.35 examples/s]Tokenizing train dataset:  92%|█████████▏| 6163/6689 [00:10<00:00, 526.98 examples/s]Tokenizing train dataset:  93%|█████████▎| 6223/6689 [00:10<00:00, 539.10 examples/s]Tokenizing train dataset:  94%|█████████▍| 6280/6689 [00:10<00:00, 544.63 examples/s]Tokenizing train dataset:  95%|█████████▍| 6353/6689 [00:10<00:00, 512.45 examples/s]Tokenizing train dataset:  96%|█████████▌| 6417/6689 [00:10<00:00, 479.65 examples/s]Tokenizing train dataset:  97%|█████████▋| 6490/6689 [00:10<00:00, 481.42 examples/s]Tokenizing train dataset:  98%|█████████▊| 6541/6689 [00:10<00:00, 486.48 examples/s]Tokenizing train dataset:  99%|█████████▉| 6621/6689 [00:11<00:00, 497.98 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 492.42 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 599.48 examples/s]
[rank4]:[W609 01:31:42.317855398 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 569/953 [00:00<00:00, 5648.30 examples/s]Extracting prompt in train dataset:   9%|▊         | 580/6689 [00:00<00:01, 5716.11 examples/s]Extracting prompt in train dataset:   9%|▉         | 590/6689 [00:00<00:01, 5788.81 examples/s]Extracting prompt in train dataset:   9%|▉         | 590/6689 [00:00<00:01, 5776.73 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5604.95 examples/s]
Extracting prompt in train dataset:  18%|█▊        | 1180/6689 [00:00<00:00, 5816.02 examples/s]Extracting prompt in train dataset:  18%|█▊        | 1190/6689 [00:00<00:00, 5860.73 examples/s]Extracting prompt in train dataset:  18%|█▊        | 1190/6689 [00:00<00:00, 5858.18 examples/s]Extracting prompt in train dataset:  27%|██▋       | 1790/6689 [00:00<00:00, 5911.43 examples/s]Extracting prompt in train dataset:  27%|██▋       | 1790/6689 [00:00<00:00, 5902.38 examples/s]Extracting prompt in train dataset:  27%|██▋       | 1780/6689 [00:00<00:00, 5850.38 examples/s]Extracting prompt in train dataset:  35%|███▌      | 2370/6689 [00:00<00:00, 5837.46 examples/s]Extracting prompt in train dataset:  40%|███▉      | 2670/6689 [00:00<00:00, 5863.56 examples/s]Extracting prompt in train dataset:  40%|████      | 2680/6689 [00:00<00:00, 5879.31 examples/s]Extracting prompt in train dataset:  44%|████▍     | 2960/6689 [00:00<00:00, 5847.34 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  49%|████▊     | 3260/6689 [00:00<00:00, 5854.78 examples/s]Extracting prompt in train dataset:  49%|████▉     | 3270/6689 [00:00<00:00, 5870.03 examples/s]Extracting prompt in train dataset:  53%|█████▎    | 3550/6689 [00:00<00:00, 5851.97 examples/s]Applying chat template to eval dataset:  33%|███▎      | 317/953 [00:00<00:00, 3145.70 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 4127/6689 [00:00<00:00, 5815.82 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 4137/6689 [00:00<00:00, 5830.76 examples/s]Applying chat template to eval dataset:  68%|██████▊   | 648/953 [00:00<00:00, 3235.41 examples/s]Extracting prompt in train dataset:  66%|██████▌   | 4428/6689 [00:00<00:00, 5824.28 examples/s]Extracting prompt in train dataset:  70%|███████   | 4713/6689 [00:00<00:00, 5810.94 examples/s]Extracting prompt in train dataset:  71%|███████   | 4723/6689 [00:00<00:00, 5822.64 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3194.19 examples/s]
Extracting prompt in train dataset:  75%|███████▍  | 5016/6689 [00:00<00:00, 5822.40 examples/s]Extracting prompt in train dataset:  79%|███████▉  | 5300/6689 [00:00<00:00, 5811.68 examples/s]Extracting prompt in train dataset:  79%|███████▉  | 5310/6689 [00:00<00:00, 5820.01 examples/s]Extracting prompt in train dataset:  84%|████████▎ | 5600/6689 [00:00<00:00, 5815.43 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 5887/6689 [00:01<00:00, 5827.56 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 5896/6689 [00:01<00:00, 5830.20 examples/s]Extracting prompt in train dataset:  93%|█████████▎| 6188/6689 [00:01<00:00, 5833.01 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5711.01 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5707.07 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5764.37 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5755.10 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5753.52 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 325.83 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 296.32 examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:03, 277.07 examples/s]Applying chat template to train dataset:   4%|▍         | 300/6689 [00:00<00:02, 2972.59 examples/s]Applying chat template to train dataset:   5%|▍         | 306/6689 [00:00<00:02, 3033.19 examples/s]Applying chat template to train dataset:   4%|▍         | 300/6689 [00:00<00:02, 2968.64 examples/s]Applying chat template to train dataset:   9%|▉         | 630/6689 [00:00<00:01, 3158.51 examples/s]Applying chat template to train dataset:  10%|▉         | 643/6689 [00:00<00:01, 3224.87 examples/s]Applying chat template to train dataset:   9%|▉         | 630/6689 [00:00<00:01, 3148.92 examples/s]Tokenizing eval dataset:  17%|█▋        | 161/953 [00:00<00:02, 270.38 examples/s]Applying chat template to train dataset:  14%|█▍        | 960/6689 [00:00<00:01, 3214.38 examples/s]Applying chat template to train dataset:  15%|█▍        | 975/6689 [00:00<00:01, 3266.79 examples/s]Applying chat template to train dataset:  14%|█▍        | 958/6689 [00:00<00:01, 3205.10 examples/s]Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:02, 260.73 examples/s]Applying chat template to train dataset:  19%|█▉        | 1290/6689 [00:00<00:01, 3239.56 examples/s]Applying chat template to train dataset:  20%|█▉        | 1308/6689 [00:00<00:01, 3288.74 examples/s]Applying chat template to train dataset:  19%|█▉        | 1285/6689 [00:00<00:01, 3225.95 examples/s]Tokenizing eval dataset:  25%|██▍       | 234/953 [00:00<00:02, 282.70 examples/s]Applying chat template to train dataset:  26%|██▋       | 1760/6689 [00:00<00:01, 3186.19 examples/s]Applying chat template to train dataset:  27%|██▋       | 1794/6689 [00:00<00:01, 3258.35 examples/s]Applying chat template to train dataset:  26%|██▌       | 1743/6689 [00:00<00:01, 3139.77 examples/s]Tokenizing eval dataset:  32%|███▏      | 301/953 [00:00<00:01, 383.10 examples/s]Applying chat template to train dataset:  31%|███       | 2088/6689 [00:00<00:01, 3212.50 examples/s]Applying chat template to train dataset:  32%|███▏      | 2127/6689 [00:00<00:01, 3272.89 examples/s]Applying chat template to train dataset:  31%|███       | 2061/6689 [00:00<00:01, 3150.99 examples/s]Tokenizing eval dataset:  38%|███▊      | 363/953 [00:01<00:01, 440.79 examples/s]Applying chat template to train dataset:  36%|███▌      | 2413/6689 [00:00<00:01, 3209.43 examples/s]Applying chat template to train dataset:  36%|███▌      | 2380/6689 [00:00<00:01, 3159.76 examples/s]Tokenizing eval dataset:  45%|████▌     | 432/953 [00:01<00:01, 505.57 examples/s]Applying chat template to train dataset:  39%|███▉      | 2619/6689 [00:00<00:01, 3272.87 examples/s]Applying chat template to train dataset:  41%|████      | 2739/6689 [00:00<00:01, 3222.29 examples/s]Tokenizing eval dataset:  53%|█████▎    | 501/953 [00:01<00:00, 552.97 examples/s]Applying chat template to train dataset:  44%|████▍     | 2950/6689 [00:00<00:01, 3275.38 examples/s]Applying chat template to train dataset:  43%|████▎     | 2852/6689 [00:00<00:01, 3150.62 examples/s]Applying chat template to train dataset:  46%|████▌     | 3063/6689 [00:00<00:01, 3223.87 examples/s]Applying chat template to train dataset:  47%|████▋     | 3171/6689 [00:01<00:01, 3159.06 examples/s]Applying chat template to train dataset:  49%|████▉     | 3290/6689 [00:01<00:01, 3300.38 examples/s]Tokenizing eval dataset:  59%|█████▉    | 566/953 [00:01<00:00, 573.58 examples/s]Applying chat template to train dataset:  51%|█████     | 3388/6689 [00:01<00:01, 3229.27 examples/s]Applying chat template to train dataset:  52%|█████▏    | 3492/6689 [00:01<00:01, 3171.69 examples/s]Applying chat template to train dataset:  54%|█████▍    | 3630/6689 [00:01<00:00, 3319.64 examples/s]Tokenizing eval dataset:  67%|██████▋   | 635/953 [00:01<00:00, 604.62 examples/s]Applying chat template to train dataset:  58%|█████▊    | 3871/6689 [00:01<00:00, 3214.42 examples/s]Applying chat template to train dataset:  59%|█████▉    | 3958/6689 [00:01<00:00, 3141.41 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4123/6689 [00:01<00:00, 3305.31 examples/s]Tokenizing eval dataset:  76%|███████▌  | 725/953 [00:01<00:00, 595.61 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4195/6689 [00:01<00:00, 3219.76 examples/s]Applying chat template to train dataset:  64%|██████▍   | 4279/6689 [00:01<00:00, 3156.34 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4460/6689 [00:01<00:00, 3321.03 examples/s]Applying chat template to train dataset:  68%|██████▊   | 4520/6689 [00:01<00:00, 3222.67 examples/s]Tokenizing eval dataset:  84%|████████▍ | 803/953 [00:01<00:00, 560.63 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4798/6689 [00:01<00:00, 3335.25 examples/s]Applying chat template to train dataset:  69%|██████▉   | 4600/6689 [00:01<00:00, 3164.41 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4845/6689 [00:01<00:00, 3227.37 examples/s]Applying chat template to train dataset:  74%|███████▎  | 4920/6689 [00:01<00:00, 3173.51 examples/s]Tokenizing eval dataset:  92%|█████████▏| 881/953 [00:01<00:00, 539.25 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5169/6689 [00:01<00:00, 3230.50 examples/s]Applying chat template to train dataset:  79%|███████▉  | 5292/6689 [00:01<00:00, 3317.27 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5240/6689 [00:01<00:00, 3180.25 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 526.22 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 460.14 examples/s]
Applying chat template to train dataset:  85%|████████▍ | 5655/6689 [00:01<00:00, 3230.55 examples/s]Applying chat template to train dataset:  87%|████████▋ | 5786/6689 [00:01<00:00, 3306.56 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5561/6689 [00:01<00:00, 3186.71 examples/s]Applying chat template to train dataset:  89%|████████▉ | 5980/6689 [00:01<00:00, 3228.95 examples/s]Applying chat template to train dataset:  88%|████████▊ | 5881/6689 [00:01<00:00, 3188.15 examples/s]Applying chat template to train dataset:  94%|█████████▎| 6261/6689 [00:01<00:00, 3254.86 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6201/6689 [00:01<00:00, 3189.89 examples/s]Applying chat template to train dataset:  96%|█████████▋| 6450/6689 [00:02<00:00, 3188.82 examples/s]Applying chat template to train dataset:  99%|█████████▊| 6589/6689 [00:02<00:00, 3258.19 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3272.71 examples/s]
Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3202.62 examples/s]
Applying chat template to train dataset: 100%|█████████▉| 6660/6689 [00:02<00:00, 3136.48 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3153.05 examples/s]
Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   2%|▏         | 110/6689 [00:00<00:06, 1085.69 examples/s]Tokenizing train dataset:   2%|▏         | 110/6689 [00:00<00:06, 1078.69 examples/s]Tokenizing train dataset:   2%|▏         | 109/6689 [00:00<00:06, 1079.21 examples/s]Tokenizing train dataset:   3%|▎         | 234/6689 [00:00<00:05, 1166.67 examples/s]Tokenizing train dataset:   3%|▎         | 233/6689 [00:00<00:05, 1155.11 examples/s]Tokenizing train dataset:   3%|▎         | 230/6689 [00:00<00:05, 1148.56 examples/s]Tokenizing train dataset:   6%|▌         | 405/6689 [00:00<00:05, 1145.75 examples/s]Tokenizing train dataset:   5%|▌         | 346/6689 [00:00<00:05, 1147.58 examples/s]Tokenizing train dataset:   6%|▌         | 402/6689 [00:00<00:05, 1137.08 examples/s]Tokenizing train dataset:   8%|▊         | 566/6689 [00:00<00:05, 1111.49 examples/s]Tokenizing train dataset:   8%|▊         | 507/6689 [00:00<00:05, 1108.06 examples/s]Tokenizing train dataset:   8%|▊         | 562/6689 [00:00<00:05, 1100.44 examples/s]Tokenizing train dataset:  11%|█         | 715/6689 [00:00<00:05, 1060.74 examples/s]Tokenizing train dataset:  10%|▉         | 661/6689 [00:00<00:05, 1072.24 examples/s]Tokenizing train dataset:  11%|█         | 711/6689 [00:00<00:05, 1056.39 examples/s]Tokenizing train dataset:  13%|█▎        | 864/6689 [00:00<00:05, 1028.04 examples/s]Tokenizing train dataset:  12%|█▏        | 804/6689 [00:00<00:05, 1023.74 examples/s]Tokenizing train dataset:  13%|█▎        | 858/6689 [00:00<00:05, 1025.14 examples/s]Tokenizing train dataset:  15%|█▍        | 999/6689 [00:00<00:05, 980.25 examples/s] Tokenizing train dataset:  14%|█▍        | 937/6689 [00:00<00:05, 970.75 examples/s] Tokenizing train dataset:  15%|█▍        | 990/6689 [00:00<00:05, 970.89 examples/s] Tokenizing train dataset:  17%|█▋        | 1128/6689 [00:01<00:05, 938.09 examples/s]Tokenizing train dataset:  16%|█▌        | 1071/6689 [00:01<00:05, 936.67 examples/s]Tokenizing train dataset:  17%|█▋        | 1116/6689 [00:01<00:06, 927.23 examples/s]Tokenizing train dataset:  19%|█▊        | 1246/6689 [00:01<00:06, 887.36 examples/s]Tokenizing train dataset:  18%|█▊        | 1192/6689 [00:01<00:06, 890.03 examples/s]Tokenizing train dataset:  18%|█▊        | 1233/6689 [00:01<00:06, 877.79 examples/s]Tokenizing train dataset:  20%|██        | 1364/6689 [00:01<00:06, 848.51 examples/s]Tokenizing train dataset:  20%|█▉        | 1312/6689 [00:01<00:06, 860.59 examples/s]Tokenizing train dataset:  20%|██        | 1351/6689 [00:01<00:06, 845.67 examples/s]Tokenizing train dataset:  22%|██▏       | 1464/6689 [00:01<00:06, 790.46 examples/s]Tokenizing train dataset:  21%|██▏       | 1426/6689 [00:01<00:06, 825.26 examples/s]Tokenizing train dataset:  22%|██▏       | 1461/6689 [00:01<00:06, 808.49 examples/s]Tokenizing train dataset:  23%|██▎       | 1570/6689 [00:01<00:06, 758.73 examples/s]Tokenizing train dataset:  23%|██▎       | 1539/6689 [00:01<00:06, 798.78 examples/s]Tokenizing train dataset:  23%|██▎       | 1570/6689 [00:01<00:06, 771.93 examples/s]Tokenizing train dataset:  25%|██▌       | 1684/6689 [00:01<00:06, 755.14 examples/s]Tokenizing train dataset:  24%|██▍       | 1638/6689 [00:01<00:06, 753.64 examples/s]Tokenizing train dataset:  25%|██▌       | 1681/6689 [00:01<00:06, 761.01 examples/s]Tokenizing train dataset:  27%|██▋       | 1790/6689 [00:02<00:06, 729.52 examples/s]Tokenizing train dataset:  26%|██▌       | 1744/6689 [00:01<00:06, 735.19 examples/s]Tokenizing train dataset:  27%|██▋       | 1783/6689 [00:02<00:06, 733.73 examples/s]Tokenizing train dataset:  28%|██▊       | 1894/6689 [00:02<00:06, 712.60 examples/s]Tokenizing train dataset:  28%|██▊       | 1845/6689 [00:02<00:06, 713.99 examples/s]Tokenizing train dataset:  28%|██▊       | 1887/6689 [00:02<00:06, 717.59 examples/s]Tokenizing train dataset:  29%|██▊       | 1919/6689 [00:02<00:06, 715.10 examples/s]Tokenizing train dataset:  29%|██▉       | 1959/6689 [00:02<00:06, 716.99 examples/s]Tokenizing train dataset:  30%|██▉       | 2003/6689 [00:02<00:06, 715.14 examples/s]Tokenizing train dataset:  30%|███       | 2018/6689 [00:02<00:06, 692.94 examples/s]Tokenizing train dataset:  31%|███       | 2064/6689 [00:02<00:06, 707.00 examples/s]Tokenizing train dataset:  32%|███▏      | 2115/6689 [00:02<00:06, 718.30 examples/s]Tokenizing train dataset:  31%|███▏      | 2091/6689 [00:02<00:06, 700.91 examples/s]Tokenizing train dataset:  32%|███▏      | 2138/6689 [00:02<00:06, 712.52 examples/s]Tokenizing train dataset:  33%|███▎      | 2211/6689 [00:02<00:06, 692.69 examples/s]Tokenizing train dataset:  33%|███▎      | 2193/6689 [00:02<00:06, 690.09 examples/s]Tokenizing train dataset:  33%|███▎      | 2232/6689 [00:02<00:06, 677.22 examples/s]Tokenizing train dataset:  34%|███▍      | 2306/6689 [00:02<00:06, 671.42 examples/s]Tokenizing train dataset:  34%|███▍      | 2285/6689 [00:02<00:06, 661.93 examples/s]Tokenizing train dataset:  35%|███▍      | 2336/6689 [00:02<00:06, 677.78 examples/s]Tokenizing train dataset:  36%|███▌      | 2376/6689 [00:02<00:06, 676.92 examples/s]Tokenizing train dataset:  35%|███▌      | 2354/6689 [00:02<00:06, 664.12 examples/s]Tokenizing train dataset:  36%|███▋      | 2431/6689 [00:02<00:06, 660.65 examples/s]Tokenizing train dataset:  37%|███▋      | 2464/6689 [00:03<00:06, 645.07 examples/s]Tokenizing train dataset:  37%|███▋      | 2449/6689 [00:03<00:06, 644.33 examples/s]Tokenizing train dataset:  38%|███▊      | 2530/6689 [00:03<00:06, 639.23 examples/s]Tokenizing train dataset:  38%|███▊      | 2519/6689 [00:03<00:06, 635.24 examples/s]Tokenizing train dataset:  39%|███▉      | 2600/6689 [00:03<00:06, 652.68 examples/s]Tokenizing train dataset:  39%|███▊      | 2587/6689 [00:03<00:06, 644.93 examples/s]Tokenizing train dataset:  38%|███▊      | 2540/6689 [00:03<00:06, 625.77 examples/s]Tokenizing train dataset:  40%|███▉      | 2669/6689 [00:03<00:06, 661.59 examples/s]Tokenizing train dataset:  40%|███▉      | 2656/6689 [00:03<00:06, 653.61 examples/s]Tokenizing train dataset:  39%|███▉      | 2610/6689 [00:03<00:06, 639.91 examples/s]Tokenizing train dataset:  40%|████      | 2677/6689 [00:03<00:06, 645.86 examples/s]Tokenizing train dataset:  41%|████      | 2749/6689 [00:03<00:06, 612.92 examples/s]Tokenizing train dataset:  41%|████      | 2740/6689 [00:03<00:06, 613.29 examples/s]Tokenizing train dataset:  41%|████      | 2759/6689 [00:03<00:06, 606.33 examples/s]Tokenizing train dataset:  43%|████▎     | 2843/6689 [00:03<00:06, 611.21 examples/s]Tokenizing train dataset:  42%|████▏     | 2832/6689 [00:03<00:06, 609.26 examples/s]Tokenizing train dataset:  43%|████▎     | 2850/6689 [00:03<00:06, 598.91 examples/s]Tokenizing train dataset:  44%|████▍     | 2930/6689 [00:03<00:06, 599.58 examples/s]Tokenizing train dataset:  44%|████▎     | 2923/6689 [00:03<00:06, 601.51 examples/s]Tokenizing train dataset:  44%|████▎     | 2912/6689 [00:03<00:06, 598.73 examples/s]Tokenizing train dataset:  45%|████▌     | 3015/6689 [00:03<00:06, 585.72 examples/s]Tokenizing train dataset:  45%|████▍     | 3010/6689 [00:03<00:06, 586.93 examples/s]Tokenizing train dataset:  45%|████▍     | 2993/6689 [00:03<00:06, 576.99 examples/s]Tokenizing train dataset:  46%|████▋     | 3102/6689 [00:04<00:06, 581.12 examples/s]Tokenizing train dataset:  46%|████▋     | 3094/6689 [00:04<00:06, 576.89 examples/s]Tokenizing train dataset:  46%|████▌     | 3078/6689 [00:04<00:06, 569.01 examples/s]Tokenizing train dataset:  47%|████▋     | 3161/6689 [00:04<00:06, 579.81 examples/s]Tokenizing train dataset:  47%|████▋     | 3153/6689 [00:04<00:06, 578.53 examples/s]Tokenizing train dataset:  47%|████▋     | 3136/6689 [00:04<00:06, 570.57 examples/s]Tokenizing train dataset:  48%|████▊     | 3221/6689 [00:04<00:05, 582.37 examples/s]Tokenizing train dataset:  48%|████▊     | 3213/6689 [00:04<00:06, 579.33 examples/s]Tokenizing train dataset:  48%|████▊     | 3196/6689 [00:04<00:06, 575.33 examples/s]Tokenizing train dataset:  49%|████▉     | 3299/6689 [00:04<00:06, 555.41 examples/s]Tokenizing train dataset:  49%|████▉     | 3290/6689 [00:04<00:06, 553.79 examples/s]Tokenizing train dataset:  49%|████▉     | 3275/6689 [00:04<00:06, 554.45 examples/s]Tokenizing train dataset:  50%|█████     | 3358/6689 [00:04<00:05, 559.04 examples/s]Tokenizing train dataset:  50%|█████     | 3346/6689 [00:04<00:06, 554.38 examples/s]Tokenizing train dataset:  51%|█████     | 3424/6689 [00:04<00:05, 580.89 examples/s]Tokenizing train dataset:  51%|█████     | 3411/6689 [00:04<00:05, 576.50 examples/s]Tokenizing train dataset:  50%|█████     | 3359/6689 [00:04<00:06, 552.14 examples/s]Tokenizing train dataset:  52%|█████▏    | 3487/6689 [00:04<00:05, 592.07 examples/s]Tokenizing train dataset:  51%|█████     | 3423/6689 [00:04<00:05, 571.02 examples/s]Tokenizing train dataset:  52%|█████▏    | 3504/6689 [00:04<00:05, 585.85 examples/s]Tokenizing train dataset:  52%|█████▏    | 3487/6689 [00:04<00:05, 584.13 examples/s]Tokenizing train dataset:  53%|█████▎    | 3571/6689 [00:04<00:05, 574.09 examples/s]Tokenizing train dataset:  54%|█████▎    | 3580/6689 [00:04<00:05, 556.54 examples/s]Tokenizing train dataset:  53%|█████▎    | 3570/6689 [00:04<00:05, 569.19 examples/s]Tokenizing train dataset:  55%|█████▍    | 3650/6689 [00:05<00:05, 552.88 examples/s]Tokenizing train dataset:  55%|█████▍    | 3662/6689 [00:05<00:05, 549.74 examples/s]Tokenizing train dataset:  55%|█████▍    | 3648/6689 [00:05<00:05, 550.19 examples/s]Tokenizing train dataset:  56%|█████▌    | 3731/6689 [00:05<00:05, 541.04 examples/s]Tokenizing train dataset:  56%|█████▌    | 3741/6689 [00:05<00:05, 536.68 examples/s]Tokenizing train dataset:  57%|█████▋    | 3789/6689 [00:05<00:05, 541.25 examples/s]Tokenizing train dataset:  56%|█████▌    | 3724/6689 [00:05<00:05, 534.69 examples/s]Tokenizing train dataset:  57%|█████▋    | 3797/6689 [00:05<00:05, 539.44 examples/s]Tokenizing train dataset:  57%|█████▋    | 3846/6689 [00:05<00:05, 544.37 examples/s]Tokenizing train dataset:  57%|█████▋    | 3780/6689 [00:05<00:05, 534.61 examples/s]Tokenizing train dataset:  57%|█████▋    | 3839/6689 [00:05<00:05, 546.01 examples/s]Tokenizing train dataset:  58%|█████▊    | 3877/6689 [00:05<00:05, 530.21 examples/s]Tokenizing train dataset:  59%|█████▊    | 3926/6689 [00:05<00:05, 535.91 examples/s]Tokenizing train dataset:  59%|█████▉    | 3933/6689 [00:05<00:05, 534.56 examples/s]Tokenizing train dataset:  59%|█████▊    | 3918/6689 [00:05<00:05, 534.86 examples/s]Tokenizing train dataset:  60%|█████▉    | 3999/6689 [00:05<00:05, 514.72 examples/s]Tokenizing train dataset:  60%|█████▉    | 4003/6689 [00:05<00:05, 506.66 examples/s]Tokenizing train dataset:  60%|█████▉    | 3988/6689 [00:05<00:05, 505.04 examples/s]Tokenizing train dataset:  61%|██████    | 4076/6689 [00:05<00:05, 508.42 examples/s]Tokenizing train dataset:  61%|██████    | 4082/6689 [00:05<00:05, 507.98 examples/s]Tokenizing train dataset:  62%|██████▏   | 4136/6689 [00:06<00:04, 526.44 examples/s]Tokenizing train dataset:  61%|██████    | 4070/6689 [00:05<00:05, 506.45 examples/s]Tokenizing train dataset:  62%|██████▏   | 4139/6689 [00:06<00:04, 521.07 examples/s]Tokenizing train dataset:  63%|██████▎   | 4193/6689 [00:06<00:04, 536.07 examples/s]Tokenizing train dataset:  62%|██████▏   | 4131/6689 [00:06<00:04, 528.35 examples/s]Tokenizing train dataset:  63%|██████▎   | 4200/6689 [00:06<00:04, 538.09 examples/s]Tokenizing train dataset:  64%|██████▎   | 4252/6689 [00:06<00:04, 546.00 examples/s]Tokenizing train dataset:  63%|██████▎   | 4186/6689 [00:06<00:04, 530.64 examples/s]Tokenizing train dataset:  64%|██████▎   | 4255/6689 [00:06<00:04, 539.45 examples/s]Tokenizing train dataset:  63%|██████▎   | 4244/6689 [00:06<00:04, 542.76 examples/s]Tokenizing train dataset:  65%|██████▍   | 4322/6689 [00:06<00:04, 516.21 examples/s]Tokenizing train dataset:  65%|██████▍   | 4330/6689 [00:06<00:04, 519.31 examples/s]Tokenizing train dataset:  66%|██████▌   | 4383/6689 [00:06<00:04, 536.54 examples/s]Tokenizing train dataset:  65%|██████▍   | 4320/6689 [00:06<00:04, 517.57 examples/s]Tokenizing train dataset:  66%|██████▌   | 4388/6689 [00:06<00:04, 532.57 examples/s]Tokenizing train dataset:  66%|██████▋   | 4440/6689 [00:06<00:04, 539.91 examples/s]Tokenizing train dataset:  65%|██████▌   | 4378/6689 [00:06<00:04, 531.22 examples/s]Tokenizing train dataset:  67%|██████▋   | 4471/6689 [00:06<00:04, 536.78 examples/s]Tokenizing train dataset:  66%|██████▋   | 4434/6689 [00:06<00:04, 536.74 examples/s]Tokenizing train dataset:  68%|██████▊   | 4524/6689 [00:06<00:03, 542.13 examples/s]Tokenizing train dataset:  68%|██████▊   | 4526/6689 [00:06<00:04, 536.57 examples/s]Tokenizing train dataset:  68%|██████▊   | 4517/6689 [00:06<00:04, 539.93 examples/s]Tokenizing train dataset:  69%|██████▊   | 4596/6689 [00:06<00:04, 513.78 examples/s]Tokenizing train dataset:  69%|██████▊   | 4596/6689 [00:06<00:04, 509.07 examples/s]Tokenizing train dataset:  69%|██████▊   | 4588/6689 [00:06<00:04, 514.92 examples/s]Tokenizing train dataset:  70%|██████▉   | 4670/6689 [00:07<00:03, 505.50 examples/s]Tokenizing train dataset:  70%|██████▉   | 4670/6689 [00:07<00:04, 501.22 examples/s]Tokenizing train dataset:  71%|███████   | 4729/6689 [00:07<00:03, 523.41 examples/s]Tokenizing train dataset:  70%|██████▉   | 4662/6689 [00:07<00:04, 505.00 examples/s]Tokenizing train dataset:  71%|███████   | 4729/6689 [00:07<00:03, 519.83 examples/s]Tokenizing train dataset:  72%|███████▏  | 4787/6689 [00:07<00:03, 535.44 examples/s]Tokenizing train dataset:  71%|███████   | 4720/6689 [00:07<00:03, 520.94 examples/s]Tokenizing train dataset:  72%|███████▏  | 4787/6689 [00:07<00:03, 532.20 examples/s]Tokenizing train dataset:  71%|███████▏  | 4776/6689 [00:07<00:03, 527.91 examples/s]Tokenizing train dataset:  73%|███████▎  | 4867/6689 [00:07<00:03, 528.16 examples/s]Tokenizing train dataset:  73%|███████▎  | 4867/6689 [00:07<00:03, 525.09 examples/s]Tokenizing train dataset:  73%|███████▎  | 4854/6689 [00:07<00:03, 522.98 examples/s]Tokenizing train dataset:  74%|███████▍  | 4938/6689 [00:07<00:03, 505.29 examples/s]Tokenizing train dataset:  74%|███████▍  | 4938/6689 [00:07<00:03, 501.98 examples/s]Tokenizing train dataset:  75%|███████▍  | 4997/6689 [00:07<00:03, 524.69 examples/s]Tokenizing train dataset:  74%|███████▎  | 4928/6689 [00:07<00:03, 509.01 examples/s]Tokenizing train dataset:  75%|███████▍  | 4997/6689 [00:07<00:03, 521.39 examples/s]Tokenizing train dataset:  75%|███████▍  | 4984/6689 [00:07<00:03, 519.92 examples/s]Tokenizing train dataset:  76%|███████▌  | 5070/6689 [00:07<00:03, 502.99 examples/s]Tokenizing train dataset:  75%|███████▌  | 5038/6689 [00:07<00:03, 520.74 examples/s]Tokenizing train dataset:  76%|███████▌  | 5069/6689 [00:07<00:03, 502.69 examples/s]Tokenizing train dataset:  77%|███████▋  | 5152/6689 [00:07<00:03, 510.84 examples/s]Tokenizing train dataset:  76%|███████▋  | 5107/6689 [00:07<00:03, 495.47 examples/s]Tokenizing train dataset:  77%|███████▋  | 5150/6689 [00:08<00:03, 510.15 examples/s]Tokenizing train dataset:  78%|███████▊  | 5205/6689 [00:08<00:02, 511.15 examples/s]Tokenizing train dataset:  77%|███████▋  | 5162/6689 [00:08<00:03, 507.67 examples/s]Tokenizing train dataset:  78%|███████▊  | 5202/6689 [00:08<00:02, 507.68 examples/s]Tokenizing train dataset:  79%|███████▊  | 5263/6689 [00:08<00:02, 526.68 examples/s]Tokenizing train dataset:  78%|███████▊  | 5216/6689 [00:08<00:02, 509.38 examples/s]Tokenizing train dataset:  79%|███████▊  | 5261/6689 [00:08<00:02, 521.84 examples/s]Tokenizing train dataset:  80%|███████▉  | 5343/6689 [00:08<00:02, 526.18 examples/s]Tokenizing train dataset:  79%|███████▉  | 5274/6689 [00:08<00:02, 525.20 examples/s]Tokenizing train dataset:  80%|███████▉  | 5341/6689 [00:08<00:02, 523.08 examples/s]Tokenizing train dataset:  80%|███████▉  | 5331/6689 [00:08<00:02, 530.52 examples/s]Tokenizing train dataset:  81%|████████  | 5411/6689 [00:08<00:02, 497.84 examples/s]Tokenizing train dataset:  81%|████████  | 5410/6689 [00:08<00:02, 494.91 examples/s]Tokenizing train dataset:  81%|████████  | 5406/6689 [00:08<00:02, 511.84 examples/s]Tokenizing train dataset:  82%|████████▏ | 5490/6689 [00:08<00:02, 505.79 examples/s]Tokenizing train dataset:  82%|████████▏ | 5490/6689 [00:08<00:02, 500.83 examples/s]Tokenizing train dataset:  82%|████████▏ | 5479/6689 [00:08<00:02, 496.92 examples/s]Tokenizing train dataset:  83%|████████▎ | 5567/6689 [00:08<00:02, 503.92 examples/s]Tokenizing train dataset:  83%|████████▎ | 5530/6689 [00:08<00:02, 497.51 examples/s]Tokenizing train dataset:  83%|████████▎ | 5566/6689 [00:08<00:02, 500.28 examples/s]Tokenizing train dataset:  84%|████████▍ | 5623/6689 [00:08<00:02, 511.69 examples/s]Tokenizing train dataset:  84%|████████▍ | 5620/6689 [00:08<00:02, 507.06 examples/s]Tokenizing train dataset:  85%|████████▍ | 5675/6689 [00:09<00:02, 505.75 examples/s]Tokenizing train dataset:  84%|████████▍ | 5609/6689 [00:08<00:02, 504.23 examples/s]Tokenizing train dataset:  86%|████████▌ | 5729/6689 [00:09<00:01, 509.14 examples/s]Tokenizing train dataset:  85%|████████▍ | 5660/6689 [00:09<00:02, 503.76 examples/s]Tokenizing train dataset:  85%|████████▌ | 5697/6689 [00:09<00:01, 505.27 examples/s]Tokenizing train dataset:  85%|████████▌ | 5713/6689 [00:09<00:01, 503.50 examples/s]Tokenizing train dataset:  87%|████████▋ | 5806/6689 [00:09<00:01, 502.48 examples/s]Tokenizing train dataset:  86%|████████▋ | 5775/6689 [00:09<00:01, 503.93 examples/s]Tokenizing train dataset:  87%|████████▋ | 5790/6689 [00:09<00:01, 504.38 examples/s]Tokenizing train dataset:  88%|████████▊ | 5875/6689 [00:09<00:01, 487.16 examples/s]Tokenizing train dataset:  87%|████████▋ | 5846/6689 [00:09<00:01, 490.00 examples/s]Tokenizing train dataset:  88%|████████▊ | 5859/6689 [00:09<00:01, 488.15 examples/s]Tokenizing train dataset:  89%|████████▉ | 5953/6689 [00:09<00:01, 493.47 examples/s]Tokenizing train dataset:  88%|████████▊ | 5916/6689 [00:09<00:01, 480.67 examples/s]Tokenizing train dataset:  90%|████████▉ | 6003/6689 [00:09<00:01, 489.49 examples/s]Tokenizing train dataset:  89%|████████▊ | 5934/6689 [00:09<00:01, 487.01 examples/s]Tokenizing train dataset:  89%|████████▉ | 5972/6689 [00:09<00:01, 496.27 examples/s]Tokenizing train dataset:  90%|█████████ | 6053/6689 [00:09<00:01, 490.17 examples/s]Tokenizing train dataset:  90%|████████▉ | 5989/6689 [00:09<00:01, 495.78 examples/s]Tokenizing train dataset:  90%|█████████ | 6045/6689 [00:09<00:01, 485.46 examples/s]Tokenizing train dataset:  91%|█████████▏| 6105/6689 [00:09<00:01, 494.74 examples/s]Tokenizing train dataset:  91%|█████████ | 6060/6689 [00:09<00:01, 485.74 examples/s]Tokenizing train dataset:  91%|█████████ | 6099/6689 [00:09<00:01, 494.27 examples/s]Tokenizing train dataset:  92%|█████████▏| 6170/6689 [00:09<00:00, 533.98 examples/s]Tokenizing train dataset:  91%|█████████▏| 6112/6689 [00:09<00:01, 492.50 examples/s]Tokenizing train dataset:  92%|█████████▏| 6160/6689 [00:10<00:01, 521.23 examples/s]Tokenizing train dataset:  93%|█████████▎| 6228/6689 [00:10<00:00, 544.08 examples/s]Tokenizing train dataset:  92%|█████████▏| 6178/6689 [00:10<00:00, 533.30 examples/s]Tokenizing train dataset:  93%|█████████▎| 6219/6689 [00:10<00:00, 535.93 examples/s]Tokenizing train dataset:  94%|█████████▍| 6286/6689 [00:10<00:00, 551.03 examples/s]Tokenizing train dataset:  93%|█████████▎| 6236/6689 [00:10<00:00, 542.48 examples/s]Tokenizing train dataset:  94%|█████████▍| 6279/6689 [00:10<00:00, 545.08 examples/s]Tokenizing train dataset:  95%|█████████▌| 6358/6689 [00:10<00:00, 515.21 examples/s]Tokenizing train dataset:  94%|█████████▍| 6317/6689 [00:10<00:00, 528.57 examples/s]Tokenizing train dataset:  95%|█████████▍| 6351/6689 [00:10<00:00, 516.23 examples/s]Tokenizing train dataset:  96%|█████████▌| 6423/6689 [00:10<00:00, 481.01 examples/s]Tokenizing train dataset:  95%|█████████▌| 6385/6689 [00:10<00:00, 501.28 examples/s]Tokenizing train dataset:  96%|█████████▌| 6414/6689 [00:10<00:00, 478.86 examples/s]Tokenizing train dataset:  97%|█████████▋| 6473/6689 [00:10<00:00, 483.63 examples/s]Tokenizing train dataset:  96%|█████████▋| 6452/6689 [00:10<00:00, 481.02 examples/s]Tokenizing train dataset:  97%|█████████▋| 6490/6689 [00:10<00:00, 482.57 examples/s]Tokenizing train dataset:  98%|█████████▊| 6551/6689 [00:10<00:00, 494.91 examples/s]Tokenizing train dataset:  97%|█████████▋| 6501/6689 [00:10<00:00, 479.75 examples/s]Tokenizing train dataset:  98%|█████████▊| 6542/6689 [00:10<00:00, 486.73 examples/s]Tokenizing train dataset:  99%|█████████▉| 6630/6689 [00:10<00:00, 498.73 examples/s]Tokenizing train dataset:  98%|█████████▊| 6556/6689 [00:10<00:00, 494.05 examples/s]Tokenizing train dataset:  99%|█████████▊| 6592/6689 [00:10<00:00, 487.75 examples/s]Tokenizing train dataset:  99%|█████████▉| 6608/6689 [00:10<00:00, 497.65 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 495.55 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 606.01 examples/s]
Tokenizing train dataset:  99%|█████████▉| 6647/6689 [00:11<00:00, 501.07 examples/s]Tokenizing train dataset: 100%|█████████▉| 6659/6689 [00:11<00:00, 499.56 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 602.86 examples/s]
Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 601.89 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5553.27 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5534.33 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5509.97 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5576.91 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5534.68 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5519.41 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  32%|███▏      | 306/953 [00:00<00:00, 3030.93 examples/s]Applying chat template to eval dataset:  32%|███▏      | 308/953 [00:00<00:00, 3051.38 examples/s]Applying chat template to eval dataset:  33%|███▎      | 318/953 [00:00<00:00, 3150.61 examples/s]Applying chat template to eval dataset:  78%|███████▊  | 739/953 [00:00<00:00, 2924.91 examples/s]Applying chat template to eval dataset:  78%|███████▊  | 741/953 [00:00<00:00, 2929.29 examples/s]Applying chat template to eval dataset:  80%|████████  | 765/953 [00:00<00:00, 3008.88 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2978.37 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2908.02 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2894.65 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 286.53 examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 283.06 examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 285.76 examples/s]Tokenizing eval dataset:   7%|▋         | 68/953 [00:00<00:03, 255.37 examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 254.02 examples/s]Tokenizing eval dataset:   7%|▋         | 68/953 [00:00<00:03, 254.17 examples/s]Tokenizing eval dataset:  10%|▉         | 94/953 [00:00<00:03, 256.97 examples/s]Tokenizing eval dataset:  10%|█         | 96/953 [00:00<00:03, 255.55 examples/s]Tokenizing eval dataset:  10%|█         | 96/953 [00:00<00:03, 254.71 examples/s]Tokenizing eval dataset:  14%|█▎        | 130/953 [00:00<00:03, 244.03 examples/s]Tokenizing eval dataset:  14%|█▍        | 134/953 [00:00<00:03, 247.72 examples/s]Tokenizing eval dataset:  14%|█▍        | 132/953 [00:00<00:03, 244.89 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 240.34 examples/s]Tokenizing eval dataset:  18%|█▊        | 168/953 [00:00<00:03, 236.88 examples/s]Tokenizing eval dataset:  17%|█▋        | 166/953 [00:00<00:03, 235.93 examples/s]Tokenizing eval dataset:  20%|█▉        | 189/953 [00:00<00:03, 225.72 examples/s]Tokenizing eval dataset:  21%|██        | 201/953 [00:00<00:03, 228.13 examples/s]Tokenizing eval dataset:  21%|██        | 198/953 [00:00<00:03, 225.50 examples/s]Tokenizing eval dataset:  22%|██▏       | 212/953 [00:00<00:03, 226.32 examples/s]Tokenizing eval dataset:  25%|██▍       | 237/953 [00:00<00:02, 259.28 examples/s]Tokenizing eval dataset:  24%|██▍       | 230/953 [00:00<00:02, 245.64 examples/s]Tokenizing eval dataset:  27%|██▋       | 260/953 [00:00<00:02, 294.70 examples/s]Tokenizing eval dataset:  31%|███       | 295/953 [00:01<00:01, 337.79 examples/s]Tokenizing eval dataset:  30%|███       | 290/953 [00:01<00:01, 333.31 examples/s]Tokenizing eval dataset:  34%|███▍      | 324/953 [00:01<00:01, 388.39 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 412.95 examples/s]Tokenizing eval dataset:  37%|███▋      | 354/953 [00:01<00:01, 412.74 examples/s]Tokenizing eval dataset:  40%|████      | 385/953 [00:01<00:01, 445.59 examples/s]Tokenizing eval dataset:  43%|████▎     | 413/953 [00:01<00:01, 460.04 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 477.59 examples/s]Tokenizing eval dataset:  48%|████▊     | 461/953 [00:01<00:00, 528.48 examples/s]Tokenizing eval dataset:  50%|█████     | 480/953 [00:01<00:00, 515.73 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 526.34 examples/s]Tokenizing eval dataset:  54%|█████▍    | 519/953 [00:01<00:00, 541.33 examples/s]Tokenizing eval dataset:  58%|█████▊    | 549/953 [00:01<00:00, 562.03 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 563.83 examples/s]Tokenizing eval dataset:  61%|██████    | 583/953 [00:01<00:00, 569.16 examples/s]Tokenizing eval dataset:  64%|██████▍   | 613/953 [00:01<00:00, 582.99 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 588.01 examples/s]Tokenizing eval dataset:  68%|██████▊   | 648/953 [00:01<00:00, 588.59 examples/s]Tokenizing eval dataset:  71%|███████   | 674/953 [00:01<00:00, 585.61 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 593.71 examples/s]Tokenizing eval dataset:  74%|███████▍  | 708/953 [00:01<00:00, 585.71 examples/s]Tokenizing eval dataset:  80%|███████▉  | 759/953 [00:01<00:00, 569.74 examples/s]Tokenizing eval dataset:  81%|████████  | 774/953 [00:01<00:00, 573.81 examples/s]Tokenizing eval dataset:  83%|████████▎ | 788/953 [00:01<00:00, 557.92 examples/s]Tokenizing eval dataset:  87%|████████▋ | 832/953 [00:01<00:00, 538.77 examples/s]Tokenizing eval dataset:  89%|████████▉ | 846/953 [00:01<00:00, 539.35 examples/s]Tokenizing eval dataset:  91%|█████████ | 863/953 [00:02<00:00, 526.96 examples/s]Tokenizing eval dataset:  95%|█████████▌| 907/953 [00:02<00:00, 521.71 examples/s]Tokenizing eval dataset:  96%|█████████▌| 916/953 [00:02<00:00, 511.83 examples/s]Tokenizing eval dataset:  99%|█████████▊| 940/953 [00:02<00:00, 518.18 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 432.59 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 430.43 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 429.77 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.456815719604492 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5481863021850586 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4753410816192627 seconds
Time to load cpu_adam op: 2.4576525688171387 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank4]:[W609 02:50:10.423550128 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 1 ---
