cpu-bind=MASK - gn04, task  2  0 [2100852]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 2 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 2     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63104150     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=1e-7 --total_epochs=3 --beta=0.1 --curriculum_stage=0
-------------------------------------------
[2025-06-12 16:15:59,859] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 16:16:01.560000 2100901 torch/distributed/run.py:792] 
W0612 16:16:01.560000 2100901 torch/distributed/run.py:792] *****************************************
W0612 16:16:01.560000 2100901 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 16:16:01.560000 2100901 torch/distributed/run.py:792] *****************************************
[2025-06-12 16:16:06,636] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 16:16:06,676] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 16:16:06,693] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 16:16:06,701] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
Namespace(rank=64, learning_rate=1e-07, total_epochs=3, beta=0.1, curriculum_stage=0)
1e-07
Namespace(rank=64, learning_rate=1e-07, total_epochs=3, beta=0.1, curriculum_stage=0)
1e-07
Namespace(rank=64, learning_rate=1e-07, total_epochs=3, beta=0.1, curriculum_stage=0)
1e-07
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=1e-07, total_epochs=3, beta=0.1, curriculum_stage=0)
1e-07
World size: 12
Setting gradient accumulation steps to: 1
[2025-06-12 16:16:12,231] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 16:16:12,279] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 16:16:12,287] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 4890
Validation dataset size: 953
Steps per epoch: 305
Evaluate each 152 steps
[2025-06-12 16:16:12,291] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: cjvt/GaMS-9B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:16, 25.35s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:17, 25.73s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:17, 25.73s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:17, 25.73s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:48<00:48, 24.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:49<00:48, 24.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:49<00:48, 24.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:49<00:48, 24.37s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:24, 24.55s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:24, 24.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:24, 24.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:24, 24.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 22.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.04s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 22.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 22.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 22.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.12s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank11]:[W612 16:17:49.312282681 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s][rank9]:[W612 16:17:49.448288705 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  11%|█         | 550/4890 [00:00<00:00, 5436.33 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1110/4890 [00:00<00:00, 5504.04 examples/s]Extracting prompt in train dataset:  40%|███▉      | 1940/4890 [00:00<00:00, 5498.27 examples/s]Extracting prompt in train dataset:  51%|█████▏    | 2510/4890 [00:00<00:00, 5549.23 examples/s]Extracting prompt in train dataset:  68%|██████▊   | 3310/4890 [00:00<00:00, 5453.07 examples/s]Extracting prompt in train dataset:  79%|███████▉  | 3877/4890 [00:00<00:00, 5505.54 examples/s]Extracting prompt in train dataset:  91%|█████████ | 4450/4890 [00:00<00:00, 5564.89 examples/s]Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5211.13 examples/s]
Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   6%|▌         | 282/4890 [00:00<00:01, 2791.45 examples/s]Applying chat template to train dataset:  12%|█▏        | 589/4890 [00:00<00:01, 2947.71 examples/s]Applying chat template to train dataset:  18%|█▊        | 900/4890 [00:00<00:01, 3015.04 examples/s]Applying chat template to train dataset:  25%|██▍       | 1214/4890 [00:00<00:01, 2360.43 examples/s]Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Applying chat template to train dataset:  31%|███       | 1523/4890 [00:00<00:01, 2579.64 examples/s]Applying chat template to train dataset:  38%|███▊      | 1836/4890 [00:00<00:01, 2742.90 examples/s]Applying chat template to train dataset:  44%|████▍     | 2150/4890 [00:00<00:00, 2856.59 examples/s]Applying chat template to train dataset:  50%|█████     | 2460/4890 [00:00<00:00, 2927.82 examples/s]Applying chat template to train dataset:  57%|█████▋    | 2776/4890 [00:00<00:00, 2995.06 examples/s]Applying chat template to train dataset:  66%|██████▌   | 3226/4890 [00:01<00:00, 2993.97 examples/s]Applying chat template to train dataset:  72%|███████▏  | 3538/4890 [00:01<00:00, 3026.96 examples/s]Applying chat template to train dataset:  79%|███████▉  | 3851/4890 [00:01<00:00, 3054.76 examples/s]Applying chat template to train dataset:  85%|████████▌ | 4164/4890 [00:01<00:00, 3073.25 examples/s]Applying chat template to train dataset:  92%|█████████▏| 4493/4890 [00:01<00:00, 3132.40 examples/s]Applying chat template to train dataset:  99%|█████████▊| 4821/4890 [00:01<00:00, 3173.05 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 2952.65 examples/s]
[rank10]:[W612 16:17:52.508765435 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 41/4890 [00:00<00:12, 391.55 examples/s]Tokenizing train dataset:   2%|▏         | 88/4890 [00:00<00:14, 334.11 examples/s]Tokenizing train dataset:   3%|▎         | 131/4890 [00:00<00:15, 307.81 examples/s]Tokenizing train dataset:   3%|▎         | 163/4890 [00:00<00:15, 309.12 examples/s]Tokenizing train dataset:   4%|▍         | 210/4890 [00:00<00:15, 305.46 examples/s]Tokenizing train dataset:   5%|▌         | 245/4890 [00:00<00:14, 313.10 examples/s]Tokenizing train dataset:   6%|▌         | 280/4890 [00:00<00:14, 319.97 examples/s]Tokenizing train dataset:   6%|▋         | 314/4890 [00:00<00:14, 320.52 examples/s]Tokenizing train dataset:   7%|▋         | 362/4890 [00:01<00:14, 315.39 examples/s]Tokenizing train dataset:   8%|▊         | 397/4890 [00:01<00:14, 317.96 examples/s]Tokenizing train dataset:   9%|▉         | 430/4890 [00:01<00:14, 318.19 examples/s]Tokenizing train dataset:  10%|▉         | 474/4890 [00:01<00:14, 306.37 examples/s]Tokenizing train dataset:  11%|█         | 517/4890 [00:01<00:14, 298.46 examples/s]Tokenizing train dataset:  11%|█▏        | 551/4890 [00:01<00:14, 304.47 examples/s]Tokenizing train dataset:  12%|█▏        | 584/4890 [00:01<00:14, 305.68 examples/s]Tokenizing train dataset:  13%|█▎        | 636/4890 [00:02<00:13, 313.32 examples/s]Tokenizing train dataset:  14%|█▍        | 682/4890 [00:02<00:13, 304.35 examples/s]Tokenizing train dataset:  15%|█▍        | 729/4890 [00:02<00:13, 305.12 examples/s]Tokenizing train dataset:  16%|█▌        | 761/4890 [00:02<00:13, 305.70 examples/s]Tokenizing train dataset:  16%|█▋        | 802/4890 [00:02<00:14, 291.62 examples/s]Tokenizing train dataset:  17%|█▋        | 847/4890 [00:02<00:13, 289.56 examples/s]Tokenizing train dataset:  18%|█▊        | 879/4890 [00:02<00:13, 290.61 examples/s]Tokenizing train dataset:  19%|█▊        | 916/4890 [00:02<00:12, 307.05 examples/s]Tokenizing train dataset:  20%|█▉        | 961/4890 [00:03<00:13, 299.97 examples/s]Tokenizing train dataset:  20%|██        | 1002/4890 [00:03<00:13, 289.45 examples/s]Tokenizing train dataset:  21%|██▏       | 1044/4890 [00:03<00:13, 282.56 examples/s]Tokenizing train dataset:  22%|██▏       | 1078/4890 [00:03<00:12, 293.88 examples/s]Tokenizing train dataset:  23%|██▎       | 1111/4890 [00:03<00:12, 299.24 examples/s]Tokenizing train dataset:  24%|██▎       | 1152/4890 [00:03<00:13, 287.26 examples/s]Tokenizing train dataset:  24%|██▍       | 1183/4890 [00:03<00:12, 288.58 examples/s]Tokenizing train dataset:  25%|██▍       | 1217/4890 [00:04<00:12, 297.91 examples/s]Tokenizing train dataset:  26%|██▌       | 1251/4890 [00:04<00:11, 305.26 examples/s]Tokenizing train dataset:  26%|██▋       | 1285/4890 [00:04<00:11, 304.89 examples/s]Tokenizing train dataset:  27%|██▋       | 1319/4890 [00:04<00:11, 312.67 examples/s]Tokenizing train dataset:  28%|██▊       | 1363/4890 [00:04<00:11, 298.79 examples/s]Tokenizing train dataset:  29%|██▉       | 1408/4890 [00:04<00:11, 292.77 examples/s]Tokenizing train dataset:  29%|██▉       | 1438/4890 [00:04<00:11, 291.29 examples/s]Tokenizing train dataset:  30%|███       | 1470/4890 [00:04<00:11, 294.80 examples/s]Tokenizing train dataset:  31%|███       | 1510/4890 [00:05<00:11, 281.90 examples/s]Tokenizing train dataset:  32%|███▏      | 1547/4890 [00:05<00:11, 297.08 examples/s]Tokenizing train dataset:  33%|███▎      | 1590/4890 [00:05<00:11, 290.13 examples/s]Tokenizing train dataset:  33%|███▎      | 1623/4890 [00:05<00:11, 296.73 examples/s]Tokenizing train dataset:  34%|███▍      | 1658/4890 [00:05<00:10, 304.92 examples/s]Tokenizing train dataset:  35%|███▍      | 1699/4890 [00:05<00:09, 326.76 examples/s]Tokenizing train dataset:  36%|███▌      | 1747/4890 [00:05<00:09, 318.94 examples/s]Tokenizing train dataset:  36%|███▋      | 1781/4890 [00:05<00:09, 323.08 examples/s]Tokenizing train dataset:  37%|███▋      | 1826/4890 [00:06<00:09, 307.49 examples/s]Tokenizing train dataset:  38%|███▊      | 1872/4890 [00:06<00:10, 300.46 examples/s]Tokenizing train dataset:  39%|███▉      | 1909/4890 [00:06<00:10, 280.75 examples/s]Tokenizing train dataset:  40%|███▉      | 1951/4890 [00:06<00:10, 276.43 examples/s]Tokenizing train dataset:  41%|████      | 1992/4890 [00:06<00:10, 274.62 examples/s]Tokenizing train dataset:  41%|████▏     | 2028/4890 [00:06<00:09, 288.92 examples/s]Tokenizing train dataset:  42%|████▏     | 2062/4890 [00:06<00:09, 297.72 examples/s]Tokenizing train dataset:  43%|████▎     | 2095/4890 [00:06<00:09, 302.93 examples/s]Tokenizing train dataset:  44%|████▎     | 2128/4890 [00:07<00:08, 307.35 examples/s]Tokenizing train dataset:  44%|████▍     | 2171/4890 [00:07<00:09, 295.79 examples/s]Tokenizing train dataset:  45%|████▌     | 2217/4890 [00:07<00:09, 295.96 examples/s]Tokenizing train dataset:  46%|████▌     | 2250/4890 [00:07<00:08, 302.29 examples/s]Tokenizing train dataset:  47%|████▋     | 2298/4890 [00:07<00:08, 302.44 examples/s]Tokenizing train dataset:  48%|████▊     | 2339/4890 [00:07<00:08, 289.85 examples/s]Tokenizing train dataset:  48%|████▊     | 2369/4890 [00:07<00:08, 291.52 examples/s]Tokenizing train dataset:  49%|████▉     | 2413/4890 [00:08<00:08, 287.85 examples/s]Tokenizing train dataset:  50%|█████     | 2445/4890 [00:08<00:08, 293.02 examples/s]Tokenizing train dataset:  51%|█████     | 2491/4890 [00:08<00:08, 295.10 examples/s]Tokenizing train dataset:  52%|█████▏    | 2524/4890 [00:08<00:08, 266.96 examples/s]Tokenizing train dataset:  52%|█████▏    | 2556/4890 [00:08<00:08, 278.02 examples/s]Tokenizing train dataset:  53%|█████▎    | 2591/4890 [00:08<00:07, 291.68 examples/s]Tokenizing train dataset:  54%|█████▎    | 2626/4890 [00:08<00:07, 302.39 examples/s]Tokenizing train dataset:  54%|█████▍    | 2658/4890 [00:08<00:07, 305.27 examples/s]Tokenizing train dataset:  55%|█████▌    | 2692/4890 [00:08<00:07, 310.69 examples/s]Tokenizing train dataset:  56%|█████▌    | 2725/4890 [00:09<00:06, 310.56 examples/s]Tokenizing train dataset:  57%|█████▋    | 2767/4890 [00:09<00:07, 297.15 examples/s]Tokenizing train dataset:  57%|█████▋    | 2800/4890 [00:09<00:06, 302.01 examples/s]Tokenizing train dataset:  58%|█████▊    | 2845/4890 [00:09<00:06, 294.12 examples/s]Tokenizing train dataset:  59%|█████▉    | 2875/4890 [00:09<00:06, 294.23 examples/s]Tokenizing train dataset:  59%|█████▉    | 2905/4890 [00:09<00:06, 295.27 examples/s]Tokenizing train dataset:  60%|██████    | 2951/4890 [00:09<00:06, 296.25 examples/s]Tokenizing train dataset:  61%|██████    | 2982/4890 [00:09<00:06, 296.86 examples/s]Tokenizing train dataset:  62%|██████▏   | 3014/4890 [00:10<00:06, 295.23 examples/s]Tokenizing train dataset:  62%|██████▏   | 3044/4890 [00:10<00:06, 292.89 examples/s]Tokenizing train dataset:  63%|██████▎   | 3087/4890 [00:10<00:06, 287.23 examples/s]Tokenizing train dataset:  64%|██████▍   | 3119/4890 [00:10<00:06, 293.24 examples/s]Tokenizing train dataset:  65%|██████▍   | 3160/4890 [00:10<00:06, 281.10 examples/s]Tokenizing train dataset:  65%|██████▌   | 3195/4890 [00:10<00:05, 297.63 examples/s]Tokenizing train dataset:  66%|██████▋   | 3240/4890 [00:10<00:05, 293.79 examples/s]Tokenizing train dataset:  67%|██████▋   | 3273/4890 [00:10<00:05, 297.34 examples/s]Tokenizing train dataset:  68%|██████▊   | 3304/4890 [00:11<00:05, 298.69 examples/s]Tokenizing train dataset:  69%|██████▊   | 3350/4890 [00:11<00:05, 298.91 examples/s]Tokenizing train dataset:  69%|██████▉   | 3388/4890 [00:11<00:05, 279.99 examples/s]Tokenizing train dataset:  70%|██████▉   | 3417/4890 [00:11<00:05, 278.56 examples/s]Tokenizing train dataset:  71%|███████   | 3455/4890 [00:11<00:05, 261.73 examples/s]Tokenizing train dataset:  71%|███████▏  | 3489/4890 [00:11<00:05, 246.49 examples/s]Tokenizing train dataset:  72%|███████▏  | 3530/4890 [00:11<00:04, 281.63 examples/s]Tokenizing train dataset:  73%|███████▎  | 3561/4890 [00:12<00:04, 284.54 examples/s]Tokenizing train dataset:  74%|███████▎  | 3595/4890 [00:12<00:04, 295.79 examples/s]Tokenizing train dataset:  74%|███████▍  | 3627/4890 [00:12<00:04, 294.70 examples/s]Tokenizing train dataset:  75%|███████▍  | 3660/4890 [00:12<00:04, 301.12 examples/s]Tokenizing train dataset:  76%|███████▌  | 3702/4890 [00:12<00:04, 290.20 examples/s]Tokenizing train dataset:  77%|███████▋  | 3745/4890 [00:12<00:03, 324.76 examples/s]Tokenizing train dataset:  78%|███████▊  | 3795/4890 [00:12<00:03, 322.85 examples/s]Tokenizing train dataset:  78%|███████▊  | 3831/4890 [00:12<00:03, 331.70 examples/s]Tokenizing train dataset:  79%|███████▉  | 3878/4890 [00:12<00:03, 319.98 examples/s]Tokenizing train dataset:  80%|████████  | 3916/4890 [00:13<00:03, 297.18 examples/s]Tokenizing train dataset:  81%|████████  | 3966/4890 [00:13<00:03, 305.48 examples/s]Tokenizing train dataset:  82%|████████▏ | 4003/4890 [00:13<00:02, 320.25 examples/s]Tokenizing train dataset:  83%|████████▎ | 4038/4890 [00:13<00:02, 322.43 examples/s]Tokenizing train dataset:  84%|████████▎ | 4084/4890 [00:13<00:02, 309.54 examples/s]Tokenizing train dataset:  84%|████████▍ | 4126/4890 [00:13<00:02, 298.73 examples/s]Tokenizing train dataset:  85%|████████▌ | 4170/4890 [00:13<00:02, 289.79 examples/s]Tokenizing train dataset:  87%|████████▋ | 4240/4890 [00:14<00:01, 379.60 examples/s]Tokenizing train dataset:  89%|████████▉ | 4365/4890 [00:14<00:00, 590.34 examples/s]Tokenizing train dataset:  92%|█████████▏| 4491/4890 [00:14<00:00, 761.12 examples/s]Tokenizing train dataset:  94%|█████████▍| 4612/4890 [00:14<00:00, 880.14 examples/s]Tokenizing train dataset:  97%|█████████▋| 4733/4890 [00:14<00:00, 970.37 examples/s]Tokenizing train dataset:  99%|█████████▉| 4860/4890 [00:14<00:00, 1046.80 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 334.44 examples/s] 
[rank8]:[W612 16:18:07.561080206 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in train dataset:  12%|█▏        | 563/4890 [00:00<00:00, 5590.56 examples/s]Extracting prompt in train dataset:  11%|█▏        | 560/4890 [00:00<00:00, 5538.43 examples/s]Extracting prompt in train dataset:  11%|█▏        | 560/4890 [00:00<00:00, 5528.78 examples/s]Extracting prompt in eval dataset:  61%|██████    | 580/953 [00:00<00:00, 5689.87 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5680.86 examples/s]
Extracting prompt in train dataset:  23%|██▎       | 1130/4890 [00:00<00:00, 5632.98 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1130/4890 [00:00<00:00, 5613.18 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1130/4890 [00:00<00:00, 5614.50 examples/s]Extracting prompt in train dataset:  35%|███▍      | 1710/4890 [00:00<00:00, 5596.04 examples/s]Extracting prompt in train dataset:  35%|███▍      | 1710/4890 [00:00<00:00, 5583.26 examples/s]Extracting prompt in train dataset:  35%|███▍      | 1710/4890 [00:00<00:00, 5582.21 examples/s]Extracting prompt in train dataset:  47%|████▋     | 2280/4890 [00:00<00:00, 5628.42 examples/s]Extracting prompt in train dataset:  47%|████▋     | 2280/4890 [00:00<00:00, 5620.92 examples/s]Extracting prompt in train dataset:  47%|████▋     | 2280/4890 [00:00<00:00, 5616.10 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  63%|██████▎   | 3097/4890 [00:00<00:00, 5534.95 examples/s]Extracting prompt in train dataset:  63%|██████▎   | 3097/4890 [00:00<00:00, 5530.80 examples/s]Extracting prompt in train dataset:  63%|██████▎   | 3096/4890 [00:00<00:00, 5523.55 examples/s]Applying chat template to eval dataset:  33%|███▎      | 314/953 [00:00<00:00, 3115.03 examples/s]Extracting prompt in train dataset:  75%|███████▍  | 3664/4890 [00:00<00:00, 5565.63 examples/s]Extracting prompt in train dataset:  75%|███████▍  | 3665/4890 [00:00<00:00, 5566.58 examples/s]Extracting prompt in train dataset:  75%|███████▍  | 3663/4890 [00:00<00:00, 5554.30 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 641/953 [00:00<00:00, 3200.84 examples/s]Extracting prompt in train dataset:  87%|████████▋ | 4230/4890 [00:00<00:00, 5582.87 examples/s]Extracting prompt in train dataset:  87%|████████▋ | 4240/4890 [00:00<00:00, 5593.08 examples/s]Extracting prompt in train dataset:  87%|████████▋ | 4240/4890 [00:00<00:00, 5593.84 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3191.70 examples/s]
Extracting prompt in train dataset:  99%|█████████▉| 4835/4890 [00:00<00:00, 5720.74 examples/s]Extracting prompt in train dataset:  99%|█████████▉| 4846/4890 [00:00<00:00, 5731.36 examples/s]Extracting prompt in train dataset:  99%|█████████▉| 4850/4890 [00:00<00:00, 5732.86 examples/s]Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5604.42 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5601.41 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5592.13 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.48 examples/s]Applying chat template to train dataset:   6%|▌         | 286/4890 [00:00<00:01, 2824.51 examples/s]Applying chat template to train dataset:   6%|▌         | 279/4890 [00:00<00:01, 2763.57 examples/s]Applying chat template to train dataset:   6%|▌         | 287/4890 [00:00<00:01, 2826.38 examples/s]Applying chat template to train dataset:  12%|█▏        | 598/4890 [00:00<00:01, 2993.57 examples/s]Applying chat template to train dataset:  12%|█▏        | 584/4890 [00:00<00:01, 2929.18 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 288.53 examples/s]Applying chat template to train dataset:  12%|█▏        | 600/4890 [00:00<00:01, 2995.02 examples/s]Applying chat template to train dataset:  19%|█▊        | 910/4890 [00:00<00:01, 3048.05 examples/s]Applying chat template to train dataset:  18%|█▊        | 891/4890 [00:00<00:01, 2988.14 examples/s]Applying chat template to train dataset:  19%|█▉        | 917/4890 [00:00<00:01, 3068.44 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 277.64 examples/s]Applying chat template to train dataset:  25%|██▌       | 1225/4890 [00:00<00:01, 3085.24 examples/s]Applying chat template to train dataset:  25%|██▍       | 1200/4890 [00:00<00:01, 3024.49 examples/s]Applying chat template to train dataset:  25%|██▌       | 1233/4890 [00:00<00:01, 3101.36 examples/s]Applying chat template to train dataset:  31%|███▏      | 1536/4890 [00:00<00:01, 3092.81 examples/s]Applying chat template to train dataset:  31%|███       | 1506/4890 [00:00<00:01, 3034.07 examples/s]Applying chat template to train dataset:  32%|███▏      | 1545/4890 [00:00<00:01, 3105.11 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 268.54 examples/s]Applying chat template to train dataset:  38%|███▊      | 1852/4890 [00:00<00:00, 3110.74 examples/s]Applying chat template to train dataset:  37%|███▋      | 1816/4890 [00:00<00:01, 3053.50 examples/s]Applying chat template to train dataset:  38%|███▊      | 1862/4890 [00:00<00:00, 3124.54 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 254.38 examples/s]Applying chat template to train dataset:  44%|████▍     | 2169/4890 [00:00<00:00, 3124.89 examples/s]Applying chat template to train dataset:  43%|████▎     | 2126/4890 [00:00<00:00, 3064.06 examples/s]Applying chat template to train dataset:  45%|████▍     | 2180/4890 [00:00<00:00, 3134.04 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 270.35 examples/s]Applying chat template to train dataset:  51%|█████     | 2484/4890 [00:00<00:00, 3128.03 examples/s]Applying chat template to train dataset:  50%|████▉     | 2435/4890 [00:00<00:00, 3069.81 examples/s]Applying chat template to train dataset:  51%|█████     | 2497/4890 [00:00<00:00, 3142.59 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:00<00:01, 371.36 examples/s]Applying chat template to train dataset:  57%|█████▋    | 2800/4890 [00:00<00:00, 3132.55 examples/s]Applying chat template to train dataset:  56%|█████▌    | 2746/4890 [00:00<00:00, 3075.47 examples/s]Applying chat template to train dataset:  61%|██████    | 2967/4890 [00:00<00:00, 3090.93 examples/s]Tokenizing eval dataset:  37%|███▋      | 356/953 [00:01<00:01, 441.65 examples/s]Applying chat template to train dataset:  66%|██████▋   | 3240/4890 [00:01<00:00, 3047.29 examples/s]Applying chat template to train dataset:  65%|██████▌   | 3192/4890 [00:01<00:00, 3028.91 examples/s]Applying chat template to train dataset:  67%|██████▋   | 3281/4890 [00:01<00:00, 3101.27 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 489.52 examples/s]Applying chat template to train dataset:  73%|███████▎  | 3552/4890 [00:01<00:00, 3067.21 examples/s]Applying chat template to train dataset:  72%|███████▏  | 3500/4890 [00:01<00:00, 3036.92 examples/s]Applying chat template to train dataset:  74%|███████▎  | 3596/4890 [00:01<00:00, 3112.61 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 542.81 examples/s]Applying chat template to train dataset:  79%|███████▉  | 3870/4890 [00:01<00:00, 3087.92 examples/s]Applying chat template to train dataset:  78%|███████▊  | 3810/4890 [00:01<00:00, 3050.98 examples/s]Applying chat template to train dataset:  80%|████████  | 3912/4890 [00:01<00:00, 3122.57 examples/s]Tokenizing eval dataset:  58%|█████▊    | 556/953 [00:01<00:00, 577.00 examples/s]Applying chat template to train dataset:  86%|████████▌ | 4184/4890 [00:01<00:00, 3101.73 examples/s]Applying chat template to train dataset:  84%|████████▍ | 4120/4890 [00:01<00:00, 3058.73 examples/s]Applying chat template to train dataset:  87%|████████▋ | 4230/4890 [00:01<00:00, 3134.93 examples/s]Tokenizing eval dataset:  65%|██████▍   | 619/953 [00:01<00:00, 590.12 examples/s]Applying chat template to train dataset:  92%|█████████▏| 4505/4890 [00:01<00:00, 3130.92 examples/s]Applying chat template to train dataset:  91%|█████████ | 4429/4890 [00:01<00:00, 3064.70 examples/s]Applying chat template to train dataset:  93%|█████████▎| 4561/4890 [00:01<00:00, 3182.99 examples/s]Tokenizing eval dataset:  74%|███████▍  | 707/953 [00:01<00:00, 585.73 examples/s]Applying chat template to train dataset:  99%|█████████▉| 4831/4890 [00:01<00:00, 3164.31 examples/s]Applying chat template to train dataset:  97%|█████████▋| 4750/4890 [00:01<00:00, 3103.87 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3098.22 examples/s]
Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3125.29 examples/s]
Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3049.05 examples/s]
Tokenizing eval dataset:  82%|████████▏ | 786/953 [00:01<00:00, 558.15 examples/s]Tokenizing eval dataset:  90%|█████████ | 862/953 [00:01<00:00, 532.28 examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing eval dataset:  99%|█████████▊| 939/953 [00:02<00:00, 522.16 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 453.58 examples/s]
Tokenizing train dataset:   1%|          | 43/4890 [00:00<00:11, 406.41 examples/s]Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:11, 408.83 examples/s]Tokenizing train dataset:   1%|          | 43/4890 [00:00<00:11, 407.93 examples/s]Tokenizing train dataset:   2%|▏         | 92/4890 [00:00<00:14, 339.33 examples/s]Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:14, 340.33 examples/s]Tokenizing train dataset:   2%|▏         | 92/4890 [00:00<00:14, 339.49 examples/s]Tokenizing train dataset:   3%|▎         | 140/4890 [00:00<00:14, 325.61 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 321.62 examples/s]Tokenizing train dataset:   3%|▎         | 140/4890 [00:00<00:14, 325.73 examples/s]Tokenizing train dataset:   4%|▍         | 186/4890 [00:00<00:14, 313.94 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 309.02 examples/s]Tokenizing train dataset:   4%|▍         | 186/4890 [00:00<00:14, 313.80 examples/s]Tokenizing train dataset:   5%|▍         | 221/4890 [00:00<00:14, 320.76 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 314.42 examples/s]Tokenizing train dataset:   5%|▍         | 221/4890 [00:00<00:14, 320.34 examples/s]Tokenizing train dataset:   5%|▌         | 257/4890 [00:00<00:14, 325.13 examples/s]Tokenizing train dataset:   5%|▌         | 251/4890 [00:00<00:14, 317.40 examples/s]Tokenizing train dataset:   5%|▌         | 257/4890 [00:00<00:14, 324.81 examples/s]Tokenizing train dataset:   6%|▌         | 295/4890 [00:00<00:13, 337.00 examples/s]Tokenizing train dataset:   6%|▌         | 289/4890 [00:00<00:13, 333.40 examples/s]Tokenizing train dataset:   6%|▌         | 295/4890 [00:00<00:13, 336.85 examples/s]Tokenizing train dataset:   7%|▋         | 344/4890 [00:01<00:13, 324.80 examples/s]Tokenizing train dataset:   7%|▋         | 337/4890 [00:01<00:13, 325.62 examples/s]Tokenizing train dataset:   7%|▋         | 344/4890 [00:01<00:14, 324.66 examples/s]Tokenizing train dataset:   8%|▊         | 378/4890 [00:01<00:13, 324.45 examples/s]Tokenizing train dataset:   8%|▊         | 387/4890 [00:01<00:13, 323.71 examples/s]Tokenizing train dataset:   8%|▊         | 378/4890 [00:01<00:13, 324.32 examples/s]Tokenizing train dataset:   9%|▉         | 429/4890 [00:01<00:13, 325.40 examples/s]Tokenizing train dataset:   9%|▉         | 434/4890 [00:01<00:14, 315.20 examples/s]Tokenizing train dataset:   9%|▉         | 429/4890 [00:01<00:13, 325.22 examples/s]Tokenizing train dataset:  10%|▉         | 475/4890 [00:01<00:14, 313.24 examples/s]Tokenizing train dataset:  10%|▉         | 480/4890 [00:01<00:14, 308.52 examples/s]Tokenizing train dataset:  10%|▉         | 475/4890 [00:01<00:14, 313.13 examples/s]Tokenizing train dataset:  11%|█         | 520/4890 [00:01<00:14, 305.55 examples/s]Tokenizing train dataset:  11%|█         | 527/4890 [00:01<00:14, 307.70 examples/s]Tokenizing train dataset:  11%|█         | 520/4890 [00:01<00:14, 305.71 examples/s]Tokenizing train dataset:  11%|█▏        | 555/4890 [00:01<00:13, 312.93 examples/s]Tokenizing train dataset:  11%|█▏        | 561/4890 [00:01<00:13, 312.93 examples/s]Tokenizing train dataset:  11%|█▏        | 555/4890 [00:01<00:13, 313.26 examples/s]Tokenizing train dataset:  12%|█▏        | 600/4890 [00:01<00:14, 305.29 examples/s]Tokenizing train dataset:  12%|█▏        | 605/4890 [00:01<00:14, 302.28 examples/s]Tokenizing train dataset:  12%|█▏        | 600/4890 [00:01<00:14, 305.61 examples/s]Tokenizing train dataset:  13%|█▎        | 638/4890 [00:01<00:13, 320.34 examples/s]Tokenizing train dataset:  13%|█▎        | 644/4890 [00:02<00:13, 319.52 examples/s]Tokenizing train dataset:  13%|█▎        | 638/4890 [00:01<00:13, 320.94 examples/s]Tokenizing train dataset:  14%|█▍        | 686/4890 [00:02<00:13, 315.82 examples/s]Tokenizing train dataset:  14%|█▍        | 689/4890 [00:02<00:13, 311.20 examples/s]Tokenizing train dataset:  14%|█▍        | 686/4890 [00:02<00:13, 316.39 examples/s]Tokenizing train dataset:  15%|█▌        | 736/4890 [00:02<00:13, 316.86 examples/s]Tokenizing train dataset:  15%|█▌        | 740/4890 [00:02<00:13, 317.48 examples/s]Tokenizing train dataset:  15%|█▌        | 736/4890 [00:02<00:13, 317.44 examples/s]Tokenizing train dataset:  16%|█▌        | 780/4890 [00:02<00:13, 307.25 examples/s]Tokenizing train dataset:  16%|█▌        | 785/4890 [00:02<00:13, 307.81 examples/s]Tokenizing train dataset:  16%|█▌        | 780/4890 [00:02<00:13, 307.67 examples/s]Tokenizing train dataset:  17%|█▋        | 824/4890 [00:02<00:13, 298.79 examples/s]Tokenizing train dataset:  17%|█▋        | 827/4890 [00:02<00:13, 294.30 examples/s]Tokenizing train dataset:  17%|█▋        | 824/4890 [00:02<00:13, 299.56 examples/s]Tokenizing train dataset:  18%|█▊        | 870/4890 [00:02<00:13, 297.65 examples/s]Tokenizing train dataset:  18%|█▊        | 860/4890 [00:02<00:13, 299.39 examples/s]Tokenizing train dataset:  18%|█▊        | 870/4890 [00:02<00:13, 298.53 examples/s]Tokenizing train dataset:  18%|█▊        | 897/4890 [00:02<00:12, 315.22 examples/s]Tokenizing train dataset:  19%|█▊        | 910/4890 [00:02<00:12, 315.79 examples/s]Tokenizing train dataset:  19%|█▊        | 910/4890 [00:02<00:12, 316.61 examples/s]Tokenizing train dataset:  19%|█▉        | 940/4890 [00:03<00:13, 302.55 examples/s]Tokenizing train dataset:  20%|█▉        | 954/4890 [00:03<00:12, 305.16 examples/s]Tokenizing train dataset:  20%|█▉        | 954/4890 [00:03<00:12, 306.24 examples/s]Tokenizing train dataset:  20%|█▉        | 972/4890 [00:03<00:12, 305.81 examples/s]Tokenizing train dataset:  20%|██        | 998/4890 [00:03<00:13, 297.77 examples/s]Tokenizing train dataset:  20%|██        | 998/4890 [00:03<00:13, 298.59 examples/s]Tokenizing train dataset:  21%|██        | 1014/4890 [00:03<00:13, 290.85 examples/s]Tokenizing train dataset:  21%|██▏       | 1040/4890 [00:03<00:13, 289.06 examples/s]Tokenizing train dataset:  21%|██▏       | 1040/4890 [00:03<00:13, 289.67 examples/s]Tokenizing train dataset:  22%|██▏       | 1060/4890 [00:03<00:13, 291.45 examples/s]Tokenizing train dataset:  22%|██▏       | 1073/4890 [00:03<00:12, 296.17 examples/s]Tokenizing train dataset:  22%|██▏       | 1073/4890 [00:03<00:12, 296.78 examples/s]Tokenizing train dataset:  22%|██▏       | 1095/4890 [00:03<00:12, 304.05 examples/s]Tokenizing train dataset:  23%|██▎       | 1109/4890 [00:03<00:12, 309.17 examples/s]Tokenizing train dataset:  23%|██▎       | 1109/4890 [00:03<00:12, 309.69 examples/s]Tokenizing train dataset:  23%|██▎       | 1137/4890 [00:03<00:12, 291.34 examples/s]Tokenizing train dataset:  24%|██▎       | 1151/4890 [00:03<00:12, 296.17 examples/s]Tokenizing train dataset:  24%|██▎       | 1151/4890 [00:03<00:12, 296.79 examples/s]Tokenizing train dataset:  24%|██▍       | 1167/4890 [00:03<00:12, 291.54 examples/s]Tokenizing train dataset:  24%|██▍       | 1183/4890 [00:03<00:12, 296.39 examples/s]Tokenizing train dataset:  24%|██▍       | 1183/4890 [00:03<00:12, 296.94 examples/s]Tokenizing train dataset:  25%|██▍       | 1202/4890 [00:03<00:12, 302.45 examples/s]Tokenizing train dataset:  25%|██▍       | 1218/4890 [00:03<00:12, 304.94 examples/s]Tokenizing train dataset:  25%|██▍       | 1218/4890 [00:03<00:12, 305.44 examples/s]Tokenizing train dataset:  25%|██▌       | 1236/4890 [00:03<00:11, 308.23 examples/s]Tokenizing train dataset:  26%|██▌       | 1252/4890 [00:04<00:11, 312.93 examples/s]Tokenizing train dataset:  26%|██▌       | 1252/4890 [00:04<00:11, 313.41 examples/s]Tokenizing train dataset:  26%|██▌       | 1270/4890 [00:04<00:11, 310.30 examples/s]Tokenizing train dataset:  26%|██▋       | 1286/4890 [00:04<00:11, 310.43 examples/s]Tokenizing train dataset:  26%|██▋       | 1286/4890 [00:04<00:11, 310.84 examples/s]Tokenizing train dataset:  27%|██▋       | 1304/4890 [00:04<00:11, 314.35 examples/s]Tokenizing train dataset:  27%|██▋       | 1320/4890 [00:04<00:11, 317.17 examples/s]Tokenizing train dataset:  27%|██▋       | 1320/4890 [00:04<00:11, 317.69 examples/s]Tokenizing train dataset:  27%|██▋       | 1339/4890 [00:04<00:11, 317.76 examples/s]Tokenizing train dataset:  28%|██▊       | 1353/4890 [00:04<00:11, 313.47 examples/s]Tokenizing train dataset:  28%|██▊       | 1353/4890 [00:04<00:11, 314.30 examples/s]Tokenizing train dataset:  28%|██▊       | 1379/4890 [00:04<00:11, 297.27 examples/s]Tokenizing train dataset:  29%|██▊       | 1397/4890 [00:04<00:11, 303.30 examples/s]Tokenizing train dataset:  29%|██▊       | 1397/4890 [00:04<00:11, 304.04 examples/s]Tokenizing train dataset:  29%|██▉       | 1410/4890 [00:04<00:11, 296.86 examples/s]Tokenizing train dataset:  29%|██▉       | 1428/4890 [00:04<00:11, 302.63 examples/s]Tokenizing train dataset:  29%|██▉       | 1428/4890 [00:04<00:11, 303.32 examples/s]Tokenizing train dataset:  30%|██▉       | 1456/4890 [00:04<00:11, 297.54 examples/s]Tokenizing train dataset:  30%|███       | 1473/4890 [00:04<00:11, 297.96 examples/s]Tokenizing train dataset:  30%|███       | 1473/4890 [00:04<00:11, 298.68 examples/s]Tokenizing train dataset:  30%|███       | 1486/4890 [00:04<00:11, 293.43 examples/s]Tokenizing train dataset:  31%|███       | 1515/4890 [00:04<00:11, 289.42 examples/s]Tokenizing train dataset:  31%|███       | 1515/4890 [00:04<00:11, 290.17 examples/s]Tokenizing train dataset:  31%|███▏      | 1533/4890 [00:04<00:11, 298.98 examples/s]Tokenizing train dataset:  32%|███▏      | 1550/4890 [00:05<00:11, 297.69 examples/s]Tokenizing train dataset:  32%|███▏      | 1550/4890 [00:05<00:11, 298.65 examples/s]Tokenizing train dataset:  32%|███▏      | 1563/4890 [00:05<00:11, 297.64 examples/s]Tokenizing train dataset:  32%|███▏      | 1581/4890 [00:05<00:11, 296.62 examples/s]Tokenizing train dataset:  32%|███▏      | 1581/4890 [00:05<00:11, 297.72 examples/s]Tokenizing train dataset:  33%|███▎      | 1614/4890 [00:05<00:10, 302.09 examples/s]Tokenizing train dataset:  33%|███▎      | 1610/4890 [00:05<00:11, 297.14 examples/s]Tokenizing train dataset:  33%|███▎      | 1614/4890 [00:05<00:10, 303.34 examples/s]Tokenizing train dataset:  34%|███▎      | 1650/4890 [00:05<00:10, 314.27 examples/s]Tokenizing train dataset:  34%|███▎      | 1645/4890 [00:05<00:10, 307.03 examples/s]Tokenizing train dataset:  34%|███▎      | 1650/4890 [00:05<00:10, 315.81 examples/s]Tokenizing train dataset:  34%|███▍      | 1685/4890 [00:05<00:10, 319.61 examples/s]Tokenizing train dataset:  34%|███▍      | 1678/4890 [00:05<00:10, 310.19 examples/s]Tokenizing train dataset:  34%|███▍      | 1685/4890 [00:05<00:09, 321.18 examples/s]Tokenizing train dataset:  35%|███▌      | 1722/4890 [00:05<00:09, 329.92 examples/s]Tokenizing train dataset:  35%|███▌      | 1715/4890 [00:05<00:09, 320.76 examples/s]Tokenizing train dataset:  35%|███▌      | 1722/4890 [00:05<00:09, 331.56 examples/s]Tokenizing train dataset:  36%|███▌      | 1751/4890 [00:05<00:09, 328.93 examples/s]Tokenizing train dataset:  36%|███▋      | 1774/4890 [00:05<00:09, 334.54 examples/s]Tokenizing train dataset:  36%|███▋      | 1774/4890 [00:05<00:09, 336.22 examples/s]Tokenizing train dataset:  37%|███▋      | 1788/4890 [00:05<00:09, 338.73 examples/s]Tokenizing train dataset:  37%|███▋      | 1820/4890 [00:05<00:09, 320.11 examples/s]Tokenizing train dataset:  37%|███▋      | 1820/4890 [00:05<00:09, 321.75 examples/s]Tokenizing train dataset:  37%|███▋      | 1829/4890 [00:05<00:09, 310.25 examples/s]Tokenizing train dataset:  38%|███▊      | 1865/4890 [00:05<00:09, 310.81 examples/s]Tokenizing train dataset:  38%|███▊      | 1865/4890 [00:05<00:09, 312.35 examples/s]Tokenizing train dataset:  38%|███▊      | 1874/4890 [00:06<00:09, 304.91 examples/s]Tokenizing train dataset:  39%|███▉      | 1904/4890 [00:06<00:10, 288.16 examples/s]Tokenizing train dataset:  39%|███▉      | 1904/4890 [00:06<00:10, 289.57 examples/s]Tokenizing train dataset:  39%|███▉      | 1910/4890 [00:06<00:10, 280.79 examples/s]Tokenizing train dataset:  40%|███▉      | 1947/4890 [00:06<00:10, 282.68 examples/s]Tokenizing train dataset:  40%|███▉      | 1948/4890 [00:06<00:10, 285.52 examples/s]Tokenizing train dataset:  40%|███▉      | 1953/4890 [00:06<00:10, 279.84 examples/s]Tokenizing train dataset:  41%|████      | 1991/4890 [00:06<00:10, 282.03 examples/s]Tokenizing train dataset:  41%|████      | 1991/4890 [00:06<00:10, 283.17 examples/s]Tokenizing train dataset:  41%|████      | 1997/4890 [00:06<00:10, 281.89 examples/s]Tokenizing train dataset:  41%|████▏     | 2028/4890 [00:06<00:09, 295.54 examples/s]Tokenizing train dataset:  41%|████▏     | 2028/4890 [00:06<00:09, 296.99 examples/s]Tokenizing train dataset:  42%|████▏     | 2030/4890 [00:06<00:09, 290.79 examples/s]Tokenizing train dataset:  42%|████▏     | 2063/4890 [00:06<00:09, 304.39 examples/s]Tokenizing train dataset:  42%|████▏     | 2063/4890 [00:06<00:09, 305.94 examples/s]Tokenizing train dataset:  42%|████▏     | 2068/4890 [00:06<00:09, 310.78 examples/s]Tokenizing train dataset:  43%|████▎     | 2097/4890 [00:06<00:08, 310.53 examples/s]Tokenizing train dataset:  43%|████▎     | 2097/4890 [00:06<00:08, 312.09 examples/s]Tokenizing train dataset:  43%|████▎     | 2101/4890 [00:06<00:08, 311.74 examples/s]Tokenizing train dataset:  44%|████▎     | 2130/4890 [00:06<00:08, 309.56 examples/s]Tokenizing train dataset:  44%|████▎     | 2130/4890 [00:06<00:08, 311.14 examples/s]Tokenizing train dataset:  44%|████▍     | 2147/4890 [00:06<00:08, 308.03 examples/s]Tokenizing train dataset:  44%|████▍     | 2176/4890 [00:07<00:08, 305.04 examples/s]Tokenizing train dataset:  44%|████▍     | 2176/4890 [00:07<00:08, 306.37 examples/s]Tokenizing train dataset:  45%|████▍     | 2179/4890 [00:07<00:08, 305.40 examples/s]Tokenizing train dataset:  45%|████▌     | 2221/4890 [00:07<00:08, 298.81 examples/s]Tokenizing train dataset:  45%|████▌     | 2221/4890 [00:07<00:08, 300.02 examples/s]Tokenizing train dataset:  45%|████▌     | 2222/4890 [00:07<00:09, 296.13 examples/s]Tokenizing train dataset:  46%|████▌     | 2257/4890 [00:07<00:08, 312.55 examples/s]Tokenizing train dataset:  46%|████▌     | 2259/4890 [00:07<00:08, 313.85 examples/s]Tokenizing train dataset:  46%|████▌     | 2259/4890 [00:07<00:08, 311.48 examples/s]Tokenizing train dataset:  47%|████▋     | 2302/4890 [00:07<00:08, 306.13 examples/s]Tokenizing train dataset:  47%|████▋     | 2305/4890 [00:07<00:08, 306.97 examples/s]Tokenizing train dataset:  47%|████▋     | 2305/4890 [00:07<00:08, 304.21 examples/s]Tokenizing train dataset:  48%|████▊     | 2346/4890 [00:07<00:08, 297.77 examples/s]Tokenizing train dataset:  48%|████▊     | 2349/4890 [00:07<00:08, 297.97 examples/s]Tokenizing train dataset:  48%|████▊     | 2349/4890 [00:07<00:08, 295.02 examples/s]Tokenizing train dataset:  49%|████▉     | 2390/4890 [00:07<00:08, 292.78 examples/s]Tokenizing train dataset:  49%|████▊     | 2380/4890 [00:07<00:08, 297.67 examples/s]Tokenizing train dataset:  49%|████▊     | 2380/4890 [00:07<00:08, 294.72 examples/s]Tokenizing train dataset:  50%|████▉     | 2423/4890 [00:07<00:08, 297.51 examples/s]Tokenizing train dataset:  49%|████▉     | 2410/4890 [00:07<00:08, 293.79 examples/s]Tokenizing train dataset:  50%|████▉     | 2428/4890 [00:07<00:08, 301.01 examples/s]Tokenizing train dataset:  50%|████▉     | 2443/4890 [00:07<00:08, 297.80 examples/s]Tokenizing train dataset:  50%|█████     | 2469/4890 [00:08<00:08, 294.63 examples/s]Tokenizing train dataset:  51%|█████     | 2471/4890 [00:08<00:08, 291.99 examples/s]Tokenizing train dataset:  51%|█████     | 2500/4890 [00:08<00:08, 289.36 examples/s]Tokenizing train dataset:  51%|█████     | 2491/4890 [00:08<00:08, 299.24 examples/s]Tokenizing train dataset:  51%|█████▏    | 2512/4890 [00:08<00:08, 282.82 examples/s]Tokenizing train dataset:  52%|█████▏    | 2540/4890 [00:08<00:08, 277.69 examples/s]Tokenizing train dataset:  52%|█████▏    | 2524/4890 [00:08<00:08, 268.82 examples/s]Tokenizing train dataset:  52%|█████▏    | 2558/4890 [00:08<00:08, 287.07 examples/s]Tokenizing train dataset:  53%|█████▎    | 2574/4890 [00:08<00:07, 290.81 examples/s]Tokenizing train dataset:  52%|█████▏    | 2558/4890 [00:08<00:08, 281.27 examples/s]Tokenizing train dataset:  53%|█████▎    | 2593/4890 [00:08<00:07, 299.90 examples/s]Tokenizing train dataset:  53%|█████▎    | 2611/4890 [00:08<00:07, 307.66 examples/s]Tokenizing train dataset:  53%|█████▎    | 2593/4890 [00:08<00:07, 296.54 examples/s]Tokenizing train dataset:  54%|█████▎    | 2627/4890 [00:08<00:07, 307.35 examples/s]Tokenizing train dataset:  54%|█████▍    | 2645/4890 [00:08<00:07, 314.02 examples/s]Tokenizing train dataset:  54%|█████▎    | 2627/4890 [00:08<00:07, 304.64 examples/s]Tokenizing train dataset:  54%|█████▍    | 2661/4890 [00:08<00:07, 312.99 examples/s]Tokenizing train dataset:  54%|█████▍    | 2660/4890 [00:08<00:07, 309.47 examples/s]Tokenizing train dataset:  55%|█████▌    | 2695/4890 [00:08<00:06, 317.85 examples/s]Tokenizing train dataset:  55%|█████▌    | 2695/4890 [00:08<00:06, 318.26 examples/s]Tokenizing train dataset:  55%|█████▌    | 2695/4890 [00:08<00:06, 316.51 examples/s]Tokenizing train dataset:  56%|█████▌    | 2741/4890 [00:08<00:06, 311.36 examples/s]Tokenizing train dataset:  56%|█████▌    | 2727/4890 [00:08<00:06, 314.35 examples/s]Tokenizing train dataset:  56%|█████▌    | 2740/4890 [00:08<00:06, 309.62 examples/s]Tokenizing train dataset:  57%|█████▋    | 2788/4890 [00:09<00:06, 307.14 examples/s]Tokenizing train dataset:  57%|█████▋    | 2770/4890 [00:09<00:07, 299.98 examples/s]Tokenizing train dataset:  57%|█████▋    | 2788/4890 [00:09<00:06, 307.55 examples/s]Tokenizing train dataset:  58%|█████▊    | 2820/4890 [00:09<00:06, 307.07 examples/s]Tokenizing train dataset:  57%|█████▋    | 2804/4890 [00:09<00:06, 307.21 examples/s]Tokenizing train dataset:  58%|█████▊    | 2820/4890 [00:09<00:06, 307.82 examples/s]Tokenizing train dataset:  59%|█████▊    | 2863/4890 [00:09<00:06, 298.70 examples/s]Tokenizing train dataset:  58%|█████▊    | 2848/4890 [00:09<00:06, 297.56 examples/s]Tokenizing train dataset:  59%|█████▊    | 2864/4890 [00:09<00:06, 300.21 examples/s]Tokenizing train dataset:  59%|█████▉    | 2895/4890 [00:09<00:06, 301.76 examples/s]Tokenizing train dataset:  59%|█████▉    | 2879/4890 [00:09<00:06, 296.55 examples/s]Tokenizing train dataset:  59%|█████▉    | 2895/4890 [00:09<00:06, 301.93 examples/s]Tokenizing train dataset:  60%|█████▉    | 2910/4890 [00:09<00:06, 298.39 examples/s]Tokenizing train dataset:  60%|██████    | 2942/4890 [00:09<00:06, 302.62 examples/s]Tokenizing train dataset:  60%|██████    | 2942/4890 [00:09<00:06, 303.05 examples/s]Tokenizing train dataset:  60%|██████    | 2942/4890 [00:09<00:06, 300.42 examples/s]Tokenizing train dataset:  61%|██████    | 2974/4890 [00:09<00:06, 304.04 examples/s]Tokenizing train dataset:  61%|██████    | 2974/4890 [00:09<00:06, 304.93 examples/s]Tokenizing train dataset:  61%|██████    | 2974/4890 [00:09<00:06, 302.23 examples/s]Tokenizing train dataset:  61%|██████▏   | 3005/4890 [00:09<00:06, 300.72 examples/s]Tokenizing train dataset:  61%|██████▏   | 3005/4890 [00:09<00:06, 301.63 examples/s]Tokenizing train dataset:  61%|██████▏   | 3005/4890 [00:09<00:06, 298.24 examples/s]Tokenizing train dataset:  62%|██████▏   | 3038/4890 [00:09<00:06, 302.62 examples/s]Tokenizing train dataset:  62%|██████▏   | 3051/4890 [00:09<00:06, 300.53 examples/s]Tokenizing train dataset:  62%|██████▏   | 3051/4890 [00:10<00:06, 298.24 examples/s]Tokenizing train dataset:  63%|██████▎   | 3084/4890 [00:10<00:06, 297.08 examples/s]Tokenizing train dataset:  63%|██████▎   | 3096/4890 [00:10<00:06, 296.09 examples/s]Tokenizing train dataset:  63%|██████▎   | 3094/4890 [00:10<00:06, 292.45 examples/s]Tokenizing train dataset:  64%|██████▎   | 3117/4890 [00:10<00:05, 302.68 examples/s]Tokenizing train dataset:  64%|██████▍   | 3127/4890 [00:10<00:05, 294.66 examples/s]Tokenizing train dataset:  64%|██████▍   | 3126/4890 [00:10<00:05, 294.65 examples/s]Tokenizing train dataset:  65%|██████▍   | 3157/4890 [00:10<00:05, 288.94 examples/s]Tokenizing train dataset:  65%|██████▍   | 3158/4890 [00:10<00:06, 288.41 examples/s]Tokenizing train dataset:  65%|██████▌   | 3189/4890 [00:10<00:05, 293.73 examples/s]Tokenizing train dataset:  65%|██████▍   | 3157/4890 [00:10<00:06, 258.89 examples/s]Tokenizing train dataset:  65%|██████▌   | 3194/4890 [00:10<00:05, 303.66 examples/s]Tokenizing train dataset:  65%|██████▌   | 3192/4890 [00:10<00:06, 279.03 examples/s]Tokenizing train dataset:  66%|██████▌   | 3235/4890 [00:10<00:05, 293.61 examples/s]Tokenizing train dataset:  66%|██████▌   | 3239/4890 [00:10<00:05, 301.04 examples/s]Tokenizing train dataset:  66%|██████▌   | 3221/4890 [00:10<00:05, 278.61 examples/s]Tokenizing train dataset:  67%|██████▋   | 3269/4890 [00:10<00:05, 303.15 examples/s]Tokenizing train dataset:  67%|██████▋   | 3272/4890 [00:10<00:05, 304.52 examples/s]Tokenizing train dataset:  67%|██████▋   | 3258/4890 [00:10<00:05, 299.41 examples/s]Tokenizing train dataset:  67%|██████▋   | 3300/4890 [00:10<00:05, 301.51 examples/s]Tokenizing train dataset:  68%|██████▊   | 3306/4890 [00:10<00:05, 307.03 examples/s]Tokenizing train dataset:  68%|██████▊   | 3302/4890 [00:10<00:05, 293.00 examples/s]Tokenizing train dataset:  68%|██████▊   | 3347/4890 [00:10<00:05, 302.52 examples/s]Tokenizing train dataset:  69%|██████▊   | 3354/4890 [00:10<00:05, 304.42 examples/s]Tokenizing train dataset:  69%|██████▊   | 3350/4890 [00:11<00:05, 298.37 examples/s]Tokenizing train dataset:  69%|██████▉   | 3387/4890 [00:11<00:05, 285.87 examples/s]Tokenizing train dataset:  69%|██████▉   | 3395/4890 [00:11<00:05, 286.83 examples/s]Tokenizing train dataset:  69%|██████▉   | 3389/4890 [00:11<00:05, 281.42 examples/s]Tokenizing train dataset:  70%|███████   | 3428/4890 [00:11<00:05, 280.10 examples/s]Tokenizing train dataset:  70%|███████   | 3437/4890 [00:11<00:05, 278.84 examples/s]Tokenizing train dataset:  70%|██████▉   | 3418/4890 [00:11<00:05, 280.13 examples/s]Tokenizing train dataset:  71%|███████   | 3465/4890 [00:11<00:05, 264.41 examples/s]Tokenizing train dataset:  71%|███████   | 3473/4890 [00:11<00:05, 262.88 examples/s]Tokenizing train dataset:  71%|███████   | 3455/4890 [00:11<00:05, 265.35 examples/s]Tokenizing train dataset:  72%|███████▏  | 3506/4890 [00:11<00:05, 262.20 examples/s]Tokenizing train dataset:  72%|███████▏  | 3523/4890 [00:11<00:04, 280.82 examples/s]Tokenizing train dataset:  71%|███████▏  | 3489/4890 [00:11<00:05, 249.83 examples/s]Tokenizing train dataset:  72%|███████▏  | 3545/4890 [00:11<00:04, 285.29 examples/s]Tokenizing train dataset:  73%|███████▎  | 3556/4890 [00:11<00:04, 285.49 examples/s]Tokenizing train dataset:  72%|███████▏  | 3530/4890 [00:11<00:04, 285.15 examples/s]Tokenizing train dataset:  73%|███████▎  | 3580/4890 [00:11<00:04, 299.65 examples/s]Tokenizing train dataset:  73%|███████▎  | 3591/4890 [00:11<00:04, 300.25 examples/s]Tokenizing train dataset:  73%|███████▎  | 3561/4890 [00:11<00:04, 287.89 examples/s]Tokenizing train dataset:  74%|███████▍  | 3627/4890 [00:11<00:04, 299.40 examples/s]Tokenizing train dataset:  74%|███████▎  | 3595/4890 [00:11<00:04, 300.03 examples/s]Tokenizing train dataset:  74%|███████▍  | 3637/4890 [00:11<00:04, 299.84 examples/s]Tokenizing train dataset:  75%|███████▍  | 3661/4890 [00:12<00:04, 306.75 examples/s]Tokenizing train dataset:  74%|███████▍  | 3627/4890 [00:12<00:04, 299.07 examples/s]Tokenizing train dataset:  75%|███████▌  | 3670/4890 [00:12<00:04, 303.30 examples/s]Tokenizing train dataset:  75%|███████▍  | 3661/4890 [00:12<00:04, 307.08 examples/s]Tokenizing train dataset:  76%|███████▌  | 3707/4890 [00:12<00:03, 299.10 examples/s]Tokenizing train dataset:  76%|███████▌  | 3716/4890 [00:12<00:03, 299.36 examples/s]Tokenizing train dataset:  77%|███████▋  | 3751/4890 [00:12<00:03, 329.02 examples/s]Tokenizing train dataset:  76%|███████▌  | 3707/4890 [00:12<00:03, 297.92 examples/s]Tokenizing train dataset:  77%|███████▋  | 3760/4890 [00:12<00:03, 330.57 examples/s]Tokenizing train dataset:  77%|███████▋  | 3750/4890 [00:12<00:03, 330.19 examples/s]Tokenizing train dataset:  78%|███████▊  | 3795/4890 [00:12<00:03, 330.68 examples/s]Tokenizing train dataset:  78%|███████▊  | 3801/4890 [00:12<00:03, 325.24 examples/s]Tokenizing train dataset:  78%|███████▊  | 3833/4890 [00:12<00:03, 341.02 examples/s]Tokenizing train dataset:  79%|███████▊  | 3840/4890 [00:12<00:03, 333.96 examples/s]Tokenizing train dataset:  78%|███████▊  | 3800/4890 [00:12<00:03, 326.07 examples/s]Tokenizing train dataset:  78%|███████▊  | 3838/4890 [00:12<00:03, 336.53 examples/s]Tokenizing train dataset:  79%|███████▉  | 3880/4890 [00:12<00:03, 326.24 examples/s]Tokenizing train dataset:  80%|███████▉  | 3888/4890 [00:12<00:03, 327.43 examples/s]Tokenizing train dataset:  79%|███████▉  | 3884/4890 [00:12<00:03, 323.96 examples/s]Tokenizing train dataset:  80%|████████  | 3921/4890 [00:12<00:03, 303.08 examples/s]Tokenizing train dataset:  80%|████████  | 3929/4890 [00:12<00:03, 306.86 examples/s]Tokenizing train dataset:  80%|████████  | 3925/4890 [00:13<00:03, 301.97 examples/s]Tokenizing train dataset:  81%|████████  | 3973/4890 [00:12<00:02, 313.60 examples/s]Tokenizing train dataset:  81%|████████▏ | 3979/4890 [00:13<00:02, 312.35 examples/s]Tokenizing train dataset:  82%|████████▏ | 4018/4890 [00:13<00:02, 328.78 examples/s]Tokenizing train dataset:  82%|████████▏ | 4013/4890 [00:13<00:02, 331.27 examples/s]Tokenizing train dataset:  81%|████████▏ | 3977/4890 [00:13<00:02, 311.57 examples/s]Tokenizing train dataset:  82%|████████▏ | 4016/4890 [00:13<00:02, 327.61 examples/s]Tokenizing train dataset:  83%|████████▎ | 4060/4890 [00:13<00:02, 310.62 examples/s]Tokenizing train dataset:  83%|████████▎ | 4057/4890 [00:13<00:02, 314.73 examples/s]Tokenizing train dataset:  84%|████████▎ | 4090/4890 [00:13<00:02, 318.08 examples/s]Tokenizing train dataset:  84%|████████▎ | 4095/4890 [00:13<00:02, 314.00 examples/s]Tokenizing train dataset:  83%|████████▎ | 4060/4890 [00:13<00:02, 310.00 examples/s]Tokenizing train dataset:  84%|████████▎ | 4095/4890 [00:13<00:02, 313.20 examples/s]Tokenizing train dataset:  85%|████████▍ | 4136/4890 [00:13<00:02, 307.68 examples/s]Tokenizing train dataset:  85%|████████▍ | 4140/4890 [00:13<00:02, 303.42 examples/s]Tokenizing train dataset:  85%|████████▍ | 4140/4890 [00:13<00:02, 302.30 examples/s]Tokenizing train dataset:  86%|████████▌ | 4181/4890 [00:13<00:02, 303.05 examples/s]Tokenizing train dataset:  86%|████████▌ | 4186/4890 [00:13<00:02, 299.99 examples/s]Tokenizing train dataset:  87%|████████▋ | 4278/4890 [00:13<00:01, 457.13 examples/s]Tokenizing train dataset:  88%|████████▊ | 4304/4890 [00:13<00:01, 501.03 examples/s]Tokenizing train dataset:  86%|████████▌ | 4186/4890 [00:13<00:02, 298.64 examples/s]Tokenizing train dataset:  90%|█████████ | 4411/4890 [00:13<00:00, 672.15 examples/s]Tokenizing train dataset:  91%|█████████ | 4438/4890 [00:13<00:00, 704.84 examples/s]Tokenizing train dataset:  88%|████████▊ | 4305/4890 [00:13<00:01, 499.43 examples/s]Tokenizing train dataset:  93%|█████████▎| 4539/4890 [00:13<00:00, 829.43 examples/s]Tokenizing train dataset:  93%|█████████▎| 4566/4890 [00:14<00:00, 850.26 examples/s]Tokenizing train dataset:  91%|█████████ | 4440/4890 [00:14<00:00, 704.22 examples/s]Tokenizing train dataset:  95%|█████████▌| 4661/4890 [00:14<00:00, 934.29 examples/s]Tokenizing train dataset:  96%|█████████▌| 4687/4890 [00:14<00:00, 944.99 examples/s]Tokenizing train dataset:  93%|█████████▎| 4569/4890 [00:14<00:00, 853.80 examples/s]Tokenizing train dataset:  98%|█████████▊| 4792/4890 [00:14<00:00, 1037.41 examples/s]Tokenizing train dataset:  99%|█████████▊| 4819/4890 [00:14<00:00, 1046.15 examples/s]Tokenizing train dataset:  96%|█████████▌| 4690/4890 [00:14<00:00, 946.03 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 341.97 examples/s] 
Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 343.36 examples/s] 
Tokenizing train dataset:  99%|█████████▊| 4822/4890 [00:14<00:00, 1046.20 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 339.30 examples/s] 
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  60%|█████▉    | 569/953 [00:00<00:00, 5652.15 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5545.86 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5594.09 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5614.86 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5594.14 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5556.77 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  28%|██▊       | 268/953 [00:00<00:00, 2638.01 examples/s]Applying chat template to eval dataset:  27%|██▋       | 258/953 [00:00<00:00, 2560.09 examples/s]Applying chat template to eval dataset:  33%|███▎      | 311/953 [00:00<00:00, 3084.05 examples/s]Applying chat template to eval dataset:  54%|█████▍    | 518/953 [00:00<00:00, 2574.34 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 638/953 [00:00<00:00, 3187.22 examples/s]Applying chat template to eval dataset:  69%|██████▉   | 662/953 [00:00<00:00, 2616.26 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3170.14 examples/s]
Applying chat template to eval dataset:  97%|█████████▋| 925/953 [00:00<00:00, 2620.18 examples/s]Applying chat template to eval dataset:  95%|█████████▍| 902/953 [00:00<00:00, 2563.84 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2601.85 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2556.02 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 33/953 [00:00<00:02, 315.74 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 253.78 examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 252.98 examples/s]Tokenizing eval dataset:   8%|▊         | 73/953 [00:00<00:03, 273.50 examples/s]Tokenizing eval dataset:   6%|▌         | 53/953 [00:00<00:03, 245.59 examples/s]Tokenizing eval dataset:   6%|▌         | 53/953 [00:00<00:03, 246.12 examples/s]Tokenizing eval dataset:  11%|█         | 103/953 [00:00<00:03, 278.33 examples/s]Tokenizing eval dataset:   9%|▉         | 90/953 [00:00<00:03, 238.49 examples/s]Tokenizing eval dataset:   9%|▉         | 90/953 [00:00<00:03, 237.80 examples/s]Tokenizing eval dataset:  15%|█▌        | 144/953 [00:00<00:03, 268.77 examples/s]Tokenizing eval dataset:  12%|█▏        | 114/953 [00:00<00:03, 232.08 examples/s]Tokenizing eval dataset:  13%|█▎        | 123/953 [00:00<00:03, 226.14 examples/s]Tokenizing eval dataset:  19%|█▉        | 180/953 [00:00<00:03, 250.04 examples/s]Tokenizing eval dataset:  15%|█▌        | 147/953 [00:00<00:03, 222.31 examples/s]Tokenizing eval dataset:  15%|█▌        | 146/953 [00:00<00:03, 224.82 examples/s]Tokenizing eval dataset:  22%|██▏       | 208/953 [00:00<00:02, 256.04 examples/s]Tokenizing eval dataset:  27%|██▋       | 257/953 [00:00<00:02, 317.38 examples/s]Tokenizing eval dataset:  19%|█▊        | 178/953 [00:00<00:03, 211.64 examples/s]Tokenizing eval dataset:  19%|█▊        | 177/953 [00:00<00:03, 213.11 examples/s]Tokenizing eval dataset:  34%|███▍      | 325/953 [00:00<00:01, 412.93 examples/s]Tokenizing eval dataset:  22%|██▏       | 209/953 [00:00<00:03, 208.30 examples/s]Tokenizing eval dataset:  22%|██▏       | 209/953 [00:00<00:03, 209.48 examples/s]Tokenizing eval dataset:  41%|████      | 386/953 [00:01<00:01, 466.60 examples/s]Tokenizing eval dataset:  26%|██▌       | 247/953 [00:01<00:02, 248.94 examples/s]Tokenizing eval dataset:  26%|██▌       | 250/953 [00:01<00:02, 256.69 examples/s]Tokenizing eval dataset:  48%|████▊     | 461/953 [00:01<00:00, 541.73 examples/s]Tokenizing eval dataset:  31%|███▏      | 300/953 [00:01<00:02, 319.27 examples/s]Tokenizing eval dataset:  32%|███▏      | 304/953 [00:01<00:01, 326.14 examples/s]Tokenizing eval dataset:  54%|█████▍    | 519/953 [00:01<00:00, 550.82 examples/s]Tokenizing eval dataset:  37%|███▋      | 351/953 [00:01<00:01, 366.49 examples/s]Tokenizing eval dataset:  38%|███▊      | 358/953 [00:01<00:01, 378.97 examples/s]Tokenizing eval dataset:  61%|██████    | 583/953 [00:01<00:00, 575.60 examples/s]Tokenizing eval dataset:  42%|████▏     | 400/953 [00:01<00:01, 397.30 examples/s]Tokenizing eval dataset:  43%|████▎     | 408/953 [00:01<00:01, 406.18 examples/s]Tokenizing eval dataset:  68%|██████▊   | 648/953 [00:01<00:00, 594.04 examples/s]Tokenizing eval dataset:  48%|████▊     | 462/953 [00:01<00:01, 458.15 examples/s]Tokenizing eval dataset:  49%|████▉     | 469/953 [00:01<00:01, 455.21 examples/s]Tokenizing eval dataset:  77%|███████▋  | 735/953 [00:01<00:00, 576.31 examples/s]Tokenizing eval dataset:  55%|█████▍    | 520/953 [00:01<00:00, 462.75 examples/s]Tokenizing eval dataset:  56%|█████▌    | 532/953 [00:01<00:00, 459.80 examples/s]Tokenizing eval dataset:  60%|██████    | 572/953 [00:01<00:00, 477.65 examples/s]Tokenizing eval dataset:  85%|████████▍ | 810/953 [00:01<00:00, 541.28 examples/s]Tokenizing eval dataset:  62%|██████▏   | 589/953 [00:01<00:00, 482.89 examples/s]Tokenizing eval dataset:  66%|██████▌   | 627/953 [00:01<00:00, 497.07 examples/s]Tokenizing eval dataset:  67%|██████▋   | 642/953 [00:01<00:00, 494.48 examples/s]Tokenizing eval dataset:  91%|█████████ | 866/953 [00:01<00:00, 480.02 examples/s]Tokenizing eval dataset:  73%|███████▎  | 699/953 [00:01<00:00, 489.13 examples/s]Tokenizing eval dataset:  75%|███████▍  | 714/953 [00:01<00:00, 482.56 examples/s]Tokenizing eval dataset:  97%|█████████▋| 920/953 [00:02<00:00, 438.56 examples/s]Tokenizing eval dataset:  81%|████████  | 768/953 [00:02<00:00, 473.58 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 429.94 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  82%|████████▏ | 780/953 [00:02<00:00, 462.30 examples/s]Tokenizing eval dataset:  87%|████████▋ | 825/953 [00:02<00:00, 437.44 examples/s]Tokenizing eval dataset:  88%|████████▊ | 838/953 [00:02<00:00, 433.34 examples/s]Tokenizing eval dataset:  93%|█████████▎| 883/953 [00:02<00:00, 417.17 examples/s]Tokenizing eval dataset:  94%|█████████▍| 900/953 [00:02<00:00, 419.88 examples/s]Tokenizing eval dataset:  99%|█████████▉| 942/953 [00:02<00:00, 406.54 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 405.22 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 369.21 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 367.75 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4505531787872314 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3659987449645996 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3435332775115967 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.439120292663574 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank8]:[W612 17:56:34.470358798 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 2 ---
