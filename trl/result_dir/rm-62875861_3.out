cpu-bind=MASK - gn30, task  3  0 [658253]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 3 ---
Total Nodes: 4
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn21
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 4     --machine_rank 3     --main_process_ip gn21     --main_process_port 29500     --num_processes 16     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62875861     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.2 --curriculum_stage=0
-------------------------------------------
[2025-06-10 00:51:44,254] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0610 00:51:46.078000 658309 torch/distributed/run.py:792] 
W0610 00:51:46.078000 658309 torch/distributed/run.py:792] *****************************************
W0610 00:51:46.078000 658309 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0610 00:51:46.078000 658309 torch/distributed/run.py:792] *****************************************
[2025-06-10 00:51:51,547] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 00:51:51,613] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 00:51:51,625] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 00:51:51,632] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
World size: 16
Setting gradient accumulation steps to: 1
[2025-06-10 00:51:57,820] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-10 00:51:57,826] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-10 00:51:57,831] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 4890
Validation dataset size: 953
Steps per epoch: 305
Evaluate each 152 steps
[2025-06-10 00:51:57,873] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: cjvt/GaMS-9B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:18, 26.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.42s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.42s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:26<01:19, 26.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:52, 26.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:52, 26.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:52, 26.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:52, 26.40s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:18<00:26, 26.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:18<00:26, 26.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:18<00:26, 26.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:18<00:26, 26.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 23.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.36s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 23.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.41s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 23.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 23.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.41s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:37<00:00, 24.41s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Loaded tokenizer
[rank14]:[W610 00:53:40.571379092 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 14]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s][rank15]:[W610 00:53:40.582903293 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 15]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank13]:[W610 00:53:40.654409282 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 13]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  11%|█         | 540/4890 [00:00<00:00, 5332.72 examples/s]Extracting prompt in train dataset:  22%|██▏       | 1096/4890 [00:00<00:00, 5464.16 examples/s]Extracting prompt in train dataset:  39%|███▉      | 1921/4890 [00:00<00:00, 5461.90 examples/s]Extracting prompt in train dataset:  51%|█████     | 2480/4890 [00:00<00:00, 5497.15 examples/s]Extracting prompt in train dataset:  67%|██████▋   | 3286/4890 [00:00<00:00, 5437.93 examples/s]Extracting prompt in train dataset:  79%|███████▊  | 3840/4890 [00:00<00:00, 5462.84 examples/s]Extracting prompt in train dataset:  90%|█████████ | 4415/4890 [00:00<00:00, 5535.64 examples/s]Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5455.53 examples/s]
Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   6%|▌         | 286/4890 [00:00<00:01, 2830.03 examples/s]Applying chat template to train dataset:  12%|█▏        | 598/4890 [00:00<00:01, 2997.53 examples/s]Applying chat template to train dataset:  19%|█▊        | 913/4890 [00:00<00:02, 1351.12 examples/s]Applying chat template to train dataset:  25%|██▌       | 1230/4890 [00:00<00:02, 1753.66 examples/s]Applying chat template to train dataset:  32%|███▏      | 1541/4890 [00:00<00:01, 2078.52 examples/s]Applying chat template to train dataset:  38%|███▊      | 1859/4890 [00:00<00:01, 2356.23 examples/s]Applying chat template to train dataset:  44%|████▍     | 2172/4890 [00:00<00:01, 2562.68 examples/s]Applying chat template to train dataset:  51%|█████     | 2486/4890 [00:01<00:00, 2721.14 examples/s]Applying chat template to train dataset:  57%|█████▋    | 2801/4890 [00:01<00:00, 2839.63 examples/s]Applying chat template to train dataset:  67%|██████▋   | 3255/4890 [00:01<00:00, 2903.49 examples/s]Applying chat template to train dataset:  73%|███████▎  | 3569/4890 [00:01<00:00, 2962.07 examples/s]Applying chat template to train dataset:  79%|███████▉  | 3884/4890 [00:01<00:00, 3009.36 examples/s]Applying chat template to train dataset:  86%|████████▌ | 4199/4890 [00:01<00:00, 3045.71 examples/s]Applying chat template to train dataset:  93%|█████████▎| 4528/4890 [00:01<00:00, 3113.54 examples/s]Applying chat template to train dataset:  99%|█████████▉| 4857/4890 [00:01<00:00, 3163.11 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 2644.27 examples/s]
Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 41/4890 [00:00<00:12, 393.80 examples/s]Tokenizing train dataset:   2%|▏         | 89/4890 [00:00<00:14, 336.93 examples/s]Tokenizing train dataset:   3%|▎         | 133/4890 [00:00<00:15, 312.73 examples/s]Tokenizing train dataset:   3%|▎         | 165/4890 [00:00<00:15, 310.37 examples/s]Tokenizing train dataset:   4%|▍         | 213/4890 [00:00<00:15, 309.26 examples/s]Tokenizing train dataset:   5%|▌         | 247/4890 [00:00<00:14, 317.00 examples/s]Tokenizing train dataset:   6%|▌         | 282/4890 [00:00<00:14, 324.97 examples/s]Tokenizing train dataset:   6%|▋         | 315/4890 [00:00<00:14, 321.58 examples/s]Tokenizing train dataset:   7%|▋         | 362/4890 [00:01<00:14, 315.90 examples/s]Tokenizing train dataset:   8%|▊         | 397/4890 [00:01<00:14, 318.25 examples/s]Tokenizing train dataset:   9%|▉         | 430/4890 [00:01<00:13, 319.51 examples/s]Tokenizing train dataset:  10%|▉         | 475/4890 [00:01<00:14, 307.65 examples/s]Tokenizing train dataset:  11%|█         | 519/4890 [00:01<00:14, 301.07 examples/s]Tokenizing train dataset:  11%|█▏        | 553/4890 [00:01<00:14, 307.95 examples/s]Tokenizing train dataset:  12%|█▏        | 585/4890 [00:01<00:14, 307.12 examples/s]Tokenizing train dataset:  13%|█▎        | 636/4890 [00:02<00:13, 316.60 examples/s]Tokenizing train dataset:  14%|█▍        | 682/4890 [00:02<00:13, 307.55 examples/s]Tokenizing train dataset:  15%|█▍        | 729/4890 [00:02<00:13, 307.90 examples/s]Tokenizing train dataset:  16%|█▌        | 761/4890 [00:02<00:13, 309.14 examples/s]Tokenizing train dataset:  16%|█▋        | 802/4890 [00:02<00:13, 294.61 examples/s]Tokenizing train dataset:  17%|█▋        | 847/4890 [00:02<00:13, 292.73 examples/s]Tokenizing train dataset:  18%|█▊        | 879/4890 [00:02<00:13, 293.51 examples/s]Tokenizing train dataset:  19%|█▊        | 916/4890 [00:02<00:12, 310.44 examples/s]Tokenizing train dataset:  20%|█▉        | 961/4890 [00:03<00:13, 301.98 examples/s]Tokenizing train dataset:  21%|██        | 1007/4890 [00:03<00:26, 149.22 examples/s]Tokenizing train dataset:  21%|██        | 1034/4890 [00:03<00:23, 164.55 examples/s]Tokenizing train dataset:  22%|██▏       | 1064/4890 [00:03<00:20, 185.59 examples/s]Tokenizing train dataset:  22%|██▏       | 1099/4890 [00:04<00:17, 215.03 examples/s]Tokenizing train dataset:  23%|██▎       | 1141/4890 [00:04<00:16, 231.73 examples/s]Tokenizing train dataset:  24%|██▍       | 1170/4890 [00:04<00:15, 238.70 examples/s]Tokenizing train dataset:  25%|██▍       | 1207/4890 [00:04<00:13, 264.42 examples/s]Tokenizing train dataset:  25%|██▌       | 1244/4890 [00:04<00:12, 286.92 examples/s]Tokenizing train dataset:  26%|██▋       | 1287/4890 [00:04<00:12, 284.34 examples/s]Tokenizing train dataset:  27%|██▋       | 1323/4890 [00:04<00:11, 297.73 examples/s]Tokenizing train dataset:  28%|██▊       | 1367/4890 [00:04<00:12, 291.37 examples/s]Tokenizing train dataset:  29%|██▊       | 1398/4890 [00:05<00:11, 294.06 examples/s]Tokenizing train dataset:  29%|██▉       | 1441/4890 [00:05<00:12, 287.31 examples/s]Tokenizing train dataset:  30%|███       | 1473/4890 [00:05<00:11, 290.87 examples/s]Tokenizing train dataset:  31%|███       | 1514/4890 [00:05<00:11, 283.31 examples/s]Tokenizing train dataset:  32%|███▏      | 1548/4890 [00:05<00:11, 294.48 examples/s]Tokenizing train dataset:  33%|███▎      | 1591/4890 [00:05<00:11, 288.91 examples/s]Tokenizing train dataset:  33%|███▎      | 1627/4890 [00:05<00:10, 298.77 examples/s]Tokenizing train dataset:  34%|███▍      | 1661/4890 [00:05<00:10, 306.58 examples/s]Tokenizing train dataset:  35%|███▍      | 1700/4890 [00:06<00:09, 326.23 examples/s]Tokenizing train dataset:  36%|███▌      | 1750/4890 [00:06<00:09, 323.69 examples/s]Tokenizing train dataset:  37%|███▋      | 1786/4890 [00:06<00:09, 329.52 examples/s]Tokenizing train dataset:  37%|███▋      | 1829/4890 [00:06<00:09, 307.05 examples/s]Tokenizing train dataset:  38%|███▊      | 1874/4890 [00:06<00:09, 301.63 examples/s]Tokenizing train dataset:  39%|███▉      | 1910/4890 [00:06<00:10, 278.68 examples/s]Tokenizing train dataset:  40%|███▉      | 1953/4890 [00:06<00:10, 277.37 examples/s]Tokenizing train dataset:  41%|████      | 1997/4890 [00:07<00:10, 279.17 examples/s]Tokenizing train dataset:  42%|████▏     | 2030/4890 [00:07<00:09, 287.89 examples/s]Tokenizing train dataset:  42%|████▏     | 2067/4890 [00:07<00:09, 307.13 examples/s]Tokenizing train dataset:  43%|████▎     | 2100/4890 [00:07<00:09, 308.72 examples/s]Tokenizing train dataset:  44%|████▍     | 2146/4890 [00:07<00:09, 304.81 examples/s]Tokenizing train dataset:  45%|████▍     | 2190/4890 [00:07<00:09, 297.49 examples/s]Tokenizing train dataset:  46%|████▌     | 2240/4890 [00:07<00:08, 302.52 examples/s]Tokenizing train dataset:  46%|████▋     | 2273/4890 [00:07<00:08, 305.81 examples/s]Tokenizing train dataset:  47%|████▋     | 2319/4890 [00:08<00:08, 301.93 examples/s]Tokenizing train dataset:  48%|████▊     | 2360/4890 [00:08<00:08, 289.54 examples/s]Tokenizing train dataset:  49%|████▉     | 2390/4890 [00:08<00:12, 205.05 examples/s]Tokenizing train dataset:  50%|████▉     | 2423/4890 [00:08<00:10, 225.27 examples/s]Tokenizing train dataset:  50%|█████     | 2452/4890 [00:08<00:10, 235.05 examples/s]Tokenizing train dataset:  51%|█████     | 2486/4890 [00:08<00:09, 257.74 examples/s]Tokenizing train dataset:  52%|█████▏    | 2524/4890 [00:09<00:09, 245.25 examples/s]Tokenizing train dataset:  52%|█████▏    | 2556/4890 [00:09<00:08, 261.66 examples/s]Tokenizing train dataset:  53%|█████▎    | 2591/4890 [00:09<00:08, 279.64 examples/s]Tokenizing train dataset:  54%|█████▎    | 2626/4890 [00:09<00:07, 293.57 examples/s]Tokenizing train dataset:  54%|█████▍    | 2659/4890 [00:09<00:07, 299.45 examples/s]Tokenizing train dataset:  55%|█████▌    | 2692/4890 [00:09<00:07, 306.73 examples/s]Tokenizing train dataset:  56%|█████▌    | 2725/4890 [00:09<00:07, 308.22 examples/s]Tokenizing train dataset:  57%|█████▋    | 2767/4890 [00:09<00:07, 295.15 examples/s]Tokenizing train dataset:  57%|█████▋    | 2800/4890 [00:09<00:06, 300.84 examples/s]Tokenizing train dataset:  58%|█████▊    | 2845/4890 [00:10<00:06, 294.07 examples/s]Tokenizing train dataset:  59%|█████▉    | 2875/4890 [00:10<00:06, 294.87 examples/s]Tokenizing train dataset:  59%|█████▉    | 2906/4890 [00:10<00:06, 296.16 examples/s]Tokenizing train dataset:  60%|██████    | 2937/4890 [00:10<00:06, 295.57 examples/s]Tokenizing train dataset:  61%|██████    | 2967/4890 [00:10<00:06, 294.62 examples/s]Tokenizing train dataset:  61%|██████▏   | 2998/4890 [00:10<00:06, 296.39 examples/s]Tokenizing train dataset:  62%|██████▏   | 3029/4890 [00:10<00:06, 295.75 examples/s]Tokenizing train dataset:  63%|██████▎   | 3071/4890 [00:10<00:06, 285.98 examples/s]Tokenizing train dataset:  63%|██████▎   | 3102/4890 [00:10<00:06, 291.54 examples/s]Tokenizing train dataset:  64%|██████▍   | 3133/4890 [00:11<00:06, 291.39 examples/s]Tokenizing train dataset:  65%|██████▍   | 3178/4890 [00:11<00:05, 289.15 examples/s]Tokenizing train dataset:  66%|██████▌   | 3210/4890 [00:11<00:05, 292.30 examples/s]Tokenizing train dataset:  66%|██████▋   | 3242/4890 [00:11<00:05, 295.43 examples/s]Tokenizing train dataset:  67%|██████▋   | 3274/4890 [00:11<00:05, 296.94 examples/s]Tokenizing train dataset:  68%|██████▊   | 3306/4890 [00:11<00:05, 302.57 examples/s]Tokenizing train dataset:  69%|██████▊   | 3353/4890 [00:11<00:05, 299.53 examples/s]Tokenizing train dataset:  69%|██████▉   | 3390/4890 [00:11<00:05, 278.12 examples/s]Tokenizing train dataset:  70%|██████▉   | 3419/4890 [00:12<00:05, 279.03 examples/s]Tokenizing train dataset:  71%|███████   | 3456/4890 [00:12<00:05, 263.69 examples/s]Tokenizing train dataset:  71%|███████▏  | 3490/4890 [00:12<00:05, 247.77 examples/s]Tokenizing train dataset:  72%|███████▏  | 3531/4890 [00:12<00:04, 283.05 examples/s]Tokenizing train dataset:  73%|███████▎  | 3561/4890 [00:12<00:04, 286.02 examples/s]Tokenizing train dataset:  74%|███████▎  | 3595/4890 [00:12<00:04, 297.76 examples/s]Tokenizing train dataset:  74%|███████▍  | 3640/4890 [00:12<00:04, 294.74 examples/s]Tokenizing train dataset:  75%|███████▌  | 3673/4890 [00:12<00:04, 298.45 examples/s]Tokenizing train dataset:  76%|███████▌  | 3720/4890 [00:13<00:03, 299.98 examples/s]Tokenizing train dataset:  77%|███████▋  | 3761/4890 [00:13<00:03, 324.26 examples/s]Tokenizing train dataset:  78%|███████▊  | 3795/4890 [00:13<00:03, 325.88 examples/s]Tokenizing train dataset:  78%|███████▊  | 3833/4890 [00:13<00:04, 239.80 examples/s]Tokenizing train dataset:  79%|███████▉  | 3863/4890 [00:13<00:04, 251.50 examples/s]Tokenizing train dataset:  80%|███████▉  | 3895/4890 [00:13<00:03, 262.48 examples/s]Tokenizing train dataset:  80%|████████  | 3936/4890 [00:13<00:03, 262.72 examples/s]Tokenizing train dataset:  81%|████████  | 3971/4890 [00:14<00:03, 281.73 examples/s]Tokenizing train dataset:  82%|████████▏ | 4009/4890 [00:14<00:02, 302.75 examples/s]Tokenizing train dataset:  83%|████████▎ | 4041/4890 [00:14<00:02, 304.87 examples/s]Tokenizing train dataset:  84%|████████▎ | 4089/4890 [00:14<00:02, 302.69 examples/s]Tokenizing train dataset:  84%|████████▍ | 4132/4890 [00:14<00:02, 294.60 examples/s]Tokenizing train dataset:  85%|████████▌ | 4175/4890 [00:14<00:02, 288.26 examples/s]Tokenizing train dataset:  87%|████████▋ | 4253/4890 [00:14<00:01, 399.84 examples/s]Tokenizing train dataset:  90%|████████▉ | 4380/4890 [00:14<00:00, 613.60 examples/s]Tokenizing train dataset:  92%|█████████▏| 4505/4890 [00:15<00:00, 778.12 examples/s]Tokenizing train dataset:  95%|█████████▍| 4628/4890 [00:15<00:00, 899.66 examples/s]Tokenizing train dataset:  97%|█████████▋| 4750/4890 [00:15<00:00, 987.17 examples/s]Tokenizing train dataset: 100%|█████████▉| 4875/4890 [00:15<00:00, 1059.50 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:15<00:00, 318.99 examples/s] 
[rank12]:[W610 00:53:59.618138550 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 12]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  11%|█▏        | 559/4890 [00:00<00:00, 5546.81 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 562/953 [00:00<00:00, 5581.02 examples/s]Extracting prompt in train dataset:  11%|█         | 550/4890 [00:00<00:00, 5446.67 examples/s]Extracting prompt in train dataset:  11%|█         | 540/4890 [00:00<00:00, 5336.33 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5561.06 examples/s]
Extracting prompt in train dataset:  23%|██▎       | 1120/4890 [00:00<00:00, 5562.63 examples/s]Extracting prompt in train dataset:  22%|██▏       | 1090/4890 [00:00<00:00, 5408.45 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1116/4890 [00:00<00:00, 5539.53 examples/s]Extracting prompt in train dataset:  34%|███▍      | 1680/4890 [00:00<00:00, 5566.79 examples/s]Extracting prompt in train dataset:  33%|███▎      | 1631/4890 [00:00<00:00, 5391.31 examples/s]Extracting prompt in train dataset:  34%|███▍      | 1670/4890 [00:00<00:00, 5514.87 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  46%|████▌     | 2240/4890 [00:00<00:00, 3264.18 examples/s]Extracting prompt in train dataset:  45%|████▍     | 2190/4890 [00:00<00:00, 3188.31 examples/s]Extracting prompt in train dataset:  46%|████▌     | 2260/4890 [00:00<00:00, 3290.78 examples/s]Applying chat template to eval dataset:  33%|███▎      | 310/953 [00:00<00:00, 3063.49 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 2809/4890 [00:00<00:00, 3862.09 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 2750/4890 [00:00<00:00, 3768.44 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 2810/4890 [00:00<00:00, 3815.16 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 638/953 [00:00<00:00, 3185.66 examples/s]Extracting prompt in train dataset:  68%|██████▊   | 3336/4890 [00:00<00:00, 4228.50 examples/s]Extracting prompt in train dataset:  67%|██████▋   | 3270/4890 [00:00<00:00, 4135.37 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 3380/4890 [00:00<00:00, 4298.51 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3161.20 examples/s]
Extracting prompt in train dataset:  80%|███████▉  | 3898/4890 [00:00<00:00, 4605.13 examples/s]Extracting prompt in train dataset:  78%|███████▊  | 3820/4890 [00:00<00:00, 4500.43 examples/s]Extracting prompt in train dataset:  81%|████████  | 3950/4890 [00:00<00:00, 4670.72 examples/s]Extracting prompt in train dataset:  91%|█████████▏| 4473/4890 [00:00<00:00, 4924.00 examples/s]Extracting prompt in train dataset:  90%|████████▉ | 4380/4890 [00:00<00:00, 4801.07 examples/s]Extracting prompt in train dataset:  93%|█████████▎| 4549/4890 [00:00<00:00, 5028.38 examples/s]Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 4650.52 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 4610.86 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 4523.40 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.49 examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 285.11 examples/s]Applying chat template to train dataset:   6%|▌         | 284/4890 [00:00<00:01, 2805.42 examples/s]Applying chat template to train dataset:   6%|▌         | 287/4890 [00:00<00:01, 2841.92 examples/s]Applying chat template to train dataset:   6%|▌         | 282/4890 [00:00<00:01, 2793.84 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 275.75 examples/s]Applying chat template to train dataset:  12%|█▏        | 595/4890 [00:00<00:01, 2975.67 examples/s]Applying chat template to train dataset:  12%|█▏        | 601/4890 [00:00<00:01, 3010.47 examples/s]Applying chat template to train dataset:  12%|█▏        | 592/4890 [00:00<00:01, 2967.79 examples/s]Applying chat template to train dataset:  19%|█▊        | 907/4890 [00:00<00:01, 3031.36 examples/s]Applying chat template to train dataset:  19%|█▉        | 918/4890 [00:00<00:01, 3078.24 examples/s]Applying chat template to train dataset:  18%|█▊        | 903/4890 [00:00<00:01, 3030.01 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 264.30 examples/s]Applying chat template to train dataset:  25%|██▍       | 1220/4890 [00:00<00:01, 3060.49 examples/s]Applying chat template to train dataset:  25%|██▌       | 1235/4890 [00:00<00:01, 3112.73 examples/s]Applying chat template to train dataset:  25%|██▍       | 1218/4890 [00:00<00:01, 3066.35 examples/s]Applying chat template to train dataset:  31%|███▏      | 1530/4890 [00:00<00:01, 3067.75 examples/s]Applying chat template to train dataset:  32%|███▏      | 1549/4890 [00:00<00:01, 3118.93 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 251.21 examples/s]Applying chat template to train dataset:  31%|███       | 1526/4890 [00:00<00:01, 3068.79 examples/s]Applying chat template to train dataset:  38%|███▊      | 1845/4890 [00:00<00:00, 3091.20 examples/s]Applying chat template to train dataset:  38%|███▊      | 1867/4890 [00:00<00:00, 3138.85 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 267.54 examples/s]Applying chat template to train dataset:  38%|███▊      | 1840/4890 [00:00<00:00, 3086.23 examples/s]Applying chat template to train dataset:  44%|████▍     | 2159/4890 [00:00<00:00, 3104.06 examples/s]Applying chat template to train dataset:  45%|████▍     | 2184/4890 [00:00<00:00, 3147.50 examples/s]Tokenizing eval dataset:  31%|███       | 291/953 [00:00<00:01, 366.14 examples/s]Applying chat template to train dataset:  44%|████▍     | 2154/4890 [00:00<00:00, 3099.16 examples/s]Applying chat template to train dataset:  51%|█████     | 2471/4890 [00:00<00:00, 3107.31 examples/s]Applying chat template to train dataset:  51%|█████     | 2502/4890 [00:00<00:00, 3153.84 examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 436.30 examples/s]Applying chat template to train dataset:  50%|█████     | 2468/4890 [00:00<00:00, 3108.82 examples/s]Applying chat template to train dataset:  57%|█████▋    | 2785/4890 [00:00<00:00, 3115.03 examples/s]Applying chat template to train dataset:  57%|█████▋    | 2780/4890 [00:00<00:00, 3110.25 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 484.70 examples/s]Applying chat template to train dataset:  61%|██████    | 2969/4890 [00:00<00:00, 3110.10 examples/s]Tokenizing eval dataset:  51%|█████     | 484/953 [00:01<00:00, 533.08 examples/s]Applying chat template to train dataset:  66%|██████▌   | 3232/4890 [00:01<00:00, 3058.47 examples/s]Applying chat template to train dataset:  67%|██████▋   | 3284/4890 [00:01<00:00, 3119.93 examples/s]Applying chat template to train dataset:  66%|██████▌   | 3231/4890 [00:01<00:00, 3063.05 examples/s]Tokenizing eval dataset:  58%|█████▊    | 551/953 [00:01<00:00, 569.20 examples/s]Applying chat template to train dataset:  72%|███████▏  | 3543/4890 [00:01<00:00, 3068.38 examples/s]Applying chat template to train dataset:  74%|███████▎  | 3600/4890 [00:01<00:00, 3125.70 examples/s]Applying chat template to train dataset:  72%|███████▏  | 3541/4890 [00:01<00:00, 3071.87 examples/s]Tokenizing eval dataset:  65%|██████▍   | 615/953 [00:01<00:00, 584.28 examples/s]Applying chat template to train dataset:  79%|███████▉  | 3859/4890 [00:01<00:00, 3086.92 examples/s]Applying chat template to train dataset:  80%|████████  | 3920/4890 [00:01<00:00, 3135.85 examples/s]Applying chat template to train dataset:  79%|███████▉  | 3855/4890 [00:01<00:00, 3086.83 examples/s]Tokenizing eval dataset:  71%|███████   | 676/953 [00:01<00:00, 587.73 examples/s]Applying chat template to train dataset:  85%|████████▌ | 4170/4890 [00:01<00:00, 3089.32 examples/s]Applying chat template to train dataset:  87%|████████▋ | 4239/4890 [00:01<00:00, 3151.32 examples/s]Applying chat template to train dataset:  85%|████████▌ | 4166/4890 [00:01<00:00, 3090.28 examples/s]Applying chat template to train dataset:  92%|█████████▏| 4498/4890 [00:01<00:00, 3140.80 examples/s]Applying chat template to train dataset:  93%|█████████▎| 4570/4890 [00:01<00:00, 3196.00 examples/s]Applying chat template to train dataset:  92%|█████████▏| 4491/4890 [00:01<00:00, 3134.55 examples/s]Tokenizing eval dataset:  80%|███████▉  | 759/953 [00:01<00:00, 569.47 examples/s]Applying chat template to train dataset:  99%|█████████▊| 4825/4890 [00:01<00:00, 3175.72 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3138.09 examples/s]
Applying chat template to train dataset:  99%|█████████▊| 4818/4890 [00:01<00:00, 3172.80 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3091.84 examples/s]
Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3090.59 examples/s]
Tokenizing eval dataset:  87%|████████▋ | 832/953 [00:01<00:00, 536.67 examples/s]Tokenizing eval dataset:  95%|█████████▌| 907/953 [00:02<00:00, 520.38 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 450.38 examples/s]
Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:11, 406.73 examples/s]Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:11, 411.20 examples/s]Tokenizing train dataset:   1%|          | 43/4890 [00:00<00:11, 405.99 examples/s]Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:14, 341.55 examples/s]Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:14, 342.20 examples/s]Tokenizing train dataset:   2%|▏         | 92/4890 [00:00<00:14, 337.70 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 323.08 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 322.65 examples/s]Tokenizing train dataset:   3%|▎         | 140/4890 [00:00<00:14, 324.15 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 310.66 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 310.15 examples/s]Tokenizing train dataset:   4%|▍         | 186/4890 [00:00<00:15, 312.52 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 316.12 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 315.51 examples/s]Tokenizing train dataset:   5%|▍         | 221/4890 [00:00<00:14, 319.16 examples/s]Tokenizing train dataset:   5%|▌         | 252/4890 [00:00<00:14, 320.85 examples/s]Tokenizing train dataset:   5%|▌         | 252/4890 [00:00<00:14, 320.11 examples/s]Tokenizing train dataset:   5%|▌         | 257/4890 [00:00<00:14, 323.53 examples/s]Tokenizing train dataset:   6%|▌         | 290/4890 [00:00<00:13, 334.08 examples/s]Tokenizing train dataset:   6%|▌         | 290/4890 [00:00<00:13, 333.22 examples/s]Tokenizing train dataset:   6%|▌         | 295/4890 [00:00<00:13, 335.13 examples/s]Tokenizing train dataset:   7%|▋         | 324/4890 [00:00<00:13, 332.79 examples/s]Tokenizing train dataset:   7%|▋         | 324/4890 [00:00<00:13, 331.93 examples/s]Tokenizing train dataset:   7%|▋         | 344/4890 [00:01<00:14, 323.14 examples/s]Tokenizing train dataset:   8%|▊         | 370/4890 [00:01<00:14, 318.81 examples/s]Tokenizing train dataset:   8%|▊         | 370/4890 [00:01<00:14, 317.80 examples/s]Tokenizing train dataset:   8%|▊         | 378/4890 [00:01<00:13, 323.07 examples/s]Tokenizing train dataset:   9%|▊         | 420/4890 [00:01<00:14, 316.80 examples/s]Tokenizing train dataset:   9%|▊         | 420/4890 [00:01<00:14, 315.75 examples/s]Tokenizing train dataset:   9%|▉         | 429/4890 [00:01<00:13, 323.73 examples/s]Tokenizing train dataset:  10%|▉         | 469/4890 [00:01<00:14, 314.82 examples/s]Tokenizing train dataset:  10%|▉         | 469/4890 [00:01<00:14, 313.71 examples/s]Tokenizing train dataset:  10%|▉         | 474/4890 [00:01<00:14, 312.44 examples/s]Tokenizing train dataset:  10%|█         | 512/4890 [00:01<00:14, 304.49 examples/s]Tokenizing train dataset:  10%|█         | 512/4890 [00:01<00:14, 303.42 examples/s]Tokenizing train dataset:  11%|█         | 519/4890 [00:01<00:14, 305.19 examples/s]Tokenizing train dataset:  11%|█         | 549/4890 [00:01<00:13, 317.55 examples/s]Tokenizing train dataset:  11%|█         | 549/4890 [00:01<00:13, 316.56 examples/s]Tokenizing train dataset:  11%|█▏        | 553/4890 [00:01<00:13, 311.63 examples/s]Tokenizing train dataset:  12%|█▏        | 585/4890 [00:01<00:13, 310.71 examples/s]Tokenizing train dataset:  12%|█▏        | 596/4890 [00:01<00:13, 308.35 examples/s]Tokenizing train dataset:  12%|█▏        | 596/4890 [00:01<00:13, 307.70 examples/s]Tokenizing train dataset:  13%|█▎        | 634/4890 [00:01<00:13, 322.58 examples/s]Tokenizing train dataset:  13%|█▎        | 634/4890 [00:01<00:13, 321.94 examples/s]Tokenizing train dataset:  13%|█▎        | 637/4890 [00:01<00:13, 317.84 examples/s]Tokenizing train dataset:  14%|█▎        | 669/4890 [00:02<00:14, 287.62 examples/s]Tokenizing train dataset:  14%|█▍        | 682/4890 [00:02<00:13, 307.32 examples/s]Tokenizing train dataset:  14%|█▍        | 681/4890 [00:02<00:14, 282.15 examples/s]Tokenizing train dataset:  15%|█▍        | 713/4890 [00:02<00:14, 285.12 examples/s]Tokenizing train dataset:  15%|█▍        | 732/4890 [00:02<00:13, 310.46 examples/s]Tokenizing train dataset:  15%|█▍        | 729/4890 [00:02<00:14, 290.10 examples/s]Tokenizing train dataset:  15%|█▌        | 753/4890 [00:02<00:13, 309.86 examples/s]Tokenizing train dataset:  16%|█▌        | 766/4890 [00:02<00:13, 312.13 examples/s]Tokenizing train dataset:  16%|█▌        | 762/4890 [00:02<00:13, 296.68 examples/s]Tokenizing train dataset:  16%|█▌        | 794/4890 [00:02<00:13, 294.27 examples/s]Tokenizing train dataset:  17%|█▋        | 810/4890 [00:02<00:13, 299.31 examples/s]Tokenizing train dataset:  16%|█▋        | 806/4890 [00:02<00:14, 289.21 examples/s]Tokenizing train dataset:  17%|█▋        | 840/4890 [00:02<00:13, 293.66 examples/s]Tokenizing train dataset:  17%|█▋        | 854/4890 [00:02<00:13, 293.71 examples/s]Tokenizing train dataset:  17%|█▋        | 850/4890 [00:02<00:14, 287.13 examples/s]Tokenizing train dataset:  18%|█▊        | 891/4890 [00:02<00:12, 308.73 examples/s]Tokenizing train dataset:  18%|█▊        | 885/4890 [00:02<00:13, 296.83 examples/s]Tokenizing train dataset:  18%|█▊        | 891/4890 [00:02<00:13, 305.25 examples/s]Tokenizing train dataset:  19%|█▉        | 920/4890 [00:02<00:13, 303.32 examples/s]Tokenizing train dataset:  19%|█▉        | 922/4890 [00:02<00:13, 300.62 examples/s]Tokenizing train dataset:  19%|█▉        | 939/4890 [00:02<00:12, 305.27 examples/s]Tokenizing train dataset:  19%|█▉        | 952/4890 [00:03<00:13, 302.52 examples/s]Tokenizing train dataset:  20%|█▉        | 954/4890 [00:03<00:13, 302.33 examples/s]Tokenizing train dataset:  20%|█▉        | 971/4890 [00:03<00:12, 305.90 examples/s]Tokenizing train dataset:  20%|██        | 983/4890 [00:03<00:13, 299.47 examples/s]Tokenizing train dataset:  20%|██        | 998/4890 [00:03<00:13, 294.60 examples/s]Tokenizing train dataset:  21%|██        | 1012/4890 [00:03<00:13, 292.05 examples/s]Tokenizing train dataset:  21%|██        | 1027/4890 [00:03<00:13, 290.61 examples/s]Tokenizing train dataset:  21%|██▏       | 1040/4890 [00:03<00:13, 285.78 examples/s]Tokenizing train dataset:  22%|██▏       | 1056/4890 [00:03<00:13, 289.29 examples/s]Tokenizing train dataset:  22%|██▏       | 1073/4890 [00:03<00:13, 292.57 examples/s]Tokenizing train dataset:  22%|██▏       | 1073/4890 [00:03<00:13, 293.50 examples/s]Tokenizing train dataset:  22%|██▏       | 1093/4890 [00:03<00:12, 304.71 examples/s]Tokenizing train dataset:  23%|██▎       | 1109/4890 [00:03<00:12, 305.95 examples/s]Tokenizing train dataset:  23%|██▎       | 1109/4890 [00:03<00:12, 307.09 examples/s]Tokenizing train dataset:  23%|██▎       | 1135/4890 [00:03<00:12, 293.10 examples/s]Tokenizing train dataset:  24%|██▎       | 1151/4890 [00:03<00:12, 294.04 examples/s]Tokenizing train dataset:  24%|██▎       | 1151/4890 [00:03<00:12, 294.27 examples/s]Tokenizing train dataset:  24%|██▍       | 1166/4890 [00:03<00:12, 293.95 examples/s]Tokenizing train dataset:  24%|██▍       | 1181/4890 [00:03<00:12, 295.09 examples/s]Tokenizing train dataset:  24%|██▍       | 1181/4890 [00:03<00:12, 295.28 examples/s]Tokenizing train dataset:  25%|██▍       | 1200/4890 [00:03<00:12, 304.97 examples/s]Tokenizing train dataset:  25%|██▍       | 1214/4890 [00:03<00:12, 299.55 examples/s]Tokenizing train dataset:  25%|██▍       | 1214/4890 [00:03<00:12, 299.56 examples/s]Tokenizing train dataset:  25%|██▌       | 1232/4890 [00:03<00:11, 306.42 examples/s]Tokenizing train dataset:  26%|██▌       | 1250/4890 [00:04<00:11, 313.86 examples/s]Tokenizing train dataset:  26%|██▌       | 1250/4890 [00:04<00:11, 313.88 examples/s]Tokenizing train dataset:  26%|██▌       | 1269/4890 [00:04<00:11, 319.78 examples/s]Tokenizing train dataset:  27%|██▋       | 1299/4890 [00:04<00:11, 315.50 examples/s]Tokenizing train dataset:  27%|██▋       | 1299/4890 [00:04<00:11, 315.07 examples/s]Tokenizing train dataset:  27%|██▋       | 1315/4890 [00:04<00:11, 313.55 examples/s]Tokenizing train dataset:  27%|██▋       | 1331/4890 [00:04<00:11, 314.11 examples/s]Tokenizing train dataset:  27%|██▋       | 1331/4890 [00:04<00:11, 313.58 examples/s]Tokenizing train dataset:  28%|██▊       | 1348/4890 [00:04<00:11, 315.29 examples/s]Tokenizing train dataset:  28%|██▊       | 1373/4890 [00:04<00:11, 298.24 examples/s]Tokenizing train dataset:  28%|██▊       | 1373/4890 [00:04<00:11, 297.61 examples/s]Tokenizing train dataset:  28%|██▊       | 1390/4890 [00:04<00:11, 296.23 examples/s]Tokenizing train dataset:  29%|██▉       | 1407/4890 [00:04<00:11, 304.66 examples/s]Tokenizing train dataset:  29%|██▉       | 1407/4890 [00:04<00:11, 303.87 examples/s]Tokenizing train dataset:  29%|██▉       | 1422/4890 [00:04<00:11, 300.98 examples/s]Tokenizing train dataset:  30%|██▉       | 1453/4890 [00:04<00:11, 298.61 examples/s]Tokenizing train dataset:  30%|██▉       | 1453/4890 [00:04<00:11, 300.98 examples/s]Tokenizing train dataset:  30%|██▉       | 1453/4890 [00:04<00:11, 299.94 examples/s]Tokenizing train dataset:  31%|███       | 1499/4890 [00:04<00:11, 296.45 examples/s]Tokenizing train dataset:  31%|███       | 1499/4890 [00:04<00:11, 298.28 examples/s]Tokenizing train dataset:  31%|███       | 1499/4890 [00:04<00:11, 297.23 examples/s]Tokenizing train dataset:  31%|███▏      | 1530/4890 [00:04<00:11, 296.03 examples/s]Tokenizing train dataset:  31%|███▏      | 1530/4890 [00:05<00:11, 297.68 examples/s]Tokenizing train dataset:  31%|███▏      | 1530/4890 [00:05<00:11, 296.53 examples/s]Tokenizing train dataset:  32%|███▏      | 1561/4890 [00:05<00:11, 297.24 examples/s]Tokenizing train dataset:  32%|███▏      | 1561/4890 [00:05<00:11, 298.30 examples/s]Tokenizing train dataset:  32%|███▏      | 1561/4890 [00:05<00:11, 297.45 examples/s]Tokenizing train dataset:  33%|███▎      | 1591/4890 [00:05<00:11, 293.04 examples/s]Tokenizing train dataset:  33%|███▎      | 1591/4890 [00:05<00:11, 293.97 examples/s]Tokenizing train dataset:  33%|███▎      | 1591/4890 [00:05<00:11, 292.88 examples/s]Tokenizing train dataset:  33%|███▎      | 1627/4890 [00:05<00:10, 303.82 examples/s]Tokenizing train dataset:  33%|███▎      | 1627/4890 [00:05<00:10, 304.11 examples/s]Tokenizing train dataset:  33%|███▎      | 1627/4890 [00:05<00:10, 303.36 examples/s]Tokenizing train dataset:  34%|███▍      | 1661/4890 [00:05<00:10, 312.04 examples/s]Tokenizing train dataset:  34%|███▍      | 1661/4890 [00:05<00:10, 312.19 examples/s]Tokenizing train dataset:  34%|███▍      | 1661/4890 [00:05<00:10, 311.20 examples/s]Tokenizing train dataset:  35%|███▍      | 1701/4890 [00:05<00:09, 332.28 examples/s]Tokenizing train dataset:  35%|███▍      | 1701/4890 [00:05<00:09, 331.93 examples/s]Tokenizing train dataset:  35%|███▍      | 1701/4890 [00:05<00:09, 330.92 examples/s]Tokenizing train dataset:  36%|███▌      | 1751/4890 [00:05<00:09, 328.92 examples/s]Tokenizing train dataset:  36%|███▌      | 1751/4890 [00:05<00:09, 328.73 examples/s]Tokenizing train dataset:  36%|███▌      | 1751/4890 [00:05<00:09, 327.78 examples/s]Tokenizing train dataset:  37%|███▋      | 1788/4890 [00:05<00:09, 338.57 examples/s]Tokenizing train dataset:  37%|███▋      | 1788/4890 [00:05<00:09, 338.26 examples/s]Tokenizing train dataset:  37%|███▋      | 1788/4890 [00:05<00:09, 337.52 examples/s]Tokenizing train dataset:  37%|███▋      | 1829/4890 [00:05<00:09, 311.79 examples/s]Tokenizing train dataset:  37%|███▋      | 1829/4890 [00:05<00:09, 311.93 examples/s]Tokenizing train dataset:  37%|███▋      | 1829/4890 [00:05<00:09, 311.02 examples/s]Tokenizing train dataset:  38%|███▊      | 1861/4890 [00:06<00:09, 307.29 examples/s]Tokenizing train dataset:  38%|███▊      | 1861/4890 [00:06<00:09, 307.58 examples/s]Tokenizing train dataset:  38%|███▊      | 1861/4890 [00:06<00:09, 306.57 examples/s]Tokenizing train dataset:  39%|███▉      | 1901/4890 [00:06<00:10, 287.38 examples/s]Tokenizing train dataset:  39%|███▉      | 1901/4890 [00:06<00:10, 287.96 examples/s]Tokenizing train dataset:  39%|███▉      | 1900/4890 [00:06<00:10, 287.60 examples/s]Tokenizing train dataset:  40%|███▉      | 1941/4890 [00:06<00:10, 276.94 examples/s]Tokenizing train dataset:  40%|███▉      | 1941/4890 [00:06<00:10, 277.43 examples/s]Tokenizing train dataset:  40%|███▉      | 1940/4890 [00:06<00:10, 275.64 examples/s]Tokenizing train dataset:  40%|████      | 1971/4890 [00:06<00:10, 278.92 examples/s]Tokenizing train dataset:  40%|████      | 1971/4890 [00:06<00:10, 279.33 examples/s]Tokenizing train dataset:  40%|████      | 1969/4890 [00:06<00:10, 276.27 examples/s]Tokenizing train dataset:  41%|████      | 2002/4890 [00:06<00:10, 285.55 examples/s]Tokenizing train dataset:  41%|████      | 2002/4890 [00:06<00:10, 285.95 examples/s]Tokenizing train dataset:  41%|████      | 2000/4890 [00:06<00:10, 283.46 examples/s]Tokenizing train dataset:  42%|████▏     | 2036/4890 [00:06<00:09, 297.06 examples/s]Tokenizing train dataset:  42%|████▏     | 2035/4890 [00:06<00:09, 296.57 examples/s]Tokenizing train dataset:  42%|████▏     | 2035/4890 [00:06<00:09, 296.00 examples/s]Tokenizing train dataset:  42%|████▏     | 2073/4890 [00:06<00:08, 313.76 examples/s]Tokenizing train dataset:  42%|████▏     | 2072/4890 [00:06<00:08, 314.84 examples/s]Tokenizing train dataset:  42%|████▏     | 2072/4890 [00:06<00:08, 313.79 examples/s]Tokenizing train dataset:  43%|████▎     | 2108/4890 [00:06<00:08, 322.38 examples/s]Tokenizing train dataset:  43%|████▎     | 2106/4890 [00:06<00:08, 319.71 examples/s]Tokenizing train dataset:  43%|████▎     | 2106/4890 [00:06<00:08, 318.52 examples/s]Tokenizing train dataset:  44%|████▍     | 2152/4890 [00:06<00:08, 304.99 examples/s]Tokenizing train dataset:  44%|████▍     | 2151/4890 [00:07<00:08, 306.52 examples/s]Tokenizing train dataset:  44%|████▍     | 2150/4890 [00:07<00:08, 305.94 examples/s]Tokenizing train dataset:  45%|████▍     | 2185/4890 [00:07<00:08, 308.53 examples/s]Tokenizing train dataset:  45%|████▍     | 2197/4890 [00:07<00:08, 302.48 examples/s]Tokenizing train dataset:  45%|████▍     | 2196/4890 [00:07<00:08, 300.92 examples/s]Tokenizing train dataset:  46%|████▌     | 2232/4890 [00:07<00:12, 221.00 examples/s]Tokenizing train dataset:  46%|████▌     | 2229/4890 [00:07<00:11, 222.31 examples/s]Tokenizing train dataset:  46%|████▌     | 2232/4890 [00:07<00:13, 201.30 examples/s]Tokenizing train dataset:  46%|████▋     | 2267/4890 [00:07<00:10, 243.94 examples/s]Tokenizing train dataset:  46%|████▋     | 2266/4890 [00:07<00:10, 249.45 examples/s]Tokenizing train dataset:  46%|████▋     | 2267/4890 [00:07<00:11, 225.91 examples/s]Tokenizing train dataset:  47%|████▋     | 2296/4890 [00:07<00:09, 259.69 examples/s]Tokenizing train dataset:  47%|████▋     | 2298/4890 [00:07<00:10, 255.95 examples/s]Tokenizing train dataset:  47%|████▋     | 2298/4890 [00:07<00:10, 240.74 examples/s]Tokenizing train dataset:  48%|████▊     | 2326/4890 [00:07<00:09, 267.43 examples/s]Tokenizing train dataset:  48%|████▊     | 2328/4890 [00:07<00:09, 264.76 examples/s]Tokenizing train dataset:  48%|████▊     | 2328/4890 [00:07<00:10, 252.43 examples/s]Tokenizing train dataset:  48%|████▊     | 2369/4890 [00:07<00:09, 270.49 examples/s]Tokenizing train dataset:  48%|████▊     | 2371/4890 [00:07<00:09, 267.49 examples/s]Tokenizing train dataset:  48%|████▊     | 2371/4890 [00:07<00:09, 259.07 examples/s]Tokenizing train dataset:  49%|████▉     | 2398/4890 [00:08<00:09, 272.71 examples/s]Tokenizing train dataset:  49%|████▉     | 2401/4890 [00:08<00:09, 273.53 examples/s]Tokenizing train dataset:  49%|████▉     | 2401/4890 [00:08<00:09, 266.99 examples/s]Tokenizing train dataset:  50%|████▉     | 2432/4890 [00:08<00:08, 281.56 examples/s]Tokenizing train dataset:  50%|████▉     | 2431/4890 [00:08<00:08, 281.64 examples/s]Tokenizing train dataset:  50%|████▉     | 2433/4890 [00:08<00:08, 277.72 examples/s]Tokenizing train dataset:  51%|█████     | 2477/4890 [00:08<00:08, 285.83 examples/s]Tokenizing train dataset:  51%|█████     | 2477/4890 [00:08<00:08, 285.43 examples/s]Tokenizing train dataset:  51%|█████     | 2480/4890 [00:08<00:08, 282.82 examples/s]Tokenizing train dataset:  51%|█████▏    | 2515/4890 [00:08<00:08, 271.83 examples/s]Tokenizing train dataset:  51%|█████▏    | 2514/4890 [00:08<00:08, 270.36 examples/s]Tokenizing train dataset:  51%|█████▏    | 2518/4890 [00:08<00:08, 267.27 examples/s]Tokenizing train dataset:  52%|█████▏    | 2545/4890 [00:08<00:08, 274.99 examples/s]Tokenizing train dataset:  52%|█████▏    | 2543/4890 [00:08<00:08, 271.99 examples/s]Tokenizing train dataset:  52%|█████▏    | 2549/4890 [00:08<00:08, 275.45 examples/s]Tokenizing train dataset:  53%|█████▎    | 2579/4890 [00:08<00:08, 287.76 examples/s]Tokenizing train dataset:  53%|█████▎    | 2577/4890 [00:08<00:08, 286.64 examples/s]Tokenizing train dataset:  53%|█████▎    | 2583/4890 [00:08<00:07, 288.97 examples/s]Tokenizing train dataset:  53%|█████▎    | 2616/4890 [00:08<00:07, 306.23 examples/s]Tokenizing train dataset:  53%|█████▎    | 2612/4890 [00:08<00:07, 302.56 examples/s]Tokenizing train dataset:  54%|█████▎    | 2619/4890 [00:08<00:07, 302.52 examples/s]Tokenizing train dataset:  54%|█████▍    | 2649/4890 [00:08<00:07, 310.03 examples/s]Tokenizing train dataset:  54%|█████▍    | 2647/4890 [00:08<00:07, 312.47 examples/s]Tokenizing train dataset:  54%|█████▍    | 2653/4890 [00:08<00:07, 307.06 examples/s]Tokenizing train dataset:  55%|█████▍    | 2683/4890 [00:08<00:07, 313.56 examples/s]Tokenizing train dataset:  55%|█████▌    | 2697/4890 [00:09<00:06, 315.65 examples/s]Tokenizing train dataset:  55%|█████▍    | 2688/4890 [00:09<00:07, 312.68 examples/s]Tokenizing train dataset:  56%|█████▌    | 2719/4890 [00:09<00:06, 321.62 examples/s]Tokenizing train dataset:  56%|█████▌    | 2724/4890 [00:09<00:06, 319.15 examples/s]Tokenizing train dataset:  56%|█████▌    | 2742/4890 [00:09<00:06, 309.03 examples/s]Tokenizing train dataset:  56%|█████▋    | 2761/4890 [00:09<00:06, 305.66 examples/s]Tokenizing train dataset:  57%|█████▋    | 2767/4890 [00:09<00:06, 303.86 examples/s]Tokenizing train dataset:  57%|█████▋    | 2793/4890 [00:09<00:06, 307.23 examples/s]Tokenizing train dataset:  57%|█████▋    | 2788/4890 [00:09<00:06, 304.25 examples/s]Tokenizing train dataset:  57%|█████▋    | 2800/4890 [00:09<00:06, 308.36 examples/s]Tokenizing train dataset:  58%|█████▊    | 2820/4890 [00:09<00:06, 304.61 examples/s]Tokenizing train dataset:  58%|█████▊    | 2839/4890 [00:09<00:06, 302.01 examples/s]Tokenizing train dataset:  58%|█████▊    | 2845/4890 [00:09<00:06, 300.78 examples/s]Tokenizing train dataset:  59%|█████▊    | 2863/4890 [00:09<00:06, 296.77 examples/s]Tokenizing train dataset:  59%|█████▉    | 2881/4890 [00:09<00:06, 294.03 examples/s]Tokenizing train dataset:  59%|█████▉    | 2877/4890 [00:09<00:06, 300.90 examples/s]Tokenizing train dataset:  59%|█████▉    | 2895/4890 [00:09<00:06, 299.94 examples/s]Tokenizing train dataset:  60%|█████▉    | 2915/4890 [00:09<00:06, 299.01 examples/s]Tokenizing train dataset:  59%|█████▉    | 2908/4890 [00:09<00:06, 301.54 examples/s]Tokenizing train dataset:  60%|██████    | 2942/4890 [00:09<00:06, 300.95 examples/s]Tokenizing train dataset:  60%|██████    | 2950/4890 [00:09<00:06, 304.46 examples/s]Tokenizing train dataset:  60%|██████    | 2939/4890 [00:09<00:06, 301.09 examples/s]Tokenizing train dataset:  61%|██████    | 2974/4890 [00:09<00:06, 302.76 examples/s]Tokenizing train dataset:  61%|██████    | 2981/4890 [00:09<00:06, 304.24 examples/s]Tokenizing train dataset:  61%|██████    | 2973/4890 [00:09<00:06, 305.47 examples/s]Tokenizing train dataset:  61%|██████▏   | 3005/4890 [00:10<00:06, 299.54 examples/s]Tokenizing train dataset:  62%|██████▏   | 3013/4890 [00:10<00:06, 305.20 examples/s]Tokenizing train dataset:  62%|██████▏   | 3020/4890 [00:10<00:06, 304.01 examples/s]Tokenizing train dataset:  62%|██████▏   | 3051/4890 [00:10<00:06, 298.51 examples/s]Tokenizing train dataset:  62%|██████▏   | 3056/4890 [00:10<00:06, 295.21 examples/s]Tokenizing train dataset:  63%|██████▎   | 3062/4890 [00:10<00:06, 290.34 examples/s]Tokenizing train dataset:  63%|██████▎   | 3087/4890 [00:10<00:06, 293.70 examples/s]Tokenizing train dataset:  63%|██████▎   | 3094/4890 [00:10<00:06, 293.28 examples/s]Tokenizing train dataset:  63%|██████▎   | 3094/4890 [00:10<00:06, 295.69 examples/s]Tokenizing train dataset:  64%|██████▍   | 3120/4890 [00:10<00:05, 297.72 examples/s]Tokenizing train dataset:  64%|██████▍   | 3126/4890 [00:10<00:05, 295.36 examples/s]Tokenizing train dataset:  64%|██████▍   | 3126/4890 [00:10<00:05, 297.38 examples/s]Tokenizing train dataset:  65%|██████▍   | 3164/4890 [00:10<00:05, 292.00 examples/s]Tokenizing train dataset:  65%|██████▍   | 3171/4890 [00:10<00:05, 290.38 examples/s]Tokenizing train dataset:  65%|██████▍   | 3171/4890 [00:10<00:05, 291.55 examples/s]Tokenizing train dataset:  65%|██████▌   | 3198/4890 [00:10<00:05, 299.35 examples/s]Tokenizing train dataset:  66%|██████▌   | 3205/4890 [00:10<00:05, 299.19 examples/s]Tokenizing train dataset:  66%|██████▌   | 3205/4890 [00:10<00:05, 300.27 examples/s]Tokenizing train dataset:  66%|██████▌   | 3237/4890 [00:10<00:05, 299.66 examples/s]Tokenizing train dataset:  66%|██████▋   | 3245/4890 [00:10<00:05, 300.93 examples/s]Tokenizing train dataset:  66%|██████▌   | 3237/4890 [00:10<00:05, 300.38 examples/s]Tokenizing train dataset:  67%|██████▋   | 3270/4890 [00:10<00:05, 305.04 examples/s]Tokenizing train dataset:  67%|██████▋   | 3270/4890 [00:10<00:05, 305.85 examples/s]Tokenizing train dataset:  67%|██████▋   | 3294/4890 [00:11<00:05, 306.46 examples/s]Tokenizing train dataset:  68%|██████▊   | 3302/4890 [00:11<00:05, 302.01 examples/s]Tokenizing train dataset:  68%|██████▊   | 3302/4890 [00:11<00:05, 302.68 examples/s]Tokenizing train dataset:  68%|██████▊   | 3337/4890 [00:11<00:05, 298.01 examples/s]Tokenizing train dataset:  68%|██████▊   | 3349/4890 [00:11<00:05, 304.99 examples/s]Tokenizing train dataset:  68%|██████▊   | 3349/4890 [00:11<00:05, 305.81 examples/s]Tokenizing train dataset:  69%|██████▉   | 3368/4890 [00:11<00:05, 298.82 examples/s]Tokenizing train dataset:  69%|██████▉   | 3388/4890 [00:11<00:05, 285.01 examples/s]Tokenizing train dataset:  69%|██████▉   | 3388/4890 [00:11<00:05, 285.31 examples/s]Tokenizing train dataset:  70%|██████▉   | 3410/4890 [00:11<00:05, 282.89 examples/s]Tokenizing train dataset:  70%|██████▉   | 3417/4890 [00:11<00:05, 283.61 examples/s]Tokenizing train dataset:  70%|██████▉   | 3417/4890 [00:11<00:05, 283.87 examples/s]Tokenizing train dataset:  71%|███████   | 3450/4890 [00:11<00:05, 270.78 examples/s]Tokenizing train dataset:  71%|███████   | 3455/4890 [00:11<00:05, 266.66 examples/s]Tokenizing train dataset:  71%|███████   | 3455/4890 [00:11<00:05, 266.75 examples/s]Tokenizing train dataset:  71%|███████▏  | 3485/4890 [00:11<00:05, 257.28 examples/s]Tokenizing train dataset:  71%|███████▏  | 3489/4890 [00:11<00:05, 250.94 examples/s]Tokenizing train dataset:  71%|███████▏  | 3489/4890 [00:11<00:05, 250.97 examples/s]Tokenizing train dataset:  72%|███████▏  | 3523/4890 [00:11<00:04, 282.33 examples/s]Tokenizing train dataset:  72%|███████▏  | 3530/4890 [00:11<00:04, 286.57 examples/s]Tokenizing train dataset:  72%|███████▏  | 3530/4890 [00:11<00:04, 286.71 examples/s]Tokenizing train dataset:  73%|███████▎  | 3556/4890 [00:11<00:04, 286.67 examples/s]Tokenizing train dataset:  73%|███████▎  | 3561/4890 [00:11<00:04, 289.12 examples/s]Tokenizing train dataset:  73%|███████▎  | 3561/4890 [00:11<00:04, 289.41 examples/s]Tokenizing train dataset:  73%|███████▎  | 3591/4890 [00:12<00:04, 301.94 examples/s]Tokenizing train dataset:  74%|███████▎  | 3596/4890 [00:12<00:04, 299.14 examples/s]Tokenizing train dataset:  74%|███████▎  | 3596/4890 [00:12<00:04, 299.48 examples/s]Tokenizing train dataset:  74%|███████▍  | 3627/4890 [00:12<00:04, 300.60 examples/s]Tokenizing train dataset:  74%|███████▍  | 3636/4890 [00:12<00:04, 299.93 examples/s]Tokenizing train dataset:  74%|███████▍  | 3627/4890 [00:12<00:04, 301.17 examples/s]Tokenizing train dataset:  75%|███████▍  | 3661/4890 [00:12<00:03, 308.35 examples/s]Tokenizing train dataset:  75%|███████▌  | 3669/4890 [00:12<00:03, 306.87 examples/s]Tokenizing train dataset:  75%|███████▌  | 3668/4890 [00:12<00:04, 282.09 examples/s]Tokenizing train dataset:  76%|███████▌  | 3707/4890 [00:12<00:03, 299.14 examples/s]Tokenizing train dataset:  76%|███████▌  | 3713/4890 [00:12<00:03, 297.63 examples/s]Tokenizing train dataset:  76%|███████▌  | 3713/4890 [00:12<00:04, 283.33 examples/s]Tokenizing train dataset:  77%|███████▋  | 3750/4890 [00:12<00:03, 331.08 examples/s]Tokenizing train dataset:  77%|███████▋  | 3757/4890 [00:12<00:03, 329.20 examples/s]Tokenizing train dataset:  77%|███████▋  | 3757/4890 [00:12<00:03, 317.60 examples/s]Tokenizing train dataset:  78%|███████▊  | 3809/4890 [00:12<00:03, 333.87 examples/s]Tokenizing train dataset:  78%|███████▊  | 3800/4890 [00:12<00:03, 326.36 examples/s]Tokenizing train dataset:  78%|███████▊  | 3790/4890 [00:12<00:03, 319.47 examples/s]Tokenizing train dataset:  79%|███████▊  | 3844/4890 [00:12<00:03, 334.20 examples/s]Tokenizing train dataset:  78%|███████▊  | 3838/4890 [00:12<00:03, 336.39 examples/s]Tokenizing train dataset:  78%|███████▊  | 3826/4890 [00:12<00:03, 329.56 examples/s]Tokenizing train dataset:  79%|███████▉  | 3860/4890 [00:12<00:03, 326.64 examples/s]Tokenizing train dataset:  79%|███████▉  | 3884/4890 [00:12<00:03, 323.60 examples/s]Tokenizing train dataset:  80%|███████▉  | 3892/4890 [00:12<00:03, 324.74 examples/s]Tokenizing train dataset:  80%|███████▉  | 3904/4890 [00:13<00:03, 309.63 examples/s]Tokenizing train dataset:  80%|████████  | 3925/4890 [00:13<00:03, 301.72 examples/s]Tokenizing train dataset:  80%|████████  | 3934/4890 [00:13<00:03, 305.71 examples/s]Tokenizing train dataset:  81%|████████  | 3969/4890 [00:13<00:02, 312.28 examples/s]Tokenizing train dataset:  81%|████████  | 3944/4890 [00:13<00:03, 294.05 examples/s]Tokenizing train dataset:  81%|████████▏ | 3977/4890 [00:13<00:02, 310.90 examples/s]Tokenizing train dataset:  82%|████████▏ | 4007/4890 [00:13<00:02, 327.25 examples/s]Tokenizing train dataset:  81%|████████▏ | 3983/4890 [00:13<00:02, 313.56 examples/s]Tokenizing train dataset:  82%|████████▏ | 4016/4890 [00:13<00:02, 326.53 examples/s]Tokenizing train dataset:  82%|████████▏ | 4024/4890 [00:13<00:02, 336.49 examples/s]Tokenizing train dataset:  83%|████████▎ | 4053/4890 [00:13<00:02, 315.13 examples/s]Tokenizing train dataset:  83%|████████▎ | 4060/4890 [00:13<00:02, 309.31 examples/s]Tokenizing train dataset:  84%|████████▎ | 4089/4890 [00:13<00:02, 320.37 examples/s]Tokenizing train dataset:  83%|████████▎ | 4068/4890 [00:13<00:02, 315.40 examples/s]Tokenizing train dataset:  84%|████████▎ | 4095/4890 [00:13<00:02, 312.58 examples/s]Tokenizing train dataset:  84%|████████▍ | 4101/4890 [00:13<00:02, 314.44 examples/s]Tokenizing train dataset:  85%|████████▍ | 4133/4890 [00:13<00:02, 306.26 examples/s]Tokenizing train dataset:  85%|████████▍ | 4140/4890 [00:13<00:02, 302.08 examples/s]Tokenizing train dataset:  85%|████████▍ | 4143/4890 [00:13<00:02, 298.71 examples/s]Tokenizing train dataset:  85%|████████▌ | 4177/4890 [00:13<00:02, 299.58 examples/s]Tokenizing train dataset:  86%|████████▌ | 4186/4890 [00:13<00:02, 298.67 examples/s]Tokenizing train dataset:  85%|████████▌ | 4176/4890 [00:13<00:02, 299.98 examples/s]Tokenizing train dataset:  87%|████████▋ | 4262/4890 [00:14<00:01, 427.59 examples/s]Tokenizing train dataset:  88%|████████▊ | 4304/4890 [00:14<00:01, 498.50 examples/s]Tokenizing train dataset:  87%|████████▋ | 4260/4890 [00:14<00:01, 436.09 examples/s]Tokenizing train dataset:  90%|████████▉ | 4393/4890 [00:14<00:00, 646.49 examples/s]Tokenizing train dataset:  91%|█████████ | 4437/4890 [00:14<00:00, 700.25 examples/s]Tokenizing train dataset:  90%|████████▉ | 4390/4890 [00:14<00:00, 664.51 examples/s]Tokenizing train dataset:  92%|█████████▏| 4519/4890 [00:14<00:00, 807.17 examples/s]Tokenizing train dataset:  93%|█████████▎| 4563/4890 [00:14<00:00, 843.96 examples/s]Tokenizing train dataset:  92%|█████████▏| 4517/4890 [00:14<00:00, 829.56 examples/s]Tokenizing train dataset:  95%|█████████▍| 4644/4890 [00:14<00:00, 924.93 examples/s]Tokenizing train dataset:  96%|█████████▌| 4684/4890 [00:14<00:00, 941.22 examples/s]Tokenizing train dataset:  95%|█████████▍| 4642/4890 [00:14<00:00, 945.25 examples/s]Tokenizing train dataset:  98%|█████████▊| 4770/4890 [00:14<00:00, 1017.33 examples/s]Tokenizing train dataset:  98%|█████████▊| 4816/4890 [00:14<00:00, 1042.83 examples/s]Tokenizing train dataset:  98%|█████████▊| 4769/4890 [00:14<00:00, 1036.99 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 336.71 examples/s] 
Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 336.37 examples/s] 
Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 335.51 examples/s] 
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5564.74 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5519.65 examples/s]Extracting prompt in eval dataset:  58%|█████▊    | 552/953 [00:00<00:00, 5435.84 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5513.99 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5504.35 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5420.63 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  28%|██▊       | 271/953 [00:00<00:00, 2679.33 examples/s]Applying chat template to eval dataset:  28%|██▊       | 269/953 [00:00<00:00, 2663.67 examples/s]Applying chat template to eval dataset:  28%|██▊       | 270/953 [00:00<00:00, 2662.10 examples/s]Applying chat template to eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 2791.57 examples/s]Applying chat template to eval dataset:  58%|█████▊    | 548/953 [00:00<00:00, 2729.57 examples/s]Applying chat template to eval dataset:  58%|█████▊    | 554/953 [00:00<00:00, 2763.34 examples/s]Applying chat template to eval dataset:  87%|████████▋ | 833/953 [00:00<00:00, 2774.75 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2762.99 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2746.76 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2740.81 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2710.39 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2695.12 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 267.20 examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 264.96 examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 262.00 examples/s]Tokenizing eval dataset:   6%|▌         | 54/953 [00:00<00:03, 256.75 examples/s]Tokenizing eval dataset:   7%|▋         | 63/953 [00:00<00:03, 242.80 examples/s]Tokenizing eval dataset:   7%|▋         | 62/953 [00:00<00:03, 235.05 examples/s]Tokenizing eval dataset:  10%|▉         | 91/953 [00:00<00:03, 247.02 examples/s]Tokenizing eval dataset:  10%|▉         | 91/953 [00:00<00:03, 249.52 examples/s]Tokenizing eval dataset:   9%|▉         | 90/953 [00:00<00:03, 245.72 examples/s]Tokenizing eval dataset:  12%|█▏        | 116/953 [00:00<00:03, 243.59 examples/s]Tokenizing eval dataset:  13%|█▎        | 125/953 [00:00<00:03, 233.92 examples/s]Tokenizing eval dataset:  13%|█▎        | 123/953 [00:00<00:03, 230.34 examples/s]Tokenizing eval dataset:  16%|█▌        | 149/953 [00:00<00:03, 229.85 examples/s]Tokenizing eval dataset:  16%|█▌        | 149/953 [00:00<00:03, 232.01 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 226.07 examples/s]Tokenizing eval dataset:  19%|█▉        | 182/953 [00:00<00:03, 218.92 examples/s]Tokenizing eval dataset:  19%|█▉        | 182/953 [00:00<00:03, 220.92 examples/s]Tokenizing eval dataset:  20%|█▉        | 190/953 [00:00<00:03, 218.47 examples/s]Tokenizing eval dataset:  22%|██▏       | 209/953 [00:00<00:03, 230.16 examples/s]Tokenizing eval dataset:  22%|██▏       | 209/953 [00:00<00:03, 231.79 examples/s]Tokenizing eval dataset:  23%|██▎       | 220/953 [00:00<00:03, 236.22 examples/s]Tokenizing eval dataset:  27%|██▋       | 260/953 [00:00<00:02, 303.27 examples/s]Tokenizing eval dataset:  27%|██▋       | 260/953 [00:00<00:02, 305.67 examples/s]Tokenizing eval dataset:  30%|██▉       | 284/953 [00:01<00:01, 339.08 examples/s]Tokenizing eval dataset:  34%|███▍      | 327/953 [00:01<00:01, 400.03 examples/s]Tokenizing eval dataset:  34%|███▍      | 327/953 [00:01<00:01, 403.31 examples/s]Tokenizing eval dataset:  36%|███▋      | 347/953 [00:01<00:01, 416.08 examples/s]Tokenizing eval dataset:  41%|████      | 387/953 [00:01<00:01, 454.89 examples/s]Tokenizing eval dataset:  41%|████      | 387/953 [00:01<00:01, 458.13 examples/s]Tokenizing eval dataset:  47%|████▋     | 450/953 [00:01<00:00, 504.06 examples/s]Tokenizing eval dataset:  48%|████▊     | 456/953 [00:01<00:00, 523.01 examples/s]Tokenizing eval dataset:  43%|████▎     | 411/953 [00:01<00:01, 417.64 examples/s]Tokenizing eval dataset:  54%|█████▎    | 512/953 [00:01<00:00, 532.72 examples/s]Tokenizing eval dataset:  54%|█████▍    | 515/953 [00:01<00:00, 540.15 examples/s]Tokenizing eval dataset:  50%|█████     | 478/953 [00:01<00:00, 481.89 examples/s]Tokenizing eval dataset:  60%|██████    | 576/953 [00:01<00:00, 562.19 examples/s]Tokenizing eval dataset:  61%|██████    | 579/953 [00:01<00:00, 567.35 examples/s]Tokenizing eval dataset:  57%|█████▋    | 541/953 [00:01<00:00, 517.06 examples/s]Tokenizing eval dataset:  67%|██████▋   | 642/953 [00:01<00:00, 589.21 examples/s]Tokenizing eval dataset:  68%|██████▊   | 644/953 [00:01<00:00, 587.37 examples/s]Tokenizing eval dataset:  64%|██████▍   | 608/953 [00:01<00:00, 556.55 examples/s]Tokenizing eval dataset:  70%|██████▉   | 667/953 [00:01<00:00, 559.38 examples/s]Tokenizing eval dataset:  77%|███████▋  | 730/953 [00:01<00:00, 579.61 examples/s]Tokenizing eval dataset:  77%|███████▋  | 733/953 [00:01<00:00, 579.54 examples/s]Tokenizing eval dataset:  76%|███████▌  | 726/953 [00:01<00:00, 557.65 examples/s]Tokenizing eval dataset:  84%|████████▍ | 804/953 [00:01<00:00, 544.79 examples/s]Tokenizing eval dataset:  85%|████████▍ | 807/953 [00:01<00:00, 547.14 examples/s]Tokenizing eval dataset:  84%|████████▍ | 800/953 [00:01<00:00, 526.82 examples/s]Tokenizing eval dataset:  92%|█████████▏| 881/953 [00:02<00:00, 525.66 examples/s]Tokenizing eval dataset:  92%|█████████▏| 881/953 [00:02<00:00, 525.03 examples/s]Tokenizing eval dataset:  92%|█████████▏| 874/953 [00:02<00:00, 509.70 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 511.49 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 511.25 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 429.77 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 428.93 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|█████████▉| 949/953 [00:02<00:00, 502.31 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 415.81 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.457075595855713 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3120996952056885 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.355691909790039 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3415162563323975 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank12]:[W610 02:06:22.878840065 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 3 ---
