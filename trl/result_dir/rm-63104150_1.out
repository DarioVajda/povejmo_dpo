cpu-bind=MASK - gn02, task  1  0 [2351694]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 1 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 1     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63104150     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=1e-7 --total_epochs=3 --beta=0.1 --curriculum_stage=0
-------------------------------------------
[2025-06-12 16:15:05,847] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 16:15:07.570000 2351742 torch/distributed/run.py:792] 
W0612 16:15:07.570000 2351742 torch/distributed/run.py:792] *****************************************
W0612 16:15:07.570000 2351742 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 16:15:07.570000 2351742 torch/distributed/run.py:792] *****************************************
[2025-06-12 16:16:06,649] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 16:16:06,698] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 16:16:06,712] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 16:16:06,723] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
Namespace(rank=64, learning_rate=1e-07, total_epochs=3, beta=0.1, curriculum_stage=0)
1e-07
Namespace(rank=64, learning_rate=1e-07, total_epochs=3, beta=0.1, curriculum_stage=0)
1e-07
Namespace(rank=64, learning_rate=1e-07, total_epochs=3, beta=0.1, curriculum_stage=0)
1e-07
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=1e-07, total_epochs=3, beta=0.1, curriculum_stage=0)
1e-07
World size: 12
Setting gradient accumulation steps to: 1
[2025-06-12 16:16:12,297] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 16:16:12,302] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 16:16:12,311] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 4890
Validation dataset size: 953
Steps per epoch: 305
Evaluate each 152 steps
[2025-06-12 16:16:12,316] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: cjvt/GaMS-9B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:15, 25.31s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:17, 25.90s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:17, 25.90s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:17, 25.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:48<00:48, 24.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:49<00:48, 24.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:49<00:48, 24.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:49<00:48, 24.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:13<00:24, 24.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:24, 24.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:24, 24.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:24, 24.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 22.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.03s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 22.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.08s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 22.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 22.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.08s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:32<00:00, 23.08s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
[rank5]:[W612 16:17:49.469149505 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W612 16:17:49.478319195 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in train dataset:  11%|█         | 540/4890 [00:00<00:00, 5308.96 examples/s]Extracting prompt in train dataset:  22%|██▏       | 1091/4890 [00:00<00:00, 5421.72 examples/s]Extracting prompt in train dataset:  39%|███▉      | 1910/4890 [00:00<00:00, 5420.22 examples/s]Extracting prompt in train dataset:  51%|█████     | 2470/4890 [00:00<00:00, 5455.34 examples/s]Extracting prompt in train dataset:  67%|██████▋   | 3270/4890 [00:00<00:00, 5393.58 examples/s]Extracting prompt in train dataset:  78%|███████▊  | 3830/4890 [00:00<00:00, 5426.91 examples/s]Extracting prompt in train dataset:  90%|████████▉ | 4393/4890 [00:00<00:00, 5485.33 examples/s]Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 4651.51 examples/s]
Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   6%|▌         | 279/4890 [00:00<00:01, 2764.15 examples/s]Applying chat template to train dataset:  12%|█▏        | 582/4890 [00:00<00:01, 2906.15 examples/s]Applying chat template to train dataset:  18%|█▊        | 886/4890 [00:00<00:01, 2962.93 examples/s]Applying chat template to train dataset:  24%|██▍       | 1191/4890 [00:00<00:01, 2993.82 examples/s]Applying chat template to train dataset:  30%|███       | 1491/4890 [00:00<00:01, 2992.53 examples/s]Applying chat template to train dataset:  37%|███▋      | 1799/4890 [00:00<00:01, 3020.26 examples/s]Applying chat template to train dataset:  43%|████▎     | 2105/4890 [00:00<00:00, 3031.29 examples/s]Applying chat template to train dataset:  49%|████▉     | 2414/4890 [00:00<00:00, 3042.98 examples/s][rank6]:[W612 16:17:52.843243420 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Applying chat template to train dataset:  56%|█████▌    | 2722/4890 [00:00<00:00, 3051.92 examples/s]Applying chat template to train dataset:  65%|██████▍   | 3160/4890 [00:01<00:00, 2992.37 examples/s]Applying chat template to train dataset:  71%|███████   | 3466/4890 [00:01<00:00, 3009.76 examples/s]Applying chat template to train dataset:  77%|███████▋  | 3769/4890 [00:01<00:00, 3013.53 examples/s]Applying chat template to train dataset:  86%|████████▋ | 4222/4890 [00:01<00:00, 3012.60 examples/s]Applying chat template to train dataset:  93%|█████████▎| 4540/4890 [00:01<00:00, 3051.02 examples/s]Applying chat template to train dataset:  99%|█████████▉| 4857/4890 [00:01<00:00, 3081.70 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3016.15 examples/s]
Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 41/4890 [00:00<00:12, 390.36 examples/s]Tokenizing train dataset:   2%|▏         | 88/4890 [00:00<00:14, 333.78 examples/s]Tokenizing train dataset:   3%|▎         | 130/4890 [00:00<00:15, 305.13 examples/s]Tokenizing train dataset:   3%|▎         | 163/4890 [00:00<00:15, 309.92 examples/s]Tokenizing train dataset:   4%|▍         | 210/4890 [00:00<00:15, 305.00 examples/s]Tokenizing train dataset:   5%|▍         | 244/4890 [00:00<00:14, 313.40 examples/s]Tokenizing train dataset:   6%|▌         | 280/4890 [00:00<00:14, 319.37 examples/s]Tokenizing train dataset:   6%|▋         | 314/4890 [00:00<00:14, 319.55 examples/s]Tokenizing train dataset:   7%|▋         | 362/4890 [00:01<00:14, 314.46 examples/s]Tokenizing train dataset:   8%|▊         | 397/4890 [00:01<00:14, 316.27 examples/s]Tokenizing train dataset:   9%|▉         | 430/4890 [00:01<00:14, 316.85 examples/s]Tokenizing train dataset:  10%|▉         | 475/4890 [00:01<00:14, 305.99 examples/s]Tokenizing train dataset:  11%|█         | 519/4890 [00:01<00:14, 300.02 examples/s]Tokenizing train dataset:  11%|█▏        | 553/4890 [00:01<00:14, 307.40 examples/s]Tokenizing train dataset:  12%|█▏        | 585/4890 [00:01<00:14, 306.86 examples/s]Tokenizing train dataset:  13%|█▎        | 636/4890 [00:02<00:13, 316.94 examples/s]Tokenizing train dataset:  14%|█▍        | 682/4890 [00:02<00:13, 308.01 examples/s]Tokenizing train dataset:  15%|█▍        | 730/4890 [00:02<00:13, 309.13 examples/s]Tokenizing train dataset:  16%|█▌        | 762/4890 [00:02<00:13, 310.59 examples/s]Tokenizing train dataset:  16%|█▋        | 802/4890 [00:02<00:13, 295.34 examples/s]Tokenizing train dataset:  17%|█▋        | 847/4890 [00:02<00:13, 293.48 examples/s]Tokenizing train dataset:  18%|█▊        | 879/4890 [00:02<00:13, 294.24 examples/s]Tokenizing train dataset:  19%|█▊        | 916/4890 [00:02<00:12, 310.70 examples/s]Tokenizing train dataset:  20%|█▉        | 961/4890 [00:03<00:12, 302.46 examples/s]Tokenizing train dataset:  21%|██        | 1003/4890 [00:03<00:13, 291.18 examples/s]Tokenizing train dataset:  21%|██▏       | 1045/4890 [00:03<00:13, 284.62 examples/s]Tokenizing train dataset:  22%|██▏       | 1080/4890 [00:03<00:12, 296.75 examples/s]Tokenizing train dataset:  23%|██▎       | 1115/4890 [00:03<00:12, 305.73 examples/s]Tokenizing train dataset:  24%|██▎       | 1156/4890 [00:03<00:12, 289.12 examples/s]Tokenizing train dataset:  24%|██▍       | 1190/4890 [00:03<00:12, 294.39 examples/s]Tokenizing train dataset:  25%|██▍       | 1222/4890 [00:04<00:12, 296.50 examples/s]Tokenizing train dataset:  26%|██▌       | 1258/4890 [00:04<00:11, 311.45 examples/s]Tokenizing train dataset:  27%|██▋       | 1309/4890 [00:04<00:11, 314.38 examples/s]Tokenizing train dataset:  28%|██▊       | 1354/4890 [00:04<00:11, 306.19 examples/s]Tokenizing train dataset:  29%|██▊       | 1399/4890 [00:04<00:11, 300.60 examples/s]Tokenizing train dataset:  29%|██▉       | 1442/4890 [00:04<00:11, 293.52 examples/s]Tokenizing train dataset:  30%|███       | 1474/4890 [00:04<00:11, 293.71 examples/s]Tokenizing train dataset:  31%|███       | 1505/4890 [00:04<00:11, 290.11 examples/s]Tokenizing train dataset:  31%|███▏      | 1538/4890 [00:05<00:11, 296.56 examples/s]Tokenizing train dataset:  32%|███▏      | 1568/4890 [00:05<00:11, 294.62 examples/s]Tokenizing train dataset:  33%|███▎      | 1598/4890 [00:05<00:11, 292.79 examples/s]Tokenizing train dataset:  33%|███▎      | 1630/4890 [00:05<00:10, 299.13 examples/s]Tokenizing train dataset:  34%|███▍      | 1666/4890 [00:05<00:10, 310.21 examples/s]Tokenizing train dataset:  35%|███▍      | 1703/4890 [00:05<00:09, 319.49 examples/s]Tokenizing train dataset:  36%|███▌      | 1738/4890 [00:05<00:09, 326.19 examples/s]Tokenizing train dataset:  36%|███▌      | 1771/4890 [00:05<00:09, 323.55 examples/s]Tokenizing train dataset:  37%|███▋      | 1818/4890 [00:05<00:09, 314.33 examples/s]Tokenizing train dataset:  38%|███▊      | 1862/4890 [00:06<00:10, 302.01 examples/s]Tokenizing train dataset:  39%|███▉      | 1900/4890 [00:06<00:10, 281.99 examples/s]Tokenizing train dataset:  40%|███▉      | 1937/4890 [00:06<00:10, 269.82 examples/s]Tokenizing train dataset:  41%|████      | 1981/4890 [00:06<00:10, 274.14 examples/s]Tokenizing train dataset:  41%|████      | 2013/4890 [00:06<00:10, 281.83 examples/s]Tokenizing train dataset:  42%|████▏     | 2046/4890 [00:06<00:09, 291.24 examples/s]Tokenizing train dataset:  43%|████▎     | 2081/4890 [00:06<00:09, 303.75 examples/s]Tokenizing train dataset:  43%|████▎     | 2113/4890 [00:06<00:09, 306.32 examples/s]Tokenizing train dataset:  44%|████▍     | 2146/4890 [00:07<00:08, 306.16 examples/s]Tokenizing train dataset:  45%|████▍     | 2190/4890 [00:07<00:09, 297.13 examples/s]Tokenizing train dataset:  46%|████▌     | 2240/4890 [00:07<00:08, 302.14 examples/s]Tokenizing train dataset:  46%|████▋     | 2273/4890 [00:07<00:08, 305.39 examples/s]Tokenizing train dataset:  47%|████▋     | 2319/4890 [00:07<00:08, 300.99 examples/s]Tokenizing train dataset:  48%|████▊     | 2360/4890 [00:07<00:08, 288.44 examples/s]Tokenizing train dataset:  49%|████▉     | 2404/4890 [00:07<00:08, 286.62 examples/s]Tokenizing train dataset:  50%|████▉     | 2435/4890 [00:08<00:08, 290.93 examples/s]Tokenizing train dataset:  51%|█████     | 2480/4890 [00:08<00:08, 288.57 examples/s]Tokenizing train dataset:  51%|█████▏    | 2516/4890 [00:08<00:08, 270.84 examples/s]Tokenizing train dataset:  52%|█████▏    | 2547/4890 [00:08<00:08, 276.39 examples/s]Tokenizing train dataset:  53%|█████▎    | 2579/4890 [00:08<00:08, 284.70 examples/s]Tokenizing train dataset:  53%|█████▎    | 2616/4890 [00:08<00:07, 301.80 examples/s]Tokenizing train dataset:  54%|█████▍    | 2649/4890 [00:08<00:07, 304.58 examples/s]Tokenizing train dataset:  55%|█████▍    | 2681/4890 [00:08<00:07, 308.67 examples/s]Tokenizing train dataset:  56%|█████▌    | 2715/4890 [00:09<00:06, 314.92 examples/s]Tokenizing train dataset:  56%|█████▋    | 2758/4890 [00:09<00:07, 302.73 examples/s]Tokenizing train dataset:  57%|█████▋    | 2804/4890 [00:09<00:06, 300.71 examples/s]Tokenizing train dataset:  58%|█████▊    | 2848/4890 [00:09<00:06, 292.80 examples/s]Tokenizing train dataset:  59%|█████▉    | 2879/4890 [00:09<00:06, 292.12 examples/s]Tokenizing train dataset:  60%|█████▉    | 2910/4890 [00:09<00:06, 293.94 examples/s]Tokenizing train dataset:  60%|██████    | 2942/4890 [00:09<00:06, 296.13 examples/s]Tokenizing train dataset:  61%|██████    | 2974/4890 [00:09<00:06, 297.83 examples/s]Tokenizing train dataset:  61%|██████▏   | 3004/4890 [00:10<00:06, 292.83 examples/s]Tokenizing train dataset:  62%|██████▏   | 3034/4890 [00:10<00:06, 289.92 examples/s]Tokenizing train dataset:  63%|██████▎   | 3079/4890 [00:10<00:06, 289.12 examples/s]Tokenizing train dataset:  64%|██████▎   | 3109/4890 [00:10<00:06, 290.70 examples/s]Tokenizing train dataset:  64%|██████▍   | 3151/4890 [00:10<00:06, 281.92 examples/s]Tokenizing train dataset:  65%|██████▌   | 3188/4890 [00:10<00:05, 296.79 examples/s]Tokenizing train dataset:  66%|██████▌   | 3231/4890 [00:10<00:05, 290.82 examples/s]Tokenizing train dataset:  67%|██████▋   | 3266/4890 [00:10<00:05, 301.68 examples/s]Tokenizing train dataset:  67%|██████▋   | 3297/4890 [00:11<00:05, 299.29 examples/s]Tokenizing train dataset:  68%|██████▊   | 3340/4890 [00:11<00:05, 291.02 examples/s]Tokenizing train dataset:  69%|██████▉   | 3371/4890 [00:11<00:05, 288.47 examples/s]Tokenizing train dataset:  70%|██████▉   | 3410/4890 [00:11<00:05, 276.50 examples/s]Tokenizing train dataset:  71%|███████   | 3449/4890 [00:11<00:05, 267.46 examples/s]Tokenizing train dataset:  71%|███████   | 3483/4890 [00:11<00:05, 249.36 examples/s]Tokenizing train dataset:  72%|███████▏  | 3520/4890 [00:11<00:05, 272.35 examples/s]Tokenizing train dataset:  73%|███████▎  | 3551/4890 [00:11<00:04, 279.22 examples/s]Tokenizing train dataset:  73%|███████▎  | 3589/4890 [00:12<00:04, 296.98 examples/s]Tokenizing train dataset:  74%|███████▍  | 3635/4890 [00:12<00:04, 295.70 examples/s]Tokenizing train dataset:  75%|███████▍  | 3667/4890 [00:12<00:04, 300.15 examples/s]Tokenizing train dataset:  76%|███████▌  | 3710/4890 [00:12<00:04, 289.98 examples/s]Tokenizing train dataset:  77%|███████▋  | 3753/4890 [00:12<00:03, 323.60 examples/s]Tokenizing train dataset:  78%|███████▊  | 3801/4890 [00:12<00:03, 319.25 examples/s]Tokenizing train dataset:  79%|███████▊  | 3839/4890 [00:12<00:03, 329.91 examples/s]Tokenizing train dataset:  79%|███████▉  | 3885/4890 [00:12<00:03, 316.05 examples/s]Tokenizing train dataset:  80%|████████  | 3925/4890 [00:13<00:03, 295.59 examples/s]Tokenizing train dataset:  81%|████████▏ | 3977/4890 [00:13<00:02, 305.06 examples/s]Tokenizing train dataset:  82%|████████▏ | 4015/4890 [00:13<00:02, 320.88 examples/s]Tokenizing train dataset:  83%|████████▎ | 4060/4890 [00:13<00:03, 272.48 examples/s]Tokenizing train dataset:  84%|████████▎ | 4095/4890 [00:13<00:02, 282.29 examples/s]Tokenizing train dataset:  84%|████████▍ | 4125/4890 [00:13<00:02, 282.40 examples/s]Tokenizing train dataset:  85%|████████▌ | 4170/4890 [00:14<00:02, 278.30 examples/s]Tokenizing train dataset:  87%|████████▋ | 4239/4890 [00:14<00:01, 370.90 examples/s]Tokenizing train dataset:  89%|████████▉ | 4364/4890 [00:14<00:00, 584.65 examples/s]Tokenizing train dataset:  92%|█████████▏| 4491/4890 [00:14<00:00, 759.69 examples/s]Tokenizing train dataset:  94%|█████████▍| 4612/4890 [00:14<00:00, 878.50 examples/s]Tokenizing train dataset:  97%|█████████▋| 4733/4890 [00:14<00:00, 969.94 examples/s]Tokenizing train dataset:  99%|█████████▉| 4859/4890 [00:14<00:00, 1049.22 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 333.53 examples/s] 
[rank4]:[W612 16:18:08.851470978 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in train dataset:  11%|█▏        | 556/4890 [00:00<00:00, 5520.25 examples/s]Extracting prompt in train dataset:  11%|█▏        | 558/4890 [00:00<00:00, 5539.88 examples/s]Extracting prompt in eval dataset:  61%|██████    | 579/953 [00:00<00:00, 5695.66 examples/s]Extracting prompt in train dataset:  12%|█▏        | 570/4890 [00:00<00:00, 5555.25 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5640.07 examples/s]
Extracting prompt in train dataset:  23%|██▎       | 1116/4890 [00:00<00:00, 5558.92 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1120/4890 [00:00<00:00, 5562.73 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1142/4890 [00:00<00:00, 5649.55 examples/s]Extracting prompt in train dataset:  34%|███▍      | 1680/4890 [00:00<00:00, 5540.80 examples/s]Extracting prompt in train dataset:  35%|███▍      | 1710/4890 [00:00<00:00, 5638.33 examples/s]Extracting prompt in train dataset:  40%|███▉      | 1955/4890 [00:00<00:00, 5550.64 examples/s]Extracting prompt in train dataset:  46%|████▌     | 2249/4890 [00:00<00:00, 5598.01 examples/s]Extracting prompt in train dataset:  47%|████▋     | 2290/4890 [00:00<00:00, 5676.78 examples/s]Extracting prompt in train dataset:  52%|█████▏    | 2520/4890 [00:00<00:00, 5569.27 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 2810/4890 [00:00<00:00, 5465.75 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  64%|██████▍   | 3121/4890 [00:00<00:00, 5602.73 examples/s]Extracting prompt in train dataset:  68%|██████▊   | 3334/4890 [00:00<00:00, 5498.62 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 3372/4890 [00:00<00:00, 5513.18 examples/s]Applying chat template to eval dataset:  32%|███▏      | 307/953 [00:00<00:00, 3040.88 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 3700/4890 [00:00<00:00, 5639.30 examples/s]Extracting prompt in train dataset:  80%|███████▉  | 3896/4890 [00:00<00:00, 5533.00 examples/s]Extracting prompt in train dataset:  80%|████████  | 3931/4890 [00:00<00:00, 5533.22 examples/s]Applying chat template to eval dataset:  66%|██████▌   | 627/953 [00:00<00:00, 3127.56 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 4282/4890 [00:00<00:00, 5680.59 examples/s]Extracting prompt in train dataset:  91%|█████████▏| 4471/4890 [00:00<00:00, 5594.07 examples/s]Extracting prompt in train dataset:  92%|█████████▏| 4510/4890 [00:00<00:00, 5609.54 examples/s]Applying chat template to eval dataset:  99%|█████████▉| 945/953 [00:00<00:00, 3148.10 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3114.82 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5798.81 examples/s]Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5664.12 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5557.79 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5554.43 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 318.42 examples/s]Applying chat template to train dataset:   6%|▌         | 282/4890 [00:00<00:01, 2792.00 examples/s]Applying chat template to train dataset:   6%|▌         | 279/4890 [00:00<00:01, 2763.55 examples/s]Applying chat template to train dataset:   6%|▌         | 286/4890 [00:00<00:01, 2827.36 examples/s]Applying chat template to train dataset:  12%|█▏        | 591/4890 [00:00<00:01, 2960.31 examples/s]Applying chat template to train dataset:  12%|█▏        | 584/4890 [00:00<00:01, 2922.25 examples/s]Applying chat template to train dataset:  12%|█▏        | 600/4890 [00:00<00:01, 2994.36 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 288.42 examples/s]Applying chat template to train dataset:  18%|█▊        | 901/4890 [00:00<00:01, 3020.60 examples/s]Applying chat template to train dataset:  18%|█▊        | 890/4890 [00:00<00:01, 2975.35 examples/s]Applying chat template to train dataset:  19%|█▊        | 914/4890 [00:00<00:01, 3056.12 examples/s]Applying chat template to train dataset:  25%|██▍       | 1211/4890 [00:00<00:01, 3049.89 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 276.54 examples/s]Applying chat template to train dataset:  24%|██▍       | 1198/4890 [00:00<00:01, 3014.35 examples/s]Applying chat template to train dataset:  25%|██▌       | 1229/4890 [00:00<00:01, 3090.54 examples/s]Applying chat template to train dataset:  31%|███       | 1519/4890 [00:00<00:01, 3057.28 examples/s]Applying chat template to train dataset:  31%|███       | 1501/4890 [00:00<00:01, 3015.68 examples/s]Applying chat template to train dataset:  31%|███▏      | 1539/4890 [00:00<00:01, 3090.37 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.42 examples/s]Applying chat template to train dataset:  37%|███▋      | 1830/4890 [00:00<00:00, 3074.77 examples/s]Applying chat template to train dataset:  37%|███▋      | 1810/4890 [00:00<00:01, 3035.38 examples/s]Applying chat template to train dataset:  38%|███▊      | 1856/4890 [00:00<00:00, 3117.27 examples/s]Applying chat template to train dataset:  44%|████▍     | 2144/4890 [00:00<00:00, 3093.69 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.04 examples/s]Applying chat template to train dataset:  43%|████▎     | 2119/4890 [00:00<00:00, 3048.78 examples/s]Applying chat template to train dataset:  44%|████▍     | 2172/4890 [00:00<00:00, 3128.58 examples/s]Applying chat template to train dataset:  50%|█████     | 2456/4890 [00:00<00:00, 3097.81 examples/s]Applying chat template to train dataset:  50%|████▉     | 2426/4890 [00:00<00:00, 3054.48 examples/s]Tokenizing eval dataset:  24%|██▍       | 230/953 [00:00<00:02, 275.86 examples/s]Applying chat template to train dataset:  51%|█████     | 2490/4890 [00:00<00:00, 3136.32 examples/s]Applying chat template to train dataset:  57%|█████▋    | 2767/4890 [00:00<00:00, 3100.68 examples/s]Applying chat template to train dataset:  56%|█████▌    | 2732/4890 [00:00<00:00, 3054.70 examples/s]Tokenizing eval dataset:  31%|███       | 295/953 [00:00<00:01, 371.57 examples/s]Applying chat template to train dataset:  57%|█████▋    | 2807/4890 [00:00<00:00, 3143.95 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 439.12 examples/s]Applying chat template to train dataset:  66%|██████▌   | 3215/4890 [00:01<00:00, 3047.65 examples/s]Applying chat template to train dataset:  65%|██████▍   | 3173/4890 [00:01<00:00, 3005.52 examples/s]Applying chat template to train dataset:  67%|██████▋   | 3260/4890 [00:01<00:00, 3090.38 examples/s]Tokenizing eval dataset:  45%|████▍     | 425/953 [00:01<00:01, 495.74 examples/s]Applying chat template to train dataset:  72%|███████▏  | 3524/4890 [00:01<00:00, 3056.14 examples/s]Applying chat template to train dataset:  71%|███████   | 3478/4890 [00:01<00:00, 3016.35 examples/s]Applying chat template to train dataset:  73%|███████▎  | 3572/4890 [00:01<00:00, 3097.91 examples/s]Tokenizing eval dataset:  52%|█████▏    | 491/953 [00:01<00:00, 540.41 examples/s]Applying chat template to train dataset:  78%|███████▊  | 3834/4890 [00:01<00:00, 3067.12 examples/s]Applying chat template to train dataset:  77%|███████▋  | 3783/4890 [00:01<00:00, 3024.27 examples/s]Applying chat template to train dataset:  80%|███████▉  | 3889/4890 [00:01<00:00, 3114.12 examples/s]Tokenizing eval dataset:  59%|█████▊    | 558/953 [00:01<00:00, 572.47 examples/s]Applying chat template to train dataset:  85%|████████▍ | 4146/4890 [00:01<00:00, 3074.40 examples/s]Applying chat template to train dataset:  84%|████████▎ | 4090/4890 [00:01<00:00, 3031.70 examples/s]Applying chat template to train dataset:  86%|████████▌ | 4203/4890 [00:01<00:00, 3118.58 examples/s]Tokenizing eval dataset:  65%|██████▌   | 622/953 [00:01<00:00, 589.20 examples/s]Applying chat template to train dataset:  91%|█████████▏| 4469/4890 [00:01<00:00, 3116.47 examples/s]Applying chat template to train dataset:  90%|█████████ | 4407/4890 [00:01<00:00, 3067.73 examples/s]Applying chat template to train dataset:  93%|█████████▎| 4532/4890 [00:01<00:00, 3165.75 examples/s]Tokenizing eval dataset:  72%|███████▏  | 685/953 [00:01<00:00, 597.46 examples/s]Applying chat template to train dataset:  98%|█████████▊| 4792/4890 [00:01<00:00, 3147.30 examples/s]Applying chat template to train dataset:  97%|█████████▋| 4726/4890 [00:01<00:00, 3102.39 examples/s]Applying chat template to train dataset:  99%|█████████▉| 4860/4890 [00:01<00:00, 3197.94 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3076.21 examples/s]
Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3117.36 examples/s]
Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3035.42 examples/s]
Tokenizing eval dataset:  80%|████████  | 767/953 [00:01<00:00, 576.33 examples/s]Tokenizing eval dataset:  88%|████████▊ | 840/953 [00:01<00:00, 539.02 examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing eval dataset:  96%|█████████▌| 912/953 [00:02<00:00, 513.19 examples/s]Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:11, 411.51 examples/s]Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:11, 411.48 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 453.47 examples/s]
Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:11, 411.00 examples/s]Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:14, 341.90 examples/s]Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:14, 342.46 examples/s]Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:14, 340.96 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 323.20 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 323.69 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 322.32 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 311.01 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 311.17 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 310.19 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 316.44 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 316.63 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 315.31 examples/s]Tokenizing train dataset:   5%|▌         | 252/4890 [00:00<00:14, 320.89 examples/s]Tokenizing train dataset:   5%|▌         | 252/4890 [00:00<00:14, 321.25 examples/s]Tokenizing train dataset:   5%|▌         | 252/4890 [00:00<00:14, 320.04 examples/s]Tokenizing train dataset:   6%|▌         | 290/4890 [00:00<00:13, 334.07 examples/s]Tokenizing train dataset:   6%|▌         | 290/4890 [00:00<00:13, 333.99 examples/s]Tokenizing train dataset:   6%|▌         | 290/4890 [00:00<00:13, 333.16 examples/s]Tokenizing train dataset:   7%|▋         | 324/4890 [00:00<00:13, 332.76 examples/s]Tokenizing train dataset:   7%|▋         | 324/4890 [00:00<00:13, 332.90 examples/s]Tokenizing train dataset:   7%|▋         | 324/4890 [00:00<00:13, 331.95 examples/s]Tokenizing train dataset:   8%|▊         | 370/4890 [00:01<00:14, 318.60 examples/s]Tokenizing train dataset:   8%|▊         | 370/4890 [00:01<00:14, 318.62 examples/s]Tokenizing train dataset:   8%|▊         | 370/4890 [00:01<00:14, 318.22 examples/s]Tokenizing train dataset:   9%|▊         | 420/4890 [00:01<00:14, 316.86 examples/s]Tokenizing train dataset:   9%|▊         | 420/4890 [00:01<00:14, 316.92 examples/s]Tokenizing train dataset:   9%|▊         | 420/4890 [00:01<00:14, 316.47 examples/s]Tokenizing train dataset:  10%|▉         | 469/4890 [00:01<00:14, 314.82 examples/s]Tokenizing train dataset:  10%|▉         | 469/4890 [00:01<00:14, 314.91 examples/s]Tokenizing train dataset:  10%|▉         | 469/4890 [00:01<00:14, 314.58 examples/s]Tokenizing train dataset:  10%|█         | 512/4890 [00:01<00:14, 304.24 examples/s]Tokenizing train dataset:  10%|█         | 512/4890 [00:01<00:14, 304.44 examples/s]Tokenizing train dataset:  10%|█         | 512/4890 [00:01<00:14, 304.23 examples/s]Tokenizing train dataset:  11%|█         | 549/4890 [00:01<00:13, 317.19 examples/s]Tokenizing train dataset:  11%|█         | 549/4890 [00:01<00:13, 317.37 examples/s]Tokenizing train dataset:  11%|█         | 549/4890 [00:01<00:13, 317.20 examples/s]Tokenizing train dataset:  12%|█▏        | 596/4890 [00:01<00:13, 308.32 examples/s]Tokenizing train dataset:  12%|█▏        | 596/4890 [00:01<00:13, 308.67 examples/s]Tokenizing train dataset:  12%|█▏        | 596/4890 [00:01<00:13, 308.13 examples/s]Tokenizing train dataset:  13%|█▎        | 634/4890 [00:01<00:13, 322.49 examples/s]Tokenizing train dataset:  13%|█▎        | 634/4890 [00:01<00:13, 322.87 examples/s]Tokenizing train dataset:  13%|█▎        | 634/4890 [00:01<00:13, 322.02 examples/s]Tokenizing train dataset:  14%|█▍        | 681/4890 [00:02<00:13, 313.60 examples/s]Tokenizing train dataset:  14%|█▍        | 681/4890 [00:02<00:13, 314.11 examples/s]Tokenizing train dataset:  14%|█▍        | 681/4890 [00:02<00:13, 313.22 examples/s]Tokenizing train dataset:  15%|█▍        | 729/4890 [00:02<00:13, 311.90 examples/s]Tokenizing train dataset:  15%|█▍        | 729/4890 [00:02<00:13, 312.20 examples/s]Tokenizing train dataset:  15%|█▍        | 729/4890 [00:02<00:13, 311.02 examples/s]Tokenizing train dataset:  16%|█▌        | 763/4890 [00:02<00:13, 313.32 examples/s]Tokenizing train dataset:  16%|█▌        | 763/4890 [00:02<00:13, 313.73 examples/s]Tokenizing train dataset:  16%|█▌        | 761/4890 [00:02<00:13, 311.89 examples/s]Tokenizing train dataset:  16%|█▋        | 806/4890 [00:02<00:13, 300.38 examples/s]Tokenizing train dataset:  16%|█▋        | 806/4890 [00:02<00:13, 300.48 examples/s]Tokenizing train dataset:  16%|█▋        | 806/4890 [00:02<00:13, 299.19 examples/s]Tokenizing train dataset:  17%|█▋        | 850/4890 [00:02<00:13, 294.35 examples/s]Tokenizing train dataset:  17%|█▋        | 850/4890 [00:02<00:13, 294.68 examples/s]Tokenizing train dataset:  17%|█▋        | 850/4890 [00:02<00:13, 293.63 examples/s]Tokenizing train dataset:  18%|█▊        | 885/4890 [00:02<00:13, 302.53 examples/s]Tokenizing train dataset:  18%|█▊        | 885/4890 [00:02<00:13, 302.91 examples/s]Tokenizing train dataset:  18%|█▊        | 885/4890 [00:02<00:13, 301.86 examples/s]Tokenizing train dataset:  19%|█▉        | 920/4890 [00:02<00:12, 307.96 examples/s]Tokenizing train dataset:  19%|█▉        | 920/4890 [00:02<00:12, 308.11 examples/s]Tokenizing train dataset:  19%|█▉        | 920/4890 [00:02<00:12, 306.92 examples/s]Tokenizing train dataset:  19%|█▉        | 952/4890 [00:03<00:12, 306.05 examples/s]Tokenizing train dataset:  19%|█▉        | 952/4890 [00:03<00:12, 306.24 examples/s]Tokenizing train dataset:  20%|█▉        | 968/4890 [00:03<00:12, 309.85 examples/s]Tokenizing train dataset:  20%|██        | 983/4890 [00:03<00:12, 302.11 examples/s]Tokenizing train dataset:  20%|██        | 983/4890 [00:03<00:12, 302.15 examples/s]Tokenizing train dataset:  21%|██        | 1009/4890 [00:03<00:13, 296.17 examples/s]Tokenizing train dataset:  21%|██        | 1027/4890 [00:03<00:13, 292.43 examples/s]Tokenizing train dataset:  21%|██        | 1027/4890 [00:03<00:13, 292.82 examples/s]Tokenizing train dataset:  22%|██▏       | 1053/4890 [00:03<00:13, 290.85 examples/s]Tokenizing train dataset:  22%|██▏       | 1073/4890 [00:03<00:12, 293.90 examples/s]Tokenizing train dataset:  22%|██▏       | 1073/4890 [00:03<00:12, 294.25 examples/s]Tokenizing train dataset:  22%|██▏       | 1087/4890 [00:03<00:12, 299.48 examples/s]Tokenizing train dataset:  23%|██▎       | 1109/4890 [00:03<00:12, 307.30 examples/s]Tokenizing train dataset:  23%|██▎       | 1109/4890 [00:03<00:12, 307.75 examples/s]Tokenizing train dataset:  23%|██▎       | 1120/4890 [00:03<00:12, 301.69 examples/s]Tokenizing train dataset:  24%|██▎       | 1151/4890 [00:03<00:12, 294.88 examples/s]Tokenizing train dataset:  24%|██▎       | 1151/4890 [00:03<00:12, 295.22 examples/s]Tokenizing train dataset:  24%|██▎       | 1161/4890 [00:03<00:12, 289.10 examples/s]Tokenizing train dataset:  24%|██▍       | 1181/4890 [00:03<00:12, 295.91 examples/s]Tokenizing train dataset:  24%|██▍       | 1183/4890 [00:03<00:12, 295.52 examples/s]Tokenizing train dataset:  24%|██▍       | 1197/4890 [00:03<00:12, 303.26 examples/s]Tokenizing train dataset:  25%|██▍       | 1214/4890 [00:03<00:12, 300.02 examples/s]Tokenizing train dataset:  25%|██▍       | 1218/4890 [00:03<00:12, 303.98 examples/s]Tokenizing train dataset:  25%|██▌       | 1229/4890 [00:03<00:12, 303.66 examples/s]Tokenizing train dataset:  26%|██▌       | 1250/4890 [00:04<00:11, 314.35 examples/s]Tokenizing train dataset:  26%|██▌       | 1252/4890 [00:04<00:11, 312.08 examples/s]Tokenizing train dataset:  26%|██▌       | 1265/4890 [00:04<00:11, 313.86 examples/s]Tokenizing train dataset:  26%|██▋       | 1286/4890 [00:04<00:11, 309.96 examples/s]Tokenizing train dataset:  27%|██▋       | 1299/4890 [00:04<00:11, 315.72 examples/s]Tokenizing train dataset:  27%|██▋       | 1298/4890 [00:04<00:11, 312.34 examples/s]Tokenizing train dataset:  27%|██▋       | 1320/4890 [00:04<00:11, 316.94 examples/s]Tokenizing train dataset:  27%|██▋       | 1331/4890 [00:04<00:11, 314.50 examples/s]Tokenizing train dataset:  27%|██▋       | 1330/4890 [00:04<00:11, 312.57 examples/s]Tokenizing train dataset:  28%|██▊       | 1353/4890 [00:04<00:11, 313.70 examples/s]Tokenizing train dataset:  28%|██▊       | 1373/4890 [00:04<00:11, 298.57 examples/s]Tokenizing train dataset:  28%|██▊       | 1373/4890 [00:04<00:11, 294.44 examples/s]Tokenizing train dataset:  29%|██▊       | 1396/4890 [00:04<00:11, 301.39 examples/s]Tokenizing train dataset:  29%|██▉       | 1406/4890 [00:04<00:11, 301.69 examples/s]Tokenizing train dataset:  29%|██▉       | 1407/4890 [00:04<00:11, 299.71 examples/s]Tokenizing train dataset:  29%|██▉       | 1427/4890 [00:04<00:11, 300.47 examples/s]Tokenizing train dataset:  30%|██▉       | 1452/4890 [00:04<00:11, 297.88 examples/s]Tokenizing train dataset:  30%|██▉       | 1452/4890 [00:04<00:11, 296.64 examples/s]Tokenizing train dataset:  30%|███       | 1472/4890 [00:04<00:11, 294.56 examples/s]Tokenizing train dataset:  31%|███       | 1495/4890 [00:04<00:11, 291.82 examples/s]Tokenizing train dataset:  31%|███       | 1495/4890 [00:04<00:11, 290.65 examples/s]Tokenizing train dataset:  31%|███       | 1514/4890 [00:04<00:11, 284.57 examples/s]Tokenizing train dataset:  31%|███       | 1526/4890 [00:04<00:11, 294.58 examples/s]Tokenizing train dataset:  31%|███       | 1526/4890 [00:04<00:11, 293.48 examples/s]Tokenizing train dataset:  32%|███▏      | 1548/4890 [00:05<00:11, 295.53 examples/s]Tokenizing train dataset:  32%|███▏      | 1558/4890 [00:05<00:11, 297.69 examples/s]Tokenizing train dataset:  32%|███▏      | 1558/4890 [00:05<00:11, 296.25 examples/s]Tokenizing train dataset:  33%|███▎      | 1591/4890 [00:05<00:11, 289.12 examples/s]Tokenizing train dataset:  33%|███▎      | 1601/4890 [00:05<00:11, 291.53 examples/s]Tokenizing train dataset:  33%|███▎      | 1601/4890 [00:05<00:11, 290.07 examples/s]Tokenizing train dataset:  33%|███▎      | 1627/4890 [00:05<00:10, 297.83 examples/s]Tokenizing train dataset:  33%|███▎      | 1637/4890 [00:05<00:10, 303.61 examples/s]Tokenizing train dataset:  33%|███▎      | 1636/4890 [00:05<00:10, 302.48 examples/s]Tokenizing train dataset:  34%|███▍      | 1661/4890 [00:05<00:10, 305.42 examples/s]Tokenizing train dataset:  34%|███▍      | 1670/4890 [00:05<00:10, 307.75 examples/s]Tokenizing train dataset:  34%|███▍      | 1670/4890 [00:05<00:10, 306.05 examples/s]Tokenizing train dataset:  35%|███▍      | 1700/4890 [00:05<00:09, 325.06 examples/s]Tokenizing train dataset:  35%|███▍      | 1710/4890 [00:05<00:09, 325.55 examples/s]Tokenizing train dataset:  35%|███▍      | 1710/4890 [00:05<00:09, 323.67 examples/s]Tokenizing train dataset:  36%|███▌      | 1746/4890 [00:05<00:09, 331.45 examples/s]Tokenizing train dataset:  36%|███▌      | 1750/4890 [00:05<00:09, 322.71 examples/s]Tokenizing train dataset:  36%|███▌      | 1746/4890 [00:05<00:09, 329.37 examples/s]Tokenizing train dataset:  36%|███▋      | 1781/4890 [00:05<00:09, 331.98 examples/s]Tokenizing train dataset:  37%|███▋      | 1786/4890 [00:05<00:09, 329.25 examples/s]Tokenizing train dataset:  36%|███▋      | 1781/4890 [00:05<00:09, 330.35 examples/s]Tokenizing train dataset:  37%|███▋      | 1826/4890 [00:05<00:09, 313.26 examples/s]Tokenizing train dataset:  37%|███▋      | 1829/4890 [00:05<00:09, 306.90 examples/s]Tokenizing train dataset:  37%|███▋      | 1826/4890 [00:05<00:09, 311.64 examples/s]Tokenizing train dataset:  38%|███▊      | 1872/4890 [00:06<00:09, 304.94 examples/s]Tokenizing train dataset:  38%|███▊      | 1874/4890 [00:06<00:09, 301.89 examples/s]Tokenizing train dataset:  38%|███▊      | 1872/4890 [00:06<00:09, 303.20 examples/s]Tokenizing train dataset:  39%|███▉      | 1910/4890 [00:06<00:10, 281.13 examples/s]Tokenizing train dataset:  39%|███▉      | 1910/4890 [00:06<00:10, 277.87 examples/s]Tokenizing train dataset:  39%|███▉      | 1910/4890 [00:06<00:10, 279.40 examples/s]Tokenizing train dataset:  40%|███▉      | 1953/4890 [00:06<00:10, 279.96 examples/s]Tokenizing train dataset:  40%|███▉      | 1953/4890 [00:06<00:10, 276.79 examples/s]Tokenizing train dataset:  40%|███▉      | 1953/4890 [00:06<00:10, 277.86 examples/s]Tokenizing train dataset:  41%|████      | 1997/4890 [00:06<00:10, 281.82 examples/s]Tokenizing train dataset:  41%|████      | 1997/4890 [00:06<00:10, 278.91 examples/s]Tokenizing train dataset:  41%|████      | 1997/4890 [00:06<00:10, 279.91 examples/s]Tokenizing train dataset:  42%|████▏     | 2030/4890 [00:06<00:09, 290.26 examples/s]Tokenizing train dataset:  42%|████▏     | 2030/4890 [00:06<00:09, 286.67 examples/s]Tokenizing train dataset:  42%|████▏     | 2030/4890 [00:06<00:09, 288.41 examples/s]Tokenizing train dataset:  42%|████▏     | 2068/4890 [00:06<00:09, 309.77 examples/s]Tokenizing train dataset:  42%|████▏     | 2067/4890 [00:06<00:09, 304.79 examples/s]Tokenizing train dataset:  42%|████▏     | 2067/4890 [00:06<00:09, 307.79 examples/s]Tokenizing train dataset:  43%|████▎     | 2101/4890 [00:06<00:08, 310.77 examples/s]Tokenizing train dataset:  43%|████▎     | 2100/4890 [00:06<00:09, 307.13 examples/s]Tokenizing train dataset:  43%|████▎     | 2100/4890 [00:06<00:09, 309.90 examples/s]Tokenizing train dataset:  44%|████▍     | 2147/4890 [00:06<00:08, 307.25 examples/s]Tokenizing train dataset:  44%|████▍     | 2146/4890 [00:07<00:09, 304.17 examples/s]Tokenizing train dataset:  44%|████▍     | 2146/4890 [00:07<00:08, 306.35 examples/s]Tokenizing train dataset:  45%|████▍     | 2192/4890 [00:07<00:09, 299.21 examples/s]Tokenizing train dataset:  45%|████▍     | 2190/4890 [00:07<00:09, 296.19 examples/s]Tokenizing train dataset:  45%|████▍     | 2191/4890 [00:07<00:09, 299.23 examples/s]Tokenizing train dataset:  46%|████▌     | 2241/4890 [00:07<00:08, 304.97 examples/s]Tokenizing train dataset:  46%|████▌     | 2240/4890 [00:07<00:08, 301.21 examples/s]Tokenizing train dataset:  46%|████▌     | 2240/4890 [00:07<00:08, 303.97 examples/s]Tokenizing train dataset:  47%|████▋     | 2275/4890 [00:07<00:08, 308.12 examples/s]Tokenizing train dataset:  46%|████▋     | 2273/4890 [00:07<00:08, 303.89 examples/s]Tokenizing train dataset:  46%|████▋     | 2273/4890 [00:07<00:08, 307.00 examples/s]Tokenizing train dataset:  47%|████▋     | 2321/4890 [00:07<00:08, 302.99 examples/s]Tokenizing train dataset:  47%|████▋     | 2319/4890 [00:07<00:08, 299.31 examples/s]Tokenizing train dataset:  47%|████▋     | 2319/4890 [00:07<00:08, 302.85 examples/s]Tokenizing train dataset:  48%|████▊     | 2365/4890 [00:07<00:08, 294.61 examples/s]Tokenizing train dataset:  48%|████▊     | 2360/4890 [00:07<00:08, 287.67 examples/s]Tokenizing train dataset:  48%|████▊     | 2360/4890 [00:07<00:08, 290.51 examples/s]Tokenizing train dataset:  49%|████▉     | 2395/4890 [00:07<00:08, 292.25 examples/s]Tokenizing train dataset:  49%|████▉     | 2389/4890 [00:07<00:08, 286.46 examples/s]Tokenizing train dataset:  49%|████▉     | 2404/4890 [00:07<00:08, 288.81 examples/s]Tokenizing train dataset:  50%|████▉     | 2428/4890 [00:07<00:08, 296.65 examples/s]Tokenizing train dataset:  49%|████▉     | 2420/4890 [00:07<00:08, 288.90 examples/s]Tokenizing train dataset:  50%|████▉     | 2435/4890 [00:07<00:08, 292.86 examples/s]Tokenizing train dataset:  51%|█████     | 2471/4890 [00:08<00:08, 287.22 examples/s]Tokenizing train dataset:  50%|█████     | 2463/4890 [00:08<00:08, 283.21 examples/s]Tokenizing train dataset:  51%|█████     | 2480/4890 [00:08<00:08, 290.64 examples/s]Tokenizing train dataset:  51%|█████     | 2500/4890 [00:08<00:08, 285.43 examples/s]Tokenizing train dataset:  51%|█████     | 2497/4890 [00:08<00:08, 292.05 examples/s]Tokenizing train dataset:  51%|█████▏    | 2516/4890 [00:08<00:08, 272.64 examples/s]Tokenizing train dataset:  52%|█████▏    | 2540/4890 [00:08<00:08, 273.70 examples/s]Tokenizing train dataset:  52%|█████▏    | 2530/4890 [00:08<00:08, 263.26 examples/s]Tokenizing train dataset:  52%|█████▏    | 2547/4890 [00:08<00:08, 278.11 examples/s]Tokenizing train dataset:  53%|█████▎    | 2574/4890 [00:08<00:08, 287.23 examples/s]Tokenizing train dataset:  52%|█████▏    | 2567/4890 [00:08<00:08, 285.24 examples/s]Tokenizing train dataset:  53%|█████▎    | 2579/4890 [00:08<00:08, 286.48 examples/s]Tokenizing train dataset:  53%|█████▎    | 2610/4890 [00:08<00:07, 305.17 examples/s]Tokenizing train dataset:  53%|█████▎    | 2601/4890 [00:08<00:07, 298.37 examples/s]Tokenizing train dataset:  53%|█████▎    | 2616/4890 [00:08<00:07, 303.56 examples/s]Tokenizing train dataset:  54%|█████▍    | 2643/4890 [00:08<00:07, 309.30 examples/s]Tokenizing train dataset:  54%|█████▍    | 2634/4890 [00:08<00:07, 302.40 examples/s]Tokenizing train dataset:  54%|█████▍    | 2649/4890 [00:08<00:07, 306.68 examples/s]Tokenizing train dataset:  55%|█████▌    | 2692/4890 [00:08<00:07, 313.75 examples/s]Tokenizing train dataset:  55%|█████▍    | 2681/4890 [00:08<00:07, 305.23 examples/s]Tokenizing train dataset:  55%|█████▍    | 2683/4890 [00:08<00:07, 310.18 examples/s]Tokenizing train dataset:  56%|█████▌    | 2725/4890 [00:08<00:06, 314.64 examples/s]Tokenizing train dataset:  56%|█████▌    | 2715/4890 [00:08<00:06, 311.47 examples/s]Tokenizing train dataset:  56%|█████▌    | 2718/4890 [00:08<00:06, 319.60 examples/s]Tokenizing train dataset:  57%|█████▋    | 2768/4890 [00:09<00:07, 303.11 examples/s]Tokenizing train dataset:  56%|█████▋    | 2759/4890 [00:09<00:07, 301.75 examples/s]Tokenizing train dataset:  56%|█████▋    | 2761/4890 [00:09<00:07, 302.41 examples/s]Tokenizing train dataset:  57%|█████▋    | 2800/4890 [00:09<00:06, 306.01 examples/s]Tokenizing train dataset:  57%|█████▋    | 2790/4890 [00:09<00:06, 300.96 examples/s]Tokenizing train dataset:  57%|█████▋    | 2793/4890 [00:09<00:06, 303.64 examples/s]Tokenizing train dataset:  58%|█████▊    | 2845/4890 [00:09<00:06, 298.63 examples/s]Tokenizing train dataset:  58%|█████▊    | 2834/4890 [00:09<00:06, 295.18 examples/s]Tokenizing train dataset:  58%|█████▊    | 2839/4890 [00:09<00:06, 298.58 examples/s]Tokenizing train dataset:  59%|█████▉    | 2877/4890 [00:09<00:06, 299.07 examples/s]Tokenizing train dataset:  59%|█████▉    | 2879/4890 [00:09<00:06, 292.60 examples/s]Tokenizing train dataset:  59%|█████▉    | 2881/4890 [00:09<00:06, 290.66 examples/s]Tokenizing train dataset:  60%|█████▉    | 2922/4890 [00:09<00:06, 296.48 examples/s]Tokenizing train dataset:  60%|█████▉    | 2910/4890 [00:09<00:06, 293.70 examples/s]Tokenizing train dataset:  60%|█████▉    | 2914/4890 [00:09<00:06, 297.29 examples/s]Tokenizing train dataset:  60%|██████    | 2956/4890 [00:09<00:06, 302.52 examples/s]Tokenizing train dataset:  60%|██████    | 2942/4890 [00:09<00:06, 296.32 examples/s]Tokenizing train dataset:  60%|██████    | 2946/4890 [00:09<00:06, 299.56 examples/s]Tokenizing train dataset:  61%|██████    | 2989/4890 [00:09<00:06, 303.59 examples/s]Tokenizing train dataset:  61%|██████    | 2974/4890 [00:09<00:06, 298.34 examples/s]Tokenizing train dataset:  61%|██████    | 2977/4890 [00:09<00:06, 300.49 examples/s]Tokenizing train dataset:  62%|██████▏   | 3020/4890 [00:09<00:06, 301.96 examples/s]Tokenizing train dataset:  61%|██████▏   | 3005/4890 [00:09<00:06, 295.14 examples/s]Tokenizing train dataset:  62%|██████▏   | 3008/4890 [00:09<00:06, 300.43 examples/s]Tokenizing train dataset:  63%|██████▎   | 3062/4890 [00:10<00:06, 287.81 examples/s]Tokenizing train dataset:  62%|██████▏   | 3051/4890 [00:10<00:06, 295.57 examples/s]Tokenizing train dataset:  62%|██████▏   | 3052/4890 [00:10<00:06, 293.62 examples/s]Tokenizing train dataset:  63%|██████▎   | 3094/4890 [00:10<00:06, 293.87 examples/s]Tokenizing train dataset:  63%|██████▎   | 3093/4890 [00:10<00:06, 289.01 examples/s]Tokenizing train dataset:  63%|██████▎   | 3097/4890 [00:10<00:06, 290.26 examples/s]Tokenizing train dataset:  64%|██████▍   | 3126/4890 [00:10<00:05, 295.57 examples/s]Tokenizing train dataset:  64%|██████▍   | 3123/4890 [00:10<00:06, 291.30 examples/s]Tokenizing train dataset:  64%|██████▍   | 3135/4890 [00:10<00:06, 275.56 examples/s]Tokenizing train dataset:  65%|██████▍   | 3167/4890 [00:10<00:06, 282.32 examples/s]Tokenizing train dataset:  65%|██████▍   | 3169/4890 [00:10<00:05, 290.36 examples/s]Tokenizing train dataset:  65%|██████▍   | 3165/4890 [00:10<00:06, 278.08 examples/s]Tokenizing train dataset:  65%|██████▌   | 3200/4890 [00:10<00:05, 291.70 examples/s]Tokenizing train dataset:  66%|██████▌   | 3203/4890 [00:10<00:05, 298.12 examples/s]Tokenizing train dataset:  65%|██████▌   | 3198/4890 [00:10<00:05, 288.19 examples/s]Tokenizing train dataset:  66%|██████▌   | 3230/4890 [00:10<00:05, 290.28 examples/s]Tokenizing train dataset:  66%|██████▌   | 3234/4890 [00:10<00:05, 296.57 examples/s]Tokenizing train dataset:  67%|██████▋   | 3266/4890 [00:10<00:05, 302.42 examples/s]Tokenizing train dataset:  66%|██████▋   | 3243/4890 [00:10<00:05, 290.19 examples/s]Tokenizing train dataset:  67%|██████▋   | 3268/4890 [00:10<00:05, 304.66 examples/s]Tokenizing train dataset:  67%|██████▋   | 3297/4890 [00:10<00:05, 301.28 examples/s]Tokenizing train dataset:  67%|██████▋   | 3274/4890 [00:10<00:05, 293.05 examples/s]Tokenizing train dataset:  67%|██████▋   | 3299/4890 [00:10<00:05, 302.36 examples/s]Tokenizing train dataset:  68%|██████▊   | 3307/4890 [00:10<00:05, 299.36 examples/s]Tokenizing train dataset:  68%|██████▊   | 3340/4890 [00:11<00:05, 293.72 examples/s]Tokenizing train dataset:  68%|██████▊   | 3342/4890 [00:11<00:05, 293.64 examples/s]Tokenizing train dataset:  69%|██████▉   | 3371/4890 [00:11<00:05, 291.36 examples/s]Tokenizing train dataset:  69%|██████▊   | 3353/4890 [00:11<00:05, 298.17 examples/s]Tokenizing train dataset:  69%|██████▉   | 3383/4890 [00:11<00:05, 283.36 examples/s]Tokenizing train dataset:  70%|██████▉   | 3410/4890 [00:11<00:05, 279.10 examples/s]Tokenizing train dataset:  69%|██████▉   | 3390/4890 [00:11<00:05, 277.64 examples/s]Tokenizing train dataset:  70%|███████   | 3424/4890 [00:11<00:05, 275.38 examples/s]Tokenizing train dataset:  70%|██████▉   | 3419/4890 [00:11<00:05, 278.92 examples/s]Tokenizing train dataset:  71%|███████   | 3449/4890 [00:11<00:05, 269.73 examples/s]Tokenizing train dataset:  71%|███████   | 3460/4890 [00:11<00:05, 259.87 examples/s]Tokenizing train dataset:  71%|███████   | 3456/4890 [00:11<00:05, 264.08 examples/s]Tokenizing train dataset:  71%|███████   | 3483/4890 [00:11<00:05, 251.68 examples/s]Tokenizing train dataset:  72%|███████▏  | 3520/4890 [00:11<00:04, 275.44 examples/s]Tokenizing train dataset:  72%|███████▏  | 3497/4890 [00:11<00:05, 250.31 examples/s]Tokenizing train dataset:  71%|███████▏  | 3490/4890 [00:11<00:05, 248.56 examples/s]Tokenizing train dataset:  73%|███████▎  | 3551/4890 [00:11<00:04, 282.56 examples/s]Tokenizing train dataset:  72%|███████▏  | 3538/4890 [00:11<00:04, 283.96 examples/s]Tokenizing train dataset:  72%|███████▏  | 3531/4890 [00:11<00:04, 283.94 examples/s]Tokenizing train dataset:  73%|███████▎  | 3589/4890 [00:11<00:04, 300.61 examples/s]Tokenizing train dataset:  73%|███████▎  | 3561/4890 [00:11<00:04, 287.33 examples/s]Tokenizing train dataset:  73%|███████▎  | 3570/4890 [00:11<00:04, 287.28 examples/s]Tokenizing train dataset:  74%|███████▎  | 3595/4890 [00:11<00:04, 299.19 examples/s]Tokenizing train dataset:  74%|███████▎  | 3603/4890 [00:12<00:04, 293.63 examples/s]Tokenizing train dataset:  74%|███████▍  | 3635/4890 [00:12<00:04, 299.39 examples/s]Tokenizing train dataset:  74%|███████▍  | 3627/4890 [00:12<00:04, 298.04 examples/s]Tokenizing train dataset:  74%|███████▍  | 3635/4890 [00:12<00:04, 297.22 examples/s]Tokenizing train dataset:  75%|███████▌  | 3668/4890 [00:12<00:04, 302.84 examples/s]Tokenizing train dataset:  75%|███████▍  | 3660/4890 [00:12<00:04, 304.40 examples/s]Tokenizing train dataset:  75%|███████▍  | 3667/4890 [00:12<00:04, 301.23 examples/s]Tokenizing train dataset:  76%|███████▌  | 3711/4890 [00:12<00:03, 295.22 examples/s]Tokenizing train dataset:  76%|███████▌  | 3703/4890 [00:12<00:04, 293.85 examples/s]Tokenizing train dataset:  76%|███████▌  | 3710/4890 [00:12<00:04, 290.90 examples/s]Tokenizing train dataset:  77%|███████▋  | 3756/4890 [00:12<00:03, 328.61 examples/s]Tokenizing train dataset:  77%|███████▋  | 3747/4890 [00:12<00:03, 328.67 examples/s]Tokenizing train dataset:  77%|███████▋  | 3753/4890 [00:12<00:03, 325.56 examples/s]Tokenizing train dataset:  78%|███████▊  | 3808/4890 [00:12<00:03, 332.29 examples/s]Tokenizing train dataset:  78%|███████▊  | 3801/4890 [00:12<00:03, 321.38 examples/s]Tokenizing train dataset:  78%|███████▊  | 3797/4890 [00:12<00:03, 326.33 examples/s]Tokenizing train dataset:  79%|███████▊  | 3843/4890 [00:12<00:03, 332.84 examples/s]Tokenizing train dataset:  78%|███████▊  | 3834/4890 [00:12<00:03, 334.57 examples/s]Tokenizing train dataset:  79%|███████▊  | 3839/4890 [00:12<00:03, 331.95 examples/s]Tokenizing train dataset:  80%|███████▉  | 3890/4890 [00:12<00:03, 319.86 examples/s]Tokenizing train dataset:  79%|███████▉  | 3885/4890 [00:12<00:03, 317.94 examples/s]Tokenizing train dataset:  79%|███████▉  | 3881/4890 [00:12<00:03, 320.48 examples/s]Tokenizing train dataset:  80%|████████  | 3932/4890 [00:12<00:03, 302.25 examples/s]Tokenizing train dataset:  80%|████████  | 3925/4890 [00:13<00:03, 297.52 examples/s]Tokenizing train dataset:  80%|████████  | 3921/4890 [00:13<00:03, 297.56 examples/s]Tokenizing train dataset:  81%|████████  | 3967/4890 [00:13<00:02, 310.95 examples/s]Tokenizing train dataset:  82%|████████▏ | 4005/4890 [00:13<00:02, 325.33 examples/s]Tokenizing train dataset:  81%|████████▏ | 3977/4890 [00:13<00:02, 308.33 examples/s]Tokenizing train dataset:  81%|████████  | 3973/4890 [00:13<00:02, 308.15 examples/s]Tokenizing train dataset:  83%|████████▎ | 4039/4890 [00:13<00:02, 324.62 examples/s]Tokenizing train dataset:  82%|████████▏ | 4016/4890 [00:13<00:02, 324.82 examples/s]Tokenizing train dataset:  82%|████████▏ | 4013/4890 [00:13<00:02, 325.68 examples/s]Tokenizing train dataset:  84%|████████▎ | 4086/4890 [00:13<00:02, 313.64 examples/s]Tokenizing train dataset:  83%|████████▎ | 4060/4890 [00:13<00:02, 308.30 examples/s]Tokenizing train dataset:  83%|████████▎ | 4057/4890 [00:13<00:02, 309.87 examples/s]Tokenizing train dataset:  84%|████████▎ | 4090/4890 [00:13<00:02, 313.51 examples/s]Tokenizing train dataset:  84%|████████▎ | 4095/4890 [00:13<00:02, 311.86 examples/s]Tokenizing train dataset:  84%|████████▍ | 4130/4890 [00:13<00:02, 301.68 examples/s]Tokenizing train dataset:  85%|████████▍ | 4134/4890 [00:13<00:02, 303.27 examples/s]Tokenizing train dataset:  85%|████████▍ | 4140/4890 [00:13<00:02, 301.12 examples/s]Tokenizing train dataset:  85%|████████▌ | 4175/4890 [00:13<00:02, 296.43 examples/s]Tokenizing train dataset:  87%|████████▋ | 4255/4890 [00:13<00:01, 410.08 examples/s]Tokenizing train dataset:  85%|████████▌ | 4179/4890 [00:13<00:02, 298.10 examples/s]Tokenizing train dataset:  86%|████████▌ | 4186/4890 [00:13<00:02, 297.94 examples/s]Tokenizing train dataset:  90%|████████▉ | 4385/4890 [00:13<00:00, 628.11 examples/s]Tokenizing train dataset:  87%|████████▋ | 4269/4890 [00:13<00:01, 436.67 examples/s]Tokenizing train dataset:  88%|████████▊ | 4304/4890 [00:14<00:01, 497.21 examples/s]Tokenizing train dataset:  92%|█████████▏| 4510/4890 [00:14<00:00, 788.61 examples/s]Tokenizing train dataset:  90%|████████▉ | 4399/4890 [00:14<00:00, 649.51 examples/s]Tokenizing train dataset:  91%|█████████ | 4435/4890 [00:14<00:00, 695.15 examples/s]Tokenizing train dataset:  95%|█████████▍| 4636/4890 [00:14<00:00, 913.44 examples/s]Tokenizing train dataset:  93%|█████████▎| 4524/4890 [00:14<00:00, 804.74 examples/s]Tokenizing train dataset:  93%|█████████▎| 4561/4890 [00:14<00:00, 838.24 examples/s]Tokenizing train dataset:  97%|█████████▋| 4760/4890 [00:14<00:00, 1001.48 examples/s]Tokenizing train dataset:  95%|█████████▌| 4647/4890 [00:14<00:00, 917.00 examples/s]Tokenizing train dataset:  96%|█████████▌| 4681/4890 [00:14<00:00, 933.61 examples/s]Tokenizing train dataset: 100%|█████████▉| 4888/4890 [00:14<00:00, 1079.46 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 339.72 examples/s] 
Tokenizing train dataset:  98%|█████████▊| 4775/4890 [00:14<00:00, 1016.83 examples/s]Tokenizing train dataset:  98%|█████████▊| 4810/4890 [00:14<00:00, 1029.62 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 337.74 examples/s] 
Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 338.01 examples/s] 
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  59%|█████▊    | 559/953 [00:00<00:00, 5530.30 examples/s]Extracting prompt in eval dataset:  58%|█████▊    | 551/953 [00:00<00:00, 5422.13 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5502.02 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5504.24 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5484.37 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5438.99 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  30%|███       | 287/953 [00:00<00:00, 2843.11 examples/s]Applying chat template to eval dataset:  31%|███       | 295/953 [00:00<00:00, 2920.61 examples/s]Applying chat template to eval dataset:  31%|███       | 293/953 [00:00<00:00, 2898.73 examples/s]Applying chat template to eval dataset:  62%|██████▏   | 589/953 [00:00<00:00, 2927.77 examples/s]Applying chat template to eval dataset:  74%|███████▍  | 704/953 [00:00<00:00, 2787.99 examples/s]Applying chat template to eval dataset:  73%|███████▎  | 700/953 [00:00<00:00, 2762.87 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2852.10 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2855.85 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2771.31 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2740.56 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 284.26 examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 281.18 examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 283.34 examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 251.35 examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 249.94 examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 252.83 examples/s]Tokenizing eval dataset:  10%|▉         | 94/953 [00:00<00:03, 253.18 examples/s]Tokenizing eval dataset:  10%|▉         | 94/953 [00:00<00:03, 251.76 examples/s]Tokenizing eval dataset:  10%|▉         | 94/953 [00:00<00:03, 253.74 examples/s]Tokenizing eval dataset:  14%|█▎        | 130/953 [00:00<00:03, 238.12 examples/s]Tokenizing eval dataset:  14%|█▎        | 130/953 [00:00<00:03, 238.63 examples/s]Tokenizing eval dataset:  14%|█▎        | 130/953 [00:00<00:03, 238.93 examples/s]Tokenizing eval dataset:  17%|█▋        | 164/953 [00:00<00:03, 231.73 examples/s]Tokenizing eval dataset:  17%|█▋        | 164/953 [00:00<00:03, 229.96 examples/s]Tokenizing eval dataset:  17%|█▋        | 164/953 [00:00<00:03, 230.43 examples/s]Tokenizing eval dataset:  21%|██        | 196/953 [00:00<00:03, 219.19 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 217.54 examples/s]Tokenizing eval dataset:  21%|██        | 196/953 [00:00<00:03, 218.01 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:03, 234.19 examples/s]Tokenizing eval dataset:  23%|██▎       | 220/953 [00:00<00:03, 224.46 examples/s]Tokenizing eval dataset:  23%|██▎       | 223/953 [00:00<00:03, 228.87 examples/s]Tokenizing eval dataset:  30%|██▉       | 285/953 [00:01<00:02, 324.28 examples/s]Tokenizing eval dataset:  29%|██▉       | 277/953 [00:01<00:02, 312.32 examples/s]Tokenizing eval dataset:  29%|██▉       | 280/953 [00:01<00:02, 314.28 examples/s]Tokenizing eval dataset:  36%|███▌      | 339/953 [00:01<00:01, 380.70 examples/s]Tokenizing eval dataset:  35%|███▍      | 333/953 [00:01<00:01, 377.27 examples/s]Tokenizing eval dataset:  36%|███▌      | 339/953 [00:01<00:01, 380.95 examples/s]Tokenizing eval dataset:  41%|████▏     | 394/953 [00:01<00:01, 424.70 examples/s]Tokenizing eval dataset:  41%|████      | 391/953 [00:01<00:01, 428.92 examples/s]Tokenizing eval dataset:  42%|████▏     | 396/953 [00:01<00:01, 424.86 examples/s]Tokenizing eval dataset:  49%|████▉     | 467/953 [00:01<00:00, 503.41 examples/s]Tokenizing eval dataset:  49%|████▉     | 465/953 [00:01<00:00, 513.68 examples/s]Tokenizing eval dataset:  49%|████▉     | 469/953 [00:01<00:00, 501.78 examples/s]Tokenizing eval dataset:  55%|█████▌    | 526/953 [00:01<00:00, 525.69 examples/s]Tokenizing eval dataset:  55%|█████▍    | 522/953 [00:01<00:00, 526.91 examples/s]Tokenizing eval dataset:  56%|█████▌    | 529/953 [00:01<00:00, 526.12 examples/s]Tokenizing eval dataset:  62%|██████▏   | 592/953 [00:01<00:00, 562.16 examples/s]Tokenizing eval dataset:  62%|██████▏   | 590/953 [00:01<00:00, 562.82 examples/s]Tokenizing eval dataset:  63%|██████▎   | 600/953 [00:01<00:00, 571.78 examples/s]Tokenizing eval dataset:  69%|██████▉   | 656/953 [00:01<00:00, 580.23 examples/s]Tokenizing eval dataset:  69%|██████▊   | 653/953 [00:01<00:00, 581.79 examples/s]Tokenizing eval dataset:  69%|██████▉   | 661/953 [00:01<00:00, 580.39 examples/s]Tokenizing eval dataset:  76%|███████▌  | 720/953 [00:01<00:00, 580.80 examples/s]Tokenizing eval dataset:  78%|███████▊  | 739/953 [00:01<00:00, 565.13 examples/s]Tokenizing eval dataset:  77%|███████▋  | 736/953 [00:01<00:00, 567.68 examples/s]Tokenizing eval dataset:  83%|████████▎ | 794/953 [00:01<00:00, 545.64 examples/s]Tokenizing eval dataset:  85%|████████▌ | 814/953 [00:01<00:00, 535.53 examples/s]Tokenizing eval dataset:  85%|████████▌ | 811/953 [00:01<00:00, 539.99 examples/s]Tokenizing eval dataset:  91%|█████████ | 866/953 [00:02<00:00, 517.36 examples/s]Tokenizing eval dataset:  93%|█████████▎| 890/953 [00:02<00:00, 518.76 examples/s]Tokenizing eval dataset:  93%|█████████▎| 888/953 [00:02<00:00, 527.37 examples/s]Tokenizing eval dataset:  99%|█████████▊| 941/953 [00:02<00:00, 509.04 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 508.22 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 420.32 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 511.80 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 421.65 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 419.63 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.442598581314087 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.33077335357666 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3429391384124756 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.367783308029175 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank4]:[W612 17:56:16.543383526 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 1 ---
