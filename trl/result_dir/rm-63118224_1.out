cpu-bind=MASK - gn02, task  1  0 [2370586]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 1 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 1     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63118224     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=1e-6 --total_epochs=3 --beta=0.1 --curriculum_stage=2
-------------------------------------------
[2025-06-12 21:15:42,959] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 21:15:44.657000 2370637 torch/distributed/run.py:792] 
W0612 21:15:44.657000 2370637 torch/distributed/run.py:792] *****************************************
W0612 21:15:44.657000 2370637 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 21:15:44.657000 2370637 torch/distributed/run.py:792] *****************************************
[2025-06-12 21:15:56,377] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:15:56,397] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:15:56,426] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:15:56,442] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
World size: 12
Setting gradient accumulation steps to: 1
[2025-06-12 21:16:01,394] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 21:16:01,407] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 21:16:01,418] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 6690
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-12 21:16:01,445] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curriculum-1
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:27,  9.01s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:27,  9.09s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:27,  9.11s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:27,  9.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:15,  7.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:16,  8.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:16,  8.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:16,  8.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:08,  8.24s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.34s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.35s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.59s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.73s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.73s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:30<00:00,  7.74s/it]
Loaded model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank7]:[W612 21:16:36.326888422 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W612 21:16:36.336867205 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W612 21:16:36.394731640 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   8%|▊         | 550/6690 [00:00<00:01, 5423.92 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1113/6690 [00:00<00:01, 5517.10 examples/s]Extracting prompt in train dataset:  25%|██▌       | 1676/6690 [00:00<00:00, 5562.87 examples/s]Extracting prompt in train dataset:  34%|███▎      | 2248/6690 [00:00<00:00, 5606.25 examples/s]Extracting prompt in train dataset:  46%|████▌     | 3079/6690 [00:00<00:00, 5567.97 examples/s]Extracting prompt in train dataset:  54%|█████▍    | 3640/6690 [00:00<00:00, 5562.63 examples/s]Extracting prompt in train dataset:  63%|██████▎   | 4206/6690 [00:00<00:00, 5590.54 examples/s]Extracting prompt in train dataset:  75%|███████▌  | 5020/6690 [00:00<00:00, 5511.61 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 5584/6690 [00:01<00:00, 5545.10 examples/s]Extracting prompt in train dataset:  92%|█████████▏| 6145/6690 [00:01<00:00, 5560.98 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5428.91 examples/s]
Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 285/6690 [00:00<00:02, 2823.91 examples/s]Applying chat template to train dataset:   9%|▉         | 594/6690 [00:00<00:02, 2975.46 examples/s]Applying chat template to train dataset:  14%|█▎        | 906/6690 [00:00<00:01, 3035.42 examples/s]Applying chat template to train dataset:  18%|█▊        | 1216/6690 [00:00<00:01, 3055.32 examples/s]Applying chat template to train dataset:  23%|██▎       | 1526/6690 [00:00<00:01, 3071.14 examples/s]Applying chat template to train dataset:  27%|██▋       | 1837/6690 [00:00<00:01, 3082.58 examples/s]Applying chat template to train dataset:  32%|███▏      | 2148/6690 [00:00<00:01, 3089.57 examples/s]Applying chat template to train dataset:  39%|███▉      | 2611/6690 [00:00<00:01, 3083.72 examples/s]Applying chat template to train dataset:  44%|████▎     | 2923/6690 [00:00<00:01, 3090.07 examples/s]Applying chat template to train dataset:  48%|████▊     | 3235/6690 [00:01<00:01, 3095.44 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3546/6690 [00:01<00:01, 3094.68 examples/s]Applying chat template to train dataset:  58%|█████▊    | 3856/6690 [00:01<00:00, 3094.95 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4168/6690 [00:01<00:00, 3100.37 examples/s]Applying chat template to train dataset:  69%|██████▉   | 4616/6690 [00:01<00:00, 3053.76 examples/s]Applying chat template to train dataset:  74%|███████▎  | 4926/6690 [00:01<00:00, 3064.69 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5236/6690 [00:01<00:00, 3072.26 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5547/6690 [00:01<00:00, 3079.30 examples/s]Applying chat template to train dataset:  88%|████████▊ | 5857/6690 [00:01<00:00, 3082.01 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6168/6690 [00:02<00:00, 3087.68 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6478/6690 [00:02<00:00, 3089.48 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3068.21 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 40/6690 [00:00<00:17, 386.33 examples/s]Tokenizing train dataset:   1%|▏         | 89/6690 [00:00<00:14, 440.40 examples/s]Tokenizing train dataset:   2%|▏         | 157/6690 [00:00<00:14, 443.12 examples/s]Tokenizing train dataset:   3%|▎         | 204/6690 [00:00<00:14, 450.88 examples/s]Tokenizing train dataset:   4%|▍         | 254/6690 [00:00<00:13, 462.17 examples/s]Tokenizing train dataset:   5%|▍         | 319/6690 [00:00<00:14, 446.17 examples/s]Tokenizing train dataset:   6%|▌         | 370/6690 [00:00<00:13, 460.96 examples/s]Tokenizing train dataset:   7%|▋         | 442/6690 [00:00<00:13, 466.96 examples/s]Tokenizing train dataset:   7%|▋         | 491/6690 [00:01<00:13, 471.93 examples/s]Tokenizing train dataset:   8%|▊         | 540/6690 [00:01<00:13, 470.74 examples/s]Tokenizing train dataset:   9%|▉         | 589/6690 [00:01<00:12, 472.57 examples/s]Tokenizing train dataset:  10%|▉         | 641/6690 [00:01<00:12, 479.15 examples/s]Tokenizing train dataset:  10%|█         | 690/6690 [00:01<00:12, 476.27 examples/s]Tokenizing train dataset:  11%|█         | 743/6690 [00:01<00:12, 488.66 examples/s]Tokenizing train dataset:  12%|█▏        | 793/6690 [00:01<00:12, 487.48 examples/s]Tokenizing train dataset:  13%|█▎        | 866/6690 [00:01<00:12, 477.60 examples/s]Tokenizing train dataset:  14%|█▍        | 926/6690 [00:02<00:12, 449.17 examples/s]Tokenizing train dataset:  15%|█▍        | 973/6690 [00:02<00:12, 453.86 examples/s]Tokenizing train dataset:  16%|█▌        | 1040/6690 [00:02<00:12, 448.65 examples/s]Tokenizing train dataset:  16%|█▋        | 1088/6690 [00:02<00:12, 454.37 examples/s]Tokenizing train dataset:  17%|█▋        | 1156/6690 [00:02<00:12, 448.28 examples/s]Tokenizing train dataset:  18%|█▊        | 1203/6690 [00:02<00:12, 448.60 examples/s]Tokenizing train dataset:  19%|█▉        | 1268/6690 [00:02<00:12, 439.96 examples/s]Tokenizing train dataset:  20%|█▉        | 1317/6690 [00:02<00:11, 450.48 examples/s]Tokenizing train dataset:  20%|██        | 1363/6690 [00:02<00:11, 449.27 examples/s]Tokenizing train dataset:  21%|██▏       | 1422/6690 [00:03<00:12, 428.00 examples/s]Tokenizing train dataset:  22%|██▏       | 1485/6690 [00:03<00:12, 424.22 examples/s]Tokenizing train dataset:  23%|██▎       | 1540/6690 [00:03<00:11, 450.38 examples/s]Tokenizing train dataset:  24%|██▍       | 1603/6690 [00:03<00:11, 437.00 examples/s]Tokenizing train dataset:  25%|██▍       | 1668/6690 [00:03<00:11, 434.16 examples/s]Tokenizing train dataset:  26%|██▌       | 1716/6690 [00:03<00:11, 443.50 examples/s]Tokenizing train dataset:  26%|██▋       | 1762/6690 [00:03<00:11, 445.10 examples/s]Tokenizing train dataset:  27%|██▋       | 1807/6690 [00:03<00:11, 441.23 examples/s]Tokenizing train dataset:  28%|██▊       | 1876/6690 [00:04<00:10, 444.78 examples/s]Tokenizing train dataset:  29%|██▉       | 1946/6690 [00:04<00:10, 442.84 examples/s]Tokenizing train dataset:  30%|██▉       | 2003/6690 [00:04<00:11, 420.93 examples/s]Tokenizing train dataset:  31%|███       | 2047/6690 [00:04<00:10, 424.02 examples/s]Tokenizing train dataset:  31%|███▏      | 2091/6690 [00:04<00:10, 426.08 examples/s]Tokenizing train dataset:  32%|███▏      | 2140/6690 [00:04<00:10, 438.52 examples/s]Tokenizing train dataset:  33%|███▎      | 2190/6690 [00:04<00:09, 451.26 examples/s]Tokenizing train dataset:  33%|███▎      | 2241/6690 [00:04<00:09, 464.85 examples/s]Tokenizing train dataset:  34%|███▍      | 2304/6690 [00:05<00:09, 442.61 examples/s]Tokenizing train dataset:  35%|███▌      | 2350/6690 [00:05<00:09, 444.90 examples/s]Tokenizing train dataset:  36%|███▌      | 2397/6690 [00:05<00:09, 448.71 examples/s]Tokenizing train dataset:  37%|███▋      | 2468/6690 [00:05<00:09, 452.58 examples/s]Tokenizing train dataset:  38%|███▊      | 2540/6690 [00:05<00:09, 458.76 examples/s]Tokenizing train dataset:  39%|███▊      | 2591/6690 [00:05<00:08, 467.58 examples/s]Tokenizing train dataset:  40%|███▉      | 2654/6690 [00:05<00:09, 447.12 examples/s]Tokenizing train dataset:  40%|████      | 2700/6690 [00:06<00:08, 444.60 examples/s]Tokenizing train dataset:  41%|████▏     | 2761/6690 [00:06<00:09, 425.67 examples/s]Tokenizing train dataset:  42%|████▏     | 2824/6690 [00:06<00:09, 419.37 examples/s]Tokenizing train dataset:  43%|████▎     | 2871/6690 [00:06<00:08, 427.58 examples/s]Tokenizing train dataset:  44%|████▎     | 2920/6690 [00:06<00:08, 442.13 examples/s]Tokenizing train dataset:  44%|████▍     | 2969/6690 [00:06<00:08, 453.48 examples/s]Tokenizing train dataset:  45%|████▌     | 3018/6690 [00:06<00:07, 459.93 examples/s]Tokenizing train dataset:  46%|████▌     | 3070/6690 [00:06<00:07, 472.26 examples/s]Tokenizing train dataset:  47%|████▋     | 3118/6690 [00:06<00:07, 473.45 examples/s]Tokenizing train dataset:  48%|████▊     | 3182/6690 [00:07<00:07, 449.45 examples/s]Tokenizing train dataset:  48%|████▊     | 3232/6690 [00:07<00:07, 460.71 examples/s]Tokenizing train dataset:  49%|████▉     | 3298/6690 [00:07<00:07, 447.28 examples/s]Tokenizing train dataset:  50%|█████     | 3358/6690 [00:07<00:07, 429.83 examples/s]Tokenizing train dataset:  51%|█████     | 3416/6690 [00:07<00:07, 413.43 examples/s]Tokenizing train dataset:  52%|█████▏    | 3483/6690 [00:07<00:07, 417.57 examples/s]Tokenizing train dataset:  53%|█████▎    | 3533/6690 [00:07<00:08, 386.61 examples/s]Tokenizing train dataset:  54%|█████▎    | 3590/6690 [00:08<00:08, 376.13 examples/s]Tokenizing train dataset:  54%|█████▍    | 3636/6690 [00:08<00:07, 391.87 examples/s]Tokenizing train dataset:  55%|█████▍    | 3679/6690 [00:08<00:07, 397.89 examples/s]Tokenizing train dataset:  56%|█████▌    | 3740/6690 [00:08<00:07, 386.86 examples/s]Tokenizing train dataset:  57%|█████▋    | 3787/6690 [00:08<00:07, 403.89 examples/s]Tokenizing train dataset:  57%|█████▋    | 3830/6690 [00:08<00:07, 407.31 examples/s]Tokenizing train dataset:  58%|█████▊    | 3875/6690 [00:08<00:06, 415.52 examples/s]Tokenizing train dataset:  59%|█████▊    | 3925/6690 [00:08<00:06, 434.31 examples/s]Tokenizing train dataset:  59%|█████▉    | 3975/6690 [00:09<00:06, 448.61 examples/s]Tokenizing train dataset:  60%|██████    | 4022/6690 [00:09<00:05, 450.72 examples/s]Tokenizing train dataset:  61%|██████    | 4084/6690 [00:09<00:05, 434.49 examples/s]Tokenizing train dataset:  62%|██████▏   | 4131/6690 [00:09<00:05, 440.49 examples/s]Tokenizing train dataset:  62%|██████▏   | 4180/6690 [00:09<00:05, 448.84 examples/s]Tokenizing train dataset:  63%|██████▎   | 4248/6690 [00:09<00:05, 449.13 examples/s]Tokenizing train dataset:  65%|██████▍   | 4316/6690 [00:09<00:05, 446.16 examples/s]Tokenizing train dataset:  65%|██████▌   | 4375/6690 [00:09<00:05, 426.81 examples/s]Tokenizing train dataset:  66%|██████▌   | 4420/6690 [00:10<00:05, 426.18 examples/s]Tokenizing train dataset:  67%|██████▋   | 4464/6690 [00:10<00:05, 427.48 examples/s]Tokenizing train dataset:  67%|██████▋   | 4507/6690 [00:10<00:05, 425.47 examples/s]Tokenizing train dataset:  68%|██████▊   | 4554/6690 [00:10<00:04, 436.81 examples/s]Tokenizing train dataset:  69%|██████▊   | 4598/6690 [00:10<00:04, 433.71 examples/s]Tokenizing train dataset:  70%|██████▉   | 4650/6690 [00:10<00:04, 448.82 examples/s]Tokenizing train dataset:  70%|███████   | 4696/6690 [00:10<00:04, 448.81 examples/s]Tokenizing train dataset:  71%|███████   | 4744/6690 [00:10<00:04, 454.59 examples/s]Tokenizing train dataset:  72%|███████▏  | 4803/6690 [00:10<00:04, 425.98 examples/s]Tokenizing train dataset:  73%|███████▎  | 4851/6690 [00:11<00:04, 433.02 examples/s]Tokenizing train dataset:  73%|███████▎  | 4912/6690 [00:11<00:04, 415.87 examples/s]Tokenizing train dataset:  74%|███████▍  | 4958/6690 [00:11<00:04, 425.64 examples/s]Tokenizing train dataset:  75%|███████▌  | 5022/6690 [00:11<00:03, 420.56 examples/s]Tokenizing train dataset:  76%|███████▌  | 5083/6690 [00:11<00:03, 413.60 examples/s]Tokenizing train dataset:  77%|███████▋  | 5146/6690 [00:11<00:03, 411.92 examples/s]Tokenizing train dataset:  78%|███████▊  | 5196/6690 [00:11<00:03, 429.64 examples/s]Tokenizing train dataset:  79%|███████▊  | 5260/6690 [00:11<00:03, 425.60 examples/s]Tokenizing train dataset:  80%|███████▉  | 5321/6690 [00:12<00:03, 417.71 examples/s]Tokenizing train dataset:  80%|████████  | 5371/6690 [00:12<00:03, 435.27 examples/s]Tokenizing train dataset:  81%|████████  | 5416/6690 [00:12<00:02, 434.68 examples/s]Tokenizing train dataset:  82%|████████▏ | 5482/6690 [00:12<00:02, 434.31 examples/s]Tokenizing train dataset:  83%|████████▎ | 5546/6690 [00:12<00:02, 425.64 examples/s]Tokenizing train dataset:  84%|████████▍ | 5608/6690 [00:12<00:02, 418.52 examples/s]Tokenizing train dataset:  84%|████████▍ | 5651/6690 [00:12<00:02, 417.74 examples/s]Tokenizing train dataset:  85%|████████▌ | 5703/6690 [00:13<00:02, 392.48 examples/s]Tokenizing train dataset:  86%|████████▌ | 5751/6690 [00:13<00:02, 412.68 examples/s]Tokenizing train dataset:  87%|████████▋ | 5794/6690 [00:13<00:02, 414.23 examples/s]Tokenizing train dataset:  88%|████████▊ | 5854/6690 [00:13<00:02, 405.45 examples/s]Tokenizing train dataset:  88%|████████▊ | 5903/6690 [00:13<00:01, 425.17 examples/s]Tokenizing train dataset:  89%|████████▉ | 5947/6690 [00:13<00:01, 428.10 examples/s]Tokenizing train dataset:  90%|████████▉ | 6013/6690 [00:13<00:01, 428.02 examples/s]Tokenizing train dataset:  91%|█████████ | 6057/6690 [00:13<00:01, 428.30 examples/s]Tokenizing train dataset:  91%|█████████▏| 6110/6690 [00:13<00:01, 452.20 examples/s]Tokenizing train dataset:  92%|█████████▏| 6156/6690 [00:14<00:01, 451.73 examples/s]Tokenizing train dataset:  93%|█████████▎| 6218/6690 [00:14<00:01, 436.16 examples/s]Tokenizing train dataset:  94%|█████████▍| 6275/6690 [00:14<00:01, 412.81 examples/s]Tokenizing train dataset:  94%|█████████▍| 6320/6690 [00:14<00:00, 416.60 examples/s]Tokenizing train dataset:  95%|█████████▌| 6380/6690 [00:14<00:00, 400.93 examples/s]Tokenizing train dataset:  96%|█████████▌| 6430/6690 [00:14<00:00, 423.17 examples/s]Tokenizing train dataset:  97%|█████████▋| 6474/6690 [00:14<00:00, 422.00 examples/s]Tokenizing train dataset:  97%|█████████▋| 6518/6690 [00:14<00:00, 423.95 examples/s]Tokenizing train dataset:  98%|█████████▊| 6581/6690 [00:15<00:00, 418.52 examples/s]Tokenizing train dataset:  99%|█████████▉| 6642/6690 [00:15<00:00, 409.96 examples/s]Tokenizing train dataset: 100%|█████████▉| 6684/6690 [00:15<00:00, 410.16 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 434.54 examples/s]
[rank4]:[W612 21:16:56.237664339 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   8%|▊         | 552/6690 [00:00<00:01, 5478.33 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5648.19 examples/s]Extracting prompt in train dataset:   8%|▊         | 560/6690 [00:00<00:01, 5520.17 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5602.93 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5570.72 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1112/6690 [00:00<00:01, 5548.41 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1123/6690 [00:00<00:00, 5583.16 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1150/6690 [00:00<00:00, 5707.66 examples/s]Extracting prompt in train dataset:  25%|██▌       | 1693/6690 [00:00<00:00, 5633.04 examples/s]Extracting prompt in train dataset:  25%|██▌       | 1690/6690 [00:00<00:00, 5615.56 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1740/6690 [00:00<00:00, 5776.34 examples/s]Extracting prompt in train dataset:  34%|███▎      | 2254/6690 [00:00<00:00, 5606.03 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2540/6690 [00:00<00:00, 5618.39 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2610/6690 [00:00<00:00, 5774.08 examples/s]Extracting prompt in train dataset:  42%|████▏     | 2830/6690 [00:00<00:00, 5624.56 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  46%|████▋     | 3110/6690 [00:00<00:00, 5643.57 examples/s]Extracting prompt in train dataset:  48%|████▊     | 3200/6690 [00:00<00:00, 5800.88 examples/s]Extracting prompt in train dataset:  51%|█████     | 3410/6690 [00:00<00:00, 5651.43 examples/s]Applying chat template to eval dataset:  32%|███▏      | 306/953 [00:00<00:00, 3017.47 examples/s]Extracting prompt in train dataset:  55%|█████▌    | 3680/6690 [00:00<00:00, 5655.03 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 3790/6690 [00:00<00:00, 5812.74 examples/s]Extracting prompt in train dataset:  59%|█████▉    | 3980/6690 [00:00<00:00, 5662.95 examples/s]Applying chat template to eval dataset:  65%|██████▌   | 621/953 [00:00<00:00, 3089.30 examples/s]Extracting prompt in train dataset:  64%|██████▎   | 4260/6690 [00:00<00:00, 5667.99 examples/s]Extracting prompt in train dataset:  65%|██████▌   | 4380/6690 [00:00<00:00, 5722.02 examples/s]Applying chat template to eval dataset:  99%|█████████▊| 939/953 [00:00<00:00, 3120.43 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3084.87 examples/s]
Extracting prompt in train dataset:  72%|███████▏  | 4810/6690 [00:00<00:00, 5581.73 examples/s]Extracting prompt in train dataset:  74%|███████▍  | 4970/6690 [00:00<00:00, 5759.84 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 5090/6690 [00:00<00:00, 5577.53 examples/s]Extracting prompt in train dataset:  80%|████████  | 5380/6690 [00:00<00:00, 5611.69 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 5560/6690 [00:00<00:00, 5785.44 examples/s]Extracting prompt in train dataset:  85%|████████▍ | 5660/6690 [00:01<00:00, 5605.95 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 5960/6690 [00:01<00:00, 5635.55 examples/s]Extracting prompt in train dataset:  92%|█████████▏| 6150/6690 [00:01<00:00, 5810.30 examples/s]Extracting prompt in train dataset:  93%|█████████▎| 6240/6690 [00:01<00:00, 5629.77 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 6540/6690 [00:01<00:00, 5652.78 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5745.66 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5589.72 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5589.27 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 322.20 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:03, 291.12 examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 290/6690 [00:00<00:02, 2865.41 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:03, 272.62 examples/s]Applying chat template to train dataset:   4%|▍         | 286/6690 [00:00<00:02, 2821.23 examples/s]Applying chat template to train dataset:   4%|▍         | 285/6690 [00:00<00:02, 2822.94 examples/s]Applying chat template to train dataset:   9%|▉         | 607/6690 [00:00<00:02, 3039.66 examples/s]Applying chat template to train dataset:   9%|▉         | 599/6690 [00:00<00:02, 2992.06 examples/s]Applying chat template to train dataset:   9%|▉         | 596/6690 [00:00<00:02, 2983.27 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:03, 263.89 examples/s]Applying chat template to train dataset:  14%|█▍        | 925/6690 [00:00<00:01, 3097.66 examples/s]Applying chat template to train dataset:  14%|█▎        | 911/6690 [00:00<00:01, 3047.42 examples/s]Applying chat template to train dataset:  14%|█▎        | 908/6690 [00:00<00:01, 3040.02 examples/s]Applying chat template to train dataset:  19%|█▊        | 1241/6690 [00:00<00:01, 3116.87 examples/s]Applying chat template to train dataset:  18%|█▊        | 1222/6690 [00:00<00:01, 3068.95 examples/s]Tokenizing eval dataset:  21%|██        | 196/953 [00:00<00:02, 254.32 examples/s]Applying chat template to train dataset:  18%|█▊        | 1217/6690 [00:00<00:01, 3057.06 examples/s]Applying chat template to train dataset:  23%|██▎       | 1560/6690 [00:00<00:01, 3133.65 examples/s]Applying chat template to train dataset:  23%|██▎       | 1535/6690 [00:00<00:01, 3087.90 examples/s]Tokenizing eval dataset:  24%|██▍       | 233/953 [00:00<00:02, 281.03 examples/s]Applying chat template to train dataset:  23%|██▎       | 1528/6690 [00:00<00:01, 3073.31 examples/s]Applying chat template to train dataset:  28%|██▊       | 1880/6690 [00:00<00:01, 3146.39 examples/s]Applying chat template to train dataset:  28%|██▊       | 1847/6690 [00:00<00:01, 3098.35 examples/s]Tokenizing eval dataset:  31%|███▏      | 300/953 [00:00<00:01, 380.83 examples/s]Applying chat template to train dataset:  27%|██▋       | 1839/6690 [00:00<00:01, 3081.95 examples/s]Applying chat template to train dataset:  32%|███▏      | 2159/6690 [00:00<00:01, 3100.99 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 438.70 examples/s]Applying chat template to train dataset:  32%|███▏      | 2150/6690 [00:00<00:01, 3085.10 examples/s]Applying chat template to train dataset:  34%|███▎      | 2247/6690 [00:00<00:01, 2849.27 examples/s]Tokenizing eval dataset:  44%|████▍     | 424/953 [00:01<00:01, 494.06 examples/s]Applying chat template to train dataset:  38%|███▊      | 2560/6690 [00:00<00:01, 2924.42 examples/s]Applying chat template to train dataset:  39%|███▉      | 2624/6690 [00:00<00:01, 3094.58 examples/s]Applying chat template to train dataset:  39%|███▉      | 2612/6690 [00:00<00:01, 3078.47 examples/s]Tokenizing eval dataset:  52%|█████▏    | 491/953 [00:01<00:00, 540.95 examples/s]Applying chat template to train dataset:  43%|████▎     | 2880/6690 [00:00<00:01, 2995.10 examples/s]Applying chat template to train dataset:  44%|████▍     | 2938/6690 [00:00<00:01, 3100.64 examples/s]Applying chat template to train dataset:  44%|████▎     | 2922/6690 [00:00<00:01, 3081.10 examples/s]Tokenizing eval dataset:  59%|█████▊    | 558/953 [00:01<00:00, 572.32 examples/s]Applying chat template to train dataset:  48%|████▊     | 3200/6690 [00:01<00:01, 3050.01 examples/s]Applying chat template to train dataset:  49%|████▊     | 3250/6690 [00:01<00:01, 3103.79 examples/s]Applying chat template to train dataset:  48%|████▊     | 3234/6690 [00:01<00:01, 3089.59 examples/s]Tokenizing eval dataset:  65%|██████▌   | 622/953 [00:01<00:00, 589.34 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3520/6690 [00:01<00:01, 3083.42 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3561/6690 [00:01<00:01, 3104.35 examples/s]Applying chat template to train dataset:  55%|█████▌    | 3699/6690 [00:01<00:00, 3091.85 examples/s]Tokenizing eval dataset:  72%|███████▏  | 685/953 [00:01<00:00, 598.08 examples/s]Applying chat template to train dataset:  57%|█████▋    | 3838/6690 [00:01<00:00, 3109.38 examples/s]Applying chat template to train dataset:  58%|█████▊    | 3873/6690 [00:01<00:00, 3106.36 examples/s]Applying chat template to train dataset:  60%|█████▉    | 4010/6690 [00:01<00:00, 3090.70 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4156/6690 [00:01<00:00, 3126.97 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4187/6690 [00:01<00:00, 3110.96 examples/s]Tokenizing eval dataset:  80%|████████  | 767/953 [00:01<00:00, 576.04 examples/s]Applying chat template to train dataset:  65%|██████▍   | 4320/6690 [00:01<00:00, 3091.91 examples/s]Applying chat template to train dataset:  69%|██████▉   | 4611/6690 [00:01<00:00, 3087.47 examples/s]Applying chat template to train dataset:  69%|██████▉   | 4637/6690 [00:01<00:00, 3066.24 examples/s]Tokenizing eval dataset:  88%|████████▊ | 840/953 [00:01<00:00, 538.31 examples/s]Applying chat template to train dataset:  71%|███████▏  | 4768/6690 [00:01<00:00, 3050.32 examples/s]Applying chat template to train dataset:  74%|███████▎  | 4929/6690 [00:01<00:00, 3108.78 examples/s]Applying chat template to train dataset:  74%|███████▍  | 4948/6690 [00:01<00:00, 3075.36 examples/s]Applying chat template to train dataset:  76%|███████▌  | 5078/6690 [00:01<00:00, 3061.53 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5245/6690 [00:01<00:00, 3120.74 examples/s]Tokenizing eval dataset:  96%|█████████▌| 913/953 [00:02<00:00, 512.18 examples/s]Applying chat template to train dataset:  79%|███████▊  | 5260/6690 [00:01<00:00, 3081.98 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 453.84 examples/s]
Applying chat template to train dataset:  81%|████████  | 5389/6690 [00:01<00:00, 3072.07 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5561/6690 [00:01<00:00, 3128.71 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5572/6690 [00:01<00:00, 3089.26 examples/s]Applying chat template to train dataset:  85%|████████▌ | 5699/6690 [00:01<00:00, 3077.44 examples/s]Applying chat template to train dataset:  88%|████████▊ | 5879/6690 [00:01<00:00, 3137.66 examples/s]Applying chat template to train dataset:  88%|████████▊ | 5883/6690 [00:01<00:00, 3093.25 examples/s]Applying chat template to train dataset:  90%|████████▉ | 6009/6690 [00:01<00:00, 3082.38 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6195/6690 [00:02<00:00, 3142.72 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6197/6690 [00:02<00:00, 3100.60 examples/s]Applying chat template to train dataset:  94%|█████████▍| 6319/6690 [00:02<00:00, 3084.77 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6510/6690 [00:02<00:00, 3141.64 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6508/6690 [00:02<00:00, 3100.40 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3079.04 examples/s]
Applying chat template to train dataset:  99%|█████████▉| 6628/6690 [00:02<00:00, 3085.03 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3079.72 examples/s]
Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3065.84 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:15, 420.84 examples/s]Tokenizing train dataset:   1%|          | 43/6690 [00:00<00:15, 425.13 examples/s]Tokenizing train dataset:   1%|          | 43/6690 [00:00<00:15, 424.50 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:14, 442.83 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:14, 444.75 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:14, 442.00 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 444.90 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 445.41 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 441.58 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 477.47 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 476.76 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 473.41 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 477.66 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 476.67 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 472.29 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 475.39 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 474.43 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 470.04 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 465.58 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 464.20 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 459.71 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 479.97 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 478.05 examples/s]Tokenizing train dataset:   6%|▌         | 407/6690 [00:00<00:13, 471.82 examples/s]Tokenizing train dataset:   7%|▋         | 460/6690 [00:00<00:12, 485.28 examples/s]Tokenizing train dataset:   7%|▋         | 460/6690 [00:00<00:12, 483.46 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 481.74 examples/s]Tokenizing train dataset:   8%|▊         | 511/6690 [00:01<00:12, 491.19 examples/s]Tokenizing train dataset:   8%|▊         | 511/6690 [00:01<00:12, 489.01 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 483.28 examples/s]Tokenizing train dataset:   8%|▊         | 561/6690 [00:01<00:12, 492.06 examples/s]Tokenizing train dataset:   8%|▊         | 561/6690 [00:01<00:12, 490.17 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 483.94 examples/s]Tokenizing train dataset:   9%|▉         | 614/6690 [00:01<00:12, 495.42 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 484.57 examples/s]Tokenizing train dataset:  10%|▉         | 640/6690 [00:01<00:12, 500.85 examples/s]Tokenizing train dataset:  10%|█         | 690/6690 [00:01<00:12, 498.19 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 484.64 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 491.31 examples/s]Tokenizing train dataset:  11%|█         | 745/6690 [00:01<00:11, 508.26 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 492.09 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 502.19 examples/s]Tokenizing train dataset:  12%|█▏        | 800/6690 [00:01<00:11, 512.62 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 503.10 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 508.07 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 507.72 examples/s]Tokenizing train dataset:  13%|█▎        | 870/6690 [00:01<00:11, 493.18 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:11, 484.61 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:12, 481.04 examples/s]Tokenizing train dataset:  14%|█▍        | 934/6690 [00:01<00:12, 463.78 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:01<00:12, 466.53 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 462.06 examples/s]Tokenizing train dataset:  15%|█▌        | 1009/6690 [00:02<00:12, 469.44 examples/s]Tokenizing train dataset:  15%|█▌        | 1026/6690 [00:02<00:12, 464.09 examples/s]Tokenizing train dataset:  15%|█▍        | 1002/6690 [00:02<00:12, 459.03 examples/s]Tokenizing train dataset:  16%|█▌        | 1058/6690 [00:02<00:11, 471.33 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 472.34 examples/s]Tokenizing train dataset:  16%|█▌        | 1050/6690 [00:02<00:12, 461.53 examples/s]Tokenizing train dataset:  17%|█▋        | 1106/6690 [00:02<00:11, 471.94 examples/s]Tokenizing train dataset:  16%|█▋        | 1100/6690 [00:02<00:11, 467.02 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:11, 463.59 examples/s]Tokenizing train dataset:  18%|█▊        | 1176/6690 [00:02<00:11, 466.61 examples/s]Tokenizing train dataset:  18%|█▊        | 1194/6690 [00:02<00:11, 462.55 examples/s]Tokenizing train dataset:  17%|█▋        | 1168/6690 [00:02<00:12, 459.37 examples/s]Tokenizing train dataset:  19%|█▊        | 1242/6690 [00:02<00:12, 453.19 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 456.05 examples/s]Tokenizing train dataset:  18%|█▊        | 1234/6690 [00:02<00:12, 446.53 examples/s]Tokenizing train dataset:  19%|█▉        | 1289/6690 [00:02<00:11, 453.90 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 456.55 examples/s]Tokenizing train dataset:  19%|█▉        | 1280/6690 [00:02<00:12, 444.24 examples/s]Tokenizing train dataset:  20%|██        | 1340/6690 [00:02<00:11, 462.61 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 470.52 examples/s]Tokenizing train dataset:  20%|█▉        | 1334/6690 [00:02<00:11, 465.34 examples/s]Tokenizing train dataset:  21%|██        | 1387/6690 [00:02<00:11, 460.28 examples/s]Tokenizing train dataset:  21%|██        | 1421/6690 [00:03<00:11, 441.28 examples/s]Tokenizing train dataset:  21%|██        | 1400/6690 [00:02<00:11, 450.39 examples/s]Tokenizing train dataset:  22%|██▏       | 1451/6690 [00:03<00:11, 444.54 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 440.66 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 438.36 examples/s]Tokenizing train dataset:  23%|██▎       | 1528/6690 [00:03<00:11, 463.35 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 455.37 examples/s]Tokenizing train dataset:  23%|██▎       | 1515/6690 [00:03<00:11, 449.53 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 456.16 examples/s]Tokenizing train dataset:  24%|██▍       | 1596/6690 [00:03<00:11, 454.74 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 451.53 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 450.56 examples/s]Tokenizing train dataset:  25%|██▍       | 1662/6690 [00:03<00:11, 448.51 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 445.92 examples/s]Tokenizing train dataset:  26%|██▌       | 1710/6690 [00:03<00:10, 454.41 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 451.60 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 446.57 examples/s]Tokenizing train dataset:  26%|██▋       | 1760/6690 [00:03<00:10, 462.53 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 464.50 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 459.05 examples/s]Tokenizing train dataset:  27%|██▋       | 1830/6690 [00:03<00:10, 459.29 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 454.11 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 448.98 examples/s]Tokenizing train dataset:  28%|██▊       | 1877/6690 [00:04<00:10, 457.82 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:03<00:10, 455.09 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:04<00:10, 450.02 examples/s]Tokenizing train dataset:  29%|██▊       | 1912/6690 [00:04<00:10, 453.41 examples/s]Tokenizing train dataset:  29%|██▉       | 1947/6690 [00:04<00:10, 456.00 examples/s]Tokenizing train dataset:  29%|██▉       | 1960/6690 [00:04<00:10, 458.06 examples/s]Tokenizing train dataset:  29%|██▉       | 1932/6690 [00:04<00:10, 445.29 examples/s]Tokenizing train dataset:  30%|███       | 2008/6690 [00:04<00:10, 434.67 examples/s]Tokenizing train dataset:  30%|██▉       | 1977/6690 [00:04<00:10, 445.67 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:10, 434.39 examples/s]Tokenizing train dataset:  31%|███       | 2054/6690 [00:04<00:10, 437.63 examples/s]Tokenizing train dataset:  31%|███       | 2041/6690 [00:04<00:10, 431.89 examples/s]Tokenizing train dataset:  31%|███▏      | 2100/6690 [00:04<00:10, 437.45 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 434.57 examples/s]Tokenizing train dataset:  31%|███       | 2085/6690 [00:04<00:10, 433.24 examples/s]Tokenizing train dataset:  32%|███▏      | 2153/6690 [00:04<00:09, 457.24 examples/s]Tokenizing train dataset:  32%|███▏      | 2141/6690 [00:04<00:10, 448.63 examples/s]Tokenizing train dataset:  32%|███▏      | 2134/6690 [00:04<00:10, 446.04 examples/s]Tokenizing train dataset:  33%|███▎      | 2207/6690 [00:04<00:09, 473.04 examples/s]Tokenizing train dataset:  33%|███▎      | 2192/6690 [00:04<00:09, 463.25 examples/s]Tokenizing train dataset:  33%|███▎      | 2184/6690 [00:04<00:09, 457.00 examples/s]Tokenizing train dataset:  34%|███▎      | 2245/6690 [00:04<00:09, 476.87 examples/s]Tokenizing train dataset:  34%|███▍      | 2280/6690 [00:04<00:09, 467.74 examples/s]Tokenizing train dataset:  33%|███▎      | 2236/6690 [00:04<00:09, 468.17 examples/s]Tokenizing train dataset:  35%|███▍      | 2309/6690 [00:04<00:09, 451.08 examples/s]Tokenizing train dataset:  35%|███▌      | 2348/6690 [00:05<00:09, 458.33 examples/s]Tokenizing train dataset:  34%|███▍      | 2300/6690 [00:05<00:09, 450.05 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:05<00:09, 460.65 examples/s]Tokenizing train dataset:  36%|███▌      | 2396/6690 [00:05<00:09, 461.59 examples/s]Tokenizing train dataset:  35%|███▌      | 2347/6690 [00:05<00:09, 453.87 examples/s]Tokenizing train dataset:  36%|███▋      | 2430/6690 [00:05<00:09, 456.31 examples/s]Tokenizing train dataset:  36%|███▌      | 2395/6690 [00:05<00:09, 458.74 examples/s]Tokenizing train dataset:  37%|███▋      | 2469/6690 [00:05<00:09, 465.09 examples/s]Tokenizing train dataset:  37%|███▋      | 2478/6690 [00:05<00:09, 459.61 examples/s]Tokenizing train dataset:  38%|███▊      | 2516/6690 [00:05<00:09, 463.33 examples/s]Tokenizing train dataset:  37%|███▋      | 2466/6690 [00:05<00:09, 459.68 examples/s]Tokenizing train dataset:  38%|███▊      | 2526/6690 [00:05<00:09, 462.47 examples/s]Tokenizing train dataset:  38%|███▊      | 2570/6690 [00:05<00:08, 481.44 examples/s]Tokenizing train dataset:  39%|███▊      | 2581/6690 [00:05<00:08, 484.71 examples/s]Tokenizing train dataset:  38%|███▊      | 2540/6690 [00:05<00:08, 464.76 examples/s]Tokenizing train dataset:  39%|███▉      | 2620/6690 [00:05<00:08, 481.49 examples/s]Tokenizing train dataset:  39%|███▊      | 2591/6690 [00:05<00:08, 474.00 examples/s]Tokenizing train dataset:  40%|███▉      | 2649/6690 [00:05<00:08, 467.27 examples/s]Tokenizing train dataset:  40%|████      | 2684/6690 [00:05<00:08, 458.85 examples/s]Tokenizing train dataset:  40%|███▉      | 2654/6690 [00:05<00:08, 453.40 examples/s]Tokenizing train dataset:  41%|████      | 2714/6690 [00:05<00:08, 451.97 examples/s]Tokenizing train dataset:  41%|████      | 2750/6690 [00:05<00:08, 445.91 examples/s]Tokenizing train dataset:  40%|████      | 2700/6690 [00:05<00:08, 451.08 examples/s]Tokenizing train dataset:  41%|████▏     | 2774/6690 [00:06<00:09, 433.40 examples/s]Tokenizing train dataset:  42%|████▏     | 2811/6690 [00:06<00:09, 428.26 examples/s]Tokenizing train dataset:  41%|████▏     | 2761/6690 [00:06<00:09, 431.55 examples/s]Tokenizing train dataset:  43%|████▎     | 2859/6690 [00:06<00:08, 438.16 examples/s]Tokenizing train dataset:  42%|████▏     | 2841/6690 [00:06<00:08, 430.86 examples/s]Tokenizing train dataset:  42%|████▏     | 2825/6690 [00:06<00:09, 426.15 examples/s]Tokenizing train dataset:  43%|████▎     | 2909/6690 [00:06<00:08, 450.93 examples/s]Tokenizing train dataset:  43%|████▎     | 2889/6690 [00:06<00:08, 439.76 examples/s]Tokenizing train dataset:  43%|████▎     | 2872/6690 [00:06<00:08, 431.12 examples/s]Tokenizing train dataset:  44%|████▍     | 2960/6690 [00:06<00:08, 460.58 examples/s]Tokenizing train dataset:  44%|████▍     | 2942/6690 [00:06<00:08, 458.82 examples/s]Tokenizing train dataset:  44%|████▎     | 2925/6690 [00:06<00:08, 450.90 examples/s]Tokenizing train dataset:  45%|████▌     | 3012/6690 [00:06<00:07, 475.75 examples/s]Tokenizing train dataset:  45%|████▍     | 2994/6690 [00:06<00:07, 469.41 examples/s]Tokenizing train dataset:  44%|████▍     | 2975/6690 [00:06<00:08, 460.33 examples/s]Tokenizing train dataset:  46%|████▌     | 3065/6690 [00:06<00:07, 488.16 examples/s]Tokenizing train dataset:  46%|████▌     | 3046/6690 [00:06<00:07, 476.95 examples/s]Tokenizing train dataset:  45%|████▌     | 3024/6690 [00:06<00:07, 466.73 examples/s]Tokenizing train dataset:  47%|████▋     | 3116/6690 [00:06<00:07, 489.75 examples/s]Tokenizing train dataset:  46%|████▋     | 3102/6690 [00:06<00:07, 498.02 examples/s]Tokenizing train dataset:  46%|████▌     | 3078/6690 [00:06<00:07, 483.94 examples/s]Tokenizing train dataset:  48%|████▊     | 3182/6690 [00:06<00:07, 464.67 examples/s]Tokenizing train dataset:  47%|████▋     | 3167/6690 [00:06<00:07, 470.36 examples/s]Tokenizing train dataset:  47%|████▋     | 3147/6690 [00:06<00:07, 469.45 examples/s]Tokenizing train dataset:  48%|████▊     | 3234/6690 [00:06<00:07, 474.40 examples/s]Tokenizing train dataset:  48%|████▊     | 3241/6690 [00:06<00:07, 474.15 examples/s]Tokenizing train dataset:  48%|████▊     | 3215/6690 [00:07<00:07, 461.22 examples/s]Tokenizing train dataset:  49%|████▉     | 3300/6690 [00:07<00:07, 454.81 examples/s]Tokenizing train dataset:  49%|████▉     | 3302/6690 [00:07<00:07, 448.83 examples/s]Tokenizing train dataset:  49%|████▉     | 3264/6690 [00:07<00:07, 462.98 examples/s]Tokenizing train dataset:  50%|█████     | 3363/6690 [00:07<00:07, 438.77 examples/s]Tokenizing train dataset:  50%|█████     | 3367/6690 [00:07<00:07, 439.02 examples/s]Tokenizing train dataset:  50%|████▉     | 3321/6690 [00:07<00:07, 431.24 examples/s]Tokenizing train dataset:  51%|█████     | 3408/6690 [00:07<00:07, 433.85 examples/s]Tokenizing train dataset:  50%|█████     | 3368/6690 [00:07<00:07, 433.70 examples/s]Tokenizing train dataset:  51%|█████▏    | 3429/6690 [00:07<00:07, 425.88 examples/s]Tokenizing train dataset:  52%|█████▏    | 3465/6690 [00:07<00:07, 412.57 examples/s]Tokenizing train dataset:  51%|█████▏    | 3429/6690 [00:07<00:07, 422.30 examples/s]Tokenizing train dataset:  53%|█████▎    | 3514/6690 [00:07<00:07, 425.66 examples/s]Tokenizing train dataset:  52%|█████▏    | 3496/6690 [00:07<00:07, 428.20 examples/s]Tokenizing train dataset:  52%|█████▏    | 3495/6690 [00:07<00:07, 423.70 examples/s]Tokenizing train dataset:  53%|█████▎    | 3572/6690 [00:07<00:07, 408.51 examples/s]Tokenizing train dataset:  53%|█████▎    | 3557/6690 [00:07<00:07, 416.55 examples/s]Tokenizing train dataset:  53%|█████▎    | 3554/6690 [00:07<00:07, 410.54 examples/s]Tokenizing train dataset:  54%|█████▍    | 3640/6690 [00:07<00:07, 418.61 examples/s]Tokenizing train dataset:  54%|█████▍    | 3615/6690 [00:07<00:07, 403.95 examples/s]Tokenizing train dataset:  55%|█████▌    | 3685/6690 [00:08<00:07, 422.94 examples/s]Tokenizing train dataset:  55%|█████▍    | 3664/6690 [00:08<00:07, 418.53 examples/s]Tokenizing train dataset:  54%|█████▍    | 3612/6690 [00:07<00:07, 398.23 examples/s]Tokenizing train dataset:  55%|█████▍    | 3663/6690 [00:08<00:07, 415.89 examples/s]Tokenizing train dataset:  56%|█████▌    | 3743/6690 [00:08<00:07, 403.77 examples/s]Tokenizing train dataset:  56%|█████▌    | 3722/6690 [00:08<00:07, 402.61 examples/s]Tokenizing train dataset:  57%|█████▋    | 3790/6690 [00:08<00:06, 416.78 examples/s]Tokenizing train dataset:  56%|█████▋    | 3770/6690 [00:08<00:07, 412.78 examples/s]Tokenizing train dataset:  56%|█████▌    | 3720/6690 [00:08<00:07, 396.88 examples/s]Tokenizing train dataset:  56%|█████▋    | 3766/6690 [00:08<00:07, 408.81 examples/s]Tokenizing train dataset:  58%|█████▊    | 3848/6690 [00:08<00:07, 404.09 examples/s]Tokenizing train dataset:  57%|█████▋    | 3835/6690 [00:08<00:06, 418.27 examples/s]Tokenizing train dataset:  57%|█████▋    | 3810/6690 [00:08<00:07, 411.06 examples/s]Tokenizing train dataset:  58%|█████▊    | 3898/6690 [00:08<00:06, 425.29 examples/s]Tokenizing train dataset:  58%|█████▊    | 3883/6690 [00:08<00:06, 430.10 examples/s]Tokenizing train dataset:  58%|█████▊    | 3856/6690 [00:08<00:06, 420.88 examples/s]Tokenizing train dataset:  59%|█████▉    | 3949/6690 [00:08<00:06, 443.31 examples/s]Tokenizing train dataset:  59%|█████▉    | 3931/6690 [00:08<00:06, 440.00 examples/s]Tokenizing train dataset:  58%|█████▊    | 3908/6690 [00:08<00:06, 446.78 examples/s]Tokenizing train dataset:  60%|█████▉    | 3996/6690 [00:08<00:06, 448.73 examples/s]Tokenizing train dataset:  59%|█████▉    | 3980/6690 [00:08<00:06, 451.02 examples/s]Tokenizing train dataset:  59%|█████▉    | 3954/6690 [00:08<00:06, 448.92 examples/s]Tokenizing train dataset:  60%|██████    | 4030/6690 [00:08<00:05, 460.50 examples/s]Tokenizing train dataset:  61%|██████    | 4064/6690 [00:08<00:05, 446.86 examples/s]Tokenizing train dataset:  60%|█████▉    | 4004/6690 [00:08<00:05, 462.00 examples/s]Tokenizing train dataset:  61%|██████    | 4094/6690 [00:08<00:05, 445.48 examples/s]Tokenizing train dataset:  62%|██████▏   | 4133/6690 [00:09<00:05, 447.34 examples/s]Tokenizing train dataset:  61%|██████    | 4070/6690 [00:09<00:05, 445.41 examples/s]Tokenizing train dataset:  62%|██████▏   | 4142/6690 [00:09<00:05, 452.37 examples/s]Tokenizing train dataset:  63%|██████▎   | 4183/6690 [00:09<00:05, 457.55 examples/s]Tokenizing train dataset:  63%|██████▎   | 4193/6690 [00:09<00:05, 464.28 examples/s]Tokenizing train dataset:  62%|██████▏   | 4139/6690 [00:09<00:05, 448.34 examples/s]Tokenizing train dataset:  64%|██████▎   | 4253/6690 [00:09<00:05, 455.52 examples/s]Tokenizing train dataset:  63%|██████▎   | 4191/6690 [00:09<00:05, 461.43 examples/s]Tokenizing train dataset:  64%|██████▎   | 4260/6690 [00:09<00:05, 451.12 examples/s]Tokenizing train dataset:  65%|██████▍   | 4323/6690 [00:09<00:05, 458.64 examples/s]Tokenizing train dataset:  64%|██████▍   | 4307/6690 [00:09<00:05, 452.42 examples/s]Tokenizing train dataset:  64%|██████▎   | 4256/6690 [00:09<00:05, 446.03 examples/s]Tokenizing train dataset:  64%|██████▍   | 4302/6690 [00:09<00:05, 448.31 examples/s]Tokenizing train dataset:  66%|██████▌   | 4382/6690 [00:09<00:05, 434.23 examples/s]Tokenizing train dataset:  65%|██████▌   | 4370/6690 [00:09<00:05, 436.98 examples/s]Tokenizing train dataset:  66%|██████▌   | 4429/6690 [00:09<00:05, 440.20 examples/s]Tokenizing train dataset:  66%|██████▌   | 4415/6690 [00:09<00:05, 435.33 examples/s]Tokenizing train dataset:  65%|██████▌   | 4366/6690 [00:09<00:05, 436.09 examples/s]Tokenizing train dataset:  67%|██████▋   | 4461/6690 [00:09<00:05, 439.21 examples/s]Tokenizing train dataset:  67%|██████▋   | 4496/6690 [00:09<00:05, 438.36 examples/s]Tokenizing train dataset:  66%|██████▋   | 4437/6690 [00:09<00:05, 439.88 examples/s]Tokenizing train dataset:  68%|██████▊   | 4545/6690 [00:09<00:04, 449.26 examples/s]Tokenizing train dataset:  68%|██████▊   | 4530/6690 [00:09<00:04, 442.13 examples/s]Tokenizing train dataset:  67%|██████▋   | 4500/6690 [00:10<00:05, 427.31 examples/s]Tokenizing train dataset:  68%|██████▊   | 4578/6690 [00:10<00:04, 449.63 examples/s]Tokenizing train dataset:  69%|██████▉   | 4619/6690 [00:10<00:04, 459.31 examples/s]Tokenizing train dataset:  68%|██████▊   | 4552/6690 [00:10<00:04, 443.28 examples/s]Tokenizing train dataset:  69%|██████▉   | 4630/6690 [00:10<00:04, 459.27 examples/s]Tokenizing train dataset:  70%|██████▉   | 4668/6690 [00:10<00:04, 465.46 examples/s]Tokenizing train dataset:  70%|██████▉   | 4677/6690 [00:10<00:04, 461.94 examples/s]Tokenizing train dataset:  69%|██████▉   | 4624/6690 [00:10<00:04, 451.25 examples/s]Tokenizing train dataset:  71%|███████   | 4740/6690 [00:10<00:04, 466.20 examples/s]Tokenizing train dataset:  70%|██████▉   | 4672/6690 [00:10<00:04, 456.32 examples/s]Tokenizing train dataset:  71%|███████   | 4748/6690 [00:10<00:04, 462.92 examples/s]Tokenizing train dataset:  72%|███████▏  | 4800/6690 [00:10<00:04, 437.62 examples/s]Tokenizing train dataset:  71%|███████   | 4744/6690 [00:10<00:04, 458.76 examples/s]Tokenizing train dataset:  72%|███████▏  | 4808/6690 [00:10<00:04, 440.33 examples/s]Tokenizing train dataset:  73%|███████▎  | 4851/6690 [00:10<00:04, 448.16 examples/s]Tokenizing train dataset:  73%|███████▎  | 4855/6690 [00:10<00:04, 444.90 examples/s]Tokenizing train dataset:  72%|███████▏  | 4804/6690 [00:10<00:04, 436.22 examples/s]Tokenizing train dataset:  73%|███████▎  | 4914/6690 [00:10<00:04, 434.55 examples/s]Tokenizing train dataset:  73%|███████▎  | 4851/6690 [00:10<00:04, 441.39 examples/s]Tokenizing train dataset:  74%|███████▎  | 4919/6690 [00:10<00:04, 434.51 examples/s]Tokenizing train dataset:  74%|███████▍  | 4962/6690 [00:10<00:03, 443.84 examples/s]Tokenizing train dataset:  74%|███████▍  | 4965/6690 [00:10<00:03, 439.89 examples/s]Tokenizing train dataset:  73%|███████▎  | 4912/6690 [00:10<00:04, 426.05 examples/s]Tokenizing train dataset:  75%|███████▌  | 5027/6690 [00:11<00:03, 433.14 examples/s]Tokenizing train dataset:  74%|███████▍  | 4962/6690 [00:11<00:03, 438.19 examples/s]Tokenizing train dataset:  75%|███████▌  | 5029/6690 [00:11<00:03, 432.11 examples/s]Tokenizing train dataset:  76%|███████▌  | 5092/6690 [00:11<00:03, 428.87 examples/s]Tokenizing train dataset:  76%|███████▌  | 5073/6690 [00:11<00:03, 433.22 examples/s]Tokenizing train dataset:  75%|███████▌  | 5026/6690 [00:11<00:03, 428.33 examples/s]Tokenizing train dataset:  77%|███████▋  | 5160/6690 [00:11<00:03, 431.96 examples/s]Tokenizing train dataset:  77%|███████▋  | 5134/6690 [00:11<00:03, 420.62 examples/s]Tokenizing train dataset:  76%|███████▌  | 5091/6690 [00:11<00:03, 425.03 examples/s]Tokenizing train dataset:  78%|███████▊  | 5210/6690 [00:11<00:03, 440.99 examples/s]Tokenizing train dataset:  78%|███████▊  | 5185/6690 [00:11<00:03, 438.48 examples/s]Tokenizing train dataset:  77%|███████▋  | 5156/6690 [00:11<00:03, 424.58 examples/s]Tokenizing train dataset:  79%|███████▊  | 5256/6690 [00:11<00:03, 441.98 examples/s]Tokenizing train dataset:  78%|███████▊  | 5230/6690 [00:11<00:03, 435.38 examples/s]Tokenizing train dataset:  78%|███████▊  | 5207/6690 [00:11<00:03, 439.47 examples/s]Tokenizing train dataset:  79%|███████▉  | 5277/6690 [00:11<00:03, 442.63 examples/s]Tokenizing train dataset:  80%|███████▉  | 5320/6690 [00:11<00:03, 431.20 examples/s]Tokenizing train dataset:  79%|███████▉  | 5272/6690 [00:11<00:03, 432.76 examples/s]Tokenizing train dataset:  80%|████████  | 5371/6690 [00:11<00:02, 449.88 examples/s]Tokenizing train dataset:  80%|███████▉  | 5339/6690 [00:11<00:03, 427.72 examples/s]Tokenizing train dataset:  81%|████████  | 5394/6690 [00:11<00:02, 458.31 examples/s]Tokenizing train dataset:  81%|████████  | 5419/6690 [00:11<00:02, 448.23 examples/s]Tokenizing train dataset:  80%|███████▉  | 5335/6690 [00:11<00:03, 425.36 examples/s]Tokenizing train dataset:  82%|████████▏ | 5466/6690 [00:12<00:02, 448.66 examples/s]Tokenizing train dataset:  81%|████████  | 5390/6690 [00:12<00:02, 447.84 examples/s]Tokenizing train dataset:  82%|████████▏ | 5461/6690 [00:12<00:02, 449.20 examples/s]Tokenizing train dataset:  82%|████████▏ | 5513/6690 [00:12<00:02, 449.59 examples/s]Tokenizing train dataset:  82%|████████▏ | 5507/6690 [00:12<00:02, 448.42 examples/s]Tokenizing train dataset:  82%|████████▏ | 5456/6690 [00:12<00:02, 439.82 examples/s]Tokenizing train dataset:  83%|████████▎ | 5578/6690 [00:12<00:02, 439.25 examples/s]Tokenizing train dataset:  82%|████████▏ | 5502/6690 [00:12<00:02, 441.71 examples/s]Tokenizing train dataset:  83%|████████▎ | 5571/6690 [00:12<00:02, 436.56 examples/s]Tokenizing train dataset:  84%|████████▍ | 5645/6690 [00:12<00:02, 436.48 examples/s]Tokenizing train dataset:  83%|████████▎ | 5566/6690 [00:12<00:02, 434.59 examples/s]Tokenizing train dataset:  84%|████████▍ | 5640/6690 [00:12<00:02, 438.15 examples/s]Tokenizing train dataset:  85%|████████▌ | 5690/6690 [00:12<00:02, 435.76 examples/s]Tokenizing train dataset:  84%|████████▍ | 5631/6690 [00:12<00:02, 430.01 examples/s]Tokenizing train dataset:  85%|████████▌ | 5705/6690 [00:12<00:02, 435.07 examples/s]Tokenizing train dataset:  86%|████████▌ | 5738/6690 [00:12<00:02, 443.35 examples/s]Tokenizing train dataset:  85%|████████▍ | 5678/6690 [00:12<00:02, 436.05 examples/s]Tokenizing train dataset:  86%|████████▌ | 5757/6690 [00:12<00:02, 448.63 examples/s]Tokenizing train dataset:  86%|████████▋ | 5784/6690 [00:12<00:02, 443.89 examples/s]Tokenizing train dataset:  86%|████████▌ | 5723/6690 [00:12<00:02, 432.16 examples/s]Tokenizing train dataset:  87%|████████▋ | 5821/6690 [00:12<00:01, 439.87 examples/s]Tokenizing train dataset:  87%|████████▋ | 5848/6690 [00:12<00:01, 433.74 examples/s]Tokenizing train dataset:  86%|████████▌ | 5768/6690 [00:12<00:02, 432.73 examples/s]Tokenizing train dataset:  88%|████████▊ | 5896/6690 [00:13<00:01, 443.58 examples/s]Tokenizing train dataset:  87%|████████▋ | 5817/6690 [00:13<00:01, 446.09 examples/s]Tokenizing train dataset:  88%|████████▊ | 5881/6690 [00:13<00:01, 425.08 examples/s]Tokenizing train dataset:  89%|████████▉ | 5944/6690 [00:13<00:01, 447.90 examples/s]Tokenizing train dataset:  89%|████████▊ | 5935/6690 [00:13<00:01, 447.02 examples/s]Tokenizing train dataset:  88%|████████▊ | 5876/6690 [00:13<00:01, 422.93 examples/s]Tokenizing train dataset:  90%|████████▉ | 6013/6690 [00:13<00:01, 447.40 examples/s]Tokenizing train dataset:  89%|████████▊ | 5926/6690 [00:13<00:01, 441.05 examples/s]Tokenizing train dataset:  90%|████████▉ | 6003/6690 [00:13<00:01, 445.47 examples/s]Tokenizing train dataset:  91%|█████████ | 6060/6690 [00:13<00:01, 446.90 examples/s]Tokenizing train dataset:  89%|████████▉ | 5973/6690 [00:13<00:01, 442.41 examples/s]Tokenizing train dataset:  91%|█████████ | 6072/6690 [00:13<00:01, 448.18 examples/s]Tokenizing train dataset:  91%|█████████▏| 6116/6690 [00:13<00:01, 474.02 examples/s]Tokenizing train dataset:  90%|█████████ | 6034/6690 [00:13<00:01, 422.82 examples/s]Tokenizing train dataset:  92%|█████████▏| 6127/6690 [00:13<00:01, 466.90 examples/s]Tokenizing train dataset:  92%|█████████▏| 6183/6690 [00:13<00:01, 461.62 examples/s]Tokenizing train dataset:  91%|█████████ | 6086/6690 [00:13<00:01, 443.02 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6690 [00:13<00:01, 455.95 examples/s]Tokenizing train dataset:  92%|█████████▏| 6140/6690 [00:13<00:01, 463.48 examples/s]Tokenizing train dataset:  93%|█████████▎| 6240/6690 [00:13<00:01, 427.12 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:13<00:01, 419.64 examples/s]Tokenizing train dataset:  94%|█████████▍| 6289/6690 [00:13<00:00, 435.00 examples/s]Tokenizing train dataset:  93%|█████████▎| 6204/6690 [00:13<00:01, 448.38 examples/s]Tokenizing train dataset:  94%|█████████▍| 6299/6690 [00:13<00:00, 434.59 examples/s]Tokenizing train dataset:  95%|█████████▍| 6350/6690 [00:14<00:00, 422.17 examples/s]Tokenizing train dataset:  94%|█████████▎| 6260/6690 [00:14<00:01, 411.97 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 419.35 examples/s]Tokenizing train dataset:  94%|█████████▍| 6310/6690 [00:14<00:00, 429.20 examples/s]Tokenizing train dataset:  96%|█████████▌| 6420/6690 [00:14<00:00, 433.59 examples/s]Tokenizing train dataset:  96%|█████████▌| 6404/6690 [00:14<00:00, 421.94 examples/s]Tokenizing train dataset:  97%|█████████▋| 6465/6690 [00:14<00:00, 435.09 examples/s]Tokenizing train dataset:  95%|█████████▌| 6369/6690 [00:14<00:00, 415.36 examples/s]Tokenizing train dataset:  96%|█████████▋| 6452/6690 [00:14<00:00, 433.47 examples/s]Tokenizing train dataset:  97%|█████████▋| 6510/6690 [00:14<00:00, 434.68 examples/s]Tokenizing train dataset:  96%|█████████▌| 6414/6690 [00:14<00:00, 423.07 examples/s]Tokenizing train dataset:  97%|█████████▋| 6497/6690 [00:14<00:00, 436.34 examples/s]Tokenizing train dataset:  98%|█████████▊| 6554/6690 [00:14<00:00, 431.52 examples/s]Tokenizing train dataset:  97%|█████████▋| 6459/6690 [00:14<00:00, 427.60 examples/s]Tokenizing train dataset:  98%|█████████▊| 6542/6690 [00:14<00:00, 435.74 examples/s]Tokenizing train dataset:  99%|█████████▊| 6599/6690 [00:14<00:00, 433.83 examples/s]Tokenizing train dataset:  97%|█████████▋| 6503/6690 [00:14<00:00, 430.70 examples/s]Tokenizing train dataset:  98%|█████████▊| 6588/6690 [00:14<00:00, 440.27 examples/s]Tokenizing train dataset:  98%|█████████▊| 6548/6690 [00:14<00:00, 432.65 examples/s]Tokenizing train dataset: 100%|█████████▉| 6658/6690 [00:14<00:00, 416.08 examples/s]Tokenizing train dataset:  99%|█████████▉| 6648/6690 [00:14<00:00, 420.47 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:14<00:00, 450.01 examples/s]
Tokenizing train dataset:  99%|█████████▉| 6609/6690 [00:14<00:00, 421.23 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:14<00:00, 448.78 examples/s]
Tokenizing train dataset: 100%|█████████▉| 6670/6690 [00:15<00:00, 412.70 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 444.23 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:  58%|█████▊    | 554/953 [00:00<00:00, 5504.86 examples/s]Extracting prompt in eval dataset:  58%|█████▊    | 551/953 [00:00<00:00, 5450.36 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5584.18 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5570.47 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5461.01 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5440.73 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[codecarbon ERROR @ 21:17:22] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.
Set up DPO trainer
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  32%|███▏      | 307/953 [00:00<00:00, 3033.97 examples/s]Applying chat template to eval dataset:  32%|███▏      | 304/953 [00:00<00:00, 2994.54 examples/s]Applying chat template to eval dataset:  31%|███▏      | 298/953 [00:00<00:00, 2949.72 examples/s]Applying chat template to eval dataset:  65%|██████▌   | 624/953 [00:00<00:00, 3107.59 examples/s]Applying chat template to eval dataset:  65%|██████▌   | 620/953 [00:00<00:00, 3080.96 examples/s]Applying chat template to eval dataset:  64%|██████▍   | 610/953 [00:00<00:00, 3034.14 examples/s]Applying chat template to eval dataset:  99%|█████████▉| 942/953 [00:00<00:00, 3136.06 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3103.53 examples/s]
Applying chat template to eval dataset:  98%|█████████▊| 936/953 [00:00<00:00, 3111.68 examples/s]Applying chat template to eval dataset:  97%|█████████▋| 920/953 [00:00<00:00, 3062.29 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3075.67 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3031.32 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 322.67 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 320.84 examples/s]Tokenizing eval dataset:   3%|▎         | 33/953 [00:00<00:02, 318.64 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 289.96 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 287.54 examples/s]Tokenizing eval dataset:   8%|▊         | 72/953 [00:00<00:03, 275.76 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 277.12 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 275.74 examples/s]Tokenizing eval dataset:  12%|█▏        | 115/953 [00:00<00:03, 276.86 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.14 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 263.74 examples/s]Tokenizing eval dataset:  16%|█▌        | 151/953 [00:00<00:03, 257.38 examples/s]Tokenizing eval dataset:  20%|█▉        | 186/953 [00:00<00:03, 237.02 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 250.19 examples/s]Tokenizing eval dataset:  19%|█▉        | 180/953 [00:00<00:03, 230.17 examples/s]Tokenizing eval dataset:  22%|██▏       | 210/953 [00:00<00:03, 234.74 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 266.56 examples/s]Tokenizing eval dataset:  22%|██▏       | 206/953 [00:00<00:03, 235.36 examples/s]Tokenizing eval dataset:  27%|██▋       | 260/953 [00:00<00:02, 303.40 examples/s]Tokenizing eval dataset:  31%|███       | 291/953 [00:00<00:01, 365.72 examples/s]Tokenizing eval dataset:  26%|██▌       | 248/953 [00:00<00:02, 280.98 examples/s]Tokenizing eval dataset:  34%|███▎      | 320/953 [00:01<00:01, 381.75 examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 435.74 examples/s]Tokenizing eval dataset:  32%|███▏      | 305/953 [00:01<00:01, 358.61 examples/s]Tokenizing eval dataset:  39%|███▉      | 374/953 [00:01<00:01, 425.10 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 485.17 examples/s]Tokenizing eval dataset:  38%|███▊      | 363/953 [00:01<00:01, 418.86 examples/s]Tokenizing eval dataset:  47%|████▋     | 448/953 [00:01<00:00, 507.45 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 538.82 examples/s]Tokenizing eval dataset:  45%|████▌     | 432/953 [00:01<00:01, 488.14 examples/s]Tokenizing eval dataset:  54%|█████▎    | 510/953 [00:01<00:00, 538.02 examples/s]Tokenizing eval dataset:  58%|█████▊    | 554/953 [00:01<00:00, 571.99 examples/s]Tokenizing eval dataset:  52%|█████▏    | 500/953 [00:01<00:00, 535.12 examples/s]Tokenizing eval dataset:  60%|██████    | 574/953 [00:01<00:00, 565.93 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 582.87 examples/s]Tokenizing eval dataset:  59%|█████▉    | 564/953 [00:01<00:00, 561.49 examples/s]Tokenizing eval dataset:  67%|██████▋   | 642/953 [00:01<00:00, 594.34 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 595.03 examples/s]Tokenizing eval dataset:  66%|██████▌   | 630/953 [00:01<00:00, 584.47 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 588.82 examples/s]Tokenizing eval dataset:  77%|███████▋  | 730/953 [00:01<00:00, 583.41 examples/s]Tokenizing eval dataset:  80%|████████  | 766/953 [00:01<00:00, 575.42 examples/s]Tokenizing eval dataset:  81%|████████  | 773/953 [00:01<00:00, 570.39 examples/s]Tokenizing eval dataset:  84%|████████▍ | 804/953 [00:01<00:00, 549.86 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 540.95 examples/s]Tokenizing eval dataset:  88%|████████▊ | 843/953 [00:01<00:00, 530.50 examples/s]Tokenizing eval dataset:  93%|█████████▎| 882/953 [00:02<00:00, 531.74 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 516.73 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 451.53 examples/s]
Tokenizing eval dataset:  96%|█████████▌| 913/953 [00:02<00:00, 504.56 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 517.07 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 441.22 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 434.66 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.446716070175171 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3622543811798096 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3576536178588867 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3565621376037598 seconds
[codecarbon WARNING @ 21:17:46] Another instance of codecarbon is already running. Exiting.
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[rank4]:[E612 21:19:30.000018028 ProcessGroupNCCL.cpp:552] [Rank 4] Collective WorkNCCL(SeqNum=3, OpType=ALLGATHER, NumelIn=1, NumelOut=12, Timeout(ms)=1800000) raised the following async exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn01.vega.pri<42730>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f310d56c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f30bb8211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f30bb82964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f30bb82b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f30bb82c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f310dc4b5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f310faf3ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f310fb85a40 in /lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[E612 21:19:30.005360961 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 4]  failure detected by watchdog at work sequence id: 3 PG status: last enqueued work: 3, last completed work: 2
[rank4]:[E612 21:19:30.005370031 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank4]:[E612 21:19:31.196573838 ProcessGroupNCCL.cpp:681] [Rank 4] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E612 21:19:31.196612108 ProcessGroupNCCL.cpp:695] [Rank 4] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E612 21:19:31.196661988 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 4] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn01.vega.pri<42730>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f310d56c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f30bb8211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f30bb82964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f30bb82b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f30bb82c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f310dc4b5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f310faf3ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f310fb85a40 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 4] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn01.vega.pri<42730>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f310d56c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f30bb8211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f30bb82964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f30bb82b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f30bb82c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f310dc4b5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f310faf3ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f310fb85a40 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f310d56c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f30bb4876fc in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f310dc4b5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f310faf3ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126a40 (0x7f310fb85a40 in /lib/x86_64-linux-gnu/libc.so.6)

W0612 21:19:31.817000 2370637 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2370826 closing signal SIGTERM
W0612 21:19:31.817000 2370637 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2370827 closing signal SIGTERM
W0612 21:19:31.819000 2370637 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2370828 closing signal SIGTERM
E0612 21:19:32.539000 2370637 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 0 (pid: 2370825) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1182, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 861, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train_curriculum.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-12_21:19:31
  host      : pm5-nod03.vega.pri
  rank      : 4 (local_rank: 0)
  exitcode  : -6 (pid: 2370825)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2370825
========================================================
