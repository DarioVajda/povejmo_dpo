cpu-bind=MASK - gn21, task  0  0 [2622540]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 0 ---
Total Nodes: 4
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn21
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 4     --machine_rank 0     --main_process_ip gn21     --main_process_port 29500     --num_processes 16     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62875861     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.2 --curriculum_stage=0
-------------------------------------------
[2025-06-10 00:51:44,406] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0610 00:51:46.244000 2622593 torch/distributed/run.py:792] 
W0610 00:51:46.244000 2622593 torch/distributed/run.py:792] *****************************************
W0610 00:51:46.244000 2622593 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0610 00:51:46.244000 2622593 torch/distributed/run.py:792] *****************************************
[2025-06-10 00:51:53,232] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 00:51:53,251] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 00:51:53,253] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 00:51:53,268] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
World size: 16
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
[2025-06-10 00:51:59,038] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-10 00:51:59,043] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 4890
Validation dataset size: 953
Steps per epoch: 305
Evaluate each 152 steps
[2025-06-10 00:51:59,054] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-10 00:51:59,054] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-06-10 00:51:59,099] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: cjvt/GaMS-9B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:15, 25.10s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:16, 25.50s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:16, 25.50s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:16, 25.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:51<00:51, 25.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:51<00:52, 26.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:51<00:52, 26.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:51<00:52, 26.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:25, 25.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:25, 25.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:26, 26.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:17<00:25, 25.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 23.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.11s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 23.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 23.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 23.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.16s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:36<00:00, 24.16s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank2]:[W610 00:53:40.550128868 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s][rank1]:[W610 00:53:40.690655911 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  11%|█         | 550/4890 [00:00<00:00, 5424.36 examples/s][rank3]:[W610 00:53:40.764206423 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  23%|██▎       | 1119/4890 [00:00<00:00, 5575.59 examples/s]Extracting prompt in train dataset:  40%|███▉      | 1951/4890 [00:00<00:00, 5545.81 examples/s]Extracting prompt in train dataset:  52%|█████▏    | 2530/4890 [00:00<00:00, 5620.20 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 3380/4890 [00:00<00:00, 5556.00 examples/s]Extracting prompt in train dataset:  81%|████████  | 3960/4890 [00:00<00:00, 5603.84 examples/s]Extracting prompt in train dataset:  93%|█████████▎| 4560/4890 [00:00<00:00, 5713.36 examples/s]Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5586.97 examples/s]
Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   6%|▌         | 280/4890 [00:00<00:01, 2755.94 examples/s]Applying chat template to train dataset:  12%|█▏        | 594/4890 [00:00<00:02, 1776.60 examples/s]Applying chat template to train dataset:  19%|█▊        | 908/4890 [00:00<00:01, 2226.78 examples/s]Applying chat template to train dataset:  25%|██▌       | 1223/4890 [00:00<00:01, 2519.99 examples/s]Applying chat template to train dataset:  31%|███▏      | 1532/4890 [00:00<00:01, 2695.12 examples/s]Applying chat template to train dataset:  38%|███▊      | 1849/4890 [00:00<00:01, 2838.84 examples/s]Applying chat template to train dataset:  44%|████▍     | 2164/4890 [00:00<00:00, 2931.35 examples/s]Applying chat template to train dataset:  51%|█████     | 2480/4890 [00:00<00:00, 2998.64 examples/s]Applying chat template to train dataset:  57%|█████▋    | 2800/4890 [00:01<00:00, 3048.07 examples/s]Applying chat template to train dataset:  66%|██████▋   | 3248/4890 [00:01<00:00, 3019.57 examples/s]Applying chat template to train dataset:  73%|███████▎  | 3560/4890 [00:01<00:00, 3043.68 examples/s]Applying chat template to train dataset:  79%|███████▉  | 3877/4890 [00:01<00:00, 3077.03 examples/s]Applying chat template to train dataset:  86%|████████▌ | 4191/4890 [00:01<00:00, 3094.73 examples/s]Applying chat template to train dataset:  92%|█████████▏| 4523/4890 [00:01<00:00, 3159.18 examples/s]Applying chat template to train dataset:  99%|█████████▉| 4855/4890 [00:01<00:00, 3204.19 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 2897.82 examples/s]
Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 41/4890 [00:00<00:12, 390.15 examples/s]Tokenizing train dataset:   2%|▏         | 89/4890 [00:00<00:14, 336.01 examples/s]Tokenizing train dataset:   3%|▎         | 133/4890 [00:00<00:15, 312.32 examples/s]Tokenizing train dataset:   3%|▎         | 165/4890 [00:00<00:15, 310.24 examples/s]Tokenizing train dataset:   4%|▍         | 213/4890 [00:00<00:15, 309.22 examples/s]Tokenizing train dataset:   5%|▌         | 247/4890 [00:00<00:14, 317.11 examples/s]Tokenizing train dataset:   6%|▌         | 282/4890 [00:00<00:14, 324.96 examples/s]Tokenizing train dataset:   6%|▋         | 315/4890 [00:00<00:14, 321.72 examples/s]Tokenizing train dataset:   7%|▋         | 362/4890 [00:01<00:14, 315.77 examples/s]Tokenizing train dataset:   8%|▊         | 397/4890 [00:01<00:14, 318.32 examples/s]Tokenizing train dataset:   9%|▉         | 430/4890 [00:01<00:13, 319.44 examples/s]Tokenizing train dataset:  10%|▉         | 476/4890 [00:01<00:14, 305.22 examples/s]Tokenizing train dataset:  11%|█         | 523/4890 [00:01<00:14, 304.38 examples/s]Tokenizing train dataset:  11%|█▏        | 555/4890 [00:01<00:14, 306.90 examples/s]Tokenizing train dataset:  12%|█▏        | 586/4890 [00:01<00:14, 305.24 examples/s]Tokenizing train dataset:  13%|█▎        | 638/4890 [00:02<00:13, 312.98 examples/s]Tokenizing train dataset:  14%|█▍        | 685/4890 [00:02<00:13, 309.57 examples/s]Tokenizing train dataset:  15%|█▌        | 735/4890 [00:02<00:13, 310.70 examples/s]Tokenizing train dataset:  16%|█▌        | 780/4890 [00:02<00:13, 301.67 examples/s]Tokenizing train dataset:  17%|█▋        | 824/4890 [00:02<00:13, 293.72 examples/s]Tokenizing train dataset:  17%|█▋        | 854/4890 [00:02<00:13, 292.19 examples/s]Tokenizing train dataset:  18%|█▊        | 890/4890 [00:02<00:13, 305.18 examples/s]Tokenizing train dataset:  19%|█▉        | 922/4890 [00:02<00:13, 301.52 examples/s]Tokenizing train dataset:  19%|█▉        | 953/4890 [00:03<00:13, 302.55 examples/s]Tokenizing train dataset:  20%|██        | 997/4890 [00:03<00:13, 292.87 examples/s]Tokenizing train dataset:  21%|██        | 1039/4890 [00:03<00:13, 285.45 examples/s]Tokenizing train dataset:  22%|██▏       | 1070/4890 [00:03<00:13, 289.06 examples/s]Tokenizing train dataset:  23%|██▎       | 1105/4890 [00:03<00:12, 300.04 examples/s]Tokenizing train dataset:  24%|██▎       | 1150/4890 [00:03<00:12, 292.41 examples/s]Tokenizing train dataset:  24%|██▍       | 1180/4890 [00:03<00:12, 290.55 examples/s]Tokenizing train dataset:  25%|██▍       | 1214/4890 [00:03<00:12, 296.74 examples/s]Tokenizing train dataset:  26%|██▌       | 1250/4890 [00:04<00:11, 310.69 examples/s]Tokenizing train dataset:  27%|██▋       | 1298/4890 [00:04<00:11, 311.25 examples/s]Tokenizing train dataset:  27%|██▋       | 1330/4890 [00:04<00:11, 310.82 examples/s]Tokenizing train dataset:  28%|██▊       | 1372/4890 [00:04<00:11, 295.36 examples/s]Tokenizing train dataset:  29%|██▊       | 1403/4890 [00:04<00:11, 297.54 examples/s]Tokenizing train dataset:  29%|██▉       | 1435/4890 [00:04<00:11, 296.03 examples/s]Tokenizing train dataset:  30%|██▉       | 1465/4890 [00:04<00:11, 296.23 examples/s]Tokenizing train dataset:  31%|███       | 1508/4890 [00:04<00:11, 286.59 examples/s]Tokenizing train dataset:  31%|███▏      | 1540/4890 [00:05<00:11, 294.71 examples/s]Tokenizing train dataset:  32%|███▏      | 1570/4890 [00:05<00:11, 291.81 examples/s]Tokenizing train dataset:  33%|███▎      | 1601/4890 [00:05<00:11, 291.17 examples/s]Tokenizing train dataset:  33%|███▎      | 1636/4890 [00:05<00:10, 304.27 examples/s]Tokenizing train dataset:  34%|███▍      | 1670/4890 [00:05<00:10, 307.51 examples/s]Tokenizing train dataset:  35%|███▍      | 1710/4890 [00:05<00:09, 325.56 examples/s]Tokenizing train dataset:  36%|███▌      | 1746/4890 [00:05<00:09, 330.42 examples/s]Tokenizing train dataset:  36%|███▋      | 1781/4890 [00:05<00:09, 330.57 examples/s]Tokenizing train dataset:  37%|███▋      | 1826/4890 [00:05<00:09, 311.12 examples/s]Tokenizing train dataset:  38%|███▊      | 1872/4890 [00:06<00:09, 302.69 examples/s]Tokenizing train dataset:  39%|███▉      | 1909/4890 [00:06<00:10, 279.10 examples/s]Tokenizing train dataset:  40%|███▉      | 1951/4890 [00:06<00:10, 276.68 examples/s]Tokenizing train dataset:  41%|████      | 1993/4890 [00:06<00:10, 274.08 examples/s]Tokenizing train dataset:  41%|████▏     | 2029/4890 [00:06<00:09, 289.55 examples/s]Tokenizing train dataset:  42%|████▏     | 2063/4890 [00:06<00:09, 300.02 examples/s]Tokenizing train dataset:  43%|████▎     | 2096/4890 [00:06<00:09, 304.24 examples/s]Tokenizing train dataset:  44%|████▎     | 2129/4890 [00:07<00:08, 308.89 examples/s]Tokenizing train dataset:  44%|████▍     | 2174/4890 [00:07<00:09, 299.85 examples/s]Tokenizing train dataset:  45%|████▌     | 2220/4890 [00:07<00:09, 294.33 examples/s]Tokenizing train dataset:  46%|████▌     | 2257/4890 [00:07<00:08, 308.50 examples/s]Tokenizing train dataset:  47%|████▋     | 2302/4890 [00:07<00:08, 302.39 examples/s]Tokenizing train dataset:  48%|████▊     | 2344/4890 [00:07<00:08, 292.80 examples/s]Tokenizing train dataset:  49%|████▉     | 2390/4890 [00:07<00:08, 289.44 examples/s]Tokenizing train dataset:  50%|████▉     | 2423/4890 [00:08<00:08, 293.62 examples/s]Tokenizing train dataset:  50%|█████     | 2468/4890 [00:08<00:08, 291.17 examples/s]Tokenizing train dataset:  51%|█████     | 2499/4890 [00:08<00:08, 294.83 examples/s]Tokenizing train dataset:  52%|█████▏    | 2534/4890 [00:08<00:08, 267.86 examples/s]Tokenizing train dataset:  53%|█████▎    | 2570/4890 [00:08<00:08, 285.23 examples/s]Tokenizing train dataset:  53%|█████▎    | 2606/4890 [00:08<00:07, 299.69 examples/s]Tokenizing train dataset:  54%|█████▍    | 2640/4890 [00:08<00:07, 305.88 examples/s]Tokenizing train dataset:  55%|█████▍    | 2673/4890 [00:08<00:08, 258.56 examples/s]Tokenizing train dataset:  55%|█████▌    | 2710/4890 [00:09<00:07, 283.12 examples/s]Tokenizing train dataset:  56%|█████▋    | 2753/4890 [00:09<00:07, 279.94 examples/s]Tokenizing train dataset:  57%|█████▋    | 2800/4890 [00:09<00:07, 286.42 examples/s]Tokenizing train dataset:  58%|█████▊    | 2830/4890 [00:09<00:07, 286.12 examples/s]Tokenizing train dataset:  59%|█████▊    | 2861/4890 [00:09<00:07, 286.83 examples/s]Tokenizing train dataset:  59%|█████▉    | 2891/4890 [00:09<00:06, 287.17 examples/s]Tokenizing train dataset:  60%|█████▉    | 2921/4890 [00:09<00:06, 289.16 examples/s]Tokenizing train dataset:  60%|██████    | 2954/4890 [00:09<00:06, 297.72 examples/s]Tokenizing train dataset:  61%|██████    | 2985/4890 [00:09<00:06, 299.25 examples/s]Tokenizing train dataset:  62%|██████▏   | 3030/4890 [00:10<00:06, 291.08 examples/s]Tokenizing train dataset:  63%|██████▎   | 3060/4890 [00:10<00:06, 284.88 examples/s]Tokenizing train dataset:  63%|██████▎   | 3090/4890 [00:10<00:06, 287.93 examples/s]Tokenizing train dataset:  64%|██████▍   | 3122/4890 [00:10<00:06, 294.07 examples/s]Tokenizing train dataset:  65%|██████▍   | 3167/4890 [00:10<00:05, 291.56 examples/s]Tokenizing train dataset:  65%|██████▌   | 3200/4890 [00:10<00:05, 298.05 examples/s]Tokenizing train dataset:  66%|██████▋   | 3246/4890 [00:10<00:05, 297.11 examples/s]Tokenizing train dataset:  67%|██████▋   | 3277/4890 [00:10<00:05, 295.91 examples/s]Tokenizing train dataset:  68%|██████▊   | 3310/4890 [00:11<00:05, 300.07 examples/s]Tokenizing train dataset:  69%|██████▊   | 3357/4890 [00:11<00:05, 297.57 examples/s]Tokenizing train dataset:  69%|██████▉   | 3396/4890 [00:11<00:05, 281.22 examples/s]Tokenizing train dataset:  70%|███████   | 3436/4890 [00:11<00:05, 272.23 examples/s]Tokenizing train dataset:  71%|███████   | 3472/4890 [00:11<00:05, 258.63 examples/s]Tokenizing train dataset:  72%|███████▏  | 3520/4890 [00:11<00:05, 271.94 examples/s]Tokenizing train dataset:  73%|███████▎  | 3551/4890 [00:11<00:04, 278.54 examples/s]Tokenizing train dataset:  73%|███████▎  | 3589/4890 [00:12<00:04, 295.81 examples/s]Tokenizing train dataset:  74%|███████▍  | 3635/4890 [00:12<00:04, 295.78 examples/s]Tokenizing train dataset:  75%|███████▍  | 3667/4890 [00:12<00:04, 300.67 examples/s]Tokenizing train dataset:  76%|███████▌  | 3710/4890 [00:12<00:04, 291.90 examples/s]Tokenizing train dataset:  77%|███████▋  | 3754/4890 [00:12<00:03, 326.74 examples/s]Tokenizing train dataset:  78%|███████▊  | 3804/4890 [00:12<00:03, 324.87 examples/s]Tokenizing train dataset:  79%|███████▊  | 3840/4890 [00:12<00:03, 329.49 examples/s]Tokenizing train dataset:  79%|███████▉  | 3887/4890 [00:13<00:03, 321.55 examples/s]Tokenizing train dataset:  80%|████████  | 3929/4890 [00:13<00:03, 302.62 examples/s]Tokenizing train dataset:  81%|████████▏ | 3978/4890 [00:13<00:02, 307.78 examples/s]Tokenizing train dataset:  82%|████████▏ | 4017/4890 [00:13<00:02, 322.70 examples/s]Tokenizing train dataset:  83%|████████▎ | 4060/4890 [00:13<00:02, 306.77 examples/s]Tokenizing train dataset:  84%|████████▎ | 4095/4890 [00:13<00:02, 310.01 examples/s]Tokenizing train dataset:  85%|████████▍ | 4140/4890 [00:13<00:02, 299.40 examples/s]Tokenizing train dataset:  86%|████████▌ | 4186/4890 [00:14<00:02, 262.67 examples/s]Tokenizing train dataset:  88%|████████▊ | 4303/4890 [00:14<00:01, 448.47 examples/s]Tokenizing train dataset:  91%|█████████ | 4433/4890 [00:14<00:00, 641.62 examples/s]Tokenizing train dataset:  93%|█████████▎| 4560/4890 [00:14<00:00, 792.20 examples/s]Tokenizing train dataset:  96%|█████████▌| 4681/4890 [00:14<00:00, 896.66 examples/s]Tokenizing train dataset:  98%|█████████▊| 4810/4890 [00:14<00:00, 998.42 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 333.65 examples/s]
[rank0]:[W610 00:53:59.852907266 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  11%|█▏        | 560/4890 [00:00<00:00, 5555.78 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5637.05 examples/s]Extracting prompt in train dataset:  11%|█         | 549/4890 [00:00<00:00, 5418.03 examples/s]Extracting prompt in train dataset:  11%|█         | 542/4890 [00:00<00:00, 5344.61 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5660.67 examples/s]
Extracting prompt in train dataset:  23%|██▎       | 1110/4890 [00:00<00:00, 5526.01 examples/s]Extracting prompt in train dataset:  24%|██▎       | 1150/4890 [00:00<00:00, 5730.47 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1110/4890 [00:00<00:00, 5509.23 examples/s]Extracting prompt in train dataset:  34%|███▍      | 1670/4890 [00:00<00:00, 5535.07 examples/s]Extracting prompt in train dataset:  34%|███▍      | 1670/4890 [00:00<00:00, 5524.98 examples/s]Extracting prompt in train dataset:  41%|████      | 2010/4890 [00:00<00:00, 5710.87 examples/s]Extracting prompt in train dataset:  46%|████▌     | 2242/4890 [00:00<00:00, 5607.08 examples/s]Extracting prompt in train dataset:  46%|████▌     | 2250/4890 [00:00<00:00, 5612.88 examples/s]Extracting prompt in train dataset:  53%|█████▎    | 2610/4890 [00:00<00:00, 5773.97 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 2810/4890 [00:00<00:00, 5523.82 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  63%|██████▎   | 3090/4890 [00:00<00:00, 5532.21 examples/s]Extracting prompt in train dataset:  71%|███████   | 3450/4890 [00:00<00:00, 5688.29 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 3373/4890 [00:00<00:00, 5555.46 examples/s]Applying chat template to eval dataset:  32%|███▏      | 309/953 [00:00<00:00, 3048.14 examples/s]Extracting prompt in train dataset:  75%|███████▍  | 3650/4890 [00:00<00:00, 5541.68 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 4040/4890 [00:00<00:00, 5741.16 examples/s]Extracting prompt in train dataset:  81%|████████  | 3940/4890 [00:00<00:00, 5589.83 examples/s]Applying chat template to eval dataset:  66%|██████▋   | 633/953 [00:00<00:00, 3147.81 examples/s]Extracting prompt in train dataset:  86%|████████▋ | 4223/4890 [00:00<00:00, 5596.62 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2176.44 examples/s]Extracting prompt in train dataset:  93%|█████████▎| 4541/4890 [00:00<00:00, 4023.88 examples/s]Extracting prompt in train dataset:  95%|█████████▌| 4660/4890 [00:00<00:00, 4206.23 examples/s]Extracting prompt in train dataset:  99%|█████████▉| 4840/4890 [00:00<00:00, 4615.34 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2357.63 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 5107.38 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:00<00:00, 4963.54 examples/s]
Extracting prompt in train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 4864.68 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 318.17 examples/s]Applying chat template to train dataset:   6%|▌         | 285/4890 [00:00<00:01, 2816.25 examples/s]Applying chat template to train dataset:   6%|▌         | 291/4890 [00:00<00:01, 2873.13 examples/s]Applying chat template to train dataset:   6%|▌         | 287/4890 [00:00<00:01, 2847.63 examples/s]Applying chat template to train dataset:  12%|█▏        | 605/4890 [00:00<00:01, 3035.21 examples/s]Applying chat template to train dataset:  13%|█▎        | 612/4890 [00:00<00:01, 3061.49 examples/s]Applying chat template to train dataset:  12%|█▏        | 602/4890 [00:00<00:01, 3017.05 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 287.04 examples/s]Applying chat template to train dataset:  19%|█▉        | 926/4890 [00:00<00:01, 3112.04 examples/s]Applying chat template to train dataset:  19%|█▉        | 940/4890 [00:00<00:01, 3152.78 examples/s]Applying chat template to train dataset:  19%|█▉        | 920/4890 [00:00<00:01, 3077.85 examples/s]Applying chat template to train dataset:  25%|██▌       | 1243/4890 [00:00<00:01, 3129.18 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 277.59 examples/s]Applying chat template to train dataset:  26%|██▌       | 1270/4890 [00:00<00:01, 3207.18 examples/s]Applying chat template to train dataset:  25%|██▌       | 1240/4890 [00:00<00:01, 3118.74 examples/s]Applying chat template to train dataset:  33%|███▎      | 1591/4890 [00:00<00:01, 3205.28 examples/s]Applying chat template to train dataset:  32%|███▏      | 1555/4890 [00:00<00:01, 3125.92 examples/s]Applying chat template to train dataset:  35%|███▌      | 1714/4890 [00:00<00:01, 3132.37 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:02, 265.88 examples/s]Applying chat template to train dataset:  39%|███▉      | 1915/4890 [00:00<00:00, 3215.33 examples/s]Applying chat template to train dataset:  38%|███▊      | 1875/4890 [00:00<00:00, 3147.08 examples/s]Applying chat template to train dataset:  42%|████▏     | 2038/4890 [00:00<00:00, 3163.26 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.11 examples/s]Applying chat template to train dataset:  46%|████▌     | 2243/4890 [00:00<00:00, 3235.59 examples/s]Applying chat template to train dataset:  45%|████▌     | 2201/4890 [00:00<00:00, 3180.73 examples/s]Applying chat template to train dataset:  48%|████▊     | 2363/4890 [00:00<00:00, 3185.11 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 269.10 examples/s]Applying chat template to train dataset:  53%|█████▎    | 2573/4890 [00:00<00:00, 3252.96 examples/s]Applying chat template to train dataset:  52%|█████▏    | 2528/4890 [00:00<00:00, 3205.57 examples/s]Applying chat template to train dataset:  55%|█████▍    | 2688/4890 [00:00<00:00, 3201.44 examples/s]Tokenizing eval dataset:  31%|███       | 291/953 [00:00<00:01, 368.53 examples/s]Applying chat template to train dataset:  62%|██████▏   | 3045/4890 [00:00<00:00, 3202.45 examples/s]Applying chat template to train dataset:  61%|██████    | 2995/4890 [00:00<00:00, 3164.53 examples/s]Applying chat template to train dataset:  64%|██████▍   | 3145/4890 [00:01<00:00, 3137.08 examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 439.53 examples/s]Applying chat template to train dataset:  69%|██████▉   | 3367/4890 [00:01<00:00, 3205.16 examples/s]Applying chat template to train dataset:  68%|██████▊   | 3313/4890 [00:01<00:00, 3167.26 examples/s]Applying chat template to train dataset:  71%|███████   | 3467/4890 [00:01<00:00, 3157.88 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 488.37 examples/s]Applying chat template to train dataset:  75%|███████▌  | 3689/4890 [00:01<00:00, 3205.01 examples/s]Applying chat template to train dataset:  74%|███████▍  | 3637/4890 [00:01<00:00, 3186.42 examples/s]Applying chat template to train dataset:  78%|███████▊  | 3791/4890 [00:01<00:00, 3178.30 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 541.71 examples/s]Applying chat template to train dataset:  82%|████████▏ | 4020/4890 [00:01<00:00, 3229.59 examples/s]Applying chat template to train dataset:  81%|████████  | 3963/4890 [00:01<00:00, 3204.02 examples/s]Applying chat template to train dataset:  84%|████████▍ | 4117/4890 [00:01<00:00, 3198.22 examples/s]Tokenizing eval dataset:  58%|█████▊    | 555/953 [00:01<00:00, 576.49 examples/s]Applying chat template to train dataset:  89%|████████▉ | 4360/4890 [00:01<00:00, 3272.71 examples/s]Applying chat template to train dataset:  88%|████████▊ | 4295/4890 [00:01<00:00, 3234.41 examples/s]Applying chat template to train dataset:  91%|█████████ | 4454/4890 [00:01<00:00, 3245.04 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 585.25 examples/s]Applying chat template to train dataset:  96%|█████████▋| 4709/4890 [00:01<00:00, 3335.82 examples/s]Applying chat template to train dataset:  95%|█████████▍| 4639/4890 [00:01<00:00, 3293.84 examples/s]Applying chat template to train dataset:  98%|█████████▊| 4796/4890 [00:01<00:00, 3293.77 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3181.91 examples/s]
Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3233.17 examples/s]
Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 596.91 examples/s]Applying chat template to train dataset: 100%|██████████| 4890/4890 [00:01<00:00, 3189.78 examples/s]
Tokenizing eval dataset:  80%|████████  | 766/953 [00:01<00:00, 577.18 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 543.04 examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/4890 [00:00<?, ? examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 516.43 examples/s]Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:11, 410.81 examples/s]Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:11, 406.73 examples/s]Tokenizing train dataset:   1%|          | 42/4890 [00:00<00:11, 409.84 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 453.52 examples/s]
Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:13, 343.09 examples/s]Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:14, 340.45 examples/s]Tokenizing train dataset:   2%|▏         | 90/4890 [00:00<00:14, 342.82 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 324.59 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 322.43 examples/s]Tokenizing train dataset:   3%|▎         | 139/4890 [00:00<00:14, 324.49 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 312.04 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 309.20 examples/s]Tokenizing train dataset:   4%|▍         | 184/4890 [00:00<00:15, 312.22 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 317.69 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 314.87 examples/s]Tokenizing train dataset:   4%|▍         | 218/4890 [00:00<00:14, 317.96 examples/s]Tokenizing train dataset:   5%|▌         | 252/4890 [00:00<00:14, 322.36 examples/s]Tokenizing train dataset:   5%|▌         | 252/4890 [00:00<00:14, 319.75 examples/s]Tokenizing train dataset:   5%|▌         | 252/4890 [00:00<00:14, 322.65 examples/s]Tokenizing train dataset:   6%|▌         | 290/4890 [00:00<00:13, 335.92 examples/s]Tokenizing train dataset:   6%|▌         | 290/4890 [00:00<00:13, 333.04 examples/s]Tokenizing train dataset:   6%|▌         | 290/4890 [00:00<00:13, 336.18 examples/s]Tokenizing train dataset:   7%|▋         | 324/4890 [00:00<00:13, 331.80 examples/s]Tokenizing train dataset:   7%|▋         | 339/4890 [00:01<00:13, 329.38 examples/s]Tokenizing train dataset:   7%|▋         | 339/4890 [00:01<00:13, 329.80 examples/s]Tokenizing train dataset:   8%|▊         | 370/4890 [00:01<00:14, 318.03 examples/s]Tokenizing train dataset:   8%|▊         | 389/4890 [00:01<00:13, 326.84 examples/s]Tokenizing train dataset:   8%|▊         | 389/4890 [00:01<00:13, 327.14 examples/s]Tokenizing train dataset:   9%|▊         | 420/4890 [00:01<00:14, 316.13 examples/s]Tokenizing train dataset:   9%|▉         | 437/4890 [00:01<00:14, 316.63 examples/s]Tokenizing train dataset:   9%|▉         | 437/4890 [00:01<00:14, 316.76 examples/s]Tokenizing train dataset:  10%|▉         | 469/4890 [00:01<00:13, 316.02 examples/s]Tokenizing train dataset:  10%|▉         | 469/4890 [00:01<00:14, 314.15 examples/s]Tokenizing train dataset:  10%|▉         | 469/4890 [00:01<00:13, 316.33 examples/s]Tokenizing train dataset:  11%|█         | 516/4890 [00:01<00:14, 309.17 examples/s]Tokenizing train dataset:  10%|█         | 512/4890 [00:01<00:14, 303.93 examples/s]Tokenizing train dataset:  11%|█         | 516/4890 [00:01<00:14, 309.66 examples/s]Tokenizing train dataset:  11%|█         | 550/4890 [00:01<00:13, 314.88 examples/s]Tokenizing train dataset:  11%|█         | 549/4890 [00:01<00:13, 316.86 examples/s]Tokenizing train dataset:  11%|█         | 550/4890 [00:01<00:13, 315.06 examples/s]Tokenizing train dataset:  12%|█▏        | 596/4890 [00:01<00:13, 310.07 examples/s]Tokenizing train dataset:  12%|█▏        | 596/4890 [00:01<00:13, 308.09 examples/s]Tokenizing train dataset:  12%|█▏        | 596/4890 [00:01<00:13, 310.56 examples/s]Tokenizing train dataset:  13%|█▎        | 634/4890 [00:01<00:13, 324.77 examples/s]Tokenizing train dataset:  13%|█▎        | 634/4890 [00:01<00:13, 322.42 examples/s]Tokenizing train dataset:  13%|█▎        | 634/4890 [00:01<00:13, 325.28 examples/s]Tokenizing train dataset:  14%|█▍        | 681/4890 [00:02<00:13, 315.64 examples/s]Tokenizing train dataset:  14%|█▍        | 681/4890 [00:02<00:13, 313.75 examples/s]Tokenizing train dataset:  14%|█▍        | 681/4890 [00:02<00:13, 316.13 examples/s]Tokenizing train dataset:  15%|█▍        | 729/4890 [00:02<00:13, 313.80 examples/s]Tokenizing train dataset:  15%|█▍        | 729/4890 [00:02<00:13, 311.90 examples/s]Tokenizing train dataset:  15%|█▍        | 729/4890 [00:02<00:13, 314.17 examples/s]Tokenizing train dataset:  16%|█▌        | 763/4890 [00:02<00:13, 315.12 examples/s]Tokenizing train dataset:  16%|█▌        | 763/4890 [00:02<00:13, 313.19 examples/s]Tokenizing train dataset:  16%|█▌        | 763/4890 [00:02<00:13, 315.26 examples/s]Tokenizing train dataset:  16%|█▋        | 806/4890 [00:02<00:13, 301.82 examples/s]Tokenizing train dataset:  16%|█▋        | 806/4890 [00:02<00:13, 300.27 examples/s]Tokenizing train dataset:  16%|█▋        | 806/4890 [00:02<00:13, 302.00 examples/s]Tokenizing train dataset:  17%|█▋        | 850/4890 [00:02<00:13, 296.04 examples/s]Tokenizing train dataset:  18%|█▊        | 885/4890 [00:02<00:13, 304.15 examples/s]Tokenizing train dataset:  17%|█▋        | 850/4890 [00:02<00:16, 246.63 examples/s]Tokenizing train dataset:  17%|█▋        | 850/4890 [00:02<00:16, 241.05 examples/s]Tokenizing train dataset:  19%|█▉        | 920/4890 [00:02<00:12, 309.19 examples/s]Tokenizing train dataset:  18%|█▊        | 885/4890 [00:02<00:15, 264.04 examples/s]Tokenizing train dataset:  18%|█▊        | 885/4890 [00:02<00:15, 258.83 examples/s]Tokenizing train dataset:  19%|█▉        | 952/4890 [00:03<00:12, 307.28 examples/s]Tokenizing train dataset:  19%|█▉        | 920/4890 [00:03<00:14, 278.11 examples/s]Tokenizing train dataset:  19%|█▉        | 920/4890 [00:03<00:14, 273.38 examples/s]Tokenizing train dataset:  20%|██        | 983/4890 [00:03<00:12, 303.23 examples/s]Tokenizing train dataset:  19%|█▉        | 952/4890 [00:03<00:13, 284.00 examples/s]Tokenizing train dataset:  19%|█▉        | 952/4890 [00:03<00:14, 279.94 examples/s]Tokenizing train dataset:  20%|██        | 983/4890 [00:03<00:13, 286.26 examples/s]Tokenizing train dataset:  20%|██        | 983/4890 [00:03<00:13, 282.91 examples/s]Tokenizing train dataset:  21%|██        | 1027/4890 [00:03<00:13, 293.55 examples/s]Tokenizing train dataset:  22%|██▏       | 1057/4890 [00:03<00:13, 292.89 examples/s]Tokenizing train dataset:  21%|██        | 1027/4890 [00:03<00:13, 283.15 examples/s]Tokenizing train dataset:  21%|██        | 1027/4890 [00:03<00:13, 280.59 examples/s]Tokenizing train dataset:  22%|██▏       | 1093/4890 [00:03<00:12, 308.47 examples/s]Tokenizing train dataset:  22%|██▏       | 1056/4890 [00:03<00:13, 281.42 examples/s]Tokenizing train dataset:  22%|██▏       | 1073/4890 [00:03<00:13, 288.22 examples/s]Tokenizing train dataset:  22%|██▏       | 1093/4890 [00:03<00:12, 300.74 examples/s]Tokenizing train dataset:  23%|██▎       | 1135/4890 [00:03<00:12, 294.78 examples/s]Tokenizing train dataset:  23%|██▎       | 1109/4890 [00:03<00:12, 302.91 examples/s]Tokenizing train dataset:  24%|██▍       | 1166/4890 [00:03<00:12, 295.63 examples/s]Tokenizing train dataset:  23%|██▎       | 1135/4890 [00:03<00:12, 289.37 examples/s]Tokenizing train dataset:  24%|██▎       | 1151/4890 [00:03<00:12, 292.79 examples/s]Tokenizing train dataset:  25%|██▍       | 1202/4890 [00:03<00:12, 306.37 examples/s]Tokenizing train dataset:  24%|██▍       | 1166/4890 [00:03<00:12, 291.43 examples/s]Tokenizing train dataset:  24%|██▍       | 1183/4890 [00:03<00:12, 294.13 examples/s]Tokenizing train dataset:  25%|██▌       | 1236/4890 [00:03<00:11, 311.86 examples/s]Tokenizing train dataset:  25%|██▍       | 1202/4890 [00:03<00:12, 303.15 examples/s]Tokenizing train dataset:  25%|██▍       | 1218/4890 [00:03<00:12, 303.39 examples/s]Tokenizing train dataset:  26%|██▌       | 1270/4890 [00:04<00:11, 313.72 examples/s]Tokenizing train dataset:  25%|██▌       | 1236/4890 [00:04<00:11, 309.08 examples/s]Tokenizing train dataset:  26%|██▌       | 1252/4890 [00:04<00:11, 312.11 examples/s]Tokenizing train dataset:  27%|██▋       | 1304/4890 [00:04<00:11, 317.46 examples/s]Tokenizing train dataset:  26%|██▌       | 1270/4890 [00:04<00:11, 311.43 examples/s]Tokenizing train dataset:  26%|██▋       | 1286/4890 [00:04<00:11, 309.96 examples/s]Tokenizing train dataset:  27%|██▋       | 1339/4890 [00:04<00:11, 320.64 examples/s]Tokenizing train dataset:  27%|██▋       | 1305/4890 [00:04<00:11, 316.23 examples/s]Tokenizing train dataset:  27%|██▋       | 1320/4890 [00:04<00:11, 317.18 examples/s]Tokenizing train dataset:  27%|██▋       | 1339/4890 [00:04<00:11, 318.57 examples/s]Tokenizing train dataset:  28%|██▊       | 1381/4890 [00:04<00:11, 300.72 examples/s]Tokenizing train dataset:  28%|██▊       | 1353/4890 [00:04<00:11, 313.93 examples/s]Tokenizing train dataset:  29%|██▉       | 1412/4890 [00:04<00:11, 301.42 examples/s]Tokenizing train dataset:  28%|██▊       | 1381/4890 [00:04<00:11, 299.06 examples/s]Tokenizing train dataset:  29%|██▊       | 1397/4890 [00:04<00:11, 304.32 examples/s]Tokenizing train dataset:  29%|██▉       | 1412/4890 [00:04<00:11, 299.83 examples/s]Tokenizing train dataset:  30%|██▉       | 1457/4890 [00:04<00:11, 299.03 examples/s]Tokenizing train dataset:  29%|██▉       | 1428/4890 [00:04<00:11, 303.86 examples/s]Tokenizing train dataset:  30%|███       | 1488/4890 [00:04<00:11, 298.25 examples/s]Tokenizing train dataset:  30%|██▉       | 1457/4890 [00:04<00:11, 297.46 examples/s]Tokenizing train dataset:  30%|███       | 1473/4890 [00:04<00:11, 299.28 examples/s]Tokenizing train dataset:  30%|███       | 1488/4890 [00:04<00:11, 296.79 examples/s]Tokenizing train dataset:  31%|███▏      | 1535/4890 [00:04<00:11, 300.14 examples/s]Tokenizing train dataset:  31%|███       | 1515/4890 [00:04<00:11, 290.70 examples/s]Tokenizing train dataset:  32%|███▏      | 1567/4890 [00:05<00:11, 299.61 examples/s]Tokenizing train dataset:  31%|███▏      | 1535/4890 [00:05<00:11, 298.71 examples/s]Tokenizing train dataset:  32%|███▏      | 1550/4890 [00:05<00:11, 299.37 examples/s]Tokenizing train dataset:  33%|███▎      | 1598/4890 [00:05<00:11, 297.75 examples/s]Tokenizing train dataset:  32%|███▏      | 1567/4890 [00:05<00:11, 298.29 examples/s]Tokenizing train dataset:  32%|███▏      | 1581/4890 [00:05<00:11, 298.62 examples/s]Tokenizing train dataset:  33%|███▎      | 1631/4890 [00:05<00:10, 304.16 examples/s]Tokenizing train dataset:  33%|███▎      | 1598/4890 [00:05<00:11, 296.49 examples/s]Tokenizing train dataset:  33%|███▎      | 1614/4890 [00:05<00:10, 304.41 examples/s]Tokenizing train dataset:  34%|███▍      | 1667/4890 [00:05<00:10, 314.89 examples/s]Tokenizing train dataset:  33%|███▎      | 1631/4890 [00:05<00:10, 302.94 examples/s]Tokenizing train dataset:  34%|███▎      | 1650/4890 [00:05<00:10, 316.85 examples/s]Tokenizing train dataset:  35%|███▍      | 1704/4890 [00:05<00:09, 329.66 examples/s]Tokenizing train dataset:  34%|███▍      | 1667/4890 [00:05<00:10, 313.68 examples/s]Tokenizing train dataset:  34%|███▍      | 1685/4890 [00:05<00:09, 322.06 examples/s]Tokenizing train dataset:  36%|███▌      | 1739/4890 [00:05<00:09, 332.14 examples/s]Tokenizing train dataset:  35%|███▍      | 1704/4890 [00:05<00:09, 328.04 examples/s]Tokenizing train dataset:  35%|███▌      | 1722/4890 [00:05<00:09, 332.40 examples/s]Tokenizing train dataset:  36%|███▋      | 1774/4890 [00:05<00:09, 337.07 examples/s]Tokenizing train dataset:  36%|███▌      | 1739/4890 [00:05<00:09, 330.38 examples/s]Tokenizing train dataset:  36%|███▌      | 1756/4890 [00:05<00:09, 333.36 examples/s]Tokenizing train dataset:  36%|███▋      | 1774/4890 [00:05<00:09, 335.16 examples/s]Tokenizing train dataset:  37%|███▋      | 1820/4890 [00:05<00:09, 320.98 examples/s]Tokenizing train dataset:  37%|███▋      | 1792/4890 [00:05<00:09, 336.26 examples/s]Tokenizing train dataset:  37%|███▋      | 1820/4890 [00:05<00:09, 319.30 examples/s]Tokenizing train dataset:  38%|███▊      | 1865/4890 [00:05<00:09, 311.40 examples/s]Tokenizing train dataset:  38%|███▊      | 1834/4890 [00:05<00:09, 307.70 examples/s]Tokenizing train dataset:  38%|███▊      | 1868/4890 [00:06<00:09, 313.23 examples/s]Tokenizing train dataset:  38%|███▊      | 1865/4890 [00:06<00:09, 310.13 examples/s]Tokenizing train dataset:  39%|███▉      | 1904/4890 [00:06<00:10, 288.21 examples/s]Tokenizing train dataset:  39%|███▉      | 1906/4890 [00:06<00:10, 290.08 examples/s]Tokenizing train dataset:  39%|███▉      | 1903/4890 [00:06<00:10, 287.11 examples/s]Tokenizing train dataset:  40%|███▉      | 1948/4890 [00:06<00:10, 284.74 examples/s]Tokenizing train dataset:  40%|███▉      | 1948/4890 [00:06<00:10, 282.82 examples/s]Tokenizing train dataset:  40%|███▉      | 1944/4890 [00:06<00:10, 279.74 examples/s]Tokenizing train dataset:  41%|████      | 1991/4890 [00:06<00:10, 282.89 examples/s]Tokenizing train dataset:  41%|████▏     | 2028/4890 [00:06<00:09, 296.89 examples/s]Tokenizing train dataset:  41%|████      | 1991/4890 [00:06<00:10, 281.38 examples/s]Tokenizing train dataset:  41%|████      | 1989/4890 [00:06<00:10, 281.49 examples/s]Tokenizing train dataset:  42%|████▏     | 2063/4890 [00:06<00:09, 306.28 examples/s]Tokenizing train dataset:  41%|████▏     | 2028/4890 [00:06<00:09, 296.26 examples/s]Tokenizing train dataset:  41%|████▏     | 2024/4890 [00:06<00:09, 294.97 examples/s]Tokenizing train dataset:  43%|████▎     | 2098/4890 [00:06<00:08, 314.26 examples/s]Tokenizing train dataset:  42%|████▏     | 2063/4890 [00:06<00:09, 305.81 examples/s]Tokenizing train dataset:  42%|████▏     | 2057/4890 [00:06<00:09, 301.86 examples/s]Tokenizing train dataset:  44%|████▎     | 2130/4890 [00:06<00:08, 311.37 examples/s]Tokenizing train dataset:  43%|████▎     | 2098/4890 [00:06<00:08, 314.01 examples/s]Tokenizing train dataset:  43%|████▎     | 2090/4890 [00:06<00:09, 307.72 examples/s]Tokenizing train dataset:  44%|████▎     | 2130/4890 [00:06<00:08, 311.19 examples/s]Tokenizing train dataset:  43%|████▎     | 2126/4890 [00:07<00:08, 315.21 examples/s]Tokenizing train dataset:  45%|████▍     | 2177/4890 [00:07<00:08, 308.00 examples/s]Tokenizing train dataset:  45%|████▍     | 2177/4890 [00:07<00:08, 307.86 examples/s]Tokenizing train dataset:  44%|████▍     | 2171/4890 [00:07<00:08, 304.29 examples/s]Tokenizing train dataset:  45%|████▌     | 2222/4890 [00:07<00:08, 300.01 examples/s]Tokenizing train dataset:  46%|████▌     | 2259/4890 [00:07<00:08, 314.77 examples/s]Tokenizing train dataset:  45%|████▌     | 2222/4890 [00:07<00:08, 299.83 examples/s]Tokenizing train dataset:  45%|████▌     | 2218/4890 [00:07<00:08, 304.52 examples/s]Tokenizing train dataset:  46%|████▌     | 2259/4890 [00:07<00:08, 314.85 examples/s]Tokenizing train dataset:  46%|████▌     | 2254/4890 [00:07<00:08, 314.27 examples/s]Tokenizing train dataset:  47%|████▋     | 2305/4890 [00:07<00:08, 307.80 examples/s]Tokenizing train dataset:  47%|████▋     | 2305/4890 [00:07<00:08, 307.97 examples/s]Tokenizing train dataset:  47%|████▋     | 2300/4890 [00:07<00:08, 307.99 examples/s]Tokenizing train dataset:  48%|████▊     | 2349/4890 [00:07<00:08, 298.57 examples/s]Tokenizing train dataset:  49%|████▊     | 2380/4890 [00:07<00:08, 298.23 examples/s]Tokenizing train dataset:  48%|████▊     | 2349/4890 [00:07<00:08, 298.63 examples/s]Tokenizing train dataset:  48%|████▊     | 2343/4890 [00:07<00:08, 295.15 examples/s]Tokenizing train dataset:  49%|████▊     | 2380/4890 [00:07<00:08, 298.27 examples/s]Tokenizing train dataset:  49%|████▊     | 2373/4890 [00:07<00:08, 293.10 examples/s]Tokenizing train dataset:  50%|████▉     | 2428/4890 [00:07<00:08, 301.83 examples/s]Tokenizing train dataset:  49%|████▉     | 2406/4890 [00:07<00:08, 294.30 examples/s]Tokenizing train dataset:  50%|████▉     | 2428/4890 [00:07<00:08, 301.46 examples/s]Tokenizing train dataset:  51%|█████     | 2471/4890 [00:08<00:08, 292.80 examples/s]Tokenizing train dataset:  50%|████▉     | 2440/4890 [00:08<00:08, 301.68 examples/s]Tokenizing train dataset:  51%|█████     | 2471/4890 [00:08<00:08, 292.22 examples/s]Tokenizing train dataset:  51%|█████▏    | 2512/4890 [00:08<00:08, 284.05 examples/s]Tokenizing train dataset:  51%|█████     | 2488/4890 [00:08<00:07, 302.90 examples/s]Tokenizing train dataset:  51%|█████▏    | 2512/4890 [00:08<00:08, 283.09 examples/s]Tokenizing train dataset:  52%|█████▏    | 2558/4890 [00:08<00:08, 287.84 examples/s]Tokenizing train dataset:  52%|█████▏    | 2525/4890 [00:08<00:08, 273.60 examples/s]Tokenizing train dataset:  53%|█████▎    | 2593/4890 [00:08<00:07, 300.80 examples/s]Tokenizing train dataset:  52%|█████▏    | 2558/4890 [00:08<00:08, 287.27 examples/s]Tokenizing train dataset:  52%|█████▏    | 2558/4890 [00:08<00:08, 284.40 examples/s]Tokenizing train dataset:  54%|█████▎    | 2627/4890 [00:08<00:07, 308.17 examples/s]Tokenizing train dataset:  53%|█████▎    | 2593/4890 [00:08<00:07, 300.15 examples/s]Tokenizing train dataset:  53%|█████▎    | 2593/4890 [00:08<00:07, 299.42 examples/s]Tokenizing train dataset:  54%|█████▍    | 2661/4890 [00:08<00:07, 314.14 examples/s]Tokenizing train dataset:  54%|█████▎    | 2627/4890 [00:08<00:07, 307.40 examples/s]Tokenizing train dataset:  54%|█████▎    | 2627/4890 [00:08<00:07, 307.48 examples/s]Tokenizing train dataset:  55%|█████▌    | 2696/4890 [00:08<00:06, 320.38 examples/s]Tokenizing train dataset:  54%|█████▍    | 2661/4890 [00:08<00:07, 313.31 examples/s]Tokenizing train dataset:  54%|█████▍    | 2661/4890 [00:08<00:07, 313.37 examples/s]Tokenizing train dataset:  55%|█████▌    | 2696/4890 [00:08<00:06, 319.71 examples/s]Tokenizing train dataset:  56%|█████▌    | 2743/4890 [00:08<00:06, 313.40 examples/s]Tokenizing train dataset:  55%|█████▌    | 2696/4890 [00:08<00:06, 319.74 examples/s]Tokenizing train dataset:  56%|█████▌    | 2743/4890 [00:08<00:06, 312.56 examples/s]Tokenizing train dataset:  57%|█████▋    | 2790/4890 [00:09<00:06, 309.47 examples/s]Tokenizing train dataset:  56%|█████▌    | 2743/4890 [00:09<00:06, 312.12 examples/s]Tokenizing train dataset:  57%|█████▋    | 2790/4890 [00:09<00:06, 308.89 examples/s]Tokenizing train dataset:  57%|█████▋    | 2790/4890 [00:09<00:06, 308.29 examples/s]Tokenizing train dataset:  58%|█████▊    | 2838/4890 [00:09<00:06, 305.94 examples/s]Tokenizing train dataset:  58%|█████▊    | 2838/4890 [00:09<00:06, 305.57 examples/s]Tokenizing train dataset:  58%|█████▊    | 2837/4890 [00:09<00:06, 305.61 examples/s]Tokenizing train dataset:  59%|█████▉    | 2881/4890 [00:09<00:06, 297.01 examples/s]Tokenizing train dataset:  60%|█████▉    | 2915/4890 [00:09<00:06, 299.54 examples/s]Tokenizing train dataset:  59%|█████▉    | 2880/4890 [00:09<00:06, 294.16 examples/s]Tokenizing train dataset:  59%|█████▉    | 2878/4890 [00:09<00:06, 292.41 examples/s]Tokenizing train dataset:  60%|██████    | 2946/4890 [00:09<00:06, 300.07 examples/s]Tokenizing train dataset:  60%|█████▉    | 2914/4890 [00:09<00:06, 300.28 examples/s]Tokenizing train dataset:  60%|█████▉    | 2910/4890 [00:09<00:06, 294.28 examples/s]Tokenizing train dataset:  61%|██████    | 2978/4890 [00:09<00:06, 303.19 examples/s]Tokenizing train dataset:  60%|██████    | 2947/4890 [00:09<00:06, 305.01 examples/s]Tokenizing train dataset:  60%|██████    | 2942/4890 [00:09<00:06, 297.83 examples/s]Tokenizing train dataset:  62%|██████▏   | 3009/4890 [00:09<00:06, 302.56 examples/s]Tokenizing train dataset:  61%|██████    | 2978/4890 [00:09<00:06, 304.64 examples/s]Tokenizing train dataset:  61%|██████    | 2974/4890 [00:09<00:06, 301.01 examples/s]Tokenizing train dataset:  62%|██████▏   | 3040/4890 [00:09<00:06, 299.42 examples/s]Tokenizing train dataset:  62%|██████▏   | 3010/4890 [00:09<00:06, 303.92 examples/s]Tokenizing train dataset:  61%|██████▏   | 3005/4890 [00:09<00:06, 298.73 examples/s]Tokenizing train dataset:  63%|██████▎   | 3085/4890 [00:10<00:06, 292.90 examples/s]Tokenizing train dataset:  62%|██████▏   | 3054/4890 [00:10<00:06, 296.21 examples/s]Tokenizing train dataset:  62%|██████▏   | 3051/4890 [00:10<00:06, 299.45 examples/s]Tokenizing train dataset:  64%|██████▍   | 3119/4890 [00:10<00:05, 299.95 examples/s]Tokenizing train dataset:  63%|██████▎   | 3085/4890 [00:10<00:06, 294.88 examples/s]Tokenizing train dataset:  63%|██████▎   | 3096/4890 [00:10<00:06, 295.39 examples/s]Tokenizing train dataset:  64%|██████▍   | 3119/4890 [00:10<00:05, 301.75 examples/s]Tokenizing train dataset:  65%|██████▍   | 3160/4890 [00:10<00:06, 287.63 examples/s]Tokenizing train dataset:  64%|██████▍   | 3127/4890 [00:10<00:05, 294.26 examples/s]Tokenizing train dataset:  65%|██████▌   | 3197/4890 [00:10<00:05, 304.01 examples/s]Tokenizing train dataset:  65%|██████▍   | 3160/4890 [00:10<00:05, 288.53 examples/s]Tokenizing train dataset:  65%|██████▍   | 3157/4890 [00:10<00:05, 289.02 examples/s]Tokenizing train dataset:  65%|██████▌   | 3197/4890 [00:10<00:05, 304.91 examples/s]Tokenizing train dataset:  65%|██████▌   | 3192/4890 [00:10<00:05, 303.18 examples/s]Tokenizing train dataset:  66%|██████▋   | 3242/4890 [00:10<00:05, 300.12 examples/s]Tokenizing train dataset:  67%|██████▋   | 3274/4890 [00:10<00:05, 302.35 examples/s]Tokenizing train dataset:  66%|██████▋   | 3242/4890 [00:10<00:05, 301.05 examples/s]Tokenizing train dataset:  66%|██████▌   | 3238/4890 [00:10<00:05, 300.30 examples/s]Tokenizing train dataset:  68%|██████▊   | 3307/4890 [00:10<00:05, 307.45 examples/s]Tokenizing train dataset:  67%|██████▋   | 3274/4890 [00:10<00:05, 302.89 examples/s]Tokenizing train dataset:  67%|██████▋   | 3271/4890 [00:10<00:05, 305.24 examples/s]Tokenizing train dataset:  68%|██████▊   | 3307/4890 [00:10<00:05, 308.08 examples/s]Tokenizing train dataset:  68%|██████▊   | 3302/4890 [00:10<00:05, 304.33 examples/s]Tokenizing train dataset:  69%|██████▊   | 3354/4890 [00:10<00:05, 305.12 examples/s]Tokenizing train dataset:  69%|██████▊   | 3354/4890 [00:11<00:05, 305.66 examples/s]Tokenizing train dataset:  69%|██████▊   | 3350/4890 [00:11<00:05, 306.61 examples/s]Tokenizing train dataset:  69%|██████▉   | 3395/4890 [00:11<00:05, 287.13 examples/s]Tokenizing train dataset:  69%|██████▉   | 3395/4890 [00:11<00:05, 287.25 examples/s]Tokenizing train dataset:  69%|██████▉   | 3389/4890 [00:11<00:05, 286.62 examples/s]Tokenizing train dataset:  70%|███████   | 3437/4890 [00:11<00:05, 278.90 examples/s]Tokenizing train dataset:  70%|██████▉   | 3418/4890 [00:11<00:05, 284.39 examples/s]Tokenizing train dataset:  70%|███████   | 3436/4890 [00:11<00:05, 277.77 examples/s]Tokenizing train dataset:  71%|███████   | 3473/4890 [00:11<00:05, 262.48 examples/s]Tokenizing train dataset:  71%|███████   | 3455/4890 [00:11<00:05, 268.35 examples/s]Tokenizing train dataset:  71%|███████   | 3473/4890 [00:11<00:05, 263.01 examples/s]Tokenizing train dataset:  72%|███████▏  | 3523/4890 [00:11<00:04, 280.44 examples/s]Tokenizing train dataset:  71%|███████▏  | 3489/4890 [00:11<00:05, 251.97 examples/s]Tokenizing train dataset:  73%|███████▎  | 3556/4890 [00:11<00:04, 285.25 examples/s]Tokenizing train dataset:  72%|███████▏  | 3523/4890 [00:11<00:04, 281.11 examples/s]Tokenizing train dataset:  72%|███████▏  | 3530/4890 [00:11<00:04, 287.57 examples/s]Tokenizing train dataset:  73%|███████▎  | 3591/4890 [00:11<00:04, 300.48 examples/s]Tokenizing train dataset:  73%|███████▎  | 3556/4890 [00:11<00:04, 285.74 examples/s]Tokenizing train dataset:  73%|███████▎  | 3562/4890 [00:11<00:04, 288.70 examples/s]Tokenizing train dataset:  73%|███████▎  | 3592/4890 [00:11<00:04, 301.38 examples/s]Tokenizing train dataset:  74%|███████▍  | 3637/4890 [00:11<00:04, 300.21 examples/s]Tokenizing train dataset:  74%|███████▎  | 3597/4890 [00:11<00:04, 301.63 examples/s]Tokenizing train dataset:  75%|███████▌  | 3670/4890 [00:12<00:04, 304.27 examples/s]Tokenizing train dataset:  74%|███████▍  | 3639/4890 [00:12<00:04, 301.45 examples/s]Tokenizing train dataset:  74%|███████▍  | 3628/4890 [00:12<00:04, 302.87 examples/s]Tokenizing train dataset:  75%|███████▌  | 3672/4890 [00:12<00:03, 304.88 examples/s]Tokenizing train dataset:  75%|███████▍  | 3662/4890 [00:12<00:03, 309.33 examples/s]Tokenizing train dataset:  76%|███████▌  | 3716/4890 [00:12<00:03, 300.19 examples/s]Tokenizing train dataset:  77%|███████▋  | 3760/4890 [00:12<00:03, 331.33 examples/s]Tokenizing train dataset:  76%|███████▌  | 3719/4890 [00:12<00:03, 303.99 examples/s]Tokenizing train dataset:  76%|███████▌  | 3707/4890 [00:12<00:03, 300.46 examples/s]Tokenizing train dataset:  78%|███████▊  | 3795/4890 [00:12<00:03, 331.59 examples/s]Tokenizing train dataset:  77%|███████▋  | 3761/4890 [00:12<00:03, 328.72 examples/s]Tokenizing train dataset:  77%|███████▋  | 3750/4890 [00:12<00:03, 332.56 examples/s]Tokenizing train dataset:  78%|███████▊  | 3833/4890 [00:12<00:03, 341.82 examples/s]Tokenizing train dataset:  78%|███████▊  | 3798/4890 [00:12<00:03, 334.78 examples/s]Tokenizing train dataset:  78%|███████▊  | 3800/4890 [00:12<00:03, 328.06 examples/s]Tokenizing train dataset:  78%|███████▊  | 3835/4890 [00:12<00:03, 339.37 examples/s]Tokenizing train dataset:  79%|███████▉  | 3880/4890 [00:12<00:03, 327.08 examples/s]Tokenizing train dataset:  78%|███████▊  | 3838/4890 [00:12<00:03, 338.30 examples/s]Tokenizing train dataset:  79%|███████▉  | 3883/4890 [00:12<00:03, 328.10 examples/s]Tokenizing train dataset:  80%|████████  | 3921/4890 [00:12<00:03, 303.78 examples/s]Tokenizing train dataset:  79%|███████▉  | 3884/4890 [00:12<00:03, 325.26 examples/s]Tokenizing train dataset:  81%|████████  | 3959/4890 [00:12<00:03, 284.90 examples/s]Tokenizing train dataset:  80%|████████  | 3923/4890 [00:13<00:03, 298.63 examples/s]Tokenizing train dataset:  80%|████████  | 3925/4890 [00:12<00:03, 266.02 examples/s]Tokenizing train dataset:  82%|████████▏ | 4000/4890 [00:13<00:02, 312.49 examples/s]Tokenizing train dataset:  81%|████████  | 3956/4890 [00:13<00:03, 271.98 examples/s]Tokenizing train dataset:  81%|████████▏ | 3975/4890 [00:13<00:02, 310.97 examples/s]Tokenizing train dataset:  82%|████████▏ | 4034/4890 [00:13<00:02, 319.01 examples/s]Tokenizing train dataset:  82%|████████▏ | 4000/4890 [00:13<00:02, 306.95 examples/s]Tokenizing train dataset:  82%|████████▏ | 4014/4890 [00:13<00:02, 327.40 examples/s]Tokenizing train dataset:  83%|████████▎ | 4037/4890 [00:13<00:02, 316.98 examples/s]Tokenizing train dataset:  83%|████████▎ | 4080/4890 [00:13<00:02, 309.68 examples/s]Tokenizing train dataset:  83%|████████▎ | 4059/4890 [00:13<00:02, 311.21 examples/s]Tokenizing train dataset:  83%|████████▎ | 4081/4890 [00:13<00:02, 307.21 examples/s]Tokenizing train dataset:  84%|████████▍ | 4125/4890 [00:13<00:02, 303.12 examples/s]Tokenizing train dataset:  84%|████████▎ | 4093/4890 [00:13<00:02, 313.03 examples/s]Tokenizing train dataset:  84%|████████▍ | 4125/4890 [00:13<00:02, 300.90 examples/s]Tokenizing train dataset:  85%|████████▌ | 4170/4890 [00:13<00:02, 295.57 examples/s]Tokenizing train dataset:  85%|████████▍ | 4139/4890 [00:13<00:02, 305.61 examples/s]Tokenizing train dataset:  87%|████████▋ | 4240/4890 [00:13<00:01, 387.03 examples/s]Tokenizing train dataset:  85%|████████▌ | 4170/4890 [00:13<00:02, 293.83 examples/s]Tokenizing train dataset:  86%|████████▌ | 4184/4890 [00:13<00:02, 301.20 examples/s]Tokenizing train dataset:  89%|████████▉ | 4371/4890 [00:13<00:00, 612.32 examples/s]Tokenizing train dataset:  87%|████████▋ | 4241/4890 [00:13<00:01, 387.72 examples/s]Tokenizing train dataset:  88%|████████▊ | 4290/4890 [00:13<00:01, 475.42 examples/s]Tokenizing train dataset:  92%|█████████▏| 4504/4890 [00:13<00:00, 793.61 examples/s]Tokenizing train dataset:  89%|████████▉ | 4373/4890 [00:13<00:00, 614.45 examples/s]Tokenizing train dataset:  90%|█████████ | 4423/4890 [00:14<00:00, 685.38 examples/s]Tokenizing train dataset:  95%|█████████▍| 4633/4890 [00:14<00:00, 925.08 examples/s]Tokenizing train dataset:  92%|█████████▏| 4504/4890 [00:14<00:00, 792.21 examples/s]Tokenizing train dataset:  93%|█████████▎| 4550/4890 [00:14<00:00, 836.84 examples/s]Tokenizing train dataset:  97%|█████████▋| 4762/4890 [00:14<00:00, 1023.88 examples/s]Tokenizing train dataset:  95%|█████████▍| 4633/4890 [00:14<00:00, 923.81 examples/s]Tokenizing train dataset:  96%|█████████▌| 4674/4890 [00:14<00:00, 944.03 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 342.98 examples/s] 
Tokenizing train dataset:  97%|█████████▋| 4761/4890 [00:14<00:00, 1021.01 examples/s]Tokenizing train dataset:  98%|█████████▊| 4804/4890 [00:14<00:00, 1040.32 examples/s]Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 340.14 examples/s] 
Tokenizing train dataset: 100%|██████████| 4890/4890 [00:14<00:00, 339.14 examples/s] 
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  59%|█████▉    | 561/953 [00:00<00:00, 5562.79 examples/s]Extracting prompt in eval dataset:  61%|██████    | 580/953 [00:00<00:00, 5738.14 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5582.21 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5750.37 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5573.65 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5570.94 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  33%|███▎      | 315/953 [00:00<00:00, 3120.54 examples/s]Applying chat template to eval dataset:  33%|███▎      | 311/953 [00:00<00:00, 3072.53 examples/s]Applying chat template to eval dataset:  33%|███▎      | 311/953 [00:00<00:00, 3079.75 examples/s]Applying chat template to eval dataset:  69%|██████▊   | 654/953 [00:00<00:00, 3272.46 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 638/953 [00:00<00:00, 3190.22 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 642/953 [00:00<00:00, 3204.68 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3263.79 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3198.26 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3197.35 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 321.43 examples/s]Tokenizing eval dataset:   3%|▎         | 32/953 [00:00<00:02, 308.93 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 321.52 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 287.88 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 287.35 examples/s]Tokenizing eval dataset:   8%|▊         | 72/953 [00:00<00:03, 272.91 examples/s]Tokenizing eval dataset:  10%|█         | 100/953 [00:00<00:03, 271.12 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 276.98 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 276.57 examples/s]Tokenizing eval dataset:  15%|█▍        | 140/953 [00:00<00:03, 264.13 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.76 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.52 examples/s]Tokenizing eval dataset:  18%|█▊        | 176/953 [00:00<00:03, 251.98 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.53 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.02 examples/s]Tokenizing eval dataset:  22%|██▏       | 214/953 [00:00<00:02, 250.51 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 269.96 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 269.36 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:00<00:01, 371.64 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:00<00:01, 371.11 examples/s]Tokenizing eval dataset:  29%|██▉       | 274/953 [00:00<00:02, 332.34 examples/s]Tokenizing eval dataset:  37%|███▋      | 357/953 [00:01<00:01, 445.18 examples/s]Tokenizing eval dataset:  37%|███▋      | 357/953 [00:01<00:01, 444.56 examples/s]Tokenizing eval dataset:  36%|███▌      | 339/953 [00:01<00:01, 412.84 examples/s]Tokenizing eval dataset:  44%|████▍     | 419/953 [00:01<00:01, 493.04 examples/s]Tokenizing eval dataset:  44%|████▍     | 419/953 [00:01<00:01, 491.99 examples/s]Tokenizing eval dataset:  42%|████▏     | 397/953 [00:01<00:01, 455.76 examples/s]Tokenizing eval dataset:  51%|█████     | 485/953 [00:01<00:00, 536.72 examples/s]Tokenizing eval dataset:  51%|█████     | 484/953 [00:01<00:00, 533.81 examples/s]Tokenizing eval dataset:  49%|████▉     | 467/953 [00:01<00:00, 519.17 examples/s]Tokenizing eval dataset:  57%|█████▋    | 544/953 [00:01<00:00, 547.68 examples/s]Tokenizing eval dataset:  57%|█████▋    | 542/953 [00:01<00:00, 545.68 examples/s]Tokenizing eval dataset:  58%|█████▊    | 549/953 [00:01<00:00, 527.75 examples/s]Tokenizing eval dataset:  64%|██████▎   | 607/953 [00:01<00:00, 568.99 examples/s]Tokenizing eval dataset:  63%|██████▎   | 605/953 [00:01<00:00, 566.44 examples/s]Tokenizing eval dataset:  64%|██████▍   | 608/953 [00:01<00:00, 537.11 examples/s]Tokenizing eval dataset:  72%|███████▏  | 686/953 [00:01<00:00, 551.72 examples/s]Tokenizing eval dataset:  72%|███████▏  | 687/953 [00:01<00:00, 553.69 examples/s]Tokenizing eval dataset:  72%|███████▏  | 689/953 [00:01<00:00, 532.01 examples/s]Tokenizing eval dataset:  79%|███████▉  | 757/953 [00:01<00:00, 521.17 examples/s]Tokenizing eval dataset:  79%|███████▉  | 756/953 [00:01<00:00, 517.00 examples/s]Tokenizing eval dataset:  79%|███████▉  | 755/953 [00:01<00:00, 496.41 examples/s]Tokenizing eval dataset:  87%|████████▋ | 825/953 [00:01<00:00, 435.32 examples/s]Tokenizing eval dataset:  87%|████████▋ | 825/953 [00:01<00:00, 438.40 examples/s]Tokenizing eval dataset:  86%|████████▌ | 820/953 [00:01<00:00, 470.16 examples/s]Tokenizing eval dataset:  93%|█████████▎| 888/953 [00:02<00:00, 432.51 examples/s]Tokenizing eval dataset:  93%|█████████▎| 889/953 [00:02<00:00, 430.28 examples/s]Tokenizing eval dataset:  93%|█████████▎| 883/953 [00:02<00:00, 450.82 examples/s]Tokenizing eval dataset:  98%|█████████▊| 936/953 [00:02<00:00, 437.42 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 420.44 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 429.99 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 419.97 examples/s]
Tokenizing eval dataset:  99%|█████████▉| 948/953 [00:02<00:00, 443.00 examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 416.72 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.426211357116699 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.328996419906616 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3147566318511963 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3308069705963135 seconds
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: vajdadario (slolama) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
wandb: Tracking run with wandb version 0.19.7
wandb: Run data is saved locally in /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/wandb/run-20250610_005455-fsfxy4dv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Curri-0-DPO_r-64_lr-4e-07_e-3_b-0.2
wandb: ⭐️ View project at https://wandb.ai/slolama/GaMS-9B-Translation-DPO
wandb: 🚀 View run at https://wandb.ai/slolama/GaMS-9B-Translation-DPO/runs/fsfxy4dv
  0%|          | 0/918 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  0%|          | 1/918 [00:15<3:50:56, 15.11s/it]  0%|          | 2/918 [00:19<2:11:24,  8.61s/it]  0%|          | 3/918 [00:23<1:38:49,  6.48s/it]  0%|          | 4/918 [00:27<1:25:57,  5.64s/it]  1%|          | 5/918 [00:31<1:16:58,  5.06s/it]  1%|          | 6/918 [00:35<1:10:56,  4.67s/it]  1%|          | 7/918 [00:39<1:08:38,  4.52s/it]  1%|          | 8/918 [00:43<1:05:47,  4.34s/it]  1%|          | 9/918 [00:47<1:04:34,  4.26s/it]  1%|          | 10/918 [00:51<1:03:04,  4.17s/it]                                                  {'loss': 0.7343, 'grad_norm': 1161.476806640625, 'learning_rate': 1.180327868852459e-08, 'rewards/chosen': -0.08358459174633026, 'rewards/rejected': -0.02193298377096653, 'rewards/accuracies': 0.35624998807907104, 'rewards/margins': -0.06154785305261612, 'logps/chosen': -831.2000122070312, 'logps/rejected': -300.39373779296875, 'logits/chosen': -5.237500190734863, 'logits/rejected': -6.78125, 'epoch': 0.03}
  1%|          | 10/918 [00:51<1:03:04,  4.17s/it]  1%|          | 11/918 [00:55<1:02:09,  4.11s/it]  1%|▏         | 12/918 [00:59<1:02:28,  4.14s/it]  1%|▏         | 13/918 [01:03<1:02:30,  4.14s/it]  2%|▏         | 14/918 [01:07<1:01:32,  4.08s/it]  2%|▏         | 15/918 [01:11<1:01:25,  4.08s/it]  2%|▏         | 16/918 [01:15<1:01:03,  4.06s/it]  2%|▏         | 17/918 [01:20<1:01:56,  4.12s/it]  2%|▏         | 18/918 [01:24<1:02:18,  4.15s/it]  2%|▏         | 19/918 [01:28<1:01:18,  4.09s/it]  2%|▏         | 20/918 [01:32<1:01:27,  4.11s/it]                                                  {'loss': 0.7771, 'grad_norm': 64.24266052246094, 'learning_rate': 2.4918032786885246e-08, 'rewards/chosen': 0.01003112830221653, 'rewards/rejected': 0.02596740797162056, 'rewards/accuracies': 0.4000000059604645, 'rewards/margins': -0.01574096642434597, 'logps/chosen': -894.0999755859375, 'logps/rejected': -307.07501220703125, 'logits/chosen': -5.254687309265137, 'logits/rejected': -6.234375, 'epoch': 0.07}
  2%|▏         | 20/918 [01:32<1:01:27,  4.11s/it]  2%|▏         | 21/918 [01:36<1:00:43,  4.06s/it]  2%|▏         | 22/918 [01:40<1:00:06,  4.02s/it]  3%|▎         | 23/918 [01:44<58:31,  3.92s/it]    3%|▎         | 24/918 [01:48<59:26,  3.99s/it]  3%|▎         | 25/918 [01:52<59:17,  3.98s/it]  3%|▎         | 26/918 [01:56<1:00:20,  4.06s/it]  3%|▎         | 27/918 [02:00<59:54,  4.03s/it]    3%|▎         | 28/918 [02:04<59:32,  4.01s/it]  3%|▎         | 29/918 [02:08<59:09,  3.99s/it]  3%|▎         | 30/918 [02:12<58:59,  3.99s/it]                                                {'loss': 0.7749, 'grad_norm': 834.4848022460938, 'learning_rate': 3.80327868852459e-08, 'rewards/chosen': -0.05074615404009819, 'rewards/rejected': 0.042359162122011185, 'rewards/accuracies': 0.42500001192092896, 'rewards/margins': -0.09291992336511612, 'logps/chosen': -769.7999877929688, 'logps/rejected': -282.57501220703125, 'logits/chosen': -5.671875, 'logits/rejected': -6.696875095367432, 'epoch': 0.1}
  3%|▎         | 30/918 [02:12<58:59,  3.99s/it]  3%|▎         | 31/918 [02:16<59:07,  4.00s/it]  3%|▎         | 32/918 [02:20<59:07,  4.00s/it]  4%|▎         | 33/918 [02:24<1:00:04,  4.07s/it]  4%|▎         | 34/918 [02:28<59:24,  4.03s/it]    4%|▍         | 35/918 [02:32<58:59,  4.01s/it]  4%|▍         | 36/918 [02:36<58:36,  3.99s/it]  4%|▍         | 37/918 [02:40<58:23,  3.98s/it]  4%|▍         | 38/918 [02:44<58:15,  3.97s/it]  4%|▍         | 39/918 [02:48<57:17,  3.91s/it]  4%|▍         | 40/918 [02:52<57:24,  3.92s/it]                                                {'loss': 0.6976, 'grad_norm': 417.8838806152344, 'learning_rate': 5.114754098360655e-08, 'rewards/chosen': 0.17631225287914276, 'rewards/rejected': -0.01380004920065403, 'rewards/accuracies': 0.4749999940395355, 'rewards/margins': 0.19000855088233948, 'logps/chosen': -946.7999877929688, 'logps/rejected': -259.3812561035156, 'logits/chosen': -5.310937404632568, 'logits/rejected': -6.715624809265137, 'epoch': 0.13}
  4%|▍         | 40/918 [02:52<57:24,  3.92s/it]  4%|▍         | 41/918 [02:56<58:39,  4.01s/it]  5%|▍         | 42/918 [03:00<58:19,  4.00s/it]  5%|▍         | 43/918 [03:04<58:02,  3.98s/it]  5%|▍         | 44/918 [03:08<57:48,  3.97s/it]  5%|▍         | 45/918 [03:12<58:32,  4.02s/it]  5%|▌         | 46/918 [03:16<59:31,  4.10s/it]  5%|▌         | 47/918 [03:20<58:50,  4.05s/it]  5%|▌         | 48/918 [03:24<58:18,  4.02s/it]  5%|▌         | 49/918 [03:28<58:00,  4.01s/it]  5%|▌         | 50/918 [03:32<56:26,  3.90s/it]                                                {'loss': 0.6963, 'grad_norm': 55.677303314208984, 'learning_rate': 6.426229508196721e-08, 'rewards/chosen': 0.19991454482078552, 'rewards/rejected': 0.01851501502096653, 'rewards/accuracies': 0.40625, 'rewards/margins': 0.18110351264476776, 'logps/chosen': -851.0999755859375, 'logps/rejected': -293.0, 'logits/chosen': -5.215624809265137, 'logits/rejected': -6.540625095367432, 'epoch': 0.16}
  5%|▌         | 50/918 [03:32<56:26,  3.90s/it]  6%|▌         | 51/918 [03:36<56:36,  3.92s/it]  6%|▌         | 52/918 [03:40<56:40,  3.93s/it]  6%|▌         | 53/918 [03:43<56:40,  3.93s/it]  6%|▌         | 54/918 [03:47<56:42,  3.94s/it]  6%|▌         | 55/918 [03:51<56:41,  3.94s/it]  6%|▌         | 56/918 [03:55<56:41,  3.95s/it]  6%|▌         | 57/918 [04:00<57:45,  4.02s/it]  6%|▋         | 58/918 [04:03<57:25,  4.01s/it]  6%|▋         | 59/918 [04:07<57:09,  3.99s/it]  7%|▋         | 60/918 [04:11<56:57,  3.98s/it]                                                {'loss': 0.6836, 'grad_norm': 622.900390625, 'learning_rate': 7.737704918032787e-08, 'rewards/chosen': 0.34638673067092896, 'rewards/rejected': 0.04725799709558487, 'rewards/accuracies': 0.48124998807907104, 'rewards/margins': 0.29912108182907104, 'logps/chosen': -862.7999877929688, 'logps/rejected': -308.6499938964844, 'logits/chosen': -5.396874904632568, 'logits/rejected': -6.556250095367432, 'epoch': 0.2}
  7%|▋         | 60/918 [04:11<56:57,  3.98s/it]  7%|▋         | 61/918 [04:15<56:47,  3.98s/it]  7%|▋         | 62/918 [04:19<56:40,  3.97s/it]  7%|▋         | 63/918 [04:23<56:28,  3.96s/it]  7%|▋         | 64/918 [04:28<57:31,  4.04s/it]  7%|▋         | 65/918 [04:31<57:02,  4.01s/it]  7%|▋         | 66/918 [04:36<57:30,  4.05s/it]  7%|▋         | 67/918 [04:39<56:35,  3.99s/it]  7%|▋         | 68/918 [04:43<56:18,  3.98s/it]  8%|▊         | 69/918 [04:47<56:09,  3.97s/it]  8%|▊         | 70/918 [04:51<55:59,  3.96s/it]                                                {'loss': 0.6419, 'grad_norm': 62.49320602416992, 'learning_rate': 9.049180327868852e-08, 'rewards/chosen': 0.4078125059604645, 'rewards/rejected': 0.03618774563074112, 'rewards/accuracies': 0.5874999761581421, 'rewards/margins': 0.3712921142578125, 'logps/chosen': -570.4000244140625, 'logps/rejected': -161.8874969482422, 'logits/chosen': -5.662499904632568, 'logits/rejected': -7.081250190734863, 'epoch': 0.23}
  8%|▊         | 70/918 [04:51<55:59,  3.96s/it]  8%|▊         | 71/918 [04:55<55:51,  3.96s/it]  8%|▊         | 72/918 [04:59<55:44,  3.95s/it]  8%|▊         | 73/918 [05:03<56:29,  4.01s/it]  8%|▊         | 74/918 [05:07<56:29,  4.02s/it]  8%|▊         | 75/918 [05:11<56:54,  4.05s/it]  8%|▊         | 76/918 [05:16<57:26,  4.09s/it]  8%|▊         | 77/918 [05:20<56:44,  4.05s/it]  8%|▊         | 78/918 [05:24<56:18,  4.02s/it]  9%|▊         | 79/918 [05:28<56:47,  4.06s/it]  9%|▊         | 80/918 [05:32<56:17,  4.03s/it]                                                {'loss': 0.6184, 'grad_norm': 54.36860656738281, 'learning_rate': 1.0360655737704918e-07, 'rewards/chosen': 0.6805053949356079, 'rewards/rejected': 0.13756713271141052, 'rewards/accuracies': 0.606249988079071, 'rewards/margins': 0.5439453125, 'logps/chosen': -732.2000122070312, 'logps/rejected': -208.89999389648438, 'logits/chosen': -5.324999809265137, 'logits/rejected': -6.465624809265137, 'epoch': 0.26}
  9%|▊         | 80/918 [05:32<56:17,  4.03s/it]  9%|▉         | 81/918 [05:35<52:40,  3.78s/it]  9%|▉         | 82/918 [05:39<54:28,  3.91s/it]  9%|▉         | 83/918 [05:43<55:09,  3.96s/it]  9%|▉         | 84/918 [05:47<54:58,  3.95s/it]  9%|▉         | 85/918 [05:51<54:52,  3.95s/it]  9%|▉         | 86/918 [05:55<54:45,  3.95s/it]  9%|▉         | 87/918 [05:59<55:51,  4.03s/it] 10%|▉         | 88/918 [06:03<54:18,  3.93s/it] 10%|▉         | 89/918 [06:07<54:22,  3.93s/it] 10%|▉         | 90/918 [06:11<54:06,  3.92s/it]                                                {'loss': 0.5633, 'grad_norm': 68.52887725830078, 'learning_rate': 1.1672131147540982e-07, 'rewards/chosen': 1.122900366783142, 'rewards/rejected': 0.192840576171875, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.9296875, 'logps/chosen': -937.9000244140625, 'logps/rejected': -323.4125061035156, 'logits/chosen': -5.157812595367432, 'logits/rejected': -6.5, 'epoch': 0.29}
 10%|▉         | 90/918 [06:11<54:06,  3.92s/it] 10%|▉         | 91/918 [06:15<54:44,  3.97s/it] 10%|█         | 92/918 [06:19<54:31,  3.96s/it] 10%|█         | 93/918 [06:23<54:26,  3.96s/it] 10%|█         | 94/918 [06:27<55:07,  4.01s/it] 10%|█         | 95/918 [06:31<54:47,  3.99s/it] 10%|█         | 96/918 [06:35<54:31,  3.98s/it] 11%|█         | 97/918 [06:39<54:20,  3.97s/it] 11%|█         | 98/918 [06:43<53:43,  3.93s/it] 11%|█         | 99/918 [06:46<53:30,  3.92s/it] 11%|█         | 100/918 [06:51<54:37,  4.01s/it]                                                 {'loss': 0.5837, 'grad_norm': 45.2774543762207, 'learning_rate': 1.2983606557377048e-07, 'rewards/chosen': 1.1257812976837158, 'rewards/rejected': 0.2792510986328125, 'rewards/accuracies': 0.675000011920929, 'rewards/margins': 0.8465820550918579, 'logps/chosen': -691.7999877929688, 'logps/rejected': -276.625, 'logits/chosen': -5.684374809265137, 'logits/rejected': -6.653124809265137, 'epoch': 0.33}
 11%|█         | 100/918 [06:51<54:37,  4.01s/it] 11%|█         | 101/918 [06:55<54:22,  3.99s/it] 11%|█         | 102/918 [06:59<54:08,  3.98s/it] 11%|█         | 103/918 [07:03<53:57,  3.97s/it] 11%|█▏        | 104/918 [07:07<55:05,  4.06s/it] 11%|█▏        | 105/918 [07:11<54:35,  4.03s/it] 12%|█▏        | 106/918 [07:15<54:13,  4.01s/it] 12%|█▏        | 107/918 [07:19<55:02,  4.07s/it] 12%|█▏        | 108/918 [07:23<54:29,  4.04s/it] 12%|█▏        | 109/918 [07:27<55:20,  4.10s/it] 12%|█▏        | 110/918 [07:31<54:42,  4.06s/it]                                                 {'loss': 0.5033, 'grad_norm': 71.33475494384766, 'learning_rate': 1.4295081967213115e-07, 'rewards/chosen': 2.126171827316284, 'rewards/rejected': 0.34150391817092896, 'rewards/accuracies': 0.7562500238418579, 'rewards/margins': 1.783203125, 'logps/chosen': -1058.0, 'logps/rejected': -429.20001220703125, 'logits/chosen': -5.098437309265137, 'logits/rejected': -6.606249809265137, 'epoch': 0.36}
 12%|█▏        | 110/918 [07:31<54:42,  4.06s/it] 12%|█▏        | 111/918 [07:35<54:13,  4.03s/it] 12%|█▏        | 112/918 [07:39<53:49,  4.01s/it] 12%|█▏        | 113/918 [07:43<53:27,  3.98s/it] 12%|█▏        | 114/918 [07:47<53:19,  3.98s/it] 13%|█▎        | 115/918 [07:51<53:08,  3.97s/it] 13%|█▎        | 116/918 [07:55<52:57,  3.96s/it] 13%|█▎        | 117/918 [07:59<52:53,  3.96s/it] 13%|█▎        | 118/918 [08:03<52:45,  3.96s/it] 13%|█▎        | 119/918 [08:07<52:38,  3.95s/it] 13%|█▎        | 120/918 [08:11<53:40,  4.04s/it]                                                 {'loss': 0.4682, 'grad_norm': 63.29143524169922, 'learning_rate': 1.560655737704918e-07, 'rewards/chosen': 2.2998046875, 'rewards/rejected': 0.3680068850517273, 'rewards/accuracies': 0.856249988079071, 'rewards/margins': 1.9363281726837158, 'logps/chosen': -892.9000244140625, 'logps/rejected': -263.6000061035156, 'logits/chosen': -5.118750095367432, 'logits/rejected': -6.412499904632568, 'epoch': 0.39}
 13%|█▎        | 120/918 [08:11<53:40,  4.04s/it] 13%|█▎        | 121/918 [08:15<53:16,  4.01s/it] 13%|█▎        | 122/918 [08:19<52:56,  3.99s/it] 13%|█▎        | 123/918 [08:23<52:59,  4.00s/it] 14%|█▎        | 124/918 [08:27<52:44,  3.99s/it] 14%|█▎        | 125/918 [08:31<52:33,  3.98s/it] 14%|█▎        | 126/918 [08:35<53:16,  4.04s/it] 14%|█▍        | 127/918 [08:39<52:48,  4.01s/it] 14%|█▍        | 128/918 [08:43<52:27,  3.98s/it] 14%|█▍        | 129/918 [08:47<52:57,  4.03s/it] 14%|█▍        | 130/918 [08:51<53:18,  4.06s/it]                                                 {'loss': 0.4401, 'grad_norm': 40.56718063354492, 'learning_rate': 1.6918032786885247e-07, 'rewards/chosen': 2.6304688453674316, 'rewards/rejected': 0.4336914122104645, 'rewards/accuracies': 0.887499988079071, 'rewards/margins': 2.196093797683716, 'logps/chosen': -984.4000244140625, 'logps/rejected': -329.0, 'logits/chosen': -5.081250190734863, 'logits/rejected': -6.34375, 'epoch': 0.42}
 14%|█▍        | 130/918 [08:51<53:18,  4.06s/it] 14%|█▍        | 131/918 [08:55<52:47,  4.02s/it] 14%|█▍        | 132/918 [08:59<52:26,  4.00s/it] 14%|█▍        | 133/918 [09:03<52:10,  3.99s/it] 15%|█▍        | 134/918 [09:07<52:53,  4.05s/it] 15%|█▍        | 135/918 [09:11<52:33,  4.03s/it] 15%|█▍        | 136/918 [09:15<53:06,  4.07s/it] 15%|█▍        | 137/918 [09:19<52:29,  4.03s/it] 15%|█▌        | 138/918 [09:23<52:01,  4.00s/it] 15%|█▌        | 139/918 [09:27<51:48,  3.99s/it] 15%|█▌        | 140/918 [09:31<51:41,  3.99s/it]                                                 {'loss': 0.3646, 'grad_norm': 235.70533752441406, 'learning_rate': 1.8229508196721311e-07, 'rewards/chosen': 3.7515625953674316, 'rewards/rejected': 0.860156238079071, 'rewards/accuracies': 0.96875, 'rewards/margins': 2.890625, 'logps/chosen': -1169.5999755859375, 'logps/rejected': -441.20001220703125, 'logits/chosen': -4.974999904632568, 'logits/rejected': -6.243750095367432, 'epoch': 0.46}
 15%|█▌        | 140/918 [09:31<51:41,  3.99s/it] 15%|█▌        | 141/918 [09:35<52:16,  4.04s/it] 15%|█▌        | 142/918 [09:39<51:55,  4.02s/it] 16%|█▌        | 143/918 [09:43<52:02,  4.03s/it] 16%|█▌        | 144/918 [09:47<52:09,  4.04s/it] 16%|█▌        | 145/918 [09:51<51:40,  4.01s/it] 16%|█▌        | 146/918 [09:55<51:21,  3.99s/it] 16%|█▌        | 147/918 [09:59<51:16,  3.99s/it] 16%|█▌        | 148/918 [10:03<51:07,  3.98s/it] 16%|█▌        | 149/918 [10:07<50:59,  3.98s/it] 16%|█▋        | 150/918 [10:11<50:48,  3.97s/it]                                                 {'loss': 0.3425, 'grad_norm': 29.418508529663086, 'learning_rate': 1.9540983606557376e-07, 'rewards/chosen': 3.0999999046325684, 'rewards/rejected': 0.600341796875, 'rewards/accuracies': 0.9437500238418579, 'rewards/margins': 2.5, 'logps/chosen': -893.7999877929688, 'logps/rejected': -346.8999938964844, 'logits/chosen': -5.287499904632568, 'logits/rejected': -6.168749809265137, 'epoch': 0.49}
 16%|█▋        | 150/918 [10:11<50:48,  3.97s/it] 16%|█▋        | 151/918 [10:15<50:45,  3.97s/it] 17%|█▋        | 152/918 [10:19<51:27,  4.03s/it]
  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|▎         | 2/60 [00:01<00:45,  1.27it/s][A
  5%|▌         | 3/60 [00:03<01:03,  1.12s/it][A
  7%|▋         | 4/60 [00:04<01:13,  1.32s/it][A
  8%|▊         | 5/60 [00:06<01:17,  1.41s/it][A
 10%|█         | 6/60 [00:08<01:20,  1.49s/it][A
 12%|█▏        | 7/60 [00:09<01:20,  1.52s/it][A
 13%|█▎        | 8/60 [00:11<01:22,  1.58s/it][A
 15%|█▌        | 9/60 [00:12<01:21,  1.60s/it][A
 17%|█▋        | 10/60 [00:14<01:19,  1.60s/it][A
 18%|█▊        | 11/60 [00:16<01:20,  1.64s/it][A
 20%|██        | 12/60 [00:17<01:17,  1.62s/it][A
 22%|██▏       | 13/60 [00:19<01:15,  1.61s/it][A
 23%|██▎       | 14/60 [00:21<01:13,  1.60s/it][A
 25%|██▌       | 15/60 [00:22<01:07,  1.50s/it][A
 27%|██▋       | 16/60 [00:23<01:00,  1.37s/it][A
 28%|██▊       | 17/60 [00:24<00:54,  1.28s/it][A
 30%|███       | 18/60 [00:25<00:45,  1.08s/it][A
 32%|███▏      | 19/60 [00:26<00:46,  1.14s/it][A
 33%|███▎      | 20/60 [00:26<00:39,  1.01it/s][A
 35%|███▌      | 21/60 [00:27<00:38,  1.01it/s][A
 37%|███▋      | 22/60 [00:29<00:41,  1.09s/it][A
 38%|███▊      | 23/60 [00:30<00:40,  1.11s/it][A
 40%|████      | 24/60 [00:31<00:38,  1.07s/it][A
 42%|████▏     | 25/60 [00:33<00:43,  1.24s/it][A
 43%|████▎     | 26/60 [00:34<00:44,  1.30s/it][A
 45%|████▌     | 27/60 [00:35<00:37,  1.15s/it][A
 47%|████▋     | 28/60 [00:36<00:34,  1.08s/it][A
 48%|████▊     | 29/60 [00:37<00:34,  1.11s/it][A
 50%|█████     | 30/60 [00:39<00:38,  1.28s/it][A
 52%|█████▏    | 31/60 [00:40<00:39,  1.36s/it][A
 53%|█████▎    | 32/60 [00:42<00:38,  1.39s/it][A
 55%|█████▌    | 33/60 [00:43<00:36,  1.37s/it][A
 57%|█████▋    | 34/60 [00:44<00:31,  1.20s/it][A
 58%|█████▊    | 35/60 [00:45<00:31,  1.27s/it][A
 60%|██████    | 36/60 [00:47<00:31,  1.32s/it][A
 62%|██████▏   | 37/60 [00:47<00:25,  1.10s/it][A
 63%|██████▎   | 38/60 [00:49<00:27,  1.24s/it][A
 65%|██████▌   | 39/60 [00:50<00:25,  1.21s/it][A
 67%|██████▋   | 40/60 [00:51<00:21,  1.08s/it][A
 68%|██████▊   | 41/60 [00:52<00:22,  1.19s/it][A
 70%|███████   | 42/60 [00:54<00:23,  1.31s/it][A
 72%|███████▏  | 43/60 [00:55<00:20,  1.21s/it][A
 73%|███████▎  | 44/60 [00:56<00:20,  1.29s/it][A
 75%|███████▌  | 45/60 [00:57<00:17,  1.13s/it][A
 77%|███████▋  | 46/60 [00:58<00:17,  1.27s/it][A
 78%|███████▊  | 47/60 [01:00<00:18,  1.40s/it][A
 80%|████████  | 48/60 [01:01<00:14,  1.22s/it][A
 82%|████████▏ | 49/60 [01:03<00:14,  1.33s/it][A
 83%|████████▎ | 50/60 [01:04<00:14,  1.44s/it][A
 85%|████████▌ | 51/60 [01:06<00:13,  1.47s/it][A
 87%|████████▋ | 52/60 [01:07<00:11,  1.41s/it][A
 88%|████████▊ | 53/60 [01:08<00:09,  1.31s/it][A
 90%|█████████ | 54/60 [01:10<00:08,  1.39s/it][A
 92%|█████████▏| 55/60 [01:10<00:05,  1.20s/it][A
 93%|█████████▎| 56/60 [01:12<00:05,  1.31s/it][A
 95%|█████████▌| 57/60 [01:14<00:04,  1.39s/it][A
 97%|█████████▋| 58/60 [01:15<00:02,  1.37s/it][A
 98%|█████████▊| 59/60 [01:17<00:01,  1.44s/it][A
100%|██████████| 60/60 [01:18<00:00,  1.50s/it][A                                                 
                                               [A{'eval_loss': 0.4188072383403778, 'eval_runtime': 80.3534, 'eval_samples_per_second': 11.86, 'eval_steps_per_second': 0.747, 'eval_rewards/chosen': 1.3723795413970947, 'eval_rewards/rejected': 0.2204686552286148, 'eval_rewards/accuracies': 0.9317129850387573, 'eval_rewards/margins': 1.1514486074447632, 'eval_logps/chosen': -371.07501220703125, 'eval_logps/rejected': -150.92916870117188, 'eval_logits/chosen': -5.889843940734863, 'eval_logits/rejected': -6.763020992279053, 'epoch': 0.5}
 17%|█▋        | 152/918 [11:40<51:27,  4.03s/it]
100%|██████████| 60/60 [01:18<00:00,  1.50s/it][A
                                               [A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 17%|█▋        | 153/918 [11:55<6:40:48, 31.44s/it] 17%|█▋        | 154/918 [11:59<4:55:40, 23.22s/it] 17%|█▋        | 155/918 [12:03<3:42:16, 17.48s/it] 17%|█▋        | 156/918 [12:07<2:50:27, 13.42s/it] 17%|█▋        | 157/918 [12:11<2:14:10, 10.58s/it] 17%|█▋        | 158/918 [12:15<1:48:57,  8.60s/it] 17%|█▋        | 159/918 [12:19<1:31:10,  7.21s/it] 17%|█▋        | 160/918 [12:22<1:18:44,  6.23s/it]                                                   {'loss': 0.3397, 'grad_norm': 24.823307037353516, 'learning_rate': 2.085245901639344e-07, 'rewards/chosen': 3.342968702316284, 'rewards/rejected': 0.56640625, 'rewards/accuracies': 0.9375, 'rewards/margins': 2.7828125953674316, 'logps/chosen': -888.0, 'logps/rejected': -306.1000061035156, 'logits/chosen': -5.203125, 'logits/rejected': -6.434374809265137, 'epoch': 0.52}
 17%|█▋        | 160/918 [12:22<1:18:44,  6.23s/it] 18%|█▊        | 161/918 [12:26<1:10:01,  5.55s/it] 18%|█▊        | 162/918 [12:30<1:04:16,  5.10s/it] 18%|█▊        | 163/918 [12:35<1:00:32,  4.81s/it] 18%|█▊        | 164/918 [12:39<58:17,  4.64s/it]   18%|█▊        | 165/918 [12:43<56:08,  4.47s/it] 18%|█▊        | 166/918 [12:46<51:36,  4.12s/it] 18%|█▊        | 167/918 [12:50<51:42,  4.13s/it] 18%|█▊        | 168/918 [12:54<51:00,  4.08s/it] 18%|█▊        | 169/918 [12:58<50:42,  4.06s/it] 19%|█▊        | 170/918 [13:02<50:41,  4.07s/it]                                                 {'loss': 0.2984, 'grad_norm': 26.12917137145996, 'learning_rate': 2.2163934426229508e-07, 'rewards/chosen': 3.860156297683716, 'rewards/rejected': 0.675537109375, 'rewards/accuracies': 0.9375, 'rewards/margins': 3.188671827316284, 'logps/chosen': -968.1500244140625, 'logps/rejected': -362.4624938964844, 'logits/chosen': -5.143750190734863, 'logits/rejected': -6.40625, 'epoch': 0.56}
 19%|█▊        | 170/918 [13:02<50:41,  4.07s/it] 19%|█▊        | 171/918 [13:06<50:13,  4.03s/it] 19%|█▊        | 172/918 [13:10<49:55,  4.02s/it] 19%|█▉        | 173/918 [13:14<49:39,  4.00s/it] 19%|█▉        | 174/918 [13:19<50:08,  4.04s/it] 19%|█▉        | 175/918 [13:22<49:51,  4.03s/it] 19%|█▉        | 176/918 [13:26<49:31,  4.01s/it] 19%|█▉        | 177/918 [13:30<49:18,  3.99s/it] 19%|█▉        | 178/918 [13:35<50:15,  4.07s/it] 19%|█▉        | 179/918 [13:38<48:08,  3.91s/it] 20%|█▉        | 180/918 [13:42<48:44,  3.96s/it]                                                 {'loss': 0.2538, 'grad_norm': 25.233482360839844, 'learning_rate': 2.3475409836065575e-07, 'rewards/chosen': 3.484375, 'rewards/rejected': 0.5586913824081421, 'rewards/accuracies': 0.9375, 'rewards/margins': 2.9242186546325684, 'logps/chosen': -743.9000244140625, 'logps/rejected': -293.86248779296875, 'logits/chosen': -5.221875190734863, 'logits/rejected': -6.331250190734863, 'epoch': 0.59}
 20%|█▉        | 180/918 [13:42<48:44,  3.96s/it] 20%|█▉        | 181/918 [13:46<48:30,  3.95s/it] 20%|█▉        | 182/918 [13:50<48:29,  3.95s/it] 20%|█▉        | 183/918 [13:54<48:28,  3.96s/it] 20%|██        | 184/918 [13:58<48:24,  3.96s/it] 20%|██        | 185/918 [14:02<48:57,  4.01s/it] 20%|██        | 186/918 [14:06<48:20,  3.96s/it] 20%|██        | 187/918 [14:10<48:13,  3.96s/it] 20%|██        | 188/918 [14:14<48:08,  3.96s/it] 21%|██        | 189/918 [14:18<48:13,  3.97s/it] 21%|██        | 190/918 [14:21<45:34,  3.76s/it]                                                 {'loss': 0.2136, 'grad_norm': 24.899492263793945, 'learning_rate': 2.4786885245901634e-07, 'rewards/chosen': 4.482031345367432, 'rewards/rejected': 0.4629882872104645, 'rewards/accuracies': 0.9624999761581421, 'rewards/margins': 4.021874904632568, 'logps/chosen': -872.5499877929688, 'logps/rejected': -283.11248779296875, 'logits/chosen': -5.0, 'logits/rejected': -6.256249904632568, 'epoch': 0.62}
 21%|██        | 190/918 [14:21<45:34,  3.76s/it] 21%|██        | 191/918 [14:25<47:17,  3.90s/it] 21%|██        | 192/918 [14:30<48:28,  4.01s/it] 21%|██        | 193/918 [14:34<48:11,  3.99s/it] 21%|██        | 194/918 [14:38<48:40,  4.03s/it] 21%|██        | 195/918 [14:42<48:37,  4.04s/it] 21%|██▏       | 196/918 [14:46<49:05,  4.08s/it] 21%|██▏       | 197/918 [14:50<49:38,  4.13s/it] 22%|██▏       | 198/918 [14:54<48:47,  4.07s/it] 22%|██▏       | 199/918 [14:58<48:18,  4.03s/it] 22%|██▏       | 200/918 [15:02<47:59,  4.01s/it]                                                 {'loss': 0.2267, 'grad_norm': 8.368980407714844, 'learning_rate': 2.60983606557377e-07, 'rewards/chosen': 3.649218797683716, 'rewards/rejected': -0.008135986514389515, 'rewards/accuracies': 0.9437500238418579, 'rewards/margins': 3.653125047683716, 'logps/chosen': -617.2000122070312, 'logps/rejected': -162.25, 'logits/chosen': -5.621874809265137, 'logits/rejected': -6.543749809265137, 'epoch': 0.65}
 22%|██▏       | 200/918 [15:02<47:59,  4.01s/it] 22%|██▏       | 201/918 [15:06<48:34,  4.06s/it] 22%|██▏       | 202/918 [15:10<48:09,  4.04s/it] 22%|██▏       | 203/918 [15:14<47:54,  4.02s/it] 22%|██▏       | 204/918 [15:18<47:38,  4.00s/it] 22%|██▏       | 205/918 [15:22<47:26,  3.99s/it] 22%|██▏       | 206/918 [15:26<47:11,  3.98s/it] 23%|██▎       | 207/918 [15:30<47:57,  4.05s/it] 23%|██▎       | 208/918 [15:34<47:35,  4.02s/it] 23%|██▎       | 209/918 [15:39<48:27,  4.10s/it] 23%|██▎       | 210/918 [15:43<48:33,  4.12s/it]                                                 {'loss': 0.2347, 'grad_norm': 12.567853927612305, 'learning_rate': 2.740983606557377e-07, 'rewards/chosen': 5.409375190734863, 'rewards/rejected': 0.863037109375, 'rewards/accuracies': 0.956250011920929, 'rewards/margins': 4.54296875, 'logps/chosen': -930.0, 'logps/rejected': -382.79998779296875, 'logits/chosen': -5.0859375, 'logits/rejected': -5.715624809265137, 'epoch': 0.69}
 23%|██▎       | 210/918 [15:43<48:33,  4.12s/it] 23%|██▎       | 211/918 [15:47<48:00,  4.07s/it] 23%|██▎       | 212/918 [15:51<47:34,  4.04s/it] 23%|██▎       | 213/918 [15:55<47:06,  4.01s/it] 23%|██▎       | 214/918 [15:59<46:50,  3.99s/it] 23%|██▎       | 215/918 [16:03<46:41,  3.98s/it] 24%|██▎       | 216/918 [16:07<47:23,  4.05s/it] 24%|██▎       | 217/918 [16:11<47:00,  4.02s/it] 24%|██▎       | 218/918 [16:15<46:44,  4.01s/it] 24%|██▍       | 219/918 [16:19<47:35,  4.09s/it] 24%|██▍       | 220/918 [16:23<48:10,  4.14s/it]                                                 {'loss': 0.2135, 'grad_norm': 30.098737716674805, 'learning_rate': 2.8721311475409835e-07, 'rewards/chosen': 5.465624809265137, 'rewards/rejected': 0.32011717557907104, 'rewards/accuracies': 0.9375, 'rewards/margins': 5.150000095367432, 'logps/chosen': -888.7999877929688, 'logps/rejected': -286.0, 'logits/chosen': -5.040625095367432, 'logits/rejected': -5.868750095367432, 'epoch': 0.72}
 24%|██▍       | 220/918 [16:23<48:10,  4.14s/it] 24%|██▍       | 221/918 [16:27<47:41,  4.11s/it] 24%|██▍       | 222/918 [16:31<47:03,  4.06s/it] 24%|██▍       | 223/918 [16:35<46:51,  4.05s/it] 24%|██▍       | 224/918 [16:39<46:28,  4.02s/it] 25%|██▍       | 225/918 [16:43<46:44,  4.05s/it] 25%|██▍       | 226/918 [16:47<46:23,  4.02s/it] 25%|██▍       | 227/918 [16:51<46:05,  4.00s/it] 25%|██▍       | 228/918 [16:55<46:06,  4.01s/it] 25%|██▍       | 229/918 [16:59<45:50,  3.99s/it] 25%|██▌       | 230/918 [17:03<46:14,  4.03s/it]                                                 {'loss': 0.1618, 'grad_norm': 15.595389366149902, 'learning_rate': 3.0032786885245897e-07, 'rewards/chosen': 4.793749809265137, 'rewards/rejected': -0.30126953125, 'rewards/accuracies': 0.96875, 'rewards/margins': 5.099999904632568, 'logps/chosen': -711.2000122070312, 'logps/rejected': -230.85000610351562, 'logits/chosen': -5.284375190734863, 'logits/rejected': -6.365624904632568, 'epoch': 0.75}
 25%|██▌       | 230/918 [17:03<46:14,  4.03s/it] 25%|██▌       | 231/918 [17:07<46:25,  4.05s/it] 25%|██▌       | 232/918 [17:11<46:03,  4.03s/it] 25%|██▌       | 233/918 [17:16<46:41,  4.09s/it] 25%|██▌       | 234/918 [17:20<46:50,  4.11s/it] 26%|██▌       | 235/918 [17:24<46:14,  4.06s/it] 26%|██▌       | 236/918 [17:28<45:58,  4.04s/it] 26%|██▌       | 237/918 [17:32<45:36,  4.02s/it] 26%|██▌       | 238/918 [17:36<46:19,  4.09s/it] 26%|██▌       | 239/918 [17:40<45:46,  4.05s/it] 26%|██▌       | 240/918 [17:44<45:24,  4.02s/it]                                                 {'loss': 0.1988, 'grad_norm': 8.199344635009766, 'learning_rate': 3.1344262295081964e-07, 'rewards/chosen': 6.557812690734863, 'rewards/rejected': 0.692675769329071, 'rewards/accuracies': 0.949999988079071, 'rewards/margins': 5.8671875, 'logps/chosen': -1018.7999877929688, 'logps/rejected': -377.3999938964844, 'logits/chosen': -5.021874904632568, 'logits/rejected': -5.790625095367432, 'epoch': 0.78}
 26%|██▌       | 240/918 [17:44<45:24,  4.02s/it] 26%|██▋       | 241/918 [17:48<45:43,  4.05s/it] 26%|██▋       | 242/918 [17:52<46:13,  4.10s/it] 26%|██▋       | 243/918 [17:56<45:41,  4.06s/it] 27%|██▋       | 244/918 [18:00<45:46,  4.07s/it] 27%|██▋       | 245/918 [18:04<45:21,  4.04s/it] 27%|██▋       | 246/918 [18:08<43:35,  3.89s/it] 27%|██▋       | 247/918 [18:12<43:41,  3.91s/it] 27%|██▋       | 248/918 [18:16<44:49,  4.01s/it] 27%|██▋       | 249/918 [18:20<45:26,  4.07s/it] 27%|██▋       | 250/918 [18:24<44:55,  4.03s/it]                                                 {'loss': 0.1233, 'grad_norm': 8.007256507873535, 'learning_rate': 3.265573770491803e-07, 'rewards/chosen': 5.137499809265137, 'rewards/rejected': -0.3739257752895355, 'rewards/accuracies': 0.981249988079071, 'rewards/margins': 5.510937690734863, 'logps/chosen': -697.7000122070312, 'logps/rejected': -241.75, 'logits/chosen': -5.296875, 'logits/rejected': -5.884375095367432, 'epoch': 0.82}
 27%|██▋       | 250/918 [18:24<44:55,  4.03s/it] 27%|██▋       | 251/918 [18:28<44:36,  4.01s/it] 27%|██▋       | 252/918 [18:32<44:56,  4.05s/it] 28%|██▊       | 253/918 [18:36<44:34,  4.02s/it] 28%|██▊       | 254/918 [18:40<44:15,  4.00s/it] 28%|██▊       | 255/918 [18:44<44:00,  3.98s/it] 28%|██▊       | 256/918 [18:48<43:44,  3.96s/it] 28%|██▊       | 257/918 [18:52<44:09,  4.01s/it] 28%|██▊       | 258/918 [18:56<43:53,  3.99s/it] 28%|██▊       | 259/918 [19:00<44:13,  4.03s/it] 28%|██▊       | 260/918 [19:04<43:56,  4.01s/it]                                                 {'loss': 0.1428, 'grad_norm': 3.9198532104492188, 'learning_rate': 3.39672131147541e-07, 'rewards/chosen': 6.4375, 'rewards/rejected': 0.24500732123851776, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 6.190625190734863, 'logps/chosen': -875.7000122070312, 'logps/rejected': -328.70001220703125, 'logits/chosen': -5.067187309265137, 'logits/rejected': -5.753125190734863, 'epoch': 0.85}
 28%|██▊       | 260/918 [19:04<43:56,  4.01s/it] 28%|██▊       | 261/918 [19:08<43:44,  3.99s/it] 29%|██▊       | 262/918 [19:12<43:32,  3.98s/it] 29%|██▊       | 263/918 [19:16<43:27,  3.98s/it] 29%|██▉       | 264/918 [19:20<43:51,  4.02s/it] 29%|██▉       | 265/918 [19:24<44:00,  4.04s/it] 29%|██▉       | 266/918 [19:28<44:19,  4.08s/it] 29%|██▉       | 267/918 [19:32<43:53,  4.05s/it] 29%|██▉       | 268/918 [19:36<43:33,  4.02s/it] 29%|██▉       | 269/918 [19:40<42:32,  3.93s/it] 29%|██▉       | 270/918 [19:44<42:32,  3.94s/it]                                                 {'loss': 0.1822, 'grad_norm': 21.652692794799805, 'learning_rate': 3.5278688524590166e-07, 'rewards/chosen': 5.8828125, 'rewards/rejected': -0.42802733182907104, 'rewards/accuracies': 0.9375, 'rewards/margins': 6.309374809265137, 'logps/chosen': -786.0999755859375, 'logps/rejected': -265.45001220703125, 'logits/chosen': -5.162499904632568, 'logits/rejected': -5.771874904632568, 'epoch': 0.88}
 29%|██▉       | 270/918 [19:44<42:32,  3.94s/it] 30%|██▉       | 271/918 [19:48<42:34,  3.95s/it] 30%|██▉       | 272/918 [19:52<42:37,  3.96s/it] 30%|██▉       | 273/918 [19:56<42:29,  3.95s/it] 30%|██▉       | 274/918 [20:00<42:27,  3.96s/it] 30%|██▉       | 275/918 [20:04<42:21,  3.95s/it] 30%|███       | 276/918 [20:08<42:18,  3.95s/it] 30%|███       | 277/918 [20:12<41:47,  3.91s/it] 30%|███       | 278/918 [20:16<42:16,  3.96s/it] 30%|███       | 279/918 [20:20<42:22,  3.98s/it] 31%|███       | 280/918 [20:24<42:21,  3.98s/it]                                                 {'loss': 0.1069, 'grad_norm': 4.124415874481201, 'learning_rate': 3.659016393442623e-07, 'rewards/chosen': 5.92578125, 'rewards/rejected': -0.5845702886581421, 'rewards/accuracies': 0.96875, 'rewards/margins': 6.5078125, 'logps/chosen': -743.0999755859375, 'logps/rejected': -281.25, 'logits/chosen': -5.068749904632568, 'logits/rejected': -5.871874809265137, 'epoch': 0.92}
 31%|███       | 280/918 [20:24<42:21,  3.98s/it] 31%|███       | 281/918 [20:28<42:55,  4.04s/it] 31%|███       | 282/918 [20:32<42:33,  4.02s/it] 31%|███       | 283/918 [20:36<43:11,  4.08s/it] 31%|███       | 284/918 [20:40<43:24,  4.11s/it] 31%|███       | 285/918 [20:44<42:50,  4.06s/it] 31%|███       | 286/918 [20:48<42:27,  4.03s/it] 31%|███▏      | 287/918 [20:52<42:42,  4.06s/it] 31%|███▏      | 288/918 [20:56<42:13,  4.02s/it] 31%|███▏      | 289/918 [21:00<42:50,  4.09s/it] 32%|███▏      | 290/918 [21:05<43:01,  4.11s/it]                                                 {'loss': 0.1524, 'grad_norm': 10.228019714355469, 'learning_rate': 3.7901639344262295e-07, 'rewards/chosen': 6.459374904632568, 'rewards/rejected': -0.7417968511581421, 'rewards/accuracies': 0.949999988079071, 'rewards/margins': 7.203125, 'logps/chosen': -809.2000122070312, 'logps/rejected': -308.45001220703125, 'logits/chosen': -5.290625095367432, 'logits/rejected': -5.856249809265137, 'epoch': 0.95}
 32%|███▏      | 290/918 [21:05<43:01,  4.11s/it] 32%|███▏      | 291/918 [21:09<42:30,  4.07s/it] 32%|███▏      | 292/918 [21:13<42:57,  4.12s/it] 32%|███▏      | 293/918 [21:17<43:23,  4.17s/it] 32%|███▏      | 294/918 [21:21<42:39,  4.10s/it] 32%|███▏      | 295/918 [21:25<42:08,  4.06s/it] 32%|███▏      | 296/918 [21:29<41:45,  4.03s/it] 32%|███▏      | 297/918 [21:33<41:27,  4.01s/it] 32%|███▏      | 298/918 [21:37<41:15,  3.99s/it] 33%|███▎      | 299/918 [21:40<39:02,  3.79s/it] 33%|███▎      | 300/918 [21:44<39:28,  3.83s/it]                                                 {'loss': 0.0871, 'grad_norm': 23.254915237426758, 'learning_rate': 3.921311475409836e-07, 'rewards/chosen': 7.295312404632568, 'rewards/rejected': -0.516796886920929, 'rewards/accuracies': 0.956250011920929, 'rewards/margins': 7.8125, 'logps/chosen': -887.5, 'logps/rejected': -332.1000061035156, 'logits/chosen': -5.084374904632568, 'logits/rejected': -5.943749904632568, 'epoch': 0.98}
 33%|███▎      | 300/918 [21:44<39:28,  3.83s/it] 33%|███▎      | 301/918 [21:48<39:47,  3.87s/it] 33%|███▎      | 302/918 [21:52<39:59,  3.90s/it] 33%|███▎      | 303/918 [21:56<40:09,  3.92s/it] 33%|███▎      | 304/918 [22:00<42:01,  4.11s/it]
  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|▎         | 2/60 [00:01<00:45,  1.27it/s][A
  5%|▌         | 3/60 [00:03<01:03,  1.11s/it][A
  7%|▋         | 4/60 [00:04<01:13,  1.32s/it][A
  8%|▊         | 5/60 [00:06<01:17,  1.41s/it][A
 10%|█         | 6/60 [00:08<01:20,  1.49s/it][A
 12%|█▏        | 7/60 [00:09<01:20,  1.52s/it][A
 13%|█▎        | 8/60 [00:11<01:22,  1.58s/it][A
 15%|█▌        | 9/60 [00:12<01:21,  1.60s/it][A
 17%|█▋        | 10/60 [00:14<01:19,  1.59s/it][A
 18%|█▊        | 11/60 [00:16<01:20,  1.63s/it][A
 20%|██        | 12/60 [00:17<01:17,  1.62s/it][A
 22%|██▏       | 13/60 [00:19<01:15,  1.61s/it][A
 23%|██▎       | 14/60 [00:21<01:13,  1.60s/it][A
 25%|██▌       | 15/60 [00:22<01:07,  1.50s/it][A
 27%|██▋       | 16/60 [00:23<01:00,  1.37s/it][A
 28%|██▊       | 17/60 [00:24<00:54,  1.28s/it][A
 30%|███       | 18/60 [00:25<00:45,  1.08s/it][A
 32%|███▏      | 19/60 [00:26<00:46,  1.14s/it][A
 33%|███▎      | 20/60 [00:26<00:39,  1.01it/s][A
 35%|███▌      | 21/60 [00:27<00:38,  1.01it/s][A
 37%|███▋      | 22/60 [00:29<00:41,  1.09s/it][A
 38%|███▊      | 23/60 [00:30<00:40,  1.11s/it][A
 40%|████      | 24/60 [00:31<00:38,  1.07s/it][A
 42%|████▏     | 25/60 [00:33<00:43,  1.24s/it][A
 43%|████▎     | 26/60 [00:34<00:44,  1.30s/it][A
 45%|████▌     | 27/60 [00:35<00:37,  1.15s/it][A
 47%|████▋     | 28/60 [00:36<00:34,  1.08s/it][A
 48%|████▊     | 29/60 [00:37<00:34,  1.11s/it][A
 50%|█████     | 30/60 [00:39<00:38,  1.28s/it][A
 52%|█████▏    | 31/60 [00:40<00:39,  1.36s/it][A
 53%|█████▎    | 32/60 [00:42<00:38,  1.38s/it][A
 55%|█████▌    | 33/60 [00:43<00:36,  1.37s/it][A
 57%|█████▋    | 34/60 [00:44<00:31,  1.20s/it][A
 58%|█████▊    | 35/60 [00:45<00:31,  1.27s/it][A
 60%|██████    | 36/60 [00:47<00:31,  1.32s/it][A
 62%|██████▏   | 37/60 [00:47<00:25,  1.10s/it][A
 63%|██████▎   | 38/60 [00:49<00:27,  1.24s/it][A
 65%|██████▌   | 39/60 [00:50<00:25,  1.21s/it][A
 67%|██████▋   | 40/60 [00:51<00:21,  1.08s/it][A
 68%|██████▊   | 41/60 [00:52<00:22,  1.19s/it][A
 70%|███████   | 42/60 [00:54<00:23,  1.31s/it][A
 72%|███████▏  | 43/60 [00:55<00:20,  1.21s/it][A
 73%|███████▎  | 44/60 [00:56<00:20,  1.29s/it][A
 75%|███████▌  | 45/60 [00:57<00:17,  1.14s/it][A
 77%|███████▋  | 46/60 [00:58<00:17,  1.27s/it][A
 78%|███████▊  | 47/60 [01:00<00:18,  1.40s/it][A
 80%|████████  | 48/60 [01:01<00:14,  1.22s/it][A
 82%|████████▏ | 49/60 [01:03<00:14,  1.33s/it][A
 83%|████████▎ | 50/60 [01:04<00:14,  1.43s/it][A
 85%|████████▌ | 51/60 [01:06<00:13,  1.47s/it][A
 87%|████████▋ | 52/60 [01:07<00:11,  1.41s/it][A
 88%|████████▊ | 53/60 [01:08<00:09,  1.31s/it][A
 90%|█████████ | 54/60 [01:10<00:08,  1.39s/it][A
 92%|█████████▏| 55/60 [01:10<00:05,  1.20s/it][A
 93%|█████████▎| 56/60 [01:12<00:05,  1.31s/it][A
 95%|█████████▌| 57/60 [01:14<00:04,  1.39s/it][A
 97%|█████████▋| 58/60 [01:15<00:02,  1.37s/it][A
 98%|█████████▊| 59/60 [01:17<00:01,  1.44s/it][A
100%|██████████| 60/60 [01:18<00:00,  1.50s/it][A                                                 
                                               [A{'eval_loss': 0.15571092069149017, 'eval_runtime': 80.3237, 'eval_samples_per_second': 11.864, 'eval_steps_per_second': 0.747, 'eval_rewards/chosen': 3.4166667461395264, 'eval_rewards/rejected': -0.5582031011581421, 'eval_rewards/accuracies': 0.9814814925193787, 'eval_rewards/margins': 3.9742839336395264, 'eval_logps/chosen': -360.5833435058594, 'eval_logps/rejected': -154.75833129882812, 'eval_logits/chosen': -5.793489456176758, 'eval_logits/rejected': -6.3848958015441895, 'epoch': 0.99}
 33%|███▎      | 304/918 [23:21<42:01,  4.11s/it]
100%|██████████| 60/60 [01:18<00:00,  1.50s/it][A
                                               [A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 33%|███▎      | 305/918 [23:36<5:21:59, 31.52s/it] 33%|███▎      | 306/918 [23:40<3:57:52, 23.32s/it] 33%|███▎      | 307/918 [23:44<2:58:19, 17.51s/it] 34%|███▎      | 308/918 [23:48<2:16:43, 13.45s/it] 34%|███▎      | 309/918 [23:52<1:48:28, 10.69s/it] 34%|███▍      | 310/918 [23:56<1:27:51,  8.67s/it]                                                   {'loss': 0.1598, 'grad_norm': 20.55390739440918, 'learning_rate': 3.99962179603345e-07, 'rewards/chosen': 7.890625, 'rewards/rejected': -0.2884765565395355, 'rewards/accuracies': 0.9312499761581421, 'rewards/margins': 8.165624618530273, 'logps/chosen': -893.2000122070312, 'logps/rejected': -378.625, 'logits/chosen': -5.162499904632568, 'logits/rejected': -5.721875190734863, 'epoch': 1.01}
 34%|███▍      | 310/918 [23:56<1:27:51,  8.67s/it] 34%|███▍      | 311/918 [24:00<1:13:24,  7.26s/it] 34%|███▍      | 312/918 [24:04<1:03:47,  6.32s/it] 34%|███▍      | 313/918 [24:08<56:32,  5.61s/it]   34%|███▍      | 314/918 [24:13<52:09,  5.18s/it] 34%|███▍      | 315/918 [24:17<49:13,  4.90s/it] 34%|███▍      | 316/918 [24:21<46:17,  4.61s/it] 35%|███▍      | 317/918 [24:25<45:10,  4.51s/it] 35%|███▍      | 318/918 [24:29<43:26,  4.34s/it] 35%|███▍      | 319/918 [24:33<43:01,  4.31s/it] 35%|███▍      | 320/918 [24:37<42:41,  4.28s/it]                                                 {'loss': 0.1034, 'grad_norm': 1.8288551568984985, 'learning_rate': 3.9953688264263893e-07, 'rewards/chosen': 7.484375, 'rewards/rejected': -1.141210913658142, 'rewards/accuracies': 0.956250011920929, 'rewards/margins': 8.628125190734863, 'logps/chosen': -899.7999877929688, 'logps/rejected': -296.82501220703125, 'logits/chosen': -4.974999904632568, 'logits/rejected': -5.59375, 'epoch': 1.05}
 35%|███▍      | 320/918 [24:37<42:41,  4.28s/it] 35%|███▍      | 321/918 [24:42<42:12,  4.24s/it] 35%|███▌      | 322/918 [24:46<41:48,  4.21s/it] 35%|███▌      | 323/918 [24:50<40:59,  4.13s/it] 35%|███▌      | 324/918 [24:54<40:21,  4.08s/it] 35%|███▌      | 325/918 [24:58<40:26,  4.09s/it] 36%|███▌      | 326/918 [25:02<40:00,  4.05s/it] 36%|███▌      | 327/918 [25:06<39:59,  4.06s/it] 36%|███▌      | 328/918 [25:10<39:39,  4.03s/it] 36%|███▌      | 329/918 [25:14<39:23,  4.01s/it] 36%|███▌      | 330/918 [25:18<39:37,  4.04s/it]                                                 {'loss': 0.0647, 'grad_norm': 2.1234118938446045, 'learning_rate': 3.986401337520265e-07, 'rewards/chosen': 7.603125095367432, 'rewards/rejected': -1.630859375, 'rewards/accuracies': 0.981249988079071, 'rewards/margins': 9.228124618530273, 'logps/chosen': -819.5999755859375, 'logps/rejected': -289.1000061035156, 'logits/chosen': -5.284375190734863, 'logits/rejected': -5.765625, 'epoch': 1.08}
 36%|███▌      | 330/918 [25:18<39:37,  4.04s/it] 36%|███▌      | 331/918 [25:22<39:52,  4.08s/it] 36%|███▌      | 332/918 [25:26<39:29,  4.04s/it] 36%|███▋      | 333/918 [25:30<39:14,  4.02s/it] 36%|███▋      | 334/918 [25:34<38:58,  4.00s/it] 36%|███▋      | 335/918 [25:38<38:46,  3.99s/it] 37%|███▋      | 336/918 [25:42<38:37,  3.98s/it] 37%|███▋      | 337/918 [25:46<38:29,  3.97s/it] 37%|███▋      | 338/918 [25:50<38:22,  3.97s/it] 37%|███▋      | 339/918 [25:54<38:18,  3.97s/it] 37%|███▋      | 340/918 [25:58<38:55,  4.04s/it]                                                 {'loss': 0.0604, 'grad_norm': 7.126620292663574, 'learning_rate': 3.972742877345368e-07, 'rewards/chosen': 8.371874809265137, 'rewards/rejected': -1.475488305091858, 'rewards/accuracies': 0.981249988079071, 'rewards/margins': 9.84375, 'logps/chosen': -949.0, 'logps/rejected': -370.20001220703125, 'logits/chosen': -5.109375, 'logits/rejected': -5.787499904632568, 'epoch': 1.11}
 37%|███▋      | 340/918 [25:58<38:55,  4.04s/it] 37%|███▋      | 341/918 [26:02<38:38,  4.02s/it] 37%|███▋      | 342/918 [26:06<38:20,  3.99s/it] 37%|███▋      | 343/918 [26:09<37:27,  3.91s/it] 37%|███▋      | 344/918 [26:13<37:35,  3.93s/it] 38%|███▊      | 345/918 [26:17<37:38,  3.94s/it] 38%|███▊      | 346/918 [26:21<37:38,  3.95s/it] 38%|███▊      | 347/918 [26:25<37:37,  3.95s/it] 38%|███▊      | 348/918 [26:30<38:08,  4.02s/it] 38%|███▊      | 349/918 [26:33<37:50,  3.99s/it] 38%|███▊      | 350/918 [26:37<37:43,  3.98s/it]                                                 {'loss': 0.1049, 'grad_norm': 20.348459243774414, 'learning_rate': 3.954429312110995e-07, 'rewards/chosen': 6.167187690734863, 'rewards/rejected': -2.177734375, 'rewards/accuracies': 0.956250011920929, 'rewards/margins': 8.353124618530273, 'logps/chosen': -695.5, 'logps/rejected': -320.32501220703125, 'logits/chosen': -5.559374809265137, 'logits/rejected': -5.849999904632568, 'epoch': 1.14}
 38%|███▊      | 350/918 [26:37<37:43,  3.98s/it] 38%|███▊      | 351/918 [26:41<37:37,  3.98s/it] 38%|███▊      | 352/918 [26:46<38:06,  4.04s/it] 38%|███▊      | 353/918 [26:50<37:49,  4.02s/it] 39%|███▊      | 354/918 [26:53<36:42,  3.91s/it] 39%|███▊      | 355/918 [26:57<36:50,  3.93s/it] 39%|███▉      | 356/918 [27:01<37:13,  3.97s/it] 39%|███▉      | 357/918 [27:05<37:06,  3.97s/it] 39%|███▉      | 358/918 [27:09<37:02,  3.97s/it] 39%|███▉      | 359/918 [27:13<37:35,  4.03s/it] 39%|███▉      | 360/918 [27:17<37:21,  4.02s/it]                                                 {'loss': 0.0591, 'grad_norm': 2.452439069747925, 'learning_rate': 3.9315087320231577e-07, 'rewards/chosen': 7.6171875, 'rewards/rejected': -2.400927782058716, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 10.015625, 'logps/chosen': -768.5999755859375, 'logps/rejected': -283.82501220703125, 'logits/chosen': -5.409375190734863, 'logits/rejected': -5.868750095367432, 'epoch': 1.18}
 39%|███▉      | 360/918 [27:17<37:21,  4.02s/it] 39%|███▉      | 361/918 [27:21<37:17,  4.02s/it] 39%|███▉      | 362/918 [27:26<37:59,  4.10s/it] 40%|███▉      | 363/918 [27:30<37:33,  4.06s/it] 40%|███▉      | 364/918 [27:34<37:15,  4.04s/it] 40%|███▉      | 365/918 [27:38<37:01,  4.02s/it] 40%|███▉      | 366/918 [27:42<36:49,  4.00s/it] 40%|███▉      | 367/918 [27:45<36:39,  3.99s/it] 40%|████      | 368/918 [27:49<36:28,  3.98s/it] 40%|████      | 369/918 [27:54<37:07,  4.06s/it] 40%|████      | 370/918 [27:58<36:51,  4.04s/it]                                                 {'loss': 0.043, 'grad_norm': 0.581983208656311, 'learning_rate': 3.904041325002901e-07, 'rewards/chosen': 6.7421875, 'rewards/rejected': -3.3375000953674316, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 10.074999809265137, 'logps/chosen': -712.4000244140625, 'logps/rejected': -249.75, 'logits/chosen': -5.300000190734863, 'logits/rejected': -6.0, 'epoch': 1.21}
 40%|████      | 370/918 [27:58<36:51,  4.04s/it] 40%|████      | 371/918 [28:02<36:40,  4.02s/it] 41%|████      | 372/918 [28:06<36:29,  4.01s/it] 41%|████      | 373/918 [28:10<36:59,  4.07s/it] 41%|████      | 374/918 [28:14<36:39,  4.04s/it] 41%|████      | 375/918 [28:18<36:53,  4.08s/it] 41%|████      | 376/918 [28:22<36:33,  4.05s/it] 41%|████      | 377/918 [28:26<36:13,  4.02s/it] 41%|████      | 378/918 [28:28<32:19,  3.59s/it] 41%|████▏     | 379/918 [28:32<32:57,  3.67s/it] 41%|████▏     | 380/918 [28:37<34:23,  3.84s/it]                                                 {'loss': 0.0621, 'grad_norm': 0.6433243155479431, 'learning_rate': 3.872099218636817e-07, 'rewards/chosen': 7.434374809265137, 'rewards/rejected': -2.891406297683716, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 10.318750381469727, 'logps/chosen': -769.5999755859375, 'logps/rejected': -300.8999938964844, 'logits/chosen': -5.3359375, 'logits/rejected': -5.846875190734863, 'epoch': 1.24}
 41%|████▏     | 380/918 [28:37<34:23,  3.84s/it] 42%|████▏     | 381/918 [28:41<34:42,  3.88s/it] 42%|████▏     | 382/918 [28:45<34:50,  3.90s/it] 42%|████▏     | 383/918 [28:48<34:58,  3.92s/it] 42%|████▏     | 384/918 [28:52<35:02,  3.94s/it] 42%|████▏     | 385/918 [28:57<35:27,  3.99s/it] 42%|████▏     | 386/918 [29:01<35:18,  3.98s/it] 42%|████▏     | 387/918 [29:05<35:52,  4.05s/it] 42%|████▏     | 388/918 [29:09<36:08,  4.09s/it] 42%|████▏     | 389/918 [29:13<35:48,  4.06s/it] 42%|████▏     | 390/918 [29:17<35:27,  4.03s/it]                                                 {'loss': 0.0235, 'grad_norm': 1.899381160736084, 'learning_rate': 3.8357662907747916e-07, 'rewards/chosen': 9.540624618530273, 'rewards/rejected': -3.3111329078674316, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 12.862500190734863, 'logps/chosen': -975.7999877929688, 'logps/rejected': -275.95001220703125, 'logits/chosen': -5.212500095367432, 'logits/rejected': -5.65625, 'epoch': 1.27}
 42%|████▏     | 390/918 [29:17<35:27,  4.03s/it] 43%|████▎     | 391/918 [29:21<35:39,  4.06s/it] 43%|████▎     | 392/918 [29:25<35:45,  4.08s/it] 43%|████▎     | 393/918 [29:29<35:24,  4.05s/it] 43%|████▎     | 394/918 [29:33<33:56,  3.89s/it] 43%|████▎     | 395/918 [29:37<34:05,  3.91s/it] 43%|████▎     | 396/918 [29:41<34:24,  3.96s/it] 43%|████▎     | 397/918 [29:45<34:21,  3.96s/it] 43%|████▎     | 398/918 [29:49<34:20,  3.96s/it] 43%|████▎     | 399/918 [29:53<34:46,  4.02s/it] 44%|████▎     | 400/918 [29:57<35:14,  4.08s/it]                                                 {'loss': 0.0617, 'grad_norm': 1.7149622440338135, 'learning_rate': 3.7951379492723467e-07, 'rewards/chosen': 5.694531440734863, 'rewards/rejected': -4.869531154632568, 'rewards/accuracies': 0.96875, 'rewards/margins': 10.578125, 'logps/chosen': -524.0, 'logps/rejected': -152.9499969482422, 'logits/chosen': -5.637499809265137, 'logits/rejected': -5.856249809265137, 'epoch': 1.31}
 44%|████▎     | 400/918 [29:57<35:14,  4.08s/it] 44%|████▎     | 401/918 [30:01<34:46,  4.04s/it] 44%|████▍     | 402/918 [30:05<35:07,  4.08s/it] 44%|████▍     | 403/918 [30:09<34:43,  4.05s/it] 44%|████▍     | 404/918 [30:12<32:25,  3.78s/it] 44%|████▍     | 405/918 [30:16<33:16,  3.89s/it] 44%|████▍     | 406/918 [30:20<33:24,  3.92s/it] 44%|████▍     | 407/918 [30:25<34:12,  4.02s/it] 44%|████▍     | 408/918 [30:29<34:07,  4.01s/it] 45%|████▍     | 409/918 [30:33<34:01,  4.01s/it] 45%|████▍     | 410/918 [30:37<33:52,  4.00s/it]                                                 {'loss': 0.0321, 'grad_norm': 0.349465936422348, 'learning_rate': 3.7503208814559416e-07, 'rewards/chosen': 7.917187690734863, 'rewards/rejected': -3.3046875, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 11.225000381469727, 'logps/chosen': -796.0, 'logps/rejected': -365.6499938964844, 'logits/chosen': -5.471875190734863, 'logits/rejected': -5.806250095367432, 'epoch': 1.34}
 45%|████▍     | 410/918 [30:37<33:52,  4.00s/it] 45%|████▍     | 411/918 [30:41<33:43,  3.99s/it] 45%|████▍     | 412/918 [30:44<33:36,  3.98s/it] 45%|████▍     | 413/918 [30:48<33:31,  3.98s/it] 45%|████▌     | 414/918 [30:53<33:50,  4.03s/it] 45%|████▌     | 415/918 [30:57<33:39,  4.01s/it] 45%|████▌     | 416/918 [31:01<34:07,  4.08s/it] 45%|████▌     | 417/918 [31:05<33:47,  4.05s/it] 46%|████▌     | 418/918 [31:09<33:37,  4.03s/it] 46%|████▌     | 419/918 [31:13<34:05,  4.10s/it] 46%|████▌     | 420/918 [31:17<33:42,  4.06s/it]                                                 {'loss': 0.0609, 'grad_norm': 9.6089506149292, 'learning_rate': 3.701432773969145e-07, 'rewards/chosen': 9.440625190734863, 'rewards/rejected': -4.509375095367432, 'rewards/accuracies': 0.981249988079071, 'rewards/margins': 13.943750381469727, 'logps/chosen': -944.5999755859375, 'logps/rejected': -422.70001220703125, 'logits/chosen': -5.328125, 'logits/rejected': -5.8125, 'epoch': 1.37}
 46%|████▌     | 420/918 [31:17<33:42,  4.06s/it] 46%|████▌     | 421/918 [31:21<33:54,  4.09s/it] 46%|████▌     | 422/918 [31:25<33:31,  4.06s/it] 46%|████▌     | 423/918 [31:29<33:16,  4.03s/it] 46%|████▌     | 424/918 [31:33<33:28,  4.07s/it] 46%|████▋     | 425/918 [31:37<33:31,  4.08s/it] 46%|████▋     | 426/918 [31:41<33:11,  4.05s/it] 47%|████▋     | 427/918 [31:45<32:55,  4.02s/it] 47%|████▋     | 428/918 [31:49<32:43,  4.01s/it] 47%|████▋     | 429/918 [31:53<31:06,  3.82s/it] 47%|████▋     | 430/918 [31:57<31:22,  3.86s/it]                                                 {'loss': 0.0512, 'grad_norm': 26.06284523010254, 'learning_rate': 3.648602003735321e-07, 'rewards/chosen': 7.787499904632568, 'rewards/rejected': -5.15625, 'rewards/accuracies': 0.981249988079071, 'rewards/margins': 12.943750381469727, 'logps/chosen': -748.7000122070312, 'logps/rejected': -234.60000610351562, 'logits/chosen': -5.578125, 'logits/rejected': -5.728125095367432, 'epoch': 1.41}
 47%|████▋     | 430/918 [31:57<31:22,  3.86s/it] 47%|████▋     | 431/918 [32:01<31:38,  3.90s/it] 47%|████▋     | 432/918 [32:05<31:47,  3.92s/it] 47%|████▋     | 433/918 [32:09<31:50,  3.94s/it] 47%|████▋     | 434/918 [32:13<31:48,  3.94s/it] 47%|████▋     | 435/918 [32:16<31:48,  3.95s/it] 47%|████▋     | 436/918 [32:20<31:48,  3.96s/it] 48%|████▊     | 437/918 [32:25<32:29,  4.05s/it] 48%|████▊     | 438/918 [32:29<32:14,  4.03s/it] 48%|████▊     | 439/918 [32:33<31:59,  4.01s/it] 48%|████▊     | 440/918 [32:37<31:51,  4.00s/it]                                                 {'loss': 0.0878, 'grad_norm': 26.715206146240234, 'learning_rate': 3.5919673008483563e-07, 'rewards/chosen': 9.637499809265137, 'rewards/rejected': -4.943749904632568, 'rewards/accuracies': 0.96875, 'rewards/margins': 14.5625, 'logps/chosen': -962.7999877929688, 'logps/rejected': -467.6000061035156, 'logits/chosen': -5.137499809265137, 'logits/rejected': -5.228125095367432, 'epoch': 1.44}
 48%|████▊     | 440/918 [32:37<31:51,  4.00s/it] 48%|████▊     | 441/918 [32:41<31:42,  3.99s/it] 48%|████▊     | 442/918 [32:45<31:36,  3.98s/it] 48%|████▊     | 443/918 [32:49<31:30,  3.98s/it] 48%|████▊     | 444/918 [32:53<31:25,  3.98s/it] 48%|████▊     | 445/918 [32:57<31:20,  3.98s/it] 49%|████▊     | 446/918 [33:00<31:17,  3.98s/it] 49%|████▊     | 447/918 [33:04<31:12,  3.98s/it] 49%|████▉     | 448/918 [33:08<31:08,  3.97s/it] 49%|████▉     | 449/918 [33:12<31:02,  3.97s/it] 49%|████▉     | 450/918 [33:16<30:55,  3.96s/it]                                                 {'loss': 0.0831, 'grad_norm': 37.5142936706543, 'learning_rate': 3.531677384276647e-07, 'rewards/chosen': 7.776562690734863, 'rewards/rejected': -6.512499809265137, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 14.306249618530273, 'logps/chosen': -744.7999877929688, 'logps/rejected': -256.6000061035156, 'logits/chosen': -5.534375190734863, 'logits/rejected': -5.759375095367432, 'epoch': 1.47}
 49%|████▉     | 450/918 [33:16<30:55,  3.96s/it] 49%|████▉     | 451/918 [33:20<31:07,  4.00s/it] 49%|████▉     | 452/918 [33:24<31:03,  4.00s/it] 49%|████▉     | 453/918 [33:28<30:54,  3.99s/it] 49%|████▉     | 454/918 [33:32<30:48,  3.98s/it] 50%|████▉     | 455/918 [33:37<31:08,  4.04s/it] 50%|████▉     | 456/918 [33:40<30:52,  4.01s/it]
  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|▎         | 2/60 [00:01<00:45,  1.27it/s][A
  5%|▌         | 3/60 [00:03<01:03,  1.12s/it][A
  7%|▋         | 4/60 [00:04<01:13,  1.32s/it][A
  8%|▊         | 5/60 [00:06<01:17,  1.41s/it][A
 10%|█         | 6/60 [00:08<01:20,  1.49s/it][A
 12%|█▏        | 7/60 [00:09<01:20,  1.52s/it][A
 13%|█▎        | 8/60 [00:11<01:22,  1.58s/it][A
 15%|█▌        | 9/60 [00:12<01:21,  1.60s/it][A
 17%|█▋        | 10/60 [00:14<01:19,  1.59s/it][A
 18%|█▊        | 11/60 [00:16<01:20,  1.63s/it][A
 20%|██        | 12/60 [00:17<01:17,  1.62s/it][A
 22%|██▏       | 13/60 [00:19<01:15,  1.61s/it][A
 23%|██▎       | 14/60 [00:21<01:13,  1.60s/it][A
 25%|██▌       | 15/60 [00:22<01:07,  1.50s/it][A
 27%|██▋       | 16/60 [00:23<01:00,  1.37s/it][A
 28%|██▊       | 17/60 [00:24<00:54,  1.28s/it][A
 30%|███       | 18/60 [00:25<00:45,  1.08s/it][A
 32%|███▏      | 19/60 [00:26<00:46,  1.13s/it][A
 33%|███▎      | 20/60 [00:26<00:39,  1.01it/s][A
 35%|███▌      | 21/60 [00:27<00:38,  1.01it/s][A
 37%|███▋      | 22/60 [00:29<00:41,  1.09s/it][A
 38%|███▊      | 23/60 [00:30<00:40,  1.11s/it][A
 40%|████      | 24/60 [00:31<00:38,  1.07s/it][A
 42%|████▏     | 25/60 [00:33<00:43,  1.24s/it][A
 43%|████▎     | 26/60 [00:34<00:44,  1.30s/it][A
 45%|████▌     | 27/60 [00:35<00:37,  1.15s/it][A
 47%|████▋     | 28/60 [00:36<00:34,  1.08s/it][A
 48%|████▊     | 29/60 [00:37<00:34,  1.11s/it][A
 50%|█████     | 30/60 [00:39<00:38,  1.28s/it][A
 52%|█████▏    | 31/60 [00:40<00:39,  1.36s/it][A
 53%|█████▎    | 32/60 [00:42<00:38,  1.39s/it][A
 55%|█████▌    | 33/60 [00:43<00:36,  1.37s/it][A
 57%|█████▋    | 34/60 [00:44<00:31,  1.20s/it][A
 58%|█████▊    | 35/60 [00:45<00:31,  1.27s/it][A
 60%|██████    | 36/60 [00:47<00:31,  1.32s/it][A
 62%|██████▏   | 37/60 [00:47<00:25,  1.10s/it][A
 63%|██████▎   | 38/60 [00:49<00:27,  1.24s/it][A
 65%|██████▌   | 39/60 [00:50<00:25,  1.21s/it][A
 67%|██████▋   | 40/60 [00:51<00:21,  1.08s/it][A
 68%|██████▊   | 41/60 [00:52<00:22,  1.19s/it][A
 70%|███████   | 42/60 [00:54<00:23,  1.31s/it][A
 72%|███████▏  | 43/60 [00:55<00:20,  1.21s/it][A
 73%|███████▎  | 44/60 [00:56<00:20,  1.29s/it][A
 75%|███████▌  | 45/60 [00:57<00:17,  1.13s/it][A
 77%|███████▋  | 46/60 [00:58<00:17,  1.27s/it][A
 78%|███████▊  | 47/60 [01:00<00:18,  1.40s/it][A
 80%|████████  | 48/60 [01:01<00:14,  1.22s/it][A
 82%|████████▏ | 49/60 [01:03<00:14,  1.33s/it][A
 83%|████████▎ | 50/60 [01:04<00:14,  1.44s/it][A
 85%|████████▌ | 51/60 [01:06<00:13,  1.47s/it][A
 87%|████████▋ | 52/60 [01:07<00:11,  1.41s/it][A
 88%|████████▊ | 53/60 [01:08<00:09,  1.31s/it][A
 90%|█████████ | 54/60 [01:10<00:08,  1.39s/it][A
 92%|█████████▏| 55/60 [01:10<00:05,  1.19s/it][A
 93%|█████████▎| 56/60 [01:12<00:05,  1.31s/it][A
 95%|█████████▌| 57/60 [01:14<00:04,  1.39s/it][A
 97%|█████████▋| 58/60 [01:15<00:02,  1.37s/it][A
 98%|█████████▊| 59/60 [01:17<00:01,  1.44s/it][A
100%|██████████| 60/60 [01:18<00:00,  1.50s/it][A                                                 
                                               [A{'eval_loss': 0.13004757463932037, 'eval_runtime': 80.3048, 'eval_samples_per_second': 11.867, 'eval_steps_per_second': 0.747, 'eval_rewards/chosen': 4.021419048309326, 'eval_rewards/rejected': -2.2272136211395264, 'eval_rewards/accuracies': 0.967129647731781, 'eval_rewards/margins': 6.2459635734558105, 'eval_logps/chosen': -357.70001220703125, 'eval_logps/rejected': -163.0749969482422, 'eval_logits/chosen': -6.133854389190674, 'eval_logits/rejected': -6.535416603088379, 'epoch': 1.49}
 50%|████▉     | 456/918 [35:01<30:52,  4.01s/it]
100%|██████████| 60/60 [01:18<00:00,  1.50s/it][A
                                               [A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 50%|████▉     | 457/918 [35:16<4:01:41, 31.46s/it] 50%|████▉     | 458/918 [35:20<2:58:23, 23.27s/it] 50%|█████     | 459/918 [35:24<2:13:42, 17.48s/it] 50%|█████     | 460/918 [35:28<1:42:38, 13.45s/it]                                                   {'loss': 0.0705, 'grad_norm': 1.6476792097091675, 'learning_rate': 3.4678905713369665e-07, 'rewards/chosen': 8.949999809265137, 'rewards/rejected': -5.16796875, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 14.09375, 'logps/chosen': -870.0, 'logps/rejected': -407.70001220703125, 'logits/chosen': -5.412499904632568, 'logits/rejected': -5.559374809265137, 'epoch': 1.5}
 50%|█████     | 460/918 [35:28<1:42:38, 13.45s/it] 50%|█████     | 461/918 [35:32<1:20:51, 10.62s/it] 50%|█████     | 462/918 [35:36<1:05:53,  8.67s/it] 50%|█████     | 463/918 [35:40<55:03,  7.26s/it]   51%|█████     | 464/918 [35:44<47:49,  6.32s/it] 51%|█████     | 465/918 [35:48<42:22,  5.61s/it] 51%|█████     | 466/918 [35:52<38:25,  5.10s/it] 51%|█████     | 467/918 [35:56<35:46,  4.76s/it] 51%|█████     | 468/918 [36:00<33:57,  4.53s/it] 51%|█████     | 469/918 [36:04<32:39,  4.36s/it] 51%|█████     | 470/918 [36:08<31:43,  4.25s/it]                                                 {'loss': 0.0514, 'grad_norm': 3.970311403274536, 'learning_rate': 3.400774361963703e-07, 'rewards/chosen': 9.659375190734863, 'rewards/rejected': -5.764843940734863, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 15.443750381469727, 'logps/chosen': -911.5999755859375, 'logps/rejected': -313.45001220703125, 'logits/chosen': -5.278124809265137, 'logits/rejected': -5.349999904632568, 'epoch': 1.54}
 51%|█████     | 470/918 [36:08<31:43,  4.25s/it] 51%|█████▏    | 471/918 [36:12<31:00,  4.16s/it] 51%|█████▏    | 472/918 [36:16<30:31,  4.11s/it] 52%|█████▏    | 473/918 [36:20<30:24,  4.10s/it] 52%|█████▏    | 474/918 [36:24<29:57,  4.05s/it] 52%|█████▏    | 475/918 [36:28<29:42,  4.02s/it] 52%|█████▏    | 476/918 [36:32<29:58,  4.07s/it] 52%|█████▏    | 477/918 [36:36<29:40,  4.04s/it] 52%|█████▏    | 478/918 [36:40<28:48,  3.93s/it] 52%|█████▏    | 479/918 [36:44<29:24,  4.02s/it] 52%|█████▏    | 480/918 [36:48<29:17,  4.01s/it]                                                 {'loss': 0.0436, 'grad_norm': 1.1878553628921509, 'learning_rate': 3.330504998865158e-07, 'rewards/chosen': 7.803124904632568, 'rewards/rejected': -8.040624618530273, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 15.865625381469727, 'logps/chosen': -726.25, 'logps/rejected': -270.6499938964844, 'logits/chosen': -5.637499809265137, 'logits/rejected': -5.837500095367432, 'epoch': 1.57}
 52%|█████▏    | 480/918 [36:48<29:17,  4.01s/it] 52%|█████▏    | 481/918 [36:52<29:43,  4.08s/it] 53%|█████▎    | 482/918 [36:56<29:26,  4.05s/it] 53%|█████▎    | 483/918 [37:00<29:20,  4.05s/it] 53%|█████▎    | 484/918 [37:04<29:02,  4.01s/it] 53%|█████▎    | 485/918 [37:09<29:30,  4.09s/it] 53%|█████▎    | 486/918 [37:13<29:10,  4.05s/it] 53%|█████▎    | 487/918 [37:17<29:35,  4.12s/it] 53%|█████▎    | 488/918 [37:21<29:09,  4.07s/it] 53%|█████▎    | 489/918 [37:25<29:04,  4.07s/it] 53%|█████▎    | 490/918 [37:29<29:21,  4.12s/it]                                                 {'loss': 0.0359, 'grad_norm': 3.9830245971679688, 'learning_rate': 3.2572670047219004e-07, 'rewards/chosen': 6.6171875, 'rewards/rejected': -8.612500190734863, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 15.231249809265137, 'logps/chosen': -607.2000122070312, 'logps/rejected': -262.95001220703125, 'logits/chosen': -5.759375095367432, 'logits/rejected': -5.912499904632568, 'epoch': 1.6}
 53%|█████▎    | 490/918 [37:29<29:21,  4.12s/it] 53%|█████▎    | 491/918 [37:33<28:09,  3.96s/it] 54%|█████▎    | 492/918 [37:37<28:41,  4.04s/it] 54%|█████▎    | 493/918 [37:41<28:28,  4.02s/it] 54%|█████▍    | 494/918 [37:45<28:22,  4.02s/it] 54%|█████▍    | 495/918 [37:49<28:11,  4.00s/it] 54%|█████▍    | 496/918 [37:53<28:13,  4.01s/it] 54%|█████▍    | 497/918 [37:56<27:14,  3.88s/it] 54%|█████▍    | 498/918 [38:00<27:20,  3.91s/it] 54%|█████▍    | 499/918 [38:04<27:25,  3.93s/it] 54%|█████▍    | 500/918 [38:08<27:27,  3.94s/it]                                                 {'loss': 0.0261, 'grad_norm': 0.8084251284599304, 'learning_rate': 3.18125269764246e-07, 'rewards/chosen': 8.80078125, 'rewards/rejected': -7.790625095367432, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 16.581249237060547, 'logps/chosen': -776.5, 'logps/rejected': -300.6000061035156, 'logits/chosen': -5.609375, 'logits/rejected': -5.534375190734863, 'epoch': 1.63}
 54%|█████▍    | 500/918 [38:08<27:27,  3.94s/it] 55%|█████▍    | 501/918 [38:13<27:53,  4.01s/it] 55%|█████▍    | 502/918 [38:17<28:02,  4.05s/it] 55%|█████▍    | 503/918 [38:21<27:51,  4.03s/it] 55%|█████▍    | 504/918 [38:25<27:40,  4.01s/it] 55%|█████▌    | 505/918 [38:29<27:55,  4.06s/it] 55%|█████▌    | 506/918 [38:33<28:17,  4.12s/it] 55%|█████▌    | 507/918 [38:37<27:56,  4.08s/it] 55%|█████▌    | 508/918 [38:41<27:40,  4.05s/it] 55%|█████▌    | 509/918 [38:45<27:35,  4.05s/it] 56%|█████▌    | 510/918 [38:49<27:22,  4.02s/it]                                                 {'loss': 0.0874, 'grad_norm': 12.41982364654541, 'learning_rate': 3.102661686148751e-07, 'rewards/chosen': 9.665624618530273, 'rewards/rejected': -7.332812309265137, 'rewards/accuracies': 0.9624999761581421, 'rewards/margins': 16.981250762939453, 'logps/chosen': -906.7999877929688, 'logps/rejected': -432.6000061035156, 'logits/chosen': -5.331250190734863, 'logits/rejected': -5.59375, 'epoch': 1.67}
 56%|█████▌    | 510/918 [38:49<27:22,  4.02s/it] 56%|█████▌    | 511/918 [38:53<27:51,  4.11s/it] 56%|█████▌    | 512/918 [38:57<27:32,  4.07s/it] 56%|█████▌    | 513/918 [39:01<26:40,  3.95s/it] 56%|█████▌    | 514/918 [39:05<26:39,  3.96s/it] 56%|█████▌    | 515/918 [39:09<26:36,  3.96s/it] 56%|█████▌    | 516/918 [39:13<27:09,  4.05s/it] 56%|█████▋    | 517/918 [39:17<26:58,  4.04s/it] 56%|█████▋    | 518/918 [39:21<26:48,  4.02s/it] 57%|█████▋    | 519/918 [39:25<26:48,  4.03s/it] 57%|█████▋    | 520/918 [39:29<26:39,  4.02s/it]                                                 {'loss': 0.0956, 'grad_norm': 1.1555564403533936, 'learning_rate': 3.021700345017346e-07, 'rewards/chosen': 9.471875190734863, 'rewards/rejected': -6.962500095367432, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 16.431249618530273, 'logps/chosen': -937.4000244140625, 'logps/rejected': -340.3999938964844, 'logits/chosen': -5.456250190734863, 'logits/rejected': -5.556250095367432, 'epoch': 1.7}
 57%|█████▋    | 520/918 [39:29<26:39,  4.02s/it] 57%|█████▋    | 521/918 [39:33<26:30,  4.01s/it] 57%|█████▋    | 522/918 [39:37<26:24,  4.00s/it] 57%|█████▋    | 523/918 [39:41<26:18,  4.00s/it] 57%|█████▋    | 524/918 [39:45<26:07,  3.98s/it] 57%|█████▋    | 525/918 [39:49<26:39,  4.07s/it] 57%|█████▋    | 526/918 [39:53<26:22,  4.04s/it] 57%|█████▋    | 527/918 [39:58<26:45,  4.11s/it] 58%|█████▊    | 528/918 [40:02<26:22,  4.06s/it] 58%|█████▊    | 529/918 [40:06<26:43,  4.12s/it] 58%|█████▊    | 530/918 [40:10<26:37,  4.12s/it]                                                 {'loss': 0.0529, 'grad_norm': 0.35292598605155945, 'learning_rate': 2.9385812733530206e-07, 'rewards/chosen': 9.231249809265137, 'rewards/rejected': -11.887499809265137, 'rewards/accuracies': 0.96875, 'rewards/margins': 21.112499237060547, 'logps/chosen': -901.2000122070312, 'logps/rejected': -466.5, 'logits/chosen': -5.478125095367432, 'logits/rejected': -5.503125190734863, 'epoch': 1.73}
 58%|█████▊    | 530/918 [40:10<26:37,  4.12s/it] 58%|█████▊    | 531/918 [40:14<26:16,  4.07s/it] 58%|█████▊    | 532/918 [40:18<25:59,  4.04s/it] 58%|█████▊    | 533/918 [40:22<25:46,  4.02s/it] 58%|█████▊    | 534/918 [40:26<25:54,  4.05s/it] 58%|█████▊    | 535/918 [40:30<25:41,  4.02s/it] 58%|█████▊    | 536/918 [40:34<25:30,  4.01s/it] 58%|█████▊    | 537/918 [40:38<25:24,  4.00s/it] 59%|█████▊    | 538/918 [40:42<25:18,  4.00s/it] 59%|█████▊    | 539/918 [40:46<25:26,  4.03s/it] 59%|█████▉    | 540/918 [40:50<24:43,  3.93s/it]                                                 {'loss': 0.0205, 'grad_norm': 0.6536598205566406, 'learning_rate': 2.853522736317627e-07, 'rewards/chosen': 8.348437309265137, 'rewards/rejected': -8.056249618530273, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 16.387500762939453, 'logps/chosen': -760.4000244140625, 'logps/rejected': -311.0, 'logits/chosen': -5.540625095367432, 'logits/rejected': -5.699999809265137, 'epoch': 1.76}
 59%|█████▉    | 540/918 [40:50<24:43,  3.93s/it] 59%|█████▉    | 541/918 [40:54<24:45,  3.94s/it] 59%|█████▉    | 542/918 [40:58<24:46,  3.95s/it] 59%|█████▉    | 543/918 [41:02<25:16,  4.04s/it] 59%|█████▉    | 544/918 [41:06<25:06,  4.03s/it] 59%|█████▉    | 545/918 [41:10<24:57,  4.01s/it] 59%|█████▉    | 546/918 [41:14<25:03,  4.04s/it] 60%|█████▉    | 547/918 [41:18<24:51,  4.02s/it] 60%|█████▉    | 548/918 [41:22<24:42,  4.01s/it] 60%|█████▉    | 549/918 [41:26<24:41,  4.01s/it] 60%|█████▉    | 550/918 [41:30<24:30,  4.00s/it]                                                 {'loss': 0.0232, 'grad_norm': 0.12850859761238098, 'learning_rate': 2.7667480919802696e-07, 'rewards/chosen': 11.090624809265137, 'rewards/rejected': -7.15625, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 18.25, 'logps/chosen': -1037.4000244140625, 'logps/rejected': -435.5, 'logits/chosen': -5.237500190734863, 'logits/rejected': -5.296875, 'epoch': 1.8}
 60%|█████▉    | 550/918 [41:30<24:30,  4.00s/it] 60%|██████    | 551/918 [41:34<24:39,  4.03s/it] 60%|██████    | 552/918 [41:38<24:26,  4.01s/it] 60%|██████    | 553/918 [41:42<23:51,  3.92s/it] 60%|██████    | 554/918 [41:46<24:17,  4.00s/it] 60%|██████    | 555/918 [41:50<24:12,  4.00s/it] 61%|██████    | 556/918 [41:54<24:36,  4.08s/it] 61%|██████    | 557/918 [41:58<24:30,  4.07s/it] 61%|██████    | 558/918 [42:02<24:28,  4.08s/it] 61%|██████    | 559/918 [42:06<24:12,  4.05s/it] 61%|██████    | 560/918 [42:10<24:19,  4.08s/it]                                                 {'loss': 0.0668, 'grad_norm': 5.415235996246338, 'learning_rate': 2.678485204793856e-07, 'rewards/chosen': 6.5078125, 'rewards/rejected': -10.265625, 'rewards/accuracies': 0.9624999761581421, 'rewards/margins': 16.787500381469727, 'logps/chosen': -575.7999877929688, 'logps/rejected': -261.6499938964844, 'logits/chosen': -5.771874904632568, 'logits/rejected': -5.668749809265137, 'epoch': 1.83}
 61%|██████    | 560/918 [42:10<24:19,  4.08s/it] 61%|██████    | 561/918 [42:15<24:31,  4.12s/it] 61%|██████    | 562/918 [42:19<24:10,  4.07s/it] 61%|██████▏   | 563/918 [42:23<24:27,  4.13s/it] 61%|██████▏   | 564/918 [42:27<24:04,  4.08s/it] 62%|██████▏   | 565/918 [42:31<23:52,  4.06s/it] 62%|██████▏   | 566/918 [42:35<23:40,  4.04s/it] 62%|██████▏   | 567/918 [42:39<23:28,  4.01s/it] 62%|██████▏   | 568/918 [42:43<23:39,  4.06s/it] 62%|██████▏   | 569/918 [42:47<23:44,  4.08s/it] 62%|██████▏   | 570/918 [42:51<23:27,  4.04s/it]                                                 {'loss': 0.024, 'grad_norm': 6.920419692993164, 'learning_rate': 2.58896584723817e-07, 'rewards/chosen': 10.050000190734863, 'rewards/rejected': -10.668749809265137, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 20.71875, 'logps/chosen': -969.2000122070312, 'logps/rejected': -383.0, 'logits/chosen': -5.375, 'logits/rejected': -5.381249904632568, 'epoch': 1.86}
 62%|██████▏   | 570/918 [42:51<23:27,  4.04s/it] 62%|██████▏   | 571/918 [42:55<23:16,  4.03s/it] 62%|██████▏   | 572/918 [42:59<23:07,  4.01s/it] 62%|██████▏   | 573/918 [43:03<23:01,  4.00s/it] 63%|██████▎   | 574/918 [43:07<22:54,  3.99s/it] 63%|██████▎   | 575/918 [43:11<22:47,  3.99s/it] 63%|██████▎   | 576/918 [43:15<23:00,  4.04s/it] 63%|██████▎   | 577/918 [43:19<22:49,  4.02s/it] 63%|██████▎   | 578/918 [43:23<22:41,  4.01s/it] 63%|██████▎   | 579/918 [43:27<23:06,  4.09s/it] 63%|██████▎   | 580/918 [43:31<22:50,  4.06s/it]                                                 {'loss': 0.1038, 'grad_norm': 13.15699577331543, 'learning_rate': 2.498425091200742e-07, 'rewards/chosen': 10.089062690734863, 'rewards/rejected': -7.787499904632568, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 17.899999618530273, 'logps/chosen': -980.4000244140625, 'logps/rejected': -428.1499938964844, 'logits/chosen': -5.521874904632568, 'logits/rejected': -5.631249904632568, 'epoch': 1.9}
 63%|██████▎   | 580/918 [43:31<22:50,  4.06s/it] 63%|██████▎   | 581/918 [43:35<22:44,  4.05s/it] 63%|██████▎   | 582/918 [43:39<22:22,  4.00s/it] 64%|██████▎   | 583/918 [43:43<22:44,  4.07s/it] 64%|██████▎   | 584/918 [43:47<22:31,  4.05s/it] 64%|██████▎   | 585/918 [43:51<22:22,  4.03s/it] 64%|██████▍   | 586/918 [43:56<22:46,  4.11s/it] 64%|██████▍   | 587/918 [44:00<22:26,  4.07s/it] 64%|██████▍   | 588/918 [44:04<22:13,  4.04s/it] 64%|██████▍   | 589/918 [44:08<22:02,  4.02s/it] 64%|██████▍   | 590/918 [44:12<21:56,  4.01s/it]                                                 {'loss': 0.1077, 'grad_norm': 5.3008904457092285, 'learning_rate': 2.407100690693674e-07, 'rewards/chosen': 9.723437309265137, 'rewards/rejected': -8.368749618530273, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 18.087499618530273, 'logps/chosen': -979.2000122070312, 'logps/rejected': -405.1499938964844, 'logits/chosen': -5.525000095367432, 'logits/rejected': -5.359375, 'epoch': 1.93}
 64%|██████▍   | 590/918 [44:12<21:56,  4.01s/it] 64%|██████▍   | 591/918 [44:16<21:51,  4.01s/it] 64%|██████▍   | 592/918 [44:20<22:04,  4.06s/it] 65%|██████▍   | 593/918 [44:24<21:54,  4.04s/it] 65%|██████▍   | 594/918 [44:28<21:44,  4.03s/it] 65%|██████▍   | 595/918 [44:32<21:36,  4.01s/it] 65%|██████▍   | 596/918 [44:36<21:27,  4.00s/it] 65%|██████▌   | 597/918 [44:39<20:57,  3.92s/it] 65%|██████▌   | 598/918 [44:43<20:58,  3.93s/it] 65%|██████▌   | 599/918 [44:48<21:15,  4.00s/it] 65%|██████▌   | 600/918 [44:52<21:13,  4.01s/it]                                                 {'loss': 0.0406, 'grad_norm': 0.410445898771286, 'learning_rate': 2.3152324575274044e-07, 'rewards/chosen': 8.25390625, 'rewards/rejected': -7.609375, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 15.850000381469727, 'logps/chosen': -776.0999755859375, 'logps/rejected': -354.3999938964844, 'logits/chosen': -5.712500095367432, 'logits/rejected': -5.640625, 'epoch': 1.96}
 65%|██████▌   | 600/918 [44:52<21:13,  4.01s/it] 65%|██████▌   | 601/918 [44:56<21:07,  4.00s/it] 66%|██████▌   | 602/918 [45:00<21:42,  4.12s/it] 66%|██████▌   | 603/918 [45:04<21:25,  4.08s/it] 66%|██████▌   | 604/918 [45:08<20:49,  3.98s/it] 66%|██████▌   | 605/918 [45:12<20:46,  3.98s/it] 66%|██████▌   | 606/918 [45:16<20:49,  4.00s/it] 66%|██████▌   | 607/918 [45:20<21:03,  4.06s/it] 66%|██████▌   | 608/918 [45:24<20:50,  4.03s/it]
  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|▎         | 2/60 [00:01<00:45,  1.27it/s][A
  5%|▌         | 3/60 [00:03<01:03,  1.12s/it][A
  7%|▋         | 4/60 [00:04<01:13,  1.32s/it][A
  8%|▊         | 5/60 [00:06<01:17,  1.41s/it][A
 10%|█         | 6/60 [00:08<01:20,  1.49s/it][A
 12%|█▏        | 7/60 [00:09<01:20,  1.52s/it][A
 13%|█▎        | 8/60 [00:11<01:22,  1.58s/it][A
 15%|█▌        | 9/60 [00:12<01:21,  1.60s/it][A
 17%|█▋        | 10/60 [00:14<01:19,  1.59s/it][A
 18%|█▊        | 11/60 [00:16<01:20,  1.63s/it][A
 20%|██        | 12/60 [00:17<01:17,  1.62s/it][A
 22%|██▏       | 13/60 [00:19<01:15,  1.61s/it][A
 23%|██▎       | 14/60 [00:21<01:13,  1.60s/it][A
 25%|██▌       | 15/60 [00:22<01:07,  1.50s/it][A
 27%|██▋       | 16/60 [00:23<01:00,  1.37s/it][A
 28%|██▊       | 17/60 [00:24<00:54,  1.28s/it][A
 30%|███       | 18/60 [00:25<00:45,  1.08s/it][A
 32%|███▏      | 19/60 [00:26<00:46,  1.13s/it][A
 33%|███▎      | 20/60 [00:26<00:39,  1.01it/s][A
 35%|███▌      | 21/60 [00:27<00:38,  1.01it/s][A
 37%|███▋      | 22/60 [00:29<00:41,  1.09s/it][A
 38%|███▊      | 23/60 [00:30<00:40,  1.10s/it][A
 40%|████      | 24/60 [00:31<00:38,  1.07s/it][A
 42%|████▏     | 25/60 [00:33<00:43,  1.24s/it][A
 43%|████▎     | 26/60 [00:34<00:44,  1.30s/it][A
 45%|████▌     | 27/60 [00:35<00:37,  1.14s/it][A
 47%|████▋     | 28/60 [00:36<00:34,  1.08s/it][A
 48%|████▊     | 29/60 [00:37<00:34,  1.11s/it][A
 50%|█████     | 30/60 [00:39<00:38,  1.28s/it][A
 52%|█████▏    | 31/60 [00:40<00:39,  1.36s/it][A
 53%|█████▎    | 32/60 [00:42<00:38,  1.38s/it][A
 55%|█████▌    | 33/60 [00:43<00:36,  1.37s/it][A
 57%|█████▋    | 34/60 [00:44<00:31,  1.20s/it][A
 58%|█████▊    | 35/60 [00:45<00:31,  1.27s/it][A
 60%|██████    | 36/60 [00:47<00:31,  1.32s/it][A
 62%|██████▏   | 37/60 [00:47<00:25,  1.10s/it][A
 63%|██████▎   | 38/60 [00:49<00:27,  1.24s/it][A
 65%|██████▌   | 39/60 [00:50<00:25,  1.21s/it][A
 67%|██████▋   | 40/60 [00:51<00:21,  1.08s/it][A
 68%|██████▊   | 41/60 [00:52<00:22,  1.19s/it][A
 70%|███████   | 42/60 [00:54<00:23,  1.31s/it][A
 72%|███████▏  | 43/60 [00:55<00:20,  1.21s/it][A
 73%|███████▎  | 44/60 [00:56<00:20,  1.29s/it][A
 75%|███████▌  | 45/60 [00:57<00:17,  1.13s/it][A
 77%|███████▋  | 46/60 [00:58<00:17,  1.27s/it][A
 78%|███████▊  | 47/60 [01:00<00:18,  1.40s/it][A
 80%|████████  | 48/60 [01:01<00:14,  1.22s/it][A
 82%|████████▏ | 49/60 [01:03<00:14,  1.33s/it][A
 83%|████████▎ | 50/60 [01:04<00:14,  1.44s/it][A
 85%|████████▌ | 51/60 [01:06<00:13,  1.47s/it][A
 87%|████████▋ | 52/60 [01:07<00:11,  1.41s/it][A
 88%|████████▊ | 53/60 [01:08<00:09,  1.31s/it][A
 90%|█████████ | 54/60 [01:10<00:08,  1.39s/it][A
 92%|█████████▏| 55/60 [01:10<00:05,  1.19s/it][A
 93%|█████████▎| 56/60 [01:12<00:05,  1.31s/it][A
 95%|█████████▌| 57/60 [01:14<00:04,  1.39s/it][A
 97%|█████████▋| 58/60 [01:15<00:02,  1.37s/it][A
 98%|█████████▊| 59/60 [01:16<00:01,  1.43s/it][A
100%|██████████| 60/60 [01:18<00:00,  1.49s/it][A                                                 
                                               [A{'eval_loss': 0.13592183589935303, 'eval_runtime': 80.2822, 'eval_samples_per_second': 11.871, 'eval_steps_per_second': 0.747, 'eval_rewards/chosen': 3.990689992904663, 'eval_rewards/rejected': -3.3736491203308105, 'eval_rewards/accuracies': 0.9629629850387573, 'eval_rewards/margins': 7.366276264190674, 'eval_logps/chosen': -357.85833740234375, 'eval_logps/rejected': -168.78750610351562, 'eval_logits/chosen': -6.348698139190674, 'eval_logits/rejected': -6.709895610809326, 'epoch': 1.99}
 66%|██████▌   | 608/918 [46:44<20:50,  4.03s/it]
100%|██████████| 60/60 [01:18<00:00,  1.49s/it][A
                                               [A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 66%|██████▋   | 609/918 [47:00<2:42:40, 31.59s/it] 66%|██████▋   | 610/918 [47:04<1:59:54, 23.36s/it]                                                   {'loss': 0.0994, 'grad_norm': 42.67393112182617, 'learning_rate': 2.2230616315808124e-07, 'rewards/chosen': 7.967187404632568, 'rewards/rejected': -9.765625, 'rewards/accuracies': 0.96875, 'rewards/margins': 17.71875, 'logps/chosen': -741.5, 'logps/rejected': -250.22500610351562, 'logits/chosen': -5.831250190734863, 'logits/rejected': -5.803124904632568, 'epoch': 1.99}
 66%|██████▋   | 610/918 [47:04<1:59:54, 23.36s/it] 67%|██████▋   | 611/918 [47:08<1:29:46, 17.55s/it] 67%|██████▋   | 612/918 [47:12<1:08:43, 13.48s/it] 67%|██████▋   | 613/918 [47:16<54:01, 10.63s/it]   67%|██████▋   | 614/918 [47:19<42:56,  8.47s/it] 67%|██████▋   | 615/918 [47:24<36:26,  7.22s/it] 67%|██████▋   | 616/918 [47:27<31:02,  6.17s/it] 67%|██████▋   | 617/918 [47:31<27:46,  5.54s/it] 67%|██████▋   | 618/918 [47:35<25:19,  5.07s/it] 67%|██████▋   | 619/918 [47:39<23:37,  4.74s/it] 68%|██████▊   | 620/918 [47:43<22:30,  4.53s/it]                                                 {'loss': 0.0091, 'grad_norm': 2.234489917755127, 'learning_rate': 2.1308302473213238e-07, 'rewards/chosen': 8.719531059265137, 'rewards/rejected': -9.796483993530273, 'rewards/accuracies': 1.0, 'rewards/margins': 18.524999618530273, 'logps/chosen': -844.9500122070312, 'logps/rejected': -341.04998779296875, 'logits/chosen': -5.668749809265137, 'logits/rejected': -5.457812309265137, 'epoch': 2.03}
 68%|██████▊   | 620/918 [47:43<22:30,  4.53s/it] 68%|██████▊   | 621/918 [47:48<21:49,  4.41s/it] 68%|██████▊   | 622/918 [47:52<21:33,  4.37s/it] 68%|██████▊   | 623/918 [47:56<21:12,  4.31s/it] 68%|██████▊   | 624/918 [48:00<20:36,  4.21s/it] 68%|██████▊   | 625/918 [48:04<20:11,  4.14s/it] 68%|██████▊   | 626/918 [48:08<19:53,  4.09s/it] 68%|██████▊   | 627/918 [48:12<19:55,  4.11s/it] 68%|██████▊   | 628/918 [48:16<19:40,  4.07s/it] 69%|██████▊   | 629/918 [48:20<19:42,  4.09s/it] 69%|██████▊   | 630/918 [48:24<19:28,  4.06s/it]                                                 {'loss': 0.0304, 'grad_norm': 3.065343141555786, 'learning_rate': 2.0387804982384713e-07, 'rewards/chosen': 9.693750381469727, 'rewards/rejected': -8.964062690734863, 'rewards/accuracies': 0.981249988079071, 'rewards/margins': 18.65625, 'logps/chosen': -947.2000122070312, 'logps/rejected': -400.8999938964844, 'logits/chosen': -5.525000095367432, 'logits/rejected': -5.784375190734863, 'epoch': 2.06}
 69%|██████▊   | 630/918 [48:24<19:28,  4.06s/it] 69%|██████▊   | 631/918 [48:28<19:17,  4.03s/it] 69%|██████▉   | 632/918 [48:32<19:07,  4.01s/it] 69%|██████▉   | 633/918 [48:36<18:59,  4.00s/it] 69%|██████▉   | 634/918 [48:40<18:54,  3.99s/it] 69%|██████▉   | 635/918 [48:44<18:48,  3.99s/it] 69%|██████▉   | 636/918 [48:48<18:48,  4.00s/it] 69%|██████▉   | 637/918 [48:52<18:56,  4.05s/it] 69%|██████▉   | 638/918 [48:56<18:27,  3.96s/it] 70%|██████▉   | 639/918 [49:00<18:45,  4.04s/it] 70%|██████▉   | 640/918 [49:04<18:37,  4.02s/it]                                                 {'loss': 0.0239, 'grad_norm': 0.4488488733768463, 'learning_rate': 1.9471541008598775e-07, 'rewards/chosen': 8.921875, 'rewards/rejected': -10.524999618530273, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 19.431249618530273, 'logps/chosen': -829.5, 'logps/rejected': -414.1000061035156, 'logits/chosen': -5.681250095367432, 'logits/rejected': -5.743750095367432, 'epoch': 2.09}
 70%|██████▉   | 640/918 [49:04<18:37,  4.02s/it] 70%|██████▉   | 641/918 [49:08<18:30,  4.01s/it] 70%|██████▉   | 642/918 [49:12<18:23,  4.00s/it] 70%|███████   | 643/918 [49:16<18:44,  4.09s/it] 70%|███████   | 644/918 [49:21<18:54,  4.14s/it] 70%|███████   | 645/918 [49:25<18:37,  4.09s/it] 70%|███████   | 646/918 [49:29<18:48,  4.15s/it] 70%|███████   | 647/918 [49:33<18:52,  4.18s/it] 71%|███████   | 648/918 [49:37<18:46,  4.17s/it] 71%|███████   | 649/918 [49:41<18:28,  4.12s/it] 71%|███████   | 650/918 [49:45<18:12,  4.08s/it]                                                 {'loss': 0.0182, 'grad_norm': 11.531756401062012, 'learning_rate': 1.8561916600197063e-07, 'rewards/chosen': 7.065625190734863, 'rewards/rejected': -11.640625, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 18.71875, 'logps/chosen': -636.2000122070312, 'logps/rejected': -249.8000030517578, 'logits/chosen': -6.003125190734863, 'logits/rejected': -5.928124904632568, 'epoch': 2.12}
 71%|███████   | 650/918 [49:45<18:12,  4.08s/it] 71%|███████   | 651/918 [49:50<18:20,  4.12s/it] 71%|███████   | 652/918 [49:54<18:30,  4.18s/it] 71%|███████   | 653/918 [49:58<18:21,  4.16s/it] 71%|███████   | 654/918 [50:02<17:58,  4.09s/it] 71%|███████▏  | 655/918 [50:06<17:58,  4.10s/it] 71%|███████▏  | 656/918 [50:10<17:49,  4.08s/it] 72%|███████▏  | 657/918 [50:14<17:49,  4.10s/it] 72%|███████▏  | 658/918 [50:18<17:36,  4.06s/it] 72%|███████▏  | 659/918 [50:22<17:27,  4.04s/it] 72%|███████▏  | 660/918 [50:26<17:29,  4.07s/it]                                                 {'loss': 0.0073, 'grad_norm': 0.4012404978275299, 'learning_rate': 1.766132037046348e-07, 'rewards/chosen': 9.278905868530273, 'rewards/rejected': -9.334375381469727, 'rewards/accuracies': 1.0, 'rewards/margins': 18.625, 'logps/chosen': -887.4000244140625, 'logps/rejected': -402.1499938964844, 'logits/chosen': -5.546875, 'logits/rejected': -5.496874809265137, 'epoch': 2.16}
 72%|███████▏  | 660/918 [50:26<17:29,  4.07s/it] 72%|███████▏  | 661/918 [50:30<17:32,  4.10s/it] 72%|███████▏  | 662/918 [50:34<17:20,  4.06s/it] 72%|███████▏  | 663/918 [50:38<17:08,  4.03s/it] 72%|███████▏  | 664/918 [50:43<17:12,  4.07s/it] 72%|███████▏  | 665/918 [50:46<17:00,  4.03s/it] 73%|███████▎  | 666/918 [50:50<16:53,  4.02s/it] 73%|███████▎  | 667/918 [50:54<16:47,  4.01s/it] 73%|███████▎  | 668/918 [50:58<16:39,  4.00s/it] 73%|███████▎  | 669/918 [51:03<16:43,  4.03s/it] 73%|███████▎  | 670/918 [51:07<16:36,  4.02s/it]                                                 {'loss': 0.0378, 'grad_norm': 1.6770063638687134, 'learning_rate': 1.6772117225284342e-07, 'rewards/chosen': 10.153124809265137, 'rewards/rejected': -12.143750190734863, 'rewards/accuracies': 0.981249988079071, 'rewards/margins': 22.299999237060547, 'logps/chosen': -1000.5999755859375, 'logps/rejected': -416.20001220703125, 'logits/chosen': -5.662499904632568, 'logits/rejected': -5.446875095367432, 'epoch': 2.19}
 73%|███████▎  | 670/918 [51:07<16:36,  4.02s/it] 73%|███████▎  | 671/918 [51:11<16:32,  4.02s/it] 73%|███████▎  | 672/918 [51:15<16:23,  4.00s/it] 73%|███████▎  | 673/918 [51:18<16:17,  3.99s/it] 73%|███████▎  | 674/918 [51:22<15:21,  3.78s/it] 74%|███████▎  | 675/918 [51:26<15:32,  3.84s/it] 74%|███████▎  | 676/918 [51:30<15:38,  3.88s/it] 74%|███████▎  | 677/918 [51:34<15:41,  3.91s/it] 74%|███████▍  | 678/918 [51:37<15:24,  3.85s/it] 74%|███████▍  | 679/918 [51:41<15:33,  3.91s/it] 74%|███████▍  | 680/918 [51:45<15:35,  3.93s/it]                                                 {'loss': 0.0293, 'grad_norm': 0.32340413331985474, 'learning_rate': 1.5896642153062624e-07, 'rewards/chosen': 7.733593940734863, 'rewards/rejected': -11.112500190734863, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 18.837499618530273, 'logps/chosen': -719.7000122070312, 'logps/rejected': -270.45001220703125, 'logits/chosen': -5.846875190734863, 'logits/rejected': -5.8125, 'epoch': 2.22}
 74%|███████▍  | 680/918 [51:45<15:35,  3.93s/it] 74%|███████▍  | 681/918 [51:49<15:35,  3.95s/it] 74%|███████▍  | 682/918 [51:54<15:51,  4.03s/it] 74%|███████▍  | 683/918 [51:58<15:44,  4.02s/it] 75%|███████▍  | 684/918 [52:02<15:37,  4.01s/it] 75%|███████▍  | 685/918 [52:06<15:32,  4.00s/it] 75%|███████▍  | 686/918 [52:10<15:27,  4.00s/it] 75%|███████▍  | 687/918 [52:14<15:21,  3.99s/it] 75%|███████▍  | 688/918 [52:18<15:22,  4.01s/it] 75%|███████▌  | 689/918 [52:22<15:17,  4.01s/it] 75%|███████▌  | 690/918 [52:26<15:10,  3.99s/it]                                                 {'loss': 0.044, 'grad_norm': 0.02312597818672657, 'learning_rate': 1.5037194093193438e-07, 'rewards/chosen': 13.46875, 'rewards/rejected': -9.6875, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 23.137500762939453, 'logps/chosen': -1304.4000244140625, 'logps/rejected': -596.2000122070312, 'logits/chosen': -5.103125095367432, 'logits/rejected': -5.142187595367432, 'epoch': 2.25}
 75%|███████▌  | 690/918 [52:26<15:10,  3.99s/it] 75%|███████▌  | 691/918 [52:30<15:05,  3.99s/it] 75%|███████▌  | 692/918 [52:34<15:01,  3.99s/it] 75%|███████▌  | 693/918 [52:38<15:08,  4.04s/it] 76%|███████▌  | 694/918 [52:41<14:32,  3.89s/it] 76%|███████▌  | 695/918 [52:45<14:32,  3.91s/it] 76%|███████▌  | 696/918 [52:49<14:48,  4.00s/it] 76%|███████▌  | 697/918 [52:53<14:41,  3.99s/it] 76%|███████▌  | 698/918 [52:57<14:37,  3.99s/it] 76%|███████▌  | 699/918 [53:02<14:48,  4.06s/it] 76%|███████▋  | 700/918 [53:06<14:39,  4.03s/it]                                                 {'loss': 0.0557, 'grad_norm': 17.228113174438477, 'learning_rate': 1.4196029899201887e-07, 'rewards/chosen': 9.128125190734863, 'rewards/rejected': -12.590624809265137, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 21.725000381469727, 'logps/chosen': -875.2999877929688, 'logps/rejected': -409.6000061035156, 'logits/chosen': -5.884375095367432, 'logits/rejected': -5.818749904632568, 'epoch': 2.29}
 76%|███████▋  | 700/918 [53:06<14:39,  4.03s/it] 76%|███████▋  | 701/918 [53:09<14:29,  4.00s/it] 76%|███████▋  | 702/918 [53:13<14:23,  4.00s/it] 77%|███████▋  | 703/918 [53:17<14:17,  3.99s/it] 77%|███████▋  | 704/918 [53:21<14:13,  3.99s/it] 77%|███████▋  | 705/918 [53:26<14:19,  4.04s/it] 77%|███████▋  | 706/918 [53:30<14:23,  4.07s/it] 77%|███████▋  | 707/918 [53:34<14:14,  4.05s/it] 77%|███████▋  | 708/918 [53:38<14:00,  4.00s/it] 77%|███████▋  | 709/918 [53:42<13:55,  4.00s/it] 77%|███████▋  | 710/918 [53:46<14:00,  4.04s/it]                                                 {'loss': 0.0282, 'grad_norm': 5.030653476715088, 'learning_rate': 1.3375358412395633e-07, 'rewards/chosen': 10.396875381469727, 'rewards/rejected': -12.353124618530273, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 22.725000381469727, 'logps/chosen': -955.7999877929688, 'logps/rejected': -387.5, 'logits/chosen': -5.571875095367432, 'logits/rejected': -5.349999904632568, 'epoch': 2.32}
 77%|███████▋  | 710/918 [53:46<14:00,  4.04s/it] 77%|███████▋  | 711/918 [53:50<13:51,  4.02s/it] 78%|███████▊  | 712/918 [53:54<13:45,  4.01s/it] 78%|███████▊  | 713/918 [53:58<13:38,  3.99s/it] 78%|███████▊  | 714/918 [54:02<13:34,  3.99s/it] 78%|███████▊  | 715/918 [54:06<13:41,  4.04s/it] 78%|███████▊  | 716/918 [54:10<13:32,  4.02s/it] 78%|███████▊  | 717/918 [54:14<13:26,  4.01s/it] 78%|███████▊  | 718/918 [54:18<13:18,  3.99s/it] 78%|███████▊  | 719/918 [54:22<13:24,  4.04s/it] 78%|███████▊  | 720/918 [54:26<13:34,  4.11s/it]                                                 {'loss': 0.061, 'grad_norm': 19.754932403564453, 'learning_rate': 1.257733466159423e-07, 'rewards/chosen': 8.762499809265137, 'rewards/rejected': -11.865625381469727, 'rewards/accuracies': 0.981249988079071, 'rewards/margins': 20.637500762939453, 'logps/chosen': -772.4000244140625, 'logps/rejected': -371.5, 'logits/chosen': -5.8125, 'logits/rejected': -5.743750095367432, 'epoch': 2.35}
 78%|███████▊  | 720/918 [54:26<13:34,  4.11s/it] 79%|███████▊  | 721/918 [54:30<13:22,  4.07s/it] 79%|███████▊  | 722/918 [54:34<13:13,  4.05s/it] 79%|███████▉  | 723/918 [54:38<12:48,  3.94s/it] 79%|███████▉  | 724/918 [54:42<12:53,  3.98s/it] 79%|███████▉  | 725/918 [54:46<12:35,  3.91s/it] 79%|███████▉  | 726/918 [54:50<12:39,  3.95s/it] 79%|███████▉  | 727/918 [54:54<12:37,  3.96s/it] 79%|███████▉  | 728/918 [54:58<12:33,  3.97s/it] 79%|███████▉  | 729/918 [55:02<12:45,  4.05s/it] 80%|███████▉  | 730/918 [55:06<12:38,  4.03s/it]                                                 {'loss': 0.0255, 'grad_norm': 7.959323406219482, 'learning_rate': 1.1804054204166691e-07, 'rewards/chosen': 6.002343654632568, 'rewards/rejected': -13.534375190734863, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 19.524999618530273, 'logps/chosen': -523.5, 'logps/rejected': -231.3000030517578, 'logits/chosen': -6.040625095367432, 'logits/rejected': -5.959374904632568, 'epoch': 2.39}
 80%|███████▉  | 730/918 [55:06<12:38,  4.03s/it] 80%|███████▉  | 731/918 [55:10<12:42,  4.08s/it] 80%|███████▉  | 732/918 [55:14<12:43,  4.10s/it] 80%|███████▉  | 733/918 [55:18<12:39,  4.11s/it] 80%|███████▉  | 734/918 [55:22<12:28,  4.07s/it] 80%|████████  | 735/918 [55:26<12:18,  4.04s/it] 80%|████████  | 736/918 [55:30<12:10,  4.01s/it] 80%|████████  | 737/918 [55:34<12:10,  4.03s/it] 80%|████████  | 738/918 [55:38<12:03,  4.02s/it] 81%|████████  | 739/918 [55:42<11:56,  4.00s/it] 81%|████████  | 740/918 [55:46<11:54,  4.01s/it]                                                 {'loss': 0.0093, 'grad_norm': 0.09856507182121277, 'learning_rate': 1.1057547623236909e-07, 'rewards/chosen': 9.040624618530273, 'rewards/rejected': -13.673437118530273, 'rewards/accuracies': 1.0, 'rewards/margins': 22.725000381469727, 'logps/chosen': -848.2000122070312, 'logps/rejected': -399.0, 'logits/chosen': -5.662499904632568, 'logits/rejected': -5.731249809265137, 'epoch': 2.42}
 81%|████████  | 740/918 [55:46<11:54,  4.01s/it] 81%|████████  | 741/918 [55:51<12:01,  4.08s/it] 81%|████████  | 742/918 [55:55<12:00,  4.09s/it] 81%|████████  | 743/918 [55:59<11:50,  4.06s/it] 81%|████████  | 744/918 [56:03<11:56,  4.12s/it] 81%|████████  | 745/918 [56:07<11:45,  4.08s/it] 81%|████████▏ | 746/918 [56:11<11:35,  4.04s/it] 81%|████████▏ | 747/918 [56:15<11:33,  4.06s/it] 81%|████████▏ | 748/918 [56:19<11:26,  4.04s/it] 82%|████████▏ | 749/918 [56:23<11:21,  4.03s/it] 82%|████████▏ | 750/918 [56:27<11:28,  4.10s/it]                                                 {'loss': 0.0336, 'grad_norm': 0.17415906488895416, 'learning_rate': 1.0339775195507309e-07, 'rewards/chosen': 7.046875, 'rewards/rejected': -16.075000762939453, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 23.106250762939453, 'logps/chosen': -640.4000244140625, 'logps/rejected': -266.1000061035156, 'logits/chosen': -5.878125190734863, 'logits/rejected': -5.721875190734863, 'epoch': 2.45}
 82%|████████▏ | 750/918 [56:27<11:28,  4.10s/it] 82%|████████▏ | 751/918 [56:31<11:18,  4.06s/it] 82%|████████▏ | 752/918 [56:35<11:17,  4.08s/it] 82%|████████▏ | 753/918 [56:39<11:09,  4.06s/it] 82%|████████▏ | 754/918 [56:43<11:02,  4.04s/it] 82%|████████▏ | 755/918 [56:48<11:11,  4.12s/it] 82%|████████▏ | 756/918 [56:52<11:13,  4.16s/it] 82%|████████▏ | 757/918 [56:56<11:01,  4.11s/it] 83%|████████▎ | 758/918 [57:00<10:50,  4.07s/it] 83%|████████▎ | 759/918 [57:04<10:46,  4.07s/it] 83%|████████▎ | 760/918 [57:08<10:39,  4.05s/it]                                                 {'loss': 0.0141, 'grad_norm': 0.02001429721713066, 'learning_rate': 9.652621743702344e-08, 'rewards/chosen': 7.759375095367432, 'rewards/rejected': -13.345312118530273, 'rewards/accuracies': 1.0, 'rewards/margins': 21.112499237060547, 'logps/chosen': -736.4000244140625, 'logps/rejected': -286.5, 'logits/chosen': -5.807812690734863, 'logits/rejected': -5.640625, 'epoch': 2.48}
 83%|████████▎ | 760/918 [57:08<10:39,  4.05s/it]
  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|▎         | 2/60 [00:01<00:45,  1.27it/s][A
  5%|▌         | 3/60 [00:03<01:03,  1.12s/it][A
  7%|▋         | 4/60 [00:04<01:13,  1.32s/it][A
  8%|▊         | 5/60 [00:06<01:17,  1.40s/it][A
 10%|█         | 6/60 [00:08<01:20,  1.49s/it][A
 12%|█▏        | 7/60 [00:09<01:20,  1.52s/it][A
 13%|█▎        | 8/60 [00:11<01:22,  1.58s/it][A
 15%|█▌        | 9/60 [00:12<01:21,  1.60s/it][A
 17%|█▋        | 10/60 [00:14<01:19,  1.59s/it][A
 18%|█▊        | 11/60 [00:16<01:20,  1.63s/it][A
 20%|██        | 12/60 [00:17<01:17,  1.62s/it][A
 22%|██▏       | 13/60 [00:19<01:15,  1.61s/it][A
 23%|██▎       | 14/60 [00:21<01:13,  1.60s/it][A
 25%|██▌       | 15/60 [00:22<01:07,  1.50s/it][A
 27%|██▋       | 16/60 [00:23<01:00,  1.37s/it][A
 28%|██▊       | 17/60 [00:24<00:54,  1.28s/it][A
 30%|███       | 18/60 [00:25<00:45,  1.08s/it][A
 32%|███▏      | 19/60 [00:26<00:46,  1.14s/it][A
 33%|███▎      | 20/60 [00:26<00:39,  1.01it/s][A
 35%|███▌      | 21/60 [00:27<00:38,  1.01it/s][A
 37%|███▋      | 22/60 [00:29<00:41,  1.09s/it][A
 38%|███▊      | 23/60 [00:30<00:40,  1.11s/it][A
 40%|████      | 24/60 [00:31<00:38,  1.07s/it][A
 42%|████▏     | 25/60 [00:33<00:43,  1.24s/it][A
 43%|████▎     | 26/60 [00:34<00:44,  1.30s/it][A
 45%|████▌     | 27/60 [00:35<00:37,  1.15s/it][A
 47%|████▋     | 28/60 [00:36<00:34,  1.08s/it][A
 48%|████▊     | 29/60 [00:37<00:34,  1.11s/it][A
 50%|█████     | 30/60 [00:39<00:38,  1.28s/it][A
 52%|█████▏    | 31/60 [00:40<00:39,  1.36s/it][A
 53%|█████▎    | 32/60 [00:42<00:38,  1.38s/it][A
 55%|█████▌    | 33/60 [00:43<00:36,  1.37s/it][A
 57%|█████▋    | 34/60 [00:44<00:31,  1.20s/it][A
 58%|█████▊    | 35/60 [00:45<00:31,  1.27s/it][A
 60%|██████    | 36/60 [00:47<00:31,  1.32s/it][A
 62%|██████▏   | 37/60 [00:47<00:25,  1.10s/it][A
 63%|██████▎   | 38/60 [00:49<00:27,  1.24s/it][A
 65%|██████▌   | 39/60 [00:50<00:25,  1.21s/it][A
 67%|██████▋   | 40/60 [00:51<00:21,  1.08s/it][A
 68%|██████▊   | 41/60 [00:52<00:22,  1.19s/it][A
 70%|███████   | 42/60 [00:54<00:23,  1.31s/it][A
 72%|███████▏  | 43/60 [00:55<00:20,  1.21s/it][A
 73%|███████▎  | 44/60 [00:56<00:20,  1.29s/it][A
 75%|███████▌  | 45/60 [00:57<00:17,  1.14s/it][A
 77%|███████▋  | 46/60 [00:58<00:17,  1.27s/it][A
 78%|███████▊  | 47/60 [01:00<00:18,  1.40s/it][A
 80%|████████  | 48/60 [01:01<00:14,  1.22s/it][A
 82%|████████▏ | 49/60 [01:03<00:14,  1.32s/it][A
 83%|████████▎ | 50/60 [01:04<00:14,  1.43s/it][A
 85%|████████▌ | 51/60 [01:06<00:13,  1.47s/it][A
 87%|████████▋ | 52/60 [01:07<00:11,  1.41s/it][A
 88%|████████▊ | 53/60 [01:08<00:09,  1.31s/it][A
 90%|█████████ | 54/60 [01:10<00:08,  1.39s/it][A
 92%|█████████▏| 55/60 [01:10<00:05,  1.20s/it][A
 93%|█████████▎| 56/60 [01:12<00:05,  1.31s/it][A
 95%|█████████▌| 57/60 [01:14<00:04,  1.39s/it][A
 97%|█████████▋| 58/60 [01:15<00:02,  1.37s/it][A
 98%|█████████▊| 59/60 [01:17<00:01,  1.43s/it][A
100%|██████████| 60/60 [01:18<00:00,  1.49s/it][A                                                 
                                               [A{'eval_loss': 0.1452765315771103, 'eval_runtime': 80.3128, 'eval_samples_per_second': 11.866, 'eval_steps_per_second': 0.747, 'eval_rewards/chosen': 3.9122395515441895, 'eval_rewards/rejected': -4.332584857940674, 'eval_rewards/accuracies': 0.95462965965271, 'eval_rewards/margins': 8.246745109558105, 'eval_logps/chosen': -358.2250061035156, 'eval_logps/rejected': -173.64166259765625, 'eval_logits/chosen': -6.505468845367432, 'eval_logits/rejected': -6.854166507720947, 'epoch': 2.48}
 83%|████████▎ | 760/918 [58:28<10:39,  4.05s/it]
100%|██████████| 60/60 [01:18<00:00,  1.49s/it][A
                                               [A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 83%|████████▎ | 761/918 [58:44<1:22:38, 31.58s/it] 83%|████████▎ | 762/918 [58:48<1:00:35, 23.30s/it] 83%|████████▎ | 763/918 [58:52<45:12, 17.50s/it]   83%|████████▎ | 764/918 [58:56<34:30, 13.45s/it] 83%|████████▎ | 765/918 [59:00<26:58, 10.58s/it] 83%|████████▎ | 766/918 [59:04<21:45,  8.59s/it] 84%|████████▎ | 767/918 [59:08<18:11,  7.23s/it] 84%|████████▎ | 768/918 [59:12<15:37,  6.25s/it] 84%|████████▍ | 769/918 [59:16<13:50,  5.57s/it] 84%|████████▍ | 770/918 [59:20<12:34,  5.10s/it]                                                 {'loss': 0.0528, 'grad_norm': 3.3484292030334473, 'learning_rate': 8.997891687149288e-08, 'rewards/chosen': 9.584375381469727, 'rewards/rejected': -11.493749618530273, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 21.075000762939453, 'logps/chosen': -904.5, 'logps/rejected': -376.1000061035156, 'logits/chosen': -5.803124904632568, 'logits/rejected': -5.646874904632568, 'epoch': 2.52}
 84%|████████▍ | 770/918 [59:20<12:34,  5.10s/it] 84%|████████▍ | 771/918 [59:23<11:39,  4.76s/it] 84%|████████▍ | 772/918 [59:27<11:00,  4.52s/it] 84%|████████▍ | 773/918 [59:32<10:44,  4.44s/it] 84%|████████▍ | 774/918 [59:36<10:19,  4.30s/it] 84%|████████▍ | 775/918 [59:40<10:04,  4.23s/it] 85%|████████▍ | 776/918 [59:44<09:49,  4.15s/it] 85%|████████▍ | 777/918 [59:48<09:50,  4.19s/it] 85%|████████▍ | 778/918 [59:52<09:37,  4.12s/it] 85%|████████▍ | 779/918 [59:56<09:26,  4.08s/it] 85%|████████▍ | 780/918 [1:00:00<09:06,  3.96s/it]                                                   {'loss': 0.0256, 'grad_norm': 0.11643136292695999, 'learning_rate': 8.377304303492845e-08, 'rewards/chosen': 7.10546875, 'rewards/rejected': -14.25, 'rewards/accuracies': 0.981249988079071, 'rewards/margins': 21.337499618530273, 'logps/chosen': -650.9000244140625, 'logps/rejected': -283.5, 'logits/chosen': -6.046875, 'logits/rejected': -5.743750095367432, 'epoch': 2.55}
 85%|████████▍ | 780/918 [1:00:00<09:06,  3.96s/it] 85%|████████▌ | 781/918 [1:00:04<09:03,  3.97s/it] 85%|████████▌ | 782/918 [1:00:08<09:00,  3.97s/it] 85%|████████▌ | 783/918 [1:00:12<09:02,  4.02s/it] 85%|████████▌ | 784/918 [1:00:16<09:04,  4.06s/it] 86%|████████▌ | 785/918 [1:00:20<08:57,  4.04s/it] 86%|████████▌ | 786/918 [1:00:24<08:51,  4.03s/it] 86%|████████▌ | 787/918 [1:00:28<08:45,  4.02s/it] 86%|████████▌ | 788/918 [1:00:32<08:49,  4.08s/it] 86%|████████▌ | 789/918 [1:00:36<08:41,  4.04s/it] 86%|████████▌ | 790/918 [1:00:40<08:42,  4.08s/it]                                                   {'loss': 0.0159, 'grad_norm': 3.8668484687805176, 'learning_rate': 7.792489213986302e-08, 'rewards/chosen': 10.620312690734863, 'rewards/rejected': -11.184374809265137, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 21.8125, 'logps/chosen': -1038.199951171875, 'logps/rejected': -468.6000061035156, 'logits/chosen': -5.621874809265137, 'logits/rejected': -5.75, 'epoch': 2.58}
 86%|████████▌ | 790/918 [1:00:40<08:42,  4.08s/it] 86%|████████▌ | 791/918 [1:00:44<08:34,  4.05s/it] 86%|████████▋ | 792/918 [1:00:48<08:35,  4.09s/it] 86%|████████▋ | 793/918 [1:00:52<08:26,  4.06s/it] 86%|████████▋ | 794/918 [1:00:56<08:19,  4.03s/it] 87%|████████▋ | 795/918 [1:01:00<08:13,  4.01s/it] 87%|████████▋ | 796/918 [1:01:05<08:17,  4.08s/it] 87%|████████▋ | 797/918 [1:01:09<08:10,  4.05s/it] 87%|████████▋ | 798/918 [1:01:13<08:13,  4.11s/it] 87%|████████▋ | 799/918 [1:01:17<08:04,  4.07s/it] 87%|████████▋ | 800/918 [1:01:21<07:57,  4.05s/it]                                                   {'loss': 0.0669, 'grad_norm': 4.9444260597229, 'learning_rate': 7.244982104214295e-08, 'rewards/chosen': 7.340624809265137, 'rewards/rejected': -12.993749618530273, 'rewards/accuracies': 0.981249988079071, 'rewards/margins': 20.337499618530273, 'logps/chosen': -677.5, 'logps/rejected': -314.3999938964844, 'logits/chosen': -6.131249904632568, 'logits/rejected': -5.756249904632568, 'epoch': 2.61}
 87%|████████▋ | 800/918 [1:01:21<07:57,  4.05s/it] 87%|████████▋ | 801/918 [1:01:25<07:51,  4.03s/it] 87%|████████▋ | 802/918 [1:01:29<07:56,  4.10s/it] 87%|████████▋ | 803/918 [1:01:33<07:49,  4.08s/it] 88%|████████▊ | 804/918 [1:01:37<07:41,  4.05s/it] 88%|████████▊ | 805/918 [1:01:41<07:35,  4.03s/it] 88%|████████▊ | 806/918 [1:01:45<07:30,  4.02s/it] 88%|████████▊ | 807/918 [1:01:49<07:22,  3.99s/it] 88%|████████▊ | 808/918 [1:01:53<07:25,  4.05s/it] 88%|████████▊ | 809/918 [1:01:57<07:15,  4.00s/it] 88%|████████▊ | 810/918 [1:02:01<07:11,  3.99s/it]                                                   {'loss': 0.0524, 'grad_norm': 1.9179438352584839, 'learning_rate': 6.736220691484477e-08, 'rewards/chosen': 6.710156440734863, 'rewards/rejected': -14.675000190734863, 'rewards/accuracies': 0.981249988079071, 'rewards/margins': 21.368749618530273, 'logps/chosen': -565.7000122070312, 'logps/rejected': -314.54998779296875, 'logits/chosen': -6.090624809265137, 'logits/rejected': -6.137499809265137, 'epoch': 2.65}
 88%|████████▊ | 810/918 [1:02:01<07:11,  3.99s/it] 88%|████████▊ | 811/918 [1:02:05<07:13,  4.05s/it] 88%|████████▊ | 812/918 [1:02:09<07:07,  4.03s/it] 89%|████████▊ | 813/918 [1:02:13<07:09,  4.09s/it] 89%|████████▊ | 814/918 [1:02:17<07:01,  4.06s/it] 89%|████████▉ | 815/918 [1:02:21<07:00,  4.08s/it] 89%|████████▉ | 816/918 [1:02:25<06:53,  4.05s/it] 89%|████████▉ | 817/918 [1:02:29<06:46,  4.03s/it] 89%|████████▉ | 818/918 [1:02:34<06:50,  4.11s/it] 89%|████████▉ | 819/918 [1:02:38<06:42,  4.07s/it] 89%|████████▉ | 820/918 [1:02:42<06:35,  4.04s/it]                                                   {'loss': 0.0656, 'grad_norm': 8.426911354064941, 'learning_rate': 6.267540949477387e-08, 'rewards/chosen': 8.890625, 'rewards/rejected': -13.09375, 'rewards/accuracies': 0.96875, 'rewards/margins': 21.987499237060547, 'logps/chosen': -860.4000244140625, 'logps/rejected': -424.1000061035156, 'logits/chosen': -5.756249904632568, 'logits/rejected': -5.481249809265137, 'epoch': 2.68}
 89%|████████▉ | 820/918 [1:02:42<06:35,  4.04s/it] 89%|████████▉ | 821/918 [1:02:46<06:29,  4.02s/it] 90%|████████▉ | 822/918 [1:02:50<06:24,  4.01s/it] 90%|████████▉ | 823/918 [1:02:54<06:19,  4.00s/it] 90%|████████▉ | 824/918 [1:02:58<06:21,  4.06s/it] 90%|████████▉ | 825/918 [1:03:01<06:04,  3.92s/it] 90%|████████▉ | 826/918 [1:03:06<06:09,  4.01s/it] 90%|█████████ | 827/918 [1:03:10<06:03,  4.00s/it] 90%|█████████ | 828/918 [1:03:14<05:59,  4.00s/it] 90%|█████████ | 829/918 [1:03:18<05:59,  4.04s/it] 90%|█████████ | 830/918 [1:03:22<06:02,  4.12s/it]                                                   {'loss': 0.0217, 'grad_norm': 0.015079447999596596, 'learning_rate': 5.840173600068237e-08, 'rewards/chosen': 10.7890625, 'rewards/rejected': -12.021875381469727, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 22.8125, 'logps/chosen': -1016.0999755859375, 'logps/rejected': -456.79998779296875, 'logits/chosen': -5.671875, 'logits/rejected': -5.884375095367432, 'epoch': 2.71}
 90%|█████████ | 830/918 [1:03:22<06:02,  4.12s/it] 91%|█████████ | 831/918 [1:03:26<05:55,  4.08s/it] 91%|█████████ | 832/918 [1:03:30<05:52,  4.10s/it] 91%|█████████ | 833/918 [1:03:34<05:45,  4.07s/it] 91%|█████████ | 834/918 [1:03:38<05:39,  4.04s/it] 91%|█████████ | 835/918 [1:03:42<05:33,  4.02s/it] 91%|█████████ | 836/918 [1:03:46<05:28,  4.01s/it] 91%|█████████ | 837/918 [1:03:50<05:23,  4.00s/it] 91%|█████████▏| 838/918 [1:03:54<05:16,  3.96s/it] 91%|█████████▏| 839/918 [1:03:58<05:13,  3.96s/it] 92%|█████████▏| 840/918 [1:04:02<05:09,  3.97s/it]                                                   {'loss': 0.0079, 'grad_norm': 0.015404673293232918, 'learning_rate': 5.455240881533048e-08, 'rewards/chosen': 8.798437118530273, 'rewards/rejected': -18.587499618530273, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 27.362499237060547, 'logps/chosen': -842.2999877929688, 'logps/rejected': -387.29998779296875, 'logits/chosen': -5.871874809265137, 'logits/rejected': -5.809374809265137, 'epoch': 2.75}
 92%|█████████▏| 840/918 [1:04:02<05:09,  3.97s/it] 92%|█████████▏| 841/918 [1:04:06<05:06,  3.98s/it] 92%|█████████▏| 842/918 [1:04:10<05:07,  4.04s/it] 92%|█████████▏| 843/918 [1:04:14<05:01,  4.02s/it] 92%|█████████▏| 844/918 [1:04:18<04:59,  4.05s/it] 92%|█████████▏| 845/918 [1:04:22<04:45,  3.91s/it] 92%|█████████▏| 846/918 [1:04:26<04:42,  3.93s/it] 92%|█████████▏| 847/918 [1:04:30<04:39,  3.94s/it] 92%|█████████▏| 848/918 [1:04:34<04:38,  3.99s/it] 92%|█████████▏| 849/918 [1:04:38<04:34,  3.98s/it] 93%|█████████▎| 850/918 [1:04:42<04:30,  3.98s/it]                                                   {'loss': 0.0507, 'grad_norm': 0.22948074340820312, 'learning_rate': 5.11375360162544e-08, 'rewards/chosen': 8.798437118530273, 'rewards/rejected': -12.646875381469727, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 21.46875, 'logps/chosen': -841.5499877929688, 'logps/rejected': -383.95001220703125, 'logits/chosen': -5.621874809265137, 'logits/rejected': -5.356249809265137, 'epoch': 2.78}
 93%|█████████▎| 850/918 [1:04:42<04:30,  3.98s/it] 93%|█████████▎| 851/918 [1:04:46<04:26,  3.98s/it] 93%|█████████▎| 852/918 [1:04:50<04:22,  3.98s/it] 93%|█████████▎| 853/918 [1:04:54<04:18,  3.97s/it] 93%|█████████▎| 854/918 [1:04:58<04:14,  3.98s/it] 93%|█████████▎| 855/918 [1:05:02<04:09,  3.97s/it] 93%|█████████▎| 856/918 [1:05:06<04:06,  3.97s/it] 93%|█████████▎| 857/918 [1:05:09<03:54,  3.85s/it] 93%|█████████▎| 858/918 [1:05:13<03:54,  3.91s/it] 94%|█████████▎| 859/918 [1:05:17<03:51,  3.93s/it] 94%|█████████▎| 860/918 [1:05:21<03:53,  4.03s/it]                                                   {'loss': 0.0373, 'grad_norm': 1.1168267726898193, 'learning_rate': 4.816608483262634e-08, 'rewards/chosen': 7.595312595367432, 'rewards/rejected': -12.103124618530273, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 19.706249237060547, 'logps/chosen': -676.0499877929688, 'logps/rejected': -330.1000061035156, 'logits/chosen': -5.915625095367432, 'logits/rejected': -5.809374809265137, 'epoch': 2.81}
 94%|█████████▎| 860/918 [1:05:21<03:53,  4.03s/it] 94%|█████████▍| 861/918 [1:05:26<03:51,  4.06s/it] 94%|█████████▍| 862/918 [1:05:30<03:48,  4.08s/it] 94%|█████████▍| 863/918 [1:05:34<03:42,  4.05s/it] 94%|█████████▍| 864/918 [1:05:38<03:38,  4.04s/it] 94%|█████████▍| 865/918 [1:05:42<03:33,  4.02s/it] 94%|█████████▍| 866/918 [1:05:46<03:31,  4.06s/it] 94%|█████████▍| 867/918 [1:05:50<03:26,  4.05s/it] 95%|█████████▍| 868/918 [1:05:54<03:24,  4.09s/it] 95%|█████████▍| 869/918 [1:05:58<03:18,  4.06s/it] 95%|█████████▍| 870/918 [1:06:02<03:15,  4.08s/it]                                                   {'loss': 0.0785, 'grad_norm': 0.09765475988388062, 'learning_rate': 4.564585809790604e-08, 'rewards/chosen': 10.721875190734863, 'rewards/rejected': -11.668749809265137, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 22.375, 'logps/chosen': -1019.2000122070312, 'logps/rejected': -426.29998779296875, 'logits/chosen': -5.609375, 'logits/rejected': -5.559374809265137, 'epoch': 2.84}
 95%|█████████▍| 870/918 [1:06:02<03:15,  4.08s/it] 95%|█████████▍| 871/918 [1:06:06<03:09,  4.03s/it] 95%|█████████▍| 872/918 [1:06:10<03:04,  4.01s/it] 95%|█████████▌| 873/918 [1:06:14<03:00,  4.00s/it] 95%|█████████▌| 874/918 [1:06:18<02:55,  3.99s/it] 95%|█████████▌| 875/918 [1:06:22<02:53,  4.03s/it] 95%|█████████▌| 876/918 [1:06:26<02:48,  4.01s/it] 96%|█████████▌| 877/918 [1:06:30<02:47,  4.09s/it] 96%|█████████▌| 878/918 [1:06:34<02:42,  4.06s/it] 96%|█████████▌| 879/918 [1:06:38<02:37,  4.04s/it] 96%|█████████▌| 880/918 [1:06:43<02:35,  4.10s/it]                                                   {'loss': 0.0197, 'grad_norm': 5.330971717834473, 'learning_rate': 4.358347376011847e-08, 'rewards/chosen': 6.96875, 'rewards/rejected': -15.143750190734863, 'rewards/accuracies': 0.9937499761581421, 'rewards/margins': 22.087499618530273, 'logps/chosen': -641.0, 'logps/rejected': -290.8999938964844, 'logits/chosen': -6.012499809265137, 'logits/rejected': -5.893750190734863, 'epoch': 2.88}
 96%|█████████▌| 880/918 [1:06:43<02:35,  4.10s/it] 96%|█████████▌| 881/918 [1:06:47<02:30,  4.07s/it] 96%|█████████▌| 882/918 [1:06:51<02:25,  4.04s/it] 96%|█████████▌| 883/918 [1:06:55<02:23,  4.11s/it] 96%|█████████▋| 884/918 [1:06:58<02:11,  3.86s/it] 96%|█████████▋| 885/918 [1:07:02<02:09,  3.92s/it] 97%|█████████▋| 886/918 [1:07:06<02:07,  3.99s/it] 97%|█████████▋| 887/918 [1:07:10<02:04,  4.00s/it] 97%|█████████▋| 888/918 [1:07:14<01:59,  4.00s/it] 97%|█████████▋| 889/918 [1:07:18<01:52,  3.87s/it] 97%|█████████▋| 890/918 [1:07:22<01:49,  3.91s/it]                                                   {'loss': 0.0433, 'grad_norm': 4.795151710510254, 'learning_rate': 4.198434750356167e-08, 'rewards/chosen': 9.035937309265137, 'rewards/rejected': -13.128125190734863, 'rewards/accuracies': 0.9750000238418579, 'rewards/margins': 22.193750381469727, 'logps/chosen': -843.9000244140625, 'logps/rejected': -363.6499938964844, 'logits/chosen': -5.987500190734863, 'logits/rejected': -5.800000190734863, 'epoch': 2.91}
 97%|█████████▋| 890/918 [1:07:22<01:49,  3.91s/it] 97%|█████████▋| 891/918 [1:07:26<01:47,  3.99s/it] 97%|█████████▋| 892/918 [1:07:30<01:43,  3.99s/it] 97%|█████████▋| 893/918 [1:07:34<01:39,  3.99s/it] 97%|█████████▋| 894/918 [1:07:38<01:35,  3.98s/it] 97%|█████████▋| 895/918 [1:07:42<01:31,  3.98s/it] 98%|█████████▊| 896/918 [1:07:46<01:29,  4.05s/it] 98%|█████████▊| 897/918 [1:07:50<01:20,  3.85s/it] 98%|█████████▊| 898/918 [1:07:54<01:19,  3.96s/it] 98%|█████████▊| 899/918 [1:07:58<01:16,  4.02s/it] 98%|█████████▊| 900/918 [1:08:02<01:12,  4.02s/it]                                                   {'loss': 0.0092, 'grad_norm': 0.07318267971277237, 'learning_rate': 4.085267852757965e-08, 'rewards/chosen': 9.832812309265137, 'rewards/rejected': -14.159765243530273, 'rewards/accuracies': 1.0, 'rewards/margins': 23.975000381469727, 'logps/chosen': -888.4000244140625, 'logps/rejected': -398.70001220703125, 'logits/chosen': -5.709374904632568, 'logits/rejected': -5.931250095367432, 'epoch': 2.94}
 98%|█████████▊| 900/918 [1:08:02<01:12,  4.02s/it] 98%|█████████▊| 901/918 [1:08:06<01:08,  4.01s/it] 98%|█████████▊| 902/918 [1:08:10<01:04,  4.00s/it] 98%|█████████▊| 903/918 [1:08:14<01:00,  4.00s/it] 98%|█████████▊| 904/918 [1:08:18<00:55,  3.99s/it] 99%|█████████▊| 905/918 [1:08:22<00:52,  4.00s/it] 99%|█████████▊| 906/918 [1:08:26<00:47,  4.00s/it] 99%|█████████▉| 907/918 [1:08:30<00:43,  3.99s/it] 99%|█████████▉| 908/918 [1:08:34<00:40,  4.08s/it] 99%|█████████▉| 909/918 [1:08:38<00:36,  4.05s/it] 99%|█████████▉| 910/918 [1:08:42<00:32,  4.01s/it]                                                   {'loss': 0.0368, 'grad_norm': 13.274035453796387, 'learning_rate': 4.019143851974377e-08, 'rewards/chosen': 8.962499618530273, 'rewards/rejected': -13.487500190734863, 'rewards/accuracies': 0.987500011920929, 'rewards/margins': 22.4375, 'logps/chosen': -810.2000122070312, 'logps/rejected': -302.3500061035156, 'logits/chosen': -5.868750095367432, 'logits/rejected': -5.737500190734863, 'epoch': 2.97}
 99%|█████████▉| 910/918 [1:08:42<00:32,  4.01s/it] 99%|█████████▉| 911/918 [1:08:46<00:28,  4.10s/it] 99%|█████████▉| 912/918 [1:08:50<00:24,  4.06s/it]
  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|▎         | 2/60 [00:01<00:45,  1.28it/s][A
  5%|▌         | 3/60 [00:03<01:03,  1.11s/it][A
  7%|▋         | 4/60 [00:04<01:13,  1.32s/it][A
  8%|▊         | 5/60 [00:06<01:17,  1.41s/it][A
 10%|█         | 6/60 [00:08<01:20,  1.49s/it][A
 12%|█▏        | 7/60 [00:09<01:20,  1.52s/it][A
 13%|█▎        | 8/60 [00:11<01:22,  1.58s/it][A
 15%|█▌        | 9/60 [00:12<01:21,  1.60s/it][A
 17%|█▋        | 10/60 [00:14<01:19,  1.60s/it][A
 18%|█▊        | 11/60 [00:16<01:20,  1.63s/it][A
 20%|██        | 12/60 [00:17<01:17,  1.62s/it][A
 22%|██▏       | 13/60 [00:19<01:15,  1.61s/it][A
 23%|██▎       | 14/60 [00:21<01:13,  1.60s/it][A
 25%|██▌       | 15/60 [00:22<01:07,  1.50s/it][A
 27%|██▋       | 16/60 [00:23<01:00,  1.37s/it][A
 28%|██▊       | 17/60 [00:24<00:54,  1.28s/it][A
 30%|███       | 18/60 [00:25<00:45,  1.08s/it][A
 32%|███▏      | 19/60 [00:26<00:46,  1.14s/it][A
 33%|███▎      | 20/60 [00:26<00:39,  1.01it/s][A
 35%|███▌      | 21/60 [00:27<00:38,  1.00it/s][A
 37%|███▋      | 22/60 [00:29<00:41,  1.09s/it][A
 38%|███▊      | 23/60 [00:30<00:40,  1.11s/it][A
 40%|████      | 24/60 [00:31<00:38,  1.07s/it][A
 42%|████▏     | 25/60 [00:33<00:43,  1.24s/it][A
 43%|████▎     | 26/60 [00:34<00:44,  1.30s/it][A
 45%|████▌     | 27/60 [00:35<00:37,  1.15s/it][A
 47%|████▋     | 28/60 [00:36<00:34,  1.08s/it][A
 48%|████▊     | 29/60 [00:37<00:34,  1.11s/it][A
 50%|█████     | 30/60 [00:39<00:38,  1.28s/it][A
 52%|█████▏    | 31/60 [00:40<00:39,  1.36s/it][A
 53%|█████▎    | 32/60 [00:42<00:38,  1.38s/it][A
 55%|█████▌    | 33/60 [00:43<00:36,  1.37s/it][A
 57%|█████▋    | 34/60 [00:44<00:31,  1.20s/it][A
 58%|█████▊    | 35/60 [00:45<00:31,  1.27s/it][A
 60%|██████    | 36/60 [00:47<00:31,  1.32s/it][A
 62%|██████▏   | 37/60 [00:47<00:25,  1.10s/it][A
 63%|██████▎   | 38/60 [00:49<00:27,  1.24s/it][A
 65%|██████▌   | 39/60 [00:50<00:25,  1.20s/it][A
 67%|██████▋   | 40/60 [00:51<00:21,  1.08s/it][A
 68%|██████▊   | 41/60 [00:52<00:22,  1.19s/it][A
 70%|███████   | 42/60 [00:54<00:23,  1.31s/it][A
 72%|███████▏  | 43/60 [00:55<00:20,  1.20s/it][A
 73%|███████▎  | 44/60 [00:56<00:20,  1.29s/it][A
 75%|███████▌  | 45/60 [00:57<00:17,  1.13s/it][A
 77%|███████▋  | 46/60 [00:58<00:17,  1.27s/it][A
 78%|███████▊  | 47/60 [01:00<00:18,  1.40s/it][A
 80%|████████  | 48/60 [01:01<00:14,  1.22s/it][A
 82%|████████▏ | 49/60 [01:03<00:14,  1.33s/it][A
 83%|████████▎ | 50/60 [01:04<00:14,  1.44s/it][A
 85%|████████▌ | 51/60 [01:06<00:13,  1.47s/it][A
 87%|████████▋ | 52/60 [01:07<00:11,  1.41s/it][A
 88%|████████▊ | 53/60 [01:08<00:09,  1.31s/it][A
 90%|█████████ | 54/60 [01:10<00:08,  1.39s/it][A
 92%|█████████▏| 55/60 [01:10<00:05,  1.20s/it][A
 93%|█████████▎| 56/60 [01:12<00:05,  1.31s/it][A
 95%|█████████▌| 57/60 [01:14<00:04,  1.39s/it][A
 97%|█████████▋| 58/60 [01:15<00:02,  1.37s/it][A
 98%|█████████▊| 59/60 [01:17<00:01,  1.43s/it][A
100%|██████████| 60/60 [01:18<00:00,  1.49s/it][A                                                   
                                               [A{'eval_loss': 0.15264956653118134, 'eval_runtime': 80.3134, 'eval_samples_per_second': 11.866, 'eval_steps_per_second': 0.747, 'eval_rewards/chosen': 3.8479816913604736, 'eval_rewards/rejected': -4.547265529632568, 'eval_rewards/accuracies': 0.9525462985038757, 'eval_rewards/margins': 8.393619537353516, 'eval_logps/chosen': -358.48333740234375, 'eval_logps/rejected': -174.71665954589844, 'eval_logits/chosen': -6.5567708015441895, 'eval_logits/rejected': -6.909895896911621, 'epoch': 2.98}
 99%|█████████▉| 912/918 [1:10:11<00:24,  4.06s/it]
100%|██████████| 60/60 [01:18<00:00,  1.49s/it][A
                                               [A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
 99%|█████████▉| 913/918 [1:10:26<02:38, 31.68s/it]100%|█████████▉| 914/918 [1:10:30<01:33, 23.37s/it]100%|█████████▉| 915/918 [1:10:34<00:52, 17.55s/it]100%|█████████▉| 916/918 [1:10:38<00:26, 13.48s/it]100%|█████████▉| 917/918 [1:10:42<00:10, 10.63s/it]100%|██████████| 918/918 [1:10:46<00:00,  8.63s/it]                                                   {'train_runtime': 4273.9922, 'train_samples_per_second': 3.432, 'train_steps_per_second': 0.215, 'train_loss': 0.16170918276789142, 'epoch': 3.0}
100%|██████████| 918/918 [1:11:05<00:00,  8.63s/it]100%|██████████| 918/918 [1:11:05<00:00,  4.65s/it]
Training complete
Saving model
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mCurri-0-DPO_r-64_lr-4e-07_e-3_b-0.2[0m at: [34mhttps://wandb.ai/slolama/GaMS-9B-Translation-DPO/runs/fsfxy4dv[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250610_005455-fsfxy4dv/logs[0m
[rank0]:[W610 02:06:07.124457016 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 0 ---
