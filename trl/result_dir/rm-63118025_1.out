cpu-bind=MASK - gn02, task  1  0 [2368769]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 1 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 1     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63118025     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=1e-6 --total_epochs=3 --beta=0.1 --curriculum_stage=2
-------------------------------------------
[2025-06-12 21:08:06,058] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 21:08:07.546000 2368820 torch/distributed/run.py:792] 
W0612 21:08:07.546000 2368820 torch/distributed/run.py:792] *****************************************
W0612 21:08:07.546000 2368820 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 21:08:07.546000 2368820 torch/distributed/run.py:792] *****************************************
[2025-06-12 21:08:56,288] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:08:56,303] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:08:56,347] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:08:56,357] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
World size: 12
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
[2025-06-12 21:09:00,858] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 6690
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-12 21:09:00,861] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 21:09:00,913] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 21:09:00,918] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curriculum-1
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:28,  9.42s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:28,  9.39s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:29,  9.74s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:28,  9.41s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:16,  8.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:16,  8.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:16,  8.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:16,  8.34s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:08,  8.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:08,  8.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.24s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:08,  8.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  6.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.10s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  6.71s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  7.36s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  6.76s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  7.44s/it]
Loaded model
Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  6.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  7.37s/it]
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank5]:[W612 21:09:33.879644525 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   8%|▊         | 512/6690 [00:00<00:01, 5079.22 examples/s][rank6]:[W612 21:09:34.467936310 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W612 21:09:34.475774569 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  16%|█▌        | 1073/6690 [00:00<00:01, 5369.60 examples/s]Extracting prompt in train dataset:  25%|██▍       | 1640/6690 [00:00<00:00, 5484.84 examples/s]Extracting prompt in train dataset:  33%|███▎      | 2220/6690 [00:00<00:00, 5573.23 examples/s]Extracting prompt in train dataset:  46%|████▌     | 3060/6690 [00:00<00:00, 5568.94 examples/s]Extracting prompt in train dataset:  54%|█████▍    | 3630/6690 [00:00<00:00, 5601.10 examples/s]Extracting prompt in train dataset:  63%|██████▎   | 4200/6690 [00:00<00:00, 5624.13 examples/s]Extracting prompt in train dataset:  75%|███████▌  | 5027/6690 [00:00<00:00, 5557.24 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 5870/6690 [00:01<00:00, 5545.06 examples/s]Extracting prompt in train dataset:  96%|█████████▋| 6450/6690 [00:01<00:00, 5580.51 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5487.53 examples/s]
Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 293/6690 [00:00<00:02, 2905.05 examples/s]Applying chat template to train dataset:   9%|▉         | 611/6690 [00:00<00:01, 3062.80 examples/s]Applying chat template to train dataset:  14%|█▍        | 930/6690 [00:00<00:01, 3118.16 examples/s]Applying chat template to train dataset:  19%|█▊        | 1247/6690 [00:00<00:01, 3134.76 examples/s]Applying chat template to train dataset:  23%|██▎       | 1564/6690 [00:00<00:01, 3142.75 examples/s]Applying chat template to train dataset:  28%|██▊       | 1882/6690 [00:00<00:01, 3152.29 examples/s]Applying chat template to train dataset:  33%|███▎      | 2200/6690 [00:00<00:01, 3156.80 examples/s]Applying chat template to train dataset:  40%|███▉      | 2674/6690 [00:00<00:01, 3151.84 examples/s]Applying chat template to train dataset:  45%|████▍     | 2991/6690 [00:00<00:01, 3154.55 examples/s]Applying chat template to train dataset:  49%|████▉     | 3310/6690 [00:01<00:01, 3158.84 examples/s]Applying chat template to train dataset:  54%|█████▍    | 3627/6690 [00:01<00:00, 3160.67 examples/s]Applying chat template to train dataset:  59%|█████▉    | 3945/6690 [00:01<00:00, 3164.29 examples/s]Applying chat template to train dataset:  64%|██████▎   | 4263/6690 [00:01<00:00, 3166.42 examples/s]Applying chat template to train dataset:  71%|███████   | 4720/6690 [00:01<00:00, 3113.98 examples/s]Applying chat template to train dataset:  75%|███████▌  | 5039/6690 [00:01<00:00, 3129.12 examples/s]Applying chat template to train dataset:  80%|████████  | 5355/6690 [00:01<00:00, 3133.86 examples/s]Applying chat template to train dataset:  85%|████████▍ | 5672/6690 [00:01<00:00, 3141.91 examples/s]Applying chat template to train dataset:  90%|████████▉ | 5990/6690 [00:01<00:00, 3147.06 examples/s]Applying chat template to train dataset:  94%|█████████▍| 6310/6690 [00:02<00:00, 3151.89 examples/s]Applying chat template to train dataset:  99%|█████████▉| 6626/6690 [00:02<00:00, 3147.96 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3132.69 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 40/6690 [00:00<00:17, 384.18 examples/s]Tokenizing train dataset:   1%|▏         | 89/6690 [00:00<00:15, 438.57 examples/s]Tokenizing train dataset:   2%|▏         | 157/6690 [00:00<00:14, 441.97 examples/s]Tokenizing train dataset:   3%|▎         | 204/6690 [00:00<00:14, 450.14 examples/s]Tokenizing train dataset:   4%|▍         | 254/6690 [00:00<00:13, 460.94 examples/s]Tokenizing train dataset:   5%|▍         | 319/6690 [00:00<00:14, 445.06 examples/s]Tokenizing train dataset:   6%|▌         | 370/6690 [00:00<00:13, 460.15 examples/s]Tokenizing train dataset:   7%|▋         | 442/6690 [00:00<00:13, 466.57 examples/s]Tokenizing train dataset:   7%|▋         | 491/6690 [00:01<00:13, 471.36 examples/s]Tokenizing train dataset:   8%|▊         | 540/6690 [00:01<00:13, 470.31 examples/s]Tokenizing train dataset:   9%|▉         | 589/6690 [00:01<00:12, 472.08 examples/s]Tokenizing train dataset:  10%|▉         | 641/6690 [00:01<00:12, 478.75 examples/s]Tokenizing train dataset:  10%|█         | 690/6690 [00:01<00:12, 476.15 examples/s]Tokenizing train dataset:  11%|█         | 743/6690 [00:01<00:12, 488.16 examples/s]Tokenizing train dataset:  12%|█▏        | 793/6690 [00:01<00:12, 486.75 examples/s]Tokenizing train dataset:  13%|█▎        | 866/6690 [00:01<00:12, 477.06 examples/s]Tokenizing train dataset:  14%|█▍        | 926/6690 [00:02<00:12, 448.77 examples/s]Tokenizing train dataset:  15%|█▍        | 973/6690 [00:02<00:12, 453.38 examples/s]Tokenizing train dataset:  16%|█▌        | 1040/6690 [00:02<00:12, 448.10 examples/s]Tokenizing train dataset:  16%|█▋        | 1088/6690 [00:02<00:12, 453.76 examples/s]Tokenizing train dataset:  17%|█▋        | 1155/6690 [00:02<00:12, 446.12 examples/s]Tokenizing train dataset:  18%|█▊        | 1203/6690 [00:02<00:12, 448.31 examples/s]Tokenizing train dataset:  19%|█▉        | 1268/6690 [00:02<00:12, 439.20 examples/s]Tokenizing train dataset:  20%|█▉        | 1317/6690 [00:02<00:11, 449.70 examples/s]Tokenizing train dataset:  20%|██        | 1363/6690 [00:02<00:11, 448.83 examples/s]Tokenizing train dataset:  21%|██▏       | 1422/6690 [00:03<00:12, 427.51 examples/s]Tokenizing train dataset:  22%|██▏       | 1485/6690 [00:03<00:12, 423.68 examples/s]Tokenizing train dataset:  23%|██▎       | 1540/6690 [00:03<00:11, 449.69 examples/s]Tokenizing train dataset:  24%|██▍       | 1604/6690 [00:03<00:11, 436.48 examples/s]Tokenizing train dataset:  25%|██▍       | 1649/6690 [00:03<00:11, 434.88 examples/s]Tokenizing train dataset:  25%|██▌       | 1695/6690 [00:03<00:11, 436.52 examples/s]Tokenizing train dataset:  26%|██▌       | 1743/6690 [00:03<00:11, 443.16 examples/s]Tokenizing train dataset:  27%|██▋       | 1788/6690 [00:03<00:11, 444.22 examples/s]Tokenizing train dataset:  27%|██▋       | 1835/6690 [00:04<00:10, 449.53 examples/s]Tokenizing train dataset:  28%|██▊       | 1901/6690 [00:04<00:10, 438.24 examples/s]Tokenizing train dataset:  29%|██▉       | 1948/6690 [00:04<00:10, 443.14 examples/s]Tokenizing train dataset:  30%|██▉       | 2006/6690 [00:04<00:11, 418.43 examples/s]Tokenizing train dataset:  31%|███       | 2050/6690 [00:04<00:11, 419.59 examples/s]Tokenizing train dataset:  31%|███▏      | 2096/6690 [00:04<00:10, 427.19 examples/s]Tokenizing train dataset:  32%|███▏      | 2144/6690 [00:04<00:10, 437.94 examples/s]Tokenizing train dataset:  33%|███▎      | 2194/6690 [00:04<00:09, 452.55 examples/s]Tokenizing train dataset:  34%|███▎      | 2245/6690 [00:04<00:09, 466.86 examples/s]Tokenizing train dataset:  34%|███▍      | 2306/6690 [00:05<00:09, 439.60 examples/s]Tokenizing train dataset:  35%|███▌      | 2354/6690 [00:05<00:09, 445.25 examples/s]Tokenizing train dataset:  36%|███▌      | 2400/6690 [00:05<00:09, 443.45 examples/s]Tokenizing train dataset:  37%|███▋      | 2470/6690 [00:05<00:09, 449.25 examples/s]Tokenizing train dataset:  38%|███▊      | 2517/6690 [00:05<00:09, 449.71 examples/s]Tokenizing train dataset:  38%|███▊      | 2569/6690 [00:05<00:08, 466.71 examples/s]Tokenizing train dataset:  39%|███▉      | 2617/6690 [00:05<00:08, 467.63 examples/s]Tokenizing train dataset:  40%|████      | 2679/6690 [00:05<00:09, 444.31 examples/s]Tokenizing train dataset:  41%|████      | 2740/6690 [00:06<00:09, 427.08 examples/s]Tokenizing train dataset:  42%|████▏     | 2803/6690 [00:06<00:09, 419.53 examples/s]Tokenizing train dataset:  43%|████▎     | 2847/6690 [00:06<00:09, 422.53 examples/s]Tokenizing train dataset:  43%|████▎     | 2894/6690 [00:06<00:08, 427.97 examples/s]Tokenizing train dataset:  44%|████▍     | 2946/6690 [00:06<00:08, 451.35 examples/s]Tokenizing train dataset:  45%|████▍     | 2994/6690 [00:06<00:08, 456.96 examples/s]Tokenizing train dataset:  46%|████▌     | 3044/6690 [00:06<00:07, 461.89 examples/s]Tokenizing train dataset:  46%|████▋     | 3100/6690 [00:06<00:07, 484.34 examples/s]Tokenizing train dataset:  47%|████▋     | 3163/6690 [00:07<00:07, 457.07 examples/s]Tokenizing train dataset:  48%|████▊     | 3210/6690 [00:07<00:07, 457.31 examples/s]Tokenizing train dataset:  49%|████▊     | 3257/6690 [00:07<00:07, 459.24 examples/s]Tokenizing train dataset:  50%|████▉     | 3318/6690 [00:07<00:07, 425.82 examples/s]Tokenizing train dataset:  50%|█████     | 3363/6690 [00:07<00:07, 425.61 examples/s]Tokenizing train dataset:  51%|█████     | 3422/6690 [00:07<00:07, 410.70 examples/s]Tokenizing train dataset:  52%|█████▏    | 3490/6690 [00:07<00:07, 417.43 examples/s]Tokenizing train dataset:  53%|█████▎    | 3549/6690 [00:07<00:07, 403.92 examples/s]Tokenizing train dataset:  54%|█████▍    | 3607/6690 [00:08<00:07, 395.40 examples/s]Tokenizing train dataset:  55%|█████▍    | 3653/6690 [00:08<00:07, 403.98 examples/s]Tokenizing train dataset:  55%|█████▌    | 3699/6690 [00:08<00:07, 412.81 examples/s]Tokenizing train dataset:  56%|█████▌    | 3754/6690 [00:08<00:07, 391.00 examples/s]Tokenizing train dataset:  57%|█████▋    | 3799/6690 [00:08<00:07, 402.03 examples/s]Tokenizing train dataset:  57%|█████▋    | 3845/6690 [00:08<00:06, 413.70 examples/s]Tokenizing train dataset:  58%|█████▊    | 3893/6690 [00:08<00:06, 428.13 examples/s]Tokenizing train dataset:  59%|█████▉    | 3942/6690 [00:08<00:06, 440.16 examples/s]Tokenizing train dataset:  60%|█████▉    | 3991/6690 [00:09<00:06, 448.81 examples/s]Tokenizing train dataset:  61%|██████    | 4058/6690 [00:09<00:05, 444.36 examples/s]Tokenizing train dataset:  61%|██████▏   | 4103/6690 [00:09<00:05, 439.66 examples/s]Tokenizing train dataset:  62%|██████▏   | 4148/6690 [00:09<00:05, 442.17 examples/s]Tokenizing train dataset:  63%|██████▎   | 4197/6690 [00:09<00:05, 452.05 examples/s]Tokenizing train dataset:  64%|██████▎   | 4261/6690 [00:09<00:05, 438.99 examples/s]Tokenizing train dataset:  64%|██████▍   | 4308/6690 [00:09<00:05, 442.49 examples/s]Tokenizing train dataset:  65%|██████▌   | 4369/6690 [00:09<00:05, 427.58 examples/s]Tokenizing train dataset:  66%|██████▋   | 4437/6690 [00:10<00:05, 433.12 examples/s]Tokenizing train dataset:  67%|██████▋   | 4498/6690 [00:10<00:05, 422.55 examples/s]Tokenizing train dataset:  68%|██████▊   | 4546/6690 [00:10<00:04, 434.90 examples/s]Tokenizing train dataset:  69%|██████▉   | 4618/6690 [00:10<00:04, 446.09 examples/s]Tokenizing train dataset:  70%|██████▉   | 4666/6690 [00:10<00:04, 450.04 examples/s]Tokenizing train dataset:  71%|███████   | 4736/6690 [00:10<00:04, 451.74 examples/s]Tokenizing train dataset:  72%|███████▏  | 4795/6690 [00:10<00:04, 423.97 examples/s]Tokenizing train dataset:  72%|███████▏  | 4844/6690 [00:10<00:04, 436.22 examples/s]Tokenizing train dataset:  73%|███████▎  | 4907/6690 [00:11<00:04, 426.14 examples/s]Tokenizing train dataset:  74%|███████▍  | 4952/6690 [00:11<00:04, 428.89 examples/s]Tokenizing train dataset:  75%|███████▍  | 4997/6690 [00:11<00:03, 428.42 examples/s]Tokenizing train dataset:  76%|███████▌  | 5060/6690 [00:11<00:03, 419.27 examples/s]Tokenizing train dataset:  76%|███████▋  | 5103/6690 [00:11<00:03, 419.93 examples/s]Tokenizing train dataset:  77%|███████▋  | 5169/6690 [00:11<00:03, 420.22 examples/s]Tokenizing train dataset:  78%|███████▊  | 5217/6690 [00:11<00:03, 431.29 examples/s]Tokenizing train dataset:  79%|███████▉  | 5282/6690 [00:12<00:03, 428.41 examples/s]Tokenizing train dataset:  80%|███████▉  | 5341/6690 [00:12<00:03, 416.23 examples/s]Tokenizing train dataset:  81%|████████  | 5394/6690 [00:12<00:02, 441.25 examples/s]Tokenizing train dataset:  82%|████████▏ | 5460/6690 [00:12<00:02, 434.94 examples/s]Tokenizing train dataset:  83%|████████▎ | 5526/6690 [00:12<00:02, 432.02 examples/s]Tokenizing train dataset:  84%|████████▎ | 5587/6690 [00:12<00:02, 422.32 examples/s]Tokenizing train dataset:  84%|████████▍ | 5630/6690 [00:12<00:02, 422.34 examples/s]Tokenizing train dataset:  85%|████████▍ | 5676/6690 [00:12<00:02, 430.13 examples/s]Tokenizing train dataset:  86%|████████▌ | 5743/6690 [00:13<00:02, 432.50 examples/s]Tokenizing train dataset:  87%|████████▋ | 5788/6690 [00:13<00:02, 431.76 examples/s]Tokenizing train dataset:  87%|████████▋ | 5851/6690 [00:13<00:01, 421.55 examples/s]Tokenizing train dataset:  88%|████████▊ | 5899/6690 [00:13<00:01, 434.23 examples/s]Tokenizing train dataset:  89%|████████▉ | 5943/6690 [00:13<00:01, 432.13 examples/s]Tokenizing train dataset:  90%|████████▉ | 6010/6690 [00:13<00:01, 432.66 examples/s]Tokenizing train dataset:  91%|█████████ | 6075/6690 [00:13<00:01, 429.18 examples/s]Tokenizing train dataset:  92%|█████████▏| 6128/6690 [00:13<00:01, 449.53 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6690 [00:14<00:01, 440.59 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:14<00:01, 406.14 examples/s]Tokenizing train dataset:  94%|█████████▍| 6298/6690 [00:14<00:00, 420.91 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 408.24 examples/s]Tokenizing train dataset:  96%|█████████▌| 6403/6690 [00:14<00:00, 409.43 examples/s]Tokenizing train dataset:  96%|█████████▋| 6451/6690 [00:14<00:00, 422.68 examples/s]Tokenizing train dataset:  97%|█████████▋| 6496/6690 [00:14<00:00, 427.45 examples/s]Tokenizing train dataset:  98%|█████████▊| 6559/6690 [00:15<00:00, 421.05 examples/s]Tokenizing train dataset:  99%|█████████▊| 6602/6690 [00:15<00:00, 418.49 examples/s]Tokenizing train dataset: 100%|█████████▉| 6660/6690 [00:15<00:00, 404.44 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 436.18 examples/s]
[rank4]:[W612 21:09:54.024504701 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 571/953 [00:00<00:00, 5652.65 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5608.88 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5613.96 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5558.81 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5643.78 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1156/6690 [00:00<00:00, 5750.72 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1150/6690 [00:00<00:00, 5695.57 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1147/6690 [00:00<00:00, 5679.07 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1740/6690 [00:00<00:00, 5786.42 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1736/6690 [00:00<00:00, 5762.40 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1721/6690 [00:00<00:00, 5703.31 examples/s]Extracting prompt in train dataset:  36%|███▋      | 2429/6690 [00:00<00:00, 5214.54 examples/s]Extracting prompt in train dataset:  37%|███▋      | 2460/6690 [00:00<00:00, 5273.27 examples/s]Extracting prompt in train dataset:  37%|███▋      | 2450/6690 [00:00<00:00, 5237.64 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  45%|████▍     | 3001/6690 [00:00<00:00, 5373.18 examples/s]Extracting prompt in train dataset:  46%|████▌     | 3050/6690 [00:00<00:00, 5461.82 examples/s]Extracting prompt in train dataset:  45%|████▌     | 3033/6690 [00:00<00:00, 5424.01 examples/s]Applying chat template to eval dataset:  33%|███▎      | 312/953 [00:00<00:00, 3091.12 examples/s]Extracting prompt in train dataset:  54%|█████▎    | 3580/6690 [00:00<00:00, 5491.04 examples/s]Extracting prompt in train dataset:  54%|█████▍    | 3613/6690 [00:00<00:00, 5540.23 examples/s]Extracting prompt in train dataset:  54%|█████▍    | 3640/6690 [00:00<00:00, 5586.13 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 637/953 [00:00<00:00, 3176.58 examples/s]Extracting prompt in train dataset:  62%|██████▏   | 4160/6690 [00:00<00:00, 5574.86 examples/s]Extracting prompt in train dataset:  63%|██████▎   | 4197/6690 [00:00<00:00, 5631.99 examples/s]Extracting prompt in train dataset:  63%|██████▎   | 4230/6690 [00:00<00:00, 5672.22 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3162.43 examples/s]
Extracting prompt in train dataset:  75%|███████▍  | 4992/6690 [00:00<00:00, 5562.71 examples/s]Extracting prompt in train dataset:  75%|███████▌  | 5037/6690 [00:00<00:00, 5610.74 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 5088/6690 [00:00<00:00, 5662.16 examples/s]Extracting prompt in train dataset:  84%|████████▍ | 5616/6690 [00:01<00:00, 5659.54 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 5570/6690 [00:01<00:00, 5611.17 examples/s]Extracting prompt in train dataset:  85%|████████▍ | 5678/6690 [00:01<00:00, 5715.40 examples/s]Extracting prompt in train dataset:  94%|█████████▎| 6269/6690 [00:01<00:00, 5762.10 examples/s]Extracting prompt in train dataset:  96%|█████████▋| 6451/6690 [00:01<00:00, 5622.63 examples/s]Extracting prompt in train dataset:  96%|█████████▌| 6410/6690 [00:01<00:00, 5592.25 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5624.71 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5545.04 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5505.73 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 321.00 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:03, 290.48 examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing eval dataset:  12%|█▏        | 119/953 [00:00<00:03, 275.54 examples/s]Applying chat template to train dataset:   4%|▍         | 297/6690 [00:00<00:02, 2940.43 examples/s]Applying chat template to train dataset:   4%|▍         | 296/6690 [00:00<00:02, 2932.05 examples/s]Applying chat template to train dataset:   4%|▍         | 299/6690 [00:00<00:02, 2963.79 examples/s]Applying chat template to train dataset:   9%|▉         | 621/6690 [00:00<00:01, 3112.05 examples/s]Applying chat template to train dataset:   9%|▉         | 619/6690 [00:00<00:01, 3103.27 examples/s]Applying chat template to train dataset:   9%|▉         | 623/6690 [00:00<00:01, 3123.43 examples/s]Tokenizing eval dataset:  17%|█▋        | 159/953 [00:00<00:02, 265.95 examples/s]Applying chat template to train dataset:  14%|█▍        | 948/6690 [00:00<00:01, 3179.80 examples/s]Applying chat template to train dataset:  14%|█▍        | 942/6690 [00:00<00:01, 3156.23 examples/s]Applying chat template to train dataset:  14%|█▍        | 950/6690 [00:00<00:01, 3179.45 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 252.87 examples/s]Applying chat template to train dataset:  19%|█▉        | 1271/6690 [00:00<00:01, 3193.99 examples/s]Applying chat template to train dataset:  19%|█▉        | 1264/6690 [00:00<00:01, 3176.38 examples/s]Applying chat template to train dataset:  19%|█▉        | 1274/6690 [00:00<00:01, 3199.89 examples/s]Tokenizing eval dataset:  24%|██▍       | 230/953 [00:00<00:02, 275.74 examples/s]Applying chat template to train dataset:  24%|██▍       | 1597/6690 [00:00<00:01, 3215.65 examples/s]Applying chat template to train dataset:  24%|██▎       | 1587/6690 [00:00<00:01, 3193.59 examples/s]Applying chat template to train dataset:  24%|██▍       | 1600/6690 [00:00<00:01, 3212.84 examples/s]Tokenizing eval dataset:  31%|███       | 295/953 [00:00<00:01, 371.67 examples/s]Applying chat template to train dataset:  29%|██▉       | 1924/6690 [00:00<00:01, 3227.90 examples/s]Applying chat template to train dataset:  29%|██▊       | 1910/6690 [00:00<00:01, 3201.41 examples/s]Applying chat template to train dataset:  29%|██▉       | 1926/6690 [00:00<00:01, 3226.95 examples/s]Tokenizing eval dataset:  38%|███▊      | 359/953 [00:01<00:01, 443.52 examples/s]Applying chat template to train dataset:  34%|███▎      | 2250/6690 [00:00<00:01, 3225.30 examples/s]Applying chat template to train dataset:  33%|███▎      | 2234/6690 [00:00<00:01, 3210.22 examples/s]Applying chat template to train dataset:  34%|███▎      | 2250/6690 [00:00<00:01, 3221.21 examples/s]Tokenizing eval dataset:  44%|████▍     | 420/953 [00:01<00:01, 488.24 examples/s]Applying chat template to train dataset:  39%|███▊      | 2576/6690 [00:00<00:01, 3231.52 examples/s]Applying chat template to train dataset:  38%|███▊      | 2574/6690 [00:00<00:01, 3225.11 examples/s]Applying chat template to train dataset:  41%|████      | 2715/6690 [00:00<00:01, 3204.17 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 540.74 examples/s]Applying chat template to train dataset:  43%|████▎     | 2904/6690 [00:00<00:01, 3244.56 examples/s]Applying chat template to train dataset:  43%|████▎     | 2899/6690 [00:00<00:01, 3231.87 examples/s]Applying chat template to train dataset:  45%|████▌     | 3038/6690 [00:00<00:01, 3208.53 examples/s]Tokenizing eval dataset:  58%|█████▊    | 554/953 [00:01<00:00, 573.67 examples/s]Applying chat template to train dataset:  48%|████▊     | 3230/6690 [00:01<00:01, 3247.68 examples/s]Applying chat template to train dataset:  48%|████▊     | 3224/6690 [00:01<00:01, 3234.05 examples/s]Applying chat template to train dataset:  50%|█████     | 3362/6690 [00:01<00:01, 3211.21 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 583.73 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3548/6690 [00:01<00:00, 3234.04 examples/s]Applying chat template to train dataset:  56%|█████▌    | 3717/6690 [00:01<00:00, 3245.58 examples/s]Applying chat template to train dataset:  55%|█████▌    | 3685/6690 [00:01<00:00, 3211.93 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 594.81 examples/s]Applying chat template to train dataset:  58%|█████▊    | 3873/6690 [00:01<00:00, 3232.08 examples/s]Applying chat template to train dataset:  60%|██████    | 4044/6690 [00:01<00:00, 3245.34 examples/s]Applying chat template to train dataset:  60%|█████▉    | 4009/6690 [00:01<00:00, 3217.70 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4199/6690 [00:01<00:00, 3238.10 examples/s]Applying chat template to train dataset:  65%|██████▌   | 4370/6690 [00:01<00:00, 3243.71 examples/s]Tokenizing eval dataset:  80%|████████  | 766/953 [00:01<00:00, 574.89 examples/s]Applying chat template to train dataset:  65%|██████▍   | 4332/6690 [00:01<00:00, 3215.99 examples/s]Applying chat template to train dataset:  70%|██████▉   | 4666/6690 [00:01<00:00, 3184.48 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4838/6690 [00:01<00:00, 3194.41 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 540.15 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4797/6690 [00:01<00:00, 3167.34 examples/s]Applying chat template to train dataset:  75%|███████▍  | 4990/6690 [00:01<00:00, 3194.10 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5163/6690 [00:01<00:00, 3202.42 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5119/6690 [00:01<00:00, 3178.87 examples/s]Tokenizing eval dataset:  96%|█████████▌| 911/953 [00:02<00:00, 516.25 examples/s]Applying chat template to train dataset:  79%|███████▉  | 5315/6690 [00:01<00:00, 3205.97 examples/s]Applying chat template to train dataset:  82%|████████▏ | 5488/6690 [00:01<00:00, 3214.59 examples/s]Applying chat template to train dataset:  81%|████████▏ | 5441/6690 [00:01<00:00, 3185.84 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 453.33 examples/s]
Applying chat template to train dataset:  84%|████████▍ | 5639/6690 [00:01<00:00, 3215.49 examples/s]Applying chat template to train dataset:  87%|████████▋ | 5813/6690 [00:01<00:00, 3219.23 examples/s]Applying chat template to train dataset:  86%|████████▌ | 5764/6690 [00:01<00:00, 3194.90 examples/s]Applying chat template to train dataset:  89%|████████▉ | 5962/6690 [00:01<00:00, 3217.22 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6145/6690 [00:01<00:00, 3246.71 examples/s]Applying chat template to train dataset:  91%|█████████ | 6086/6690 [00:01<00:00, 3200.04 examples/s]Applying chat template to train dataset:  94%|█████████▍| 6287/6690 [00:01<00:00, 3222.42 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6476/6690 [00:02<00:00, 3262.56 examples/s]Applying chat template to train dataset:  96%|█████████▌| 6407/6690 [00:02<00:00, 3201.53 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3218.67 examples/s]
Applying chat template to train dataset:  99%|█████████▉| 6610/6690 [00:02<00:00, 3220.41 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3204.35 examples/s]
Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3184.73 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 43/6690 [00:00<00:15, 417.08 examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:15, 421.42 examples/s]Tokenizing train dataset:   1%|          | 43/6690 [00:00<00:15, 420.35 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:15, 439.25 examples/s]Tokenizing train dataset:   1%|▏         | 91/6690 [00:00<00:14, 441.56 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:14, 440.46 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 441.25 examples/s]Tokenizing train dataset:   2%|▏         | 137/6690 [00:00<00:14, 447.44 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 441.50 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 473.36 examples/s]Tokenizing train dataset:   3%|▎         | 190/6690 [00:00<00:13, 473.74 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 473.00 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 472.62 examples/s]Tokenizing train dataset:   4%|▎         | 238/6690 [00:00<00:13, 474.55 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 472.17 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 470.31 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 469.54 examples/s]Tokenizing train dataset:   5%|▍         | 307/6690 [00:00<00:13, 460.83 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 460.56 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 463.24 examples/s]Tokenizing train dataset:   5%|▌         | 353/6690 [00:00<00:13, 461.41 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 474.80 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 477.06 examples/s]Tokenizing train dataset:   6%|▌         | 404/6690 [00:00<00:13, 467.01 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 482.85 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 484.62 examples/s]Tokenizing train dataset:   7%|▋         | 458/6690 [00:00<00:12, 483.47 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 484.67 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 486.66 examples/s]Tokenizing train dataset:   8%|▊         | 509/6690 [00:01<00:12, 490.49 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 485.33 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 486.95 examples/s]Tokenizing train dataset:   8%|▊         | 559/6690 [00:01<00:12, 486.79 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 486.18 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 487.47 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 484.50 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 486.69 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 487.68 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 485.42 examples/s]Tokenizing train dataset:  11%|█         | 710/6690 [00:01<00:12, 488.39 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 494.60 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 492.67 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 501.82 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 506.04 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 504.08 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 507.66 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 510.83 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 509.07 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:12, 481.22 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:12, 483.39 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:12, 482.38 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 462.44 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 464.21 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 463.05 examples/s]Tokenizing train dataset:  15%|█▌        | 1025/6690 [00:02<00:12, 459.16 examples/s]Tokenizing train dataset:  15%|█▌        | 1025/6690 [00:02<00:12, 460.55 examples/s]Tokenizing train dataset:  15%|█▌        | 1025/6690 [00:02<00:12, 459.68 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 469.07 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 470.11 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 469.54 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:12, 460.41 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:12, 460.94 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:12, 460.78 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 459.70 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 459.94 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 460.13 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 452.84 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 452.56 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 452.88 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 453.38 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 452.84 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 452.93 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 467.63 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 466.88 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 467.24 examples/s]Tokenizing train dataset:  21%|██        | 1420/6690 [00:03<00:12, 437.53 examples/s]Tokenizing train dataset:  21%|██        | 1420/6690 [00:03<00:12, 436.74 examples/s]Tokenizing train dataset:  21%|██        | 1420/6690 [00:03<00:12, 436.77 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 438.70 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 437.99 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 437.71 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 453.41 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 452.61 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 452.44 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 453.81 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 453.13 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 452.87 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 448.13 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 447.54 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 447.07 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 448.66 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 448.24 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 447.93 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 461.35 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 461.10 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 460.80 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 450.86 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 450.54 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 450.13 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:04<00:10, 452.24 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:04<00:10, 451.62 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:04<00:10, 451.21 examples/s]Tokenizing train dataset:  29%|██▊       | 1912/6690 [00:04<00:10, 449.50 examples/s]Tokenizing train dataset:  29%|██▉       | 1933/6690 [00:04<00:10, 447.77 examples/s]Tokenizing train dataset:  29%|██▊       | 1912/6690 [00:04<00:10, 449.44 examples/s]Tokenizing train dataset:  29%|██▉       | 1960/6690 [00:04<00:10, 454.08 examples/s]Tokenizing train dataset:  29%|██▉       | 1960/6690 [00:04<00:10, 453.78 examples/s]Tokenizing train dataset:  30%|██▉       | 1997/6690 [00:04<00:10, 437.96 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:10, 430.80 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:10, 430.54 examples/s]Tokenizing train dataset:  31%|███       | 2061/6690 [00:04<00:10, 431.40 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 431.00 examples/s]Tokenizing train dataset:  32%|███▏      | 2112/6690 [00:04<00:10, 445.84 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 431.05 examples/s]Tokenizing train dataset:  32%|███▏      | 2141/6690 [00:04<00:10, 445.23 examples/s]Tokenizing train dataset:  32%|███▏      | 2161/6690 [00:04<00:10, 450.56 examples/s]Tokenizing train dataset:  32%|███▏      | 2140/6690 [00:04<00:10, 444.54 examples/s]Tokenizing train dataset:  33%|███▎      | 2192/6690 [00:04<00:09, 460.04 examples/s]Tokenizing train dataset:  33%|███▎      | 2216/6690 [00:04<00:09, 473.56 examples/s]Tokenizing train dataset:  33%|███▎      | 2190/6690 [00:04<00:09, 457.39 examples/s]Tokenizing train dataset:  34%|███▎      | 2245/6690 [00:04<00:09, 473.92 examples/s]Tokenizing train dataset:  34%|███▎      | 2245/6690 [00:04<00:09, 474.58 examples/s]Tokenizing train dataset:  34%|███▍      | 2285/6690 [00:04<00:09, 462.19 examples/s]Tokenizing train dataset:  35%|███▍      | 2309/6690 [00:05<00:09, 448.48 examples/s]Tokenizing train dataset:  35%|███▍      | 2309/6690 [00:05<00:09, 448.29 examples/s]Tokenizing train dataset:  35%|███▌      | 2354/6690 [00:05<00:09, 453.71 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:05<00:09, 458.28 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:05<00:09, 457.80 examples/s]Tokenizing train dataset:  36%|███▌      | 2400/6690 [00:05<00:09, 452.58 examples/s]Tokenizing train dataset:  36%|███▋      | 2430/6690 [00:05<00:09, 453.87 examples/s]Tokenizing train dataset:  37%|███▋      | 2447/6690 [00:05<00:09, 452.45 examples/s]Tokenizing train dataset:  36%|███▋      | 2430/6690 [00:05<00:09, 453.54 examples/s]Tokenizing train dataset:  37%|███▋      | 2478/6690 [00:05<00:09, 457.24 examples/s]Tokenizing train dataset:  37%|███▋      | 2496/6690 [00:05<00:09, 460.37 examples/s]Tokenizing train dataset:  37%|███▋      | 2478/6690 [00:05<00:09, 456.78 examples/s]Tokenizing train dataset:  38%|███▊      | 2526/6690 [00:05<00:09, 460.17 examples/s]Tokenizing train dataset:  38%|███▊      | 2550/6690 [00:05<00:08, 477.32 examples/s]Tokenizing train dataset:  38%|███▊      | 2526/6690 [00:05<00:09, 459.61 examples/s]Tokenizing train dataset:  39%|███▊      | 2581/6690 [00:05<00:08, 482.15 examples/s]Tokenizing train dataset:  39%|███▉      | 2601/6690 [00:05<00:08, 481.17 examples/s]Tokenizing train dataset:  39%|███▊      | 2580/6690 [00:05<00:08, 479.95 examples/s]Tokenizing train dataset:  40%|███▉      | 2649/6690 [00:05<00:08, 464.40 examples/s]Tokenizing train dataset:  40%|███▉      | 2664/6690 [00:05<00:08, 453.44 examples/s]Tokenizing train dataset:  40%|███▉      | 2647/6690 [00:05<00:08, 466.17 examples/s]Tokenizing train dataset:  41%|████      | 2713/6690 [00:05<00:08, 449.19 examples/s]Tokenizing train dataset:  41%|████      | 2730/6690 [00:05<00:08, 444.66 examples/s]Tokenizing train dataset:  41%|████      | 2712/6690 [00:05<00:08, 449.74 examples/s]Tokenizing train dataset:  41%|████▏     | 2774/6690 [00:06<00:09, 431.39 examples/s]Tokenizing train dataset:  42%|████▏     | 2794/6690 [00:06<00:09, 432.79 examples/s]Tokenizing train dataset:  41%|████▏     | 2774/6690 [00:06<00:09, 429.89 examples/s]Tokenizing train dataset:  42%|████▏     | 2839/6690 [00:06<00:08, 431.46 examples/s]Tokenizing train dataset:  43%|████▎     | 2860/6690 [00:06<00:08, 432.77 examples/s]Tokenizing train dataset:  42%|████▏     | 2839/6690 [00:06<00:08, 430.21 examples/s]Tokenizing train dataset:  43%|████▎     | 2885/6690 [00:06<00:08, 435.40 examples/s]Tokenizing train dataset:  43%|████▎     | 2910/6690 [00:06<00:08, 445.14 examples/s]Tokenizing train dataset:  43%|████▎     | 2885/6690 [00:06<00:08, 434.51 examples/s]Tokenizing train dataset:  44%|████▍     | 2936/6690 [00:06<00:08, 452.45 examples/s]Tokenizing train dataset:  44%|████▍     | 2960/6690 [00:06<00:08, 454.98 examples/s]Tokenizing train dataset:  44%|████▍     | 2936/6690 [00:06<00:08, 451.43 examples/s]Tokenizing train dataset:  45%|████▍     | 2988/6690 [00:06<00:07, 466.51 examples/s]Tokenizing train dataset:  45%|████▌     | 3012/6690 [00:06<00:07, 469.54 examples/s]Tokenizing train dataset:  45%|████▍     | 2987/6690 [00:06<00:07, 465.74 examples/s]Tokenizing train dataset:  45%|████▌     | 3040/6690 [00:06<00:08, 417.04 examples/s]Tokenizing train dataset:  46%|████▌     | 3075/6690 [00:06<00:08, 445.30 examples/s]Tokenizing train dataset:  46%|████▌     | 3047/6690 [00:06<00:08, 438.71 examples/s]Tokenizing train dataset:  46%|████▋     | 3099/6690 [00:06<00:07, 458.11 examples/s]Tokenizing train dataset:  47%|████▋     | 3121/6690 [00:06<00:08, 445.86 examples/s]Tokenizing train dataset:  46%|████▋     | 3103/6690 [00:06<00:07, 465.23 examples/s]Tokenizing train dataset:  47%|████▋     | 3163/6690 [00:06<00:07, 441.65 examples/s]Tokenizing train dataset:  48%|████▊     | 3186/6690 [00:06<00:07, 439.39 examples/s]Tokenizing train dataset:  47%|████▋     | 3169/6690 [00:06<00:07, 448.03 examples/s]Tokenizing train dataset:  48%|████▊     | 3211/6690 [00:07<00:07, 446.12 examples/s]Tokenizing train dataset:  48%|████▊     | 3240/6690 [00:07<00:07, 460.33 examples/s]Tokenizing train dataset:  48%|████▊     | 3242/6690 [00:07<00:07, 456.16 examples/s]Tokenizing train dataset:  49%|████▊     | 3258/6690 [00:07<00:07, 450.45 examples/s]Tokenizing train dataset:  49%|████▉     | 3301/6690 [00:07<00:07, 439.65 examples/s]Tokenizing train dataset:  49%|████▉     | 3289/6690 [00:07<00:07, 456.88 examples/s]Tokenizing train dataset:  50%|████▉     | 3318/6690 [00:07<00:07, 425.38 examples/s]Tokenizing train dataset:  50%|█████     | 3365/6690 [00:07<00:07, 430.97 examples/s]Tokenizing train dataset:  50%|█████     | 3345/6690 [00:07<00:07, 424.78 examples/s]Tokenizing train dataset:  50%|█████     | 3363/6690 [00:07<00:07, 427.15 examples/s]Tokenizing train dataset:  51%|█████     | 3427/6690 [00:07<00:07, 419.80 examples/s]Tokenizing train dataset:  51%|█████     | 3408/6690 [00:07<00:07, 424.04 examples/s]Tokenizing train dataset:  51%|█████     | 3410/6690 [00:07<00:07, 420.76 examples/s]Tokenizing train dataset:  52%|█████▏    | 3464/6690 [00:07<00:08, 402.00 examples/s]Tokenizing train dataset:  52%|█████▏    | 3494/6690 [00:07<00:07, 421.80 examples/s]Tokenizing train dataset:  52%|█████▏    | 3469/6690 [00:07<00:07, 408.64 examples/s]Tokenizing train dataset:  52%|█████▏    | 3511/6690 [00:07<00:07, 418.85 examples/s]Tokenizing train dataset:  53%|█████▎    | 3515/6690 [00:07<00:07, 417.53 examples/s]Tokenizing train dataset:  53%|█████▎    | 3554/6690 [00:07<00:07, 410.28 examples/s]Tokenizing train dataset:  53%|█████▎    | 3570/6690 [00:07<00:07, 404.28 examples/s]Tokenizing train dataset:  53%|█████▎    | 3572/6690 [00:07<00:07, 402.39 examples/s]Tokenizing train dataset:  54%|█████▍    | 3612/6690 [00:07<00:07, 398.69 examples/s]Tokenizing train dataset:  54%|█████▍    | 3637/6690 [00:08<00:07, 411.95 examples/s]Tokenizing train dataset:  55%|█████▍    | 3663/6690 [00:08<00:07, 416.32 examples/s]Tokenizing train dataset:  54%|█████▍    | 3640/6690 [00:08<00:07, 412.92 examples/s]Tokenizing train dataset:  55%|█████▍    | 3679/6690 [00:08<00:07, 413.39 examples/s]Tokenizing train dataset:  55%|█████▌    | 3685/6690 [00:08<00:07, 417.38 examples/s]Tokenizing train dataset:  56%|█████▌    | 3720/6690 [00:08<00:07, 397.58 examples/s]Tokenizing train dataset:  56%|█████▌    | 3740/6690 [00:08<00:07, 396.52 examples/s]Tokenizing train dataset:  56%|█████▌    | 3740/6690 [00:08<00:07, 395.44 examples/s]Tokenizing train dataset:  56%|█████▋    | 3767/6690 [00:08<00:07, 409.55 examples/s]Tokenizing train dataset:  57%|█████▋    | 3787/6690 [00:08<00:07, 412.63 examples/s]Tokenizing train dataset:  57%|█████▋    | 3788/6690 [00:08<00:07, 412.88 examples/s]Tokenizing train dataset:  57%|█████▋    | 3810/6690 [00:08<00:06, 412.29 examples/s]Tokenizing train dataset:  57%|█████▋    | 3831/6690 [00:08<00:06, 415.17 examples/s]Tokenizing train dataset:  57%|█████▋    | 3831/6690 [00:08<00:06, 414.59 examples/s]Tokenizing train dataset:  58%|█████▊    | 3856/6690 [00:08<00:06, 422.33 examples/s]Tokenizing train dataset:  58%|█████▊    | 3876/6690 [00:08<00:06, 422.22 examples/s]Tokenizing train dataset:  58%|█████▊    | 3876/6690 [00:08<00:06, 422.05 examples/s]Tokenizing train dataset:  58%|█████▊    | 3909/6690 [00:08<00:06, 445.98 examples/s]Tokenizing train dataset:  59%|█████▊    | 3926/6690 [00:08<00:06, 441.72 examples/s]Tokenizing train dataset:  59%|█████▊    | 3926/6690 [00:08<00:06, 441.94 examples/s]Tokenizing train dataset:  59%|█████▉    | 3956/6690 [00:08<00:06, 449.99 examples/s]Tokenizing train dataset:  59%|█████▉    | 3976/6690 [00:08<00:06, 451.39 examples/s]Tokenizing train dataset:  59%|█████▉    | 3976/6690 [00:08<00:06, 451.71 examples/s]Tokenizing train dataset:  60%|█████▉    | 4007/6690 [00:08<00:05, 461.34 examples/s]Tokenizing train dataset:  60%|██████    | 4025/6690 [00:08<00:05, 459.86 examples/s]Tokenizing train dataset:  60%|██████    | 4025/6690 [00:08<00:05, 459.91 examples/s]Tokenizing train dataset:  61%|██████    | 4072/6690 [00:09<00:05, 447.31 examples/s]Tokenizing train dataset:  61%|██████    | 4087/6690 [00:09<00:05, 439.42 examples/s]Tokenizing train dataset:  61%|██████    | 4087/6690 [00:09<00:05, 439.73 examples/s]Tokenizing train dataset:  62%|██████▏   | 4118/6690 [00:09<00:05, 448.82 examples/s]Tokenizing train dataset:  62%|██████▏   | 4136/6690 [00:09<00:05, 450.43 examples/s]Tokenizing train dataset:  62%|██████▏   | 4136/6690 [00:09<00:05, 450.88 examples/s]Tokenizing train dataset:  62%|██████▏   | 4167/6690 [00:09<00:05, 458.03 examples/s]Tokenizing train dataset:  63%|██████▎   | 4188/6690 [00:09<00:05, 462.95 examples/s]Tokenizing train dataset:  63%|██████▎   | 4188/6690 [00:09<00:05, 463.42 examples/s]Tokenizing train dataset:  63%|██████▎   | 4215/6690 [00:09<00:05, 457.56 examples/s]Tokenizing train dataset:  64%|██████▎   | 4255/6690 [00:09<00:05, 449.51 examples/s]Tokenizing train dataset:  64%|██████▎   | 4255/6690 [00:09<00:05, 449.67 examples/s]Tokenizing train dataset:  64%|██████▍   | 4282/6690 [00:09<00:05, 447.50 examples/s]Tokenizing train dataset:  64%|██████▍   | 4301/6690 [00:09<00:05, 448.77 examples/s]Tokenizing train dataset:  64%|██████▍   | 4301/6690 [00:09<00:05, 448.92 examples/s]Tokenizing train dataset:  65%|██████▍   | 4333/6690 [00:09<00:05, 459.30 examples/s]Tokenizing train dataset:  65%|██████▍   | 4347/6690 [00:09<00:05, 445.72 examples/s]Tokenizing train dataset:  65%|██████▌   | 4364/6690 [00:09<00:05, 438.24 examples/s]Tokenizing train dataset:  66%|██████▌   | 4393/6690 [00:09<00:05, 431.92 examples/s]Tokenizing train dataset:  66%|██████▋   | 4441/6690 [00:09<00:05, 439.54 examples/s]Tokenizing train dataset:  66%|██████▌   | 4410/6690 [00:09<00:05, 427.10 examples/s]Tokenizing train dataset:  66%|██████▌   | 4430/6690 [00:09<00:05, 433.72 examples/s]Tokenizing train dataset:  67%|██████▋   | 4458/6690 [00:09<00:05, 439.16 examples/s]Tokenizing train dataset:  67%|██████▋   | 4504/6690 [00:10<00:05, 429.43 examples/s]Tokenizing train dataset:  67%|██████▋   | 4496/6690 [00:10<00:05, 432.46 examples/s]Tokenizing train dataset:  68%|██████▊   | 4555/6690 [00:10<00:04, 445.70 examples/s]Tokenizing train dataset:  68%|██████▊   | 4527/6690 [00:10<00:04, 436.77 examples/s]Tokenizing train dataset:  68%|██████▊   | 4544/6690 [00:10<00:04, 442.03 examples/s]Tokenizing train dataset:  68%|██████▊   | 4574/6690 [00:10<00:04, 443.49 examples/s]Tokenizing train dataset:  69%|██████▉   | 4629/6690 [00:10<00:04, 456.78 examples/s]Tokenizing train dataset:  69%|██████▉   | 4617/6690 [00:10<00:04, 454.07 examples/s]Tokenizing train dataset:  69%|██████▉   | 4624/6690 [00:10<00:04, 454.23 examples/s]Tokenizing train dataset:  70%|██████▉   | 4677/6690 [00:10<00:04, 457.72 examples/s]Tokenizing train dataset:  70%|██████▉   | 4666/6690 [00:10<00:04, 458.26 examples/s]Tokenizing train dataset:  70%|██████▉   | 4673/6690 [00:10<00:04, 461.36 examples/s]Tokenizing train dataset:  71%|███████   | 4747/6690 [00:10<00:04, 459.29 examples/s]Tokenizing train dataset:  71%|███████   | 4736/6690 [00:10<00:04, 459.18 examples/s]Tokenizing train dataset:  71%|███████   | 4745/6690 [00:10<00:04, 459.91 examples/s]Tokenizing train dataset:  72%|███████▏  | 4808/6690 [00:10<00:04, 437.75 examples/s]Tokenizing train dataset:  72%|███████▏  | 4795/6690 [00:10<00:04, 430.59 examples/s]Tokenizing train dataset:  72%|███████▏  | 4806/6690 [00:10<00:04, 436.62 examples/s]Tokenizing train dataset:  73%|███████▎  | 4855/6690 [00:10<00:04, 442.25 examples/s]Tokenizing train dataset:  72%|███████▏  | 4845/6690 [00:10<00:04, 442.45 examples/s]Tokenizing train dataset:  73%|███████▎  | 4853/6690 [00:10<00:04, 442.16 examples/s]Tokenizing train dataset:  73%|███████▎  | 4890/6690 [00:10<00:04, 436.21 examples/s]Tokenizing train dataset:  74%|███████▎  | 4918/6690 [00:10<00:04, 432.11 examples/s]Tokenizing train dataset:  73%|███████▎  | 4915/6690 [00:10<00:04, 429.35 examples/s]Tokenizing train dataset:  74%|███████▍  | 4934/6690 [00:11<00:04, 432.78 examples/s]Tokenizing train dataset:  74%|███████▍  | 4964/6690 [00:11<00:03, 436.03 examples/s]Tokenizing train dataset:  74%|███████▍  | 4963/6690 [00:11<00:03, 436.54 examples/s]Tokenizing train dataset:  75%|███████▍  | 5008/6690 [00:11<00:03, 434.79 examples/s]Tokenizing train dataset:  75%|███████▍  | 5000/6690 [00:11<00:03, 431.52 examples/s]Tokenizing train dataset:  75%|███████▌  | 5027/6690 [00:11<00:03, 429.17 examples/s]Tokenizing train dataset:  76%|███████▌  | 5070/6690 [00:11<00:03, 426.30 examples/s]Tokenizing train dataset:  76%|███████▌  | 5064/6690 [00:11<00:03, 427.33 examples/s]Tokenizing train dataset:  76%|███████▋  | 5115/6690 [00:11<00:03, 426.65 examples/s]Tokenizing train dataset:  76%|███████▌  | 5092/6690 [00:11<00:03, 424.87 examples/s]Tokenizing train dataset:  77%|███████▋  | 5125/6690 [00:11<00:03, 418.96 examples/s]Tokenizing train dataset:  77%|███████▋  | 5160/6690 [00:11<00:03, 427.86 examples/s]Tokenizing train dataset:  77%|███████▋  | 5160/6690 [00:11<00:03, 428.07 examples/s]Tokenizing train dataset:  77%|███████▋  | 5174/6690 [00:11<00:03, 432.53 examples/s]Tokenizing train dataset:  78%|███████▊  | 5210/6690 [00:11<00:03, 439.07 examples/s]Tokenizing train dataset:  78%|███████▊  | 5209/6690 [00:11<00:03, 441.49 examples/s]Tokenizing train dataset:  78%|███████▊  | 5220/6690 [00:11<00:03, 436.77 examples/s]Tokenizing train dataset:  79%|███████▊  | 5260/6690 [00:11<00:03, 396.71 examples/s]Tokenizing train dataset:  79%|███████▉  | 5275/6690 [00:11<00:03, 436.82 examples/s]Tokenizing train dataset:  79%|███████▉  | 5287/6690 [00:11<00:03, 437.98 examples/s]Tokenizing train dataset:  79%|███████▉  | 5303/6690 [00:11<00:03, 403.71 examples/s]Tokenizing train dataset:  80%|███████▉  | 5346/6690 [00:11<00:03, 407.81 examples/s]Tokenizing train dataset:  80%|███████▉  | 5338/6690 [00:11<00:03, 424.74 examples/s]Tokenizing train dataset:  80%|███████▉  | 5350/6690 [00:11<00:03, 428.21 examples/s]Tokenizing train dataset:  81%|████████  | 5400/6690 [00:12<00:02, 436.36 examples/s]Tokenizing train dataset:  81%|████████  | 5392/6690 [00:12<00:02, 450.48 examples/s]Tokenizing train dataset:  81%|████████  | 5404/6690 [00:12<00:02, 452.06 examples/s]Tokenizing train dataset:  82%|████████▏ | 5466/6690 [00:12<00:02, 435.37 examples/s]Tokenizing train dataset:  82%|████████▏ | 5460/6690 [00:12<00:02, 444.40 examples/s]Tokenizing train dataset:  82%|████████▏ | 5469/6690 [00:12<00:02, 441.38 examples/s]Tokenizing train dataset:  82%|████████▏ | 5512/6690 [00:12<00:02, 438.04 examples/s]Tokenizing train dataset:  82%|████████▏ | 5515/6690 [00:12<00:02, 444.37 examples/s]Tokenizing train dataset:  83%|████████▎ | 5526/6690 [00:12<00:02, 440.79 examples/s]Tokenizing train dataset:  83%|████████▎ | 5575/6690 [00:12<00:02, 428.24 examples/s]Tokenizing train dataset:  83%|████████▎ | 5579/6690 [00:12<00:02, 436.18 examples/s]Tokenizing train dataset:  84%|████████▎ | 5590/6690 [00:12<00:02, 430.08 examples/s]Tokenizing train dataset:  84%|████████▍ | 5642/6690 [00:12<00:02, 431.59 examples/s]Tokenizing train dataset:  84%|████████▍ | 5639/6690 [00:12<00:02, 439.64 examples/s]Tokenizing train dataset:  84%|████████▍ | 5645/6690 [00:12<00:02, 431.64 examples/s]Tokenizing train dataset:  85%|████████▌ | 5690/6690 [00:12<00:02, 431.04 examples/s]Tokenizing train dataset:  85%|████████▌ | 5707/6690 [00:12<00:02, 430.42 examples/s]Tokenizing train dataset:  85%|████████▌ | 5703/6690 [00:12<00:02, 431.14 examples/s]Tokenizing train dataset:  86%|████████▌ | 5738/6690 [00:12<00:02, 438.35 examples/s]Tokenizing train dataset:  86%|████████▌ | 5757/6690 [00:12<00:02, 445.13 examples/s]Tokenizing train dataset:  86%|████████▌ | 5753/6690 [00:12<00:02, 446.14 examples/s]Tokenizing train dataset:  86%|████████▋ | 5784/6690 [00:12<00:02, 438.63 examples/s]Tokenizing train dataset:  87%|████████▋ | 5821/6690 [00:13<00:01, 436.85 examples/s]Tokenizing train dataset:  87%|████████▋ | 5820/6690 [00:13<00:01, 437.80 examples/s]Tokenizing train dataset:  87%|████████▋ | 5848/6690 [00:13<00:01, 428.14 examples/s]Tokenizing train dataset:  88%|████████▊ | 5881/6690 [00:13<00:01, 422.53 examples/s]Tokenizing train dataset:  88%|████████▊ | 5880/6690 [00:13<00:01, 422.51 examples/s]Tokenizing train dataset:  88%|████████▊ | 5895/6690 [00:13<00:01, 436.63 examples/s]Tokenizing train dataset:  89%|████████▊ | 5935/6690 [00:13<00:01, 444.73 examples/s]Tokenizing train dataset:  89%|████████▊ | 5932/6690 [00:13<00:01, 440.84 examples/s]Tokenizing train dataset:  89%|████████▉ | 5943/6690 [00:13<00:01, 441.74 examples/s]Tokenizing train dataset:  89%|████████▉ | 5977/6690 [00:13<00:01, 434.71 examples/s]Tokenizing train dataset:  90%|████████▉ | 6003/6690 [00:13<00:01, 443.67 examples/s]Tokenizing train dataset:  90%|████████▉ | 6010/6690 [00:13<00:01, 441.49 examples/s]Tokenizing train dataset:  90%|█████████ | 6023/6690 [00:13<00:01, 439.76 examples/s]Tokenizing train dataset:  91%|█████████ | 6072/6690 [00:13<00:01, 446.96 examples/s]Tokenizing train dataset:  91%|█████████ | 6055/6690 [00:13<00:01, 441.63 examples/s]Tokenizing train dataset:  91%|█████████ | 6072/6690 [00:13<00:01, 450.27 examples/s]Tokenizing train dataset:  91%|█████████▏| 6108/6690 [00:13<00:01, 463.53 examples/s]Tokenizing train dataset:  92%|█████████▏| 6127/6690 [00:13<00:01, 465.45 examples/s]Tokenizing train dataset:  92%|█████████▏| 6127/6690 [00:13<00:01, 470.97 examples/s]Tokenizing train dataset:  92%|█████████▏| 6155/6690 [00:13<00:01, 462.71 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6690 [00:13<00:01, 454.66 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6690 [00:13<00:01, 456.43 examples/s]Tokenizing train dataset:  93%|█████████▎| 6219/6690 [00:13<00:01, 447.45 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:14<00:01, 417.83 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:14<00:01, 415.94 examples/s]Tokenizing train dataset:  94%|█████████▍| 6277/6690 [00:14<00:00, 422.42 examples/s]Tokenizing train dataset:  94%|█████████▍| 6299/6690 [00:14<00:00, 432.67 examples/s]Tokenizing train dataset:  94%|█████████▍| 6299/6690 [00:14<00:00, 431.65 examples/s]Tokenizing train dataset:  94%|█████████▍| 6321/6690 [00:14<00:00, 422.88 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 417.33 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 415.91 examples/s]Tokenizing train dataset:  95%|█████████▌| 6381/6690 [00:14<00:00, 408.87 examples/s]Tokenizing train dataset:  96%|█████████▌| 6404/6690 [00:14<00:00, 420.04 examples/s]Tokenizing train dataset:  96%|█████████▌| 6403/6690 [00:14<00:00, 416.64 examples/s]Tokenizing train dataset:  96%|█████████▌| 6431/6690 [00:14<00:00, 429.92 examples/s]Tokenizing train dataset:  96%|█████████▋| 6451/6690 [00:14<00:00, 430.89 examples/s]Tokenizing train dataset:  96%|█████████▋| 6451/6690 [00:14<00:00, 429.97 examples/s]Tokenizing train dataset:  97%|█████████▋| 6497/6690 [00:14<00:00, 434.29 examples/s]Tokenizing train dataset:  97%|█████████▋| 6496/6690 [00:14<00:00, 434.49 examples/s]Tokenizing train dataset:  97%|█████████▋| 6497/6690 [00:14<00:00, 428.76 examples/s]Tokenizing train dataset:  98%|█████████▊| 6541/6690 [00:14<00:00, 432.71 examples/s]Tokenizing train dataset:  98%|█████████▊| 6540/6690 [00:14<00:00, 430.77 examples/s]Tokenizing train dataset:  98%|█████████▊| 6541/6690 [00:14<00:00, 428.11 examples/s]Tokenizing train dataset:  98%|█████████▊| 6588/6690 [00:14<00:00, 438.25 examples/s]Tokenizing train dataset:  98%|█████████▊| 6584/6690 [00:14<00:00, 432.76 examples/s]Tokenizing train dataset:  98%|█████████▊| 6587/6690 [00:14<00:00, 432.79 examples/s]Tokenizing train dataset:  99%|█████████▉| 6648/6690 [00:14<00:00, 416.75 examples/s]Tokenizing train dataset:  99%|█████████▉| 6641/6690 [00:14<00:00, 411.25 examples/s]Tokenizing train dataset:  99%|█████████▉| 6643/6690 [00:14<00:00, 409.73 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 444.04 examples/s]
Tokenizing train dataset: 100%|█████████▉| 6684/6690 [00:15<00:00, 412.43 examples/s]Tokenizing train dataset: 100%|█████████▉| 6686/6690 [00:15<00:00, 413.85 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 444.42 examples/s]
Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 444.02 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5521.25 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5593.26 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5573.55 examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:10:28] Error running `scontrol show job $SLURM_JOB_ID` to count SLURM-available cpus. Using the machine's cpu count.
[codecarbon INFO @ 21:10:28] [setup] RAM Tracking...
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:10:28] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:10:28] [setup] CPU Tracking...
[codecarbon WARNING @ 21:10:28] No CPU tracking mode found. Falling back on CPU constant mode. 
 Linux OS detected: Please ensure RAPL files exist at \sys\class\powercap\intel-rapl to measure CPU

Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5576.91 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5553.25 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5511.22 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  33%|███▎      | 312/953 [00:00<00:00, 3093.88 examples/s]Applying chat template to eval dataset:  33%|███▎      | 315/953 [00:00<00:00, 3118.16 examples/s]Applying chat template to eval dataset:  33%|███▎      | 318/953 [00:00<00:00, 3144.74 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 635/953 [00:00<00:00, 3171.00 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 640/953 [00:00<00:00, 3191.53 examples/s]Applying chat template to eval dataset:  68%|██████▊   | 646/953 [00:00<00:00, 3216.98 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3182.24 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3186.91 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3204.79 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.69 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 318.55 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 320.75 examples/s][codecarbon INFO @ 21:10:29] CPU Model on constant consumption mode: AMD EPYC 7H12 64-Core Processor
[codecarbon INFO @ 21:10:29] [setup] GPU Tracking...
[codecarbon INFO @ 21:10:29] Tracking Nvidia GPU via pynvml
[codecarbon INFO @ 21:10:29] >>> Tracker's metadata:
[codecarbon INFO @ 21:10:29]   Platform system: Linux-5.15.112-1.el8.vega.x86_64-x86_64-with-glibc2.35
[codecarbon INFO @ 21:10:29]   Python version: 3.10.12
[codecarbon INFO @ 21:10:29]   CodeCarbon version: 2.8.3
[codecarbon INFO @ 21:10:29]   Available RAM : 503.683 GB
[codecarbon INFO @ 21:10:29]   CPU count: 256
[codecarbon INFO @ 21:10:29]   CPU model: AMD EPYC 7H12 64-Core Processor
[codecarbon INFO @ 21:10:29]   GPU count: 4
[codecarbon INFO @ 21:10:29]   GPU model: 4 x NVIDIA A100-SXM4-40GB BUT only tracking these GPU ids : [0, 1, 2, 3]
Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 287.07 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 285.73 examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 287.16 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 275.44 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 275.82 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 276.66 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 266.45 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 264.14 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 264.81 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 252.11 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 250.99 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 251.60 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 268.68 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 267.59 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:02, 268.24 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:00<00:01, 369.34 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:00<00:01, 368.17 examples/s]Tokenizing eval dataset:  31%|███       | 292/953 [00:00<00:01, 368.93 examples/s]Tokenizing eval dataset:  37%|███▋      | 357/953 [00:01<00:01, 441.27 examples/s]Tokenizing eval dataset:  37%|███▋      | 357/953 [00:01<00:01, 440.65 examples/s]Tokenizing eval dataset:  37%|███▋      | 357/953 [00:01<00:01, 440.94 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 487.36 examples/s]Tokenizing eval dataset:  44%|████▍     | 419/953 [00:01<00:01, 487.76 examples/s]Tokenizing eval dataset:  44%|████▍     | 418/953 [00:01<00:01, 487.08 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 541.11 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 540.73 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 540.98 examples/s]Tokenizing eval dataset:  58%|█████▊    | 555/953 [00:01<00:00, 575.47 examples/s]Tokenizing eval dataset:  58%|█████▊    | 555/953 [00:01<00:00, 575.27 examples/s]Tokenizing eval dataset:  58%|█████▊    | 555/953 [00:01<00:00, 575.71 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 584.52 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 584.72 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 584.82 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 595.61 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 595.96 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 595.79 examples/s]Tokenizing eval dataset:  80%|████████  | 766/953 [00:01<00:00, 575.46 examples/s]Tokenizing eval dataset:  80%|████████  | 766/953 [00:01<00:00, 576.09 examples/s]Tokenizing eval dataset:  80%|████████  | 766/953 [00:01<00:00, 575.75 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 541.37 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 541.43 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 541.73 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 516.50 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 516.42 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 516.69 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 452.99 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 452.40 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 453.00 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[codecarbon INFO @ 21:10:33] Saving emissions data to file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/training_run/curriculum-2_r-64_lr-1e-06_b-0.1/emissions.csv
Set up DPO trainer
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.440253734588623 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3637728691101074 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3613154888153076 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3477094173431396 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:25] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:25] Energy consumed for RAM : 0.000788 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:25] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:25] Energy consumed for all CPUs : 0.000584 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:11:25] Energy consumed for all GPUs : 0.001196 kWh. Total GPU Power : 286.8416634595693 W
[codecarbon INFO @ 21:11:25] 0.002568 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:40] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:40] Energy consumed for RAM : 0.001574 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:40] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:40] Energy consumed for all CPUs : 0.001167 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:11:40] Energy consumed for all GPUs : 0.002680 kWh. Total GPU Power : 356.2871643611251 W
[codecarbon INFO @ 21:11:40] 0.005420 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:55] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:55] Energy consumed for RAM : 0.002360 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:11:55] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:11:55] Energy consumed for all CPUs : 0.001750 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:11:55] Energy consumed for all GPUs : 0.004165 kWh. Total GPU Power : 356.69930940025944 W
[codecarbon INFO @ 21:11:55] 0.008275 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:10] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:10] Energy consumed for RAM : 0.003147 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:10] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:10] Energy consumed for all CPUs : 0.002333 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:12:10] Energy consumed for all GPUs : 0.005646 kWh. Total GPU Power : 355.7652803676502 W
[codecarbon INFO @ 21:12:10] 0.011126 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:25] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:25] Energy consumed for RAM : 0.003933 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:25] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:25] Energy consumed for all CPUs : 0.002915 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:12:25] Energy consumed for all GPUs : 0.007127 kWh. Total GPU Power : 355.68174996259125 W
[codecarbon INFO @ 21:12:25] 0.013975 kWh of electricity used since the beginning.
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:40] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:40] Energy consumed for RAM : 0.004719 kWh. RAM Power : 188.881121635437 W
/bin/sh: 1: scontrol: not found
[codecarbon WARNING @ 21:12:40] Error running `scontrol show job $SLURM_JOB_ID` to retrieve SLURM-available RAM.Using the machine's total RAM.
[codecarbon INFO @ 21:12:40] Energy consumed for all CPUs : 0.003498 kWh. Total CPU Power : 140.0 W
[codecarbon INFO @ 21:12:40] Energy consumed for all GPUs : 0.008610 kWh. Total GPU Power : 356.20464884593156 W
[codecarbon INFO @ 21:12:40] 0.016828 kWh of electricity used since the beginning.
[rank4]:[E612 21:12:53.622070173 ProcessGroupNCCL.cpp:552] [Rank 4] Collective WorkNCCL(SeqNum=3, OpType=ALLGATHER, NumelIn=1, NumelOut=12, Timeout(ms)=1800000) raised the following async exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn01.vega.pri<58574>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f61a3d6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f61520211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f615202964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f615202b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f615202c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f61a44425c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f61a62eaac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f61a637ca40 in /lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[E612 21:12:53.623580442 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 4]  failure detected by watchdog at work sequence id: 3 PG status: last enqueued work: 3, last completed work: 2
[rank4]:[E612 21:12:53.623588202 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank4]:[E612 21:12:54.683323194 ProcessGroupNCCL.cpp:681] [Rank 4] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E612 21:12:54.683337934 ProcessGroupNCCL.cpp:695] [Rank 4] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E612 21:12:54.683368904 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 4] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn01.vega.pri<58574>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f61a3d6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f61520211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f615202964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f615202b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f615202c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f61a44425c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f61a62eaac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f61a637ca40 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 4] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn01.vega.pri<58574>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f61a3d6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f61520211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f615202964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f615202b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f615202c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f61a44425c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f61a62eaac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f61a637ca40 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f61a3d6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f6151c876fc in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f61a44425c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f61a62eaac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126a40 (0x7f61a637ca40 in /lib/x86_64-linux-gnu/libc.so.6)

W0612 21:12:57.500000 2368820 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2369005 closing signal SIGTERM
W0612 21:12:57.501000 2368820 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2369006 closing signal SIGTERM
W0612 21:12:57.502000 2368820 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2369007 closing signal SIGTERM
E0612 21:12:58.233000 2368820 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 0 (pid: 2369004) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1182, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 861, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train_curriculum.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-12_21:12:57
  host      : pm5-nod03.vega.pri
  rank      : 4 (local_rank: 0)
  exitcode  : -6 (pid: 2369004)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2369004
========================================================
