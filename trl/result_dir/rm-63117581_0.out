cpu-bind=MASK - gn01, task  0  0 [3243260]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 0 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 0     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63117581     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=1e-6 --total_epochs=3 --beta=0.1 --curriculum_stage=2
-------------------------------------------
[2025-06-12 20:54:46,339] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 20:54:48.236000 3243317 torch/distributed/run.py:792] 
W0612 20:54:48.236000 3243317 torch/distributed/run.py:792] *****************************************
W0612 20:54:48.236000 3243317 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 20:54:48.236000 3243317 torch/distributed/run.py:792] *****************************************
[2025-06-12 20:55:17,292] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 20:55:17,311] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 20:55:17,329] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 20:55:17,337] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
1e-06
1e-06
[2025-06-12 20:55:23,489] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 20:55:23,523] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 20:55:23,573] [INFO] [comm.py:658:init_distributed] cdb=None
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
World size: 12
Setting gradient accumulation steps to: 1
Created datasets
Train dataset size: 6690
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-12 20:55:24,799] [INFO] [comm.py:658:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][2025-06-12 20:55:24,905] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curriculum-1
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:22<01:08, 22.73s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:09, 23.17s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:09, 23.15s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:22<01:08, 22.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:08<00:22, 22.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:08<00:22, 22.99s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.04s/it]
Loaded model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07MTotal Parameters: 9457.78M

Percentage of Trainable Params: 2.2846%Trainable Parameters (LoRA): 216.07M

Total Parameters: 9457.78M
Percentage of Trainable Params: 2.2846%
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
[rank2]:[W612 20:56:56.227686469 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank3]:[W612 20:56:56.367418114 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank1]:[W612 20:56:56.369596882 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   8%|▊         | 546/6690 [00:00<00:01, 5366.00 examples/s]Extracting prompt in train dataset:  20%|██        | 1340/6690 [00:00<00:01, 5261.74 examples/s]Extracting prompt in train dataset:  31%|███       | 2060/6690 [00:00<00:00, 5021.86 examples/s]Extracting prompt in train dataset:  40%|████      | 2690/6690 [00:00<00:00, 4667.90 examples/s]Extracting prompt in train dataset:  48%|████▊     | 3211/6690 [00:00<00:00, 4819.13 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3730/6690 [00:00<00:00, 4922.95 examples/s]Extracting prompt in train dataset:  65%|██████▌   | 4380/6690 [00:00<00:00, 4638.50 examples/s]Extracting prompt in train dataset:  73%|███████▎  | 4910/6690 [00:01<00:00, 4811.11 examples/s]Extracting prompt in train dataset:  81%|████████  | 5410/6690 [00:01<00:00, 4847.70 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 5940/6690 [00:01<00:00, 4958.19 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 6470/6690 [00:01<00:00, 5033.40 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 4764.49 examples/s]
Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 280/6690 [00:00<00:02, 2779.12 examples/s]Applying chat template to train dataset:  10%|█         | 686/6690 [00:00<00:02, 2724.09 examples/s]Applying chat template to train dataset:  16%|█▋        | 1089/6690 [00:00<00:02, 2700.50 examples/s]Applying chat template to train dataset:  21%|██        | 1395/6690 [00:00<00:02, 2413.47 examples/s]Applying chat template to train dataset:  25%|██▌       | 1686/6690 [00:00<00:01, 2551.60 examples/s]Applying chat template to train dataset:  31%|███       | 2087/6690 [00:00<00:01, 2592.87 examples/s]Applying chat template to train dataset:  37%|███▋      | 2474/6690 [00:00<00:01, 2582.86 examples/s]Applying chat template to train dataset:  41%|████      | 2759/6690 [00:01<00:01, 2647.70 examples/s]Applying chat template to train dataset:  46%|████▌     | 3057/6690 [00:01<00:01, 2732.60 examples/s]Applying chat template to train dataset:  51%|█████     | 3427/6690 [00:01<00:01, 2635.50 examples/s]Applying chat template to train dataset:  55%|█████▌    | 3710/6690 [00:01<00:01, 2682.76 examples/s]Applying chat template to train dataset:  60%|█████▉    | 4000/6690 [00:01<00:00, 2737.74 examples/s]Applying chat template to train dataset:  64%|██████▍   | 4288/6690 [00:01<00:00, 2776.38 examples/s]Applying chat template to train dataset:  70%|███████   | 4712/6690 [00:01<00:00, 2791.81 examples/s]Applying chat template to train dataset:  75%|███████▍  | 4998/6690 [00:01<00:00, 2808.56 examples/s]Applying chat template to train dataset:  79%|███████▉  | 5286/6690 [00:01<00:00, 2825.11 examples/s]Applying chat template to train dataset:  85%|████████▌ | 5695/6690 [00:02<00:00, 2785.68 examples/s]Applying chat template to train dataset:  90%|████████▉ | 5991/6690 [00:02<00:00, 2827.29 examples/s]Applying chat template to train dataset:  94%|█████████▍| 6277/6690 [00:02<00:00, 2833.29 examples/s]Applying chat template to train dataset:  98%|█████████▊| 6565/6690 [00:02<00:00, 2844.68 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 2703.62 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 38/6690 [00:00<00:17, 369.80 examples/s]Tokenizing train dataset:   1%|▏         | 93/6690 [00:00<00:18, 357.72 examples/s]Tokenizing train dataset:   2%|▏         | 130/6690 [00:00<00:18, 362.17 examples/s]Tokenizing train dataset:   3%|▎         | 169/6690 [00:00<00:17, 363.99 examples/s]Tokenizing train dataset:   3%|▎         | 216/6690 [00:00<00:16, 396.13 examples/s]Tokenizing train dataset:   4%|▍         | 277/6690 [00:00<00:16, 393.90 examples/s]Tokenizing train dataset:   5%|▍         | 330/6690 [00:00<00:17, 370.65 examples/s]Tokenizing train dataset:   6%|▌         | 380/6690 [00:00<00:15, 399.28 examples/s]Tokenizing train dataset:   6%|▋         | 434/6690 [00:01<00:16, 380.74 examples/s]Tokenizing train dataset:   7%|▋         | 486/6690 [00:01<00:16, 366.09 examples/s]Tokenizing train dataset:   8%|▊         | 528/6690 [00:01<00:16, 376.94 examples/s]Tokenizing train dataset:   9%|▊         | 574/6690 [00:01<00:15, 394.81 examples/s]Tokenizing train dataset:  10%|▉         | 640/6690 [00:01<00:14, 403.72 examples/s]Tokenizing train dataset:  10%|█         | 685/6690 [00:01<00:14, 412.81 examples/s]Tokenizing train dataset:  11%|█         | 731/6690 [00:01<00:14, 418.04 examples/s]Tokenizing train dataset:  12%|█▏        | 780/6690 [00:01<00:13, 433.34 examples/s]Tokenizing train dataset:  12%|█▏        | 826/6690 [00:02<00:13, 440.36 examples/s]Tokenizing train dataset:  13%|█▎        | 890/6690 [00:02<00:13, 426.98 examples/s]Tokenizing train dataset:  14%|█▍        | 952/6690 [00:02<00:13, 415.40 examples/s]Tokenizing train dataset:  15%|█▌        | 1015/6690 [00:02<00:13, 415.04 examples/s]Tokenizing train dataset:  16%|█▌        | 1071/6690 [00:02<00:14, 399.23 examples/s]Tokenizing train dataset:  17%|█▋        | 1129/6690 [00:02<00:14, 391.31 examples/s]Tokenizing train dataset:  17%|█▋        | 1169/6690 [00:02<00:14, 391.32 examples/s]Tokenizing train dataset:  18%|█▊        | 1210/6690 [00:03<00:14, 390.93 examples/s]Tokenizing train dataset:  19%|█▊        | 1250/6690 [00:03<00:14, 387.98 examples/s]Tokenizing train dataset:  19%|█▉        | 1290/6690 [00:03<00:13, 387.22 examples/s]Tokenizing train dataset:  20%|█▉        | 1336/6690 [00:03<00:13, 403.17 examples/s]Tokenizing train dataset:  21%|██        | 1395/6690 [00:03<00:13, 395.53 examples/s]Tokenizing train dataset:  21%|██▏       | 1435/6690 [00:03<00:13, 391.56 examples/s]Tokenizing train dataset:  22%|██▏       | 1495/6690 [00:03<00:13, 390.95 examples/s]Tokenizing train dataset:  23%|██▎       | 1542/6690 [00:03<00:12, 407.30 examples/s]Tokenizing train dataset:  24%|██▍       | 1598/6690 [00:04<00:12, 392.36 examples/s]Tokenizing train dataset:  25%|██▍       | 1640/6690 [00:04<00:12, 396.98 examples/s]Tokenizing train dataset:  25%|██▌       | 1704/6690 [00:04<00:12, 401.19 examples/s]Tokenizing train dataset:  26%|██▌       | 1750/6690 [00:04<00:11, 412.35 examples/s]Tokenizing train dataset:  27%|██▋       | 1811/6690 [00:04<00:12, 405.70 examples/s]Tokenizing train dataset:  28%|██▊       | 1872/6690 [00:04<00:11, 402.36 examples/s]Tokenizing train dataset:  29%|██▊       | 1916/6690 [00:04<00:13, 361.70 examples/s]Tokenizing train dataset:  29%|██▉       | 1958/6690 [00:04<00:12, 374.73 examples/s]Tokenizing train dataset:  30%|███       | 2011/6690 [00:05<00:12, 364.95 examples/s]Tokenizing train dataset:  31%|███       | 2052/6690 [00:05<00:12, 373.02 examples/s]Tokenizing train dataset:  32%|███▏      | 2114/6690 [00:05<00:11, 383.28 examples/s]Tokenizing train dataset:  32%|███▏      | 2172/6690 [00:05<00:11, 379.76 examples/s]Tokenizing train dataset:  33%|███▎      | 2218/6690 [00:05<00:11, 394.95 examples/s]Tokenizing train dataset:  34%|███▍      | 2260/6690 [00:05<00:11, 398.89 examples/s]Tokenizing train dataset:  35%|███▍      | 2315/6690 [00:05<00:11, 381.76 examples/s]Tokenizing train dataset:  35%|███▌      | 2355/6690 [00:05<00:11, 382.76 examples/s]Tokenizing train dataset:  36%|███▌      | 2411/6690 [00:06<00:11, 376.65 examples/s]Tokenizing train dataset:  37%|███▋      | 2459/6690 [00:06<00:10, 398.67 examples/s]Tokenizing train dataset:  37%|███▋      | 2503/6690 [00:06<00:10, 403.05 examples/s]Tokenizing train dataset:  38%|███▊      | 2553/6690 [00:06<00:09, 424.53 examples/s]Tokenizing train dataset:  39%|███▉      | 2600/6690 [00:06<00:09, 434.19 examples/s]Tokenizing train dataset:  40%|███▉      | 2660/6690 [00:06<00:09, 413.32 examples/s]Tokenizing train dataset:  41%|████      | 2722/6690 [00:06<00:09, 407.55 examples/s]Tokenizing train dataset:  42%|████▏     | 2783/6690 [00:07<00:11, 348.50 examples/s]Tokenizing train dataset:  42%|████▏     | 2820/6690 [00:07<00:11, 349.04 examples/s]Tokenizing train dataset:  43%|████▎     | 2865/6690 [00:07<00:10, 368.48 examples/s]Tokenizing train dataset:  43%|████▎     | 2908/6690 [00:07<00:09, 379.72 examples/s]Tokenizing train dataset:  44%|████▍     | 2954/6690 [00:07<00:09, 395.98 examples/s]Tokenizing train dataset:  45%|████▍     | 2998/6690 [00:07<00:09, 407.42 examples/s]Tokenizing train dataset:  45%|████▌     | 3040/6690 [00:07<00:08, 405.69 examples/s]Tokenizing train dataset:  46%|████▌     | 3094/6690 [00:07<00:08, 439.92 examples/s]Tokenizing train dataset:  47%|████▋     | 3152/6690 [00:07<00:08, 412.83 examples/s]Tokenizing train dataset:  48%|████▊     | 3216/6690 [00:08<00:08, 412.27 examples/s]Tokenizing train dataset:  49%|████▊     | 3260/6690 [00:08<00:08, 414.89 examples/s]Tokenizing train dataset:  50%|████▉     | 3316/6690 [00:08<00:08, 398.50 examples/s]Tokenizing train dataset:  50%|█████     | 3375/6690 [00:08<00:08, 393.70 examples/s]Tokenizing train dataset:  51%|█████     | 3421/6690 [00:08<00:09, 361.05 examples/s]Tokenizing train dataset:  52%|█████▏    | 3485/6690 [00:08<00:08, 376.86 examples/s]Tokenizing train dataset:  53%|█████▎    | 3536/6690 [00:09<00:08, 362.79 examples/s]Tokenizing train dataset:  54%|█████▎    | 3584/6690 [00:09<00:08, 348.85 examples/s]Tokenizing train dataset:  54%|█████▍    | 3636/6690 [00:09<00:08, 344.47 examples/s]Tokenizing train dataset:  55%|█████▌    | 3689/6690 [00:09<00:08, 346.03 examples/s]Tokenizing train dataset:  56%|█████▌    | 3742/6690 [00:09<00:08, 343.03 examples/s]Tokenizing train dataset:  57%|█████▋    | 3782/6690 [00:09<00:08, 353.74 examples/s]Tokenizing train dataset:  57%|█████▋    | 3837/6690 [00:09<00:08, 355.44 examples/s]Tokenizing train dataset:  58%|█████▊    | 3875/6690 [00:09<00:07, 360.79 examples/s]Tokenizing train dataset:  59%|█████▊    | 3919/6690 [00:10<00:07, 378.34 examples/s]Tokenizing train dataset:  59%|█████▉    | 3959/6690 [00:10<00:07, 382.20 examples/s]Tokenizing train dataset:  60%|█████▉    | 4006/6690 [00:10<00:06, 401.99 examples/s]Tokenizing train dataset:  61%|██████    | 4064/6690 [00:10<00:06, 383.82 examples/s]Tokenizing train dataset:  61%|██████▏   | 4105/6690 [00:10<00:06, 385.48 examples/s]Tokenizing train dataset:  62%|██████▏   | 4151/6690 [00:10<00:06, 398.55 examples/s]Tokenizing train dataset:  63%|██████▎   | 4196/6690 [00:10<00:06, 412.12 examples/s]Tokenizing train dataset:  64%|██████▎   | 4254/6690 [00:10<00:06, 396.14 examples/s]Tokenizing train dataset:  65%|██████▍   | 4318/6690 [00:11<00:05, 404.13 examples/s]Tokenizing train dataset:  65%|██████▌   | 4365/6690 [00:11<00:06, 367.96 examples/s]Tokenizing train dataset:  66%|██████▌   | 4409/6690 [00:11<00:06, 342.44 examples/s]Tokenizing train dataset:  67%|██████▋   | 4453/6690 [00:11<00:06, 361.69 examples/s]Tokenizing train dataset:  67%|██████▋   | 4492/6690 [00:11<00:06, 364.43 examples/s]Tokenizing train dataset:  68%|██████▊   | 4534/6690 [00:11<00:05, 374.35 examples/s]Tokenizing train dataset:  68%|██████▊   | 4576/6690 [00:11<00:05, 383.07 examples/s]Tokenizing train dataset:  69%|██████▉   | 4623/6690 [00:11<00:05, 403.12 examples/s]Tokenizing train dataset:  70%|██████▉   | 4668/6690 [00:12<00:04, 414.80 examples/s]Tokenizing train dataset:  70%|███████   | 4710/6690 [00:12<00:04, 412.26 examples/s]Tokenizing train dataset:  71%|███████▏  | 4770/6690 [00:12<00:04, 403.77 examples/s]Tokenizing train dataset:  72%|███████▏  | 4822/6690 [00:12<00:04, 380.75 examples/s]Tokenizing train dataset:  73%|███████▎  | 4869/6690 [00:12<00:04, 398.10 examples/s]Tokenizing train dataset:  74%|███████▎  | 4928/6690 [00:12<00:04, 392.27 examples/s]Tokenizing train dataset:  74%|███████▍  | 4969/6690 [00:12<00:04, 394.07 examples/s]Tokenizing train dataset:  75%|███████▌  | 5027/6690 [00:12<00:04, 389.21 examples/s]Tokenizing train dataset:  76%|███████▌  | 5067/6690 [00:13<00:04, 390.59 examples/s]Tokenizing train dataset:  76%|███████▋  | 5116/6690 [00:13<00:04, 366.21 examples/s]Tokenizing train dataset:  77%|███████▋  | 5180/6690 [00:13<00:03, 383.51 examples/s]Tokenizing train dataset:  78%|███████▊  | 5222/6690 [00:13<00:03, 387.34 examples/s]Tokenizing train dataset:  79%|███████▊  | 5262/6690 [00:13<00:03, 388.93 examples/s]Tokenizing train dataset:  79%|███████▉  | 5316/6690 [00:13<00:03, 374.75 examples/s]Tokenizing train dataset:  80%|████████  | 5362/6690 [00:13<00:03, 394.47 examples/s]Tokenizing train dataset:  81%|████████  | 5407/6690 [00:13<00:03, 407.94 examples/s]Tokenizing train dataset:  82%|████████▏ | 5464/6690 [00:14<00:03, 392.82 examples/s]Tokenizing train dataset:  82%|████████▏ | 5508/6690 [00:14<00:02, 401.21 examples/s]Tokenizing train dataset:  83%|████████▎ | 5562/6690 [00:14<00:02, 384.21 examples/s]Tokenizing train dataset:  84%|████████▍ | 5622/6690 [00:14<00:02, 382.67 examples/s]Tokenizing train dataset:  85%|████████▍ | 5684/6690 [00:14<00:02, 387.10 examples/s]Tokenizing train dataset:  86%|████████▌ | 5726/6690 [00:14<00:02, 393.73 examples/s]Tokenizing train dataset:  86%|████████▌ | 5767/6690 [00:14<00:02, 392.63 examples/s]Tokenizing train dataset:  87%|████████▋ | 5812/6690 [00:14<00:02, 406.43 examples/s]Tokenizing train dataset:  88%|████████▊ | 5868/6690 [00:15<00:02, 391.57 examples/s]Tokenizing train dataset:  88%|████████▊ | 5919/6690 [00:15<00:02, 362.43 examples/s]Tokenizing train dataset:  89%|████████▉ | 5957/6690 [00:15<00:02, 362.74 examples/s]Tokenizing train dataset:  90%|████████▉ | 6000/6690 [00:15<00:01, 375.35 examples/s]Tokenizing train dataset:  91%|█████████ | 6055/6690 [00:15<00:01, 365.64 examples/s]Tokenizing train dataset:  91%|█████████ | 6096/6690 [00:15<00:01, 373.66 examples/s]Tokenizing train dataset:  92%|█████████▏| 6143/6690 [00:15<00:01, 395.09 examples/s]Tokenizing train dataset:  93%|█████████▎| 6202/6690 [00:15<00:01, 391.28 examples/s]Tokenizing train dataset:  93%|█████████▎| 6254/6690 [00:16<00:01, 369.67 examples/s]Tokenizing train dataset:  94%|█████████▍| 6295/6690 [00:16<00:01, 376.04 examples/s]Tokenizing train dataset:  95%|█████████▍| 6352/6690 [00:16<00:00, 373.35 examples/s]Tokenizing train dataset:  96%|█████████▌| 6390/6690 [00:16<00:00, 369.63 examples/s]Tokenizing train dataset:  96%|█████████▌| 6434/6690 [00:16<00:00, 385.47 examples/s]Tokenizing train dataset:  97%|█████████▋| 6475/6690 [00:16<00:00, 390.21 examples/s]Tokenizing train dataset:  98%|█████████▊| 6525/6690 [00:16<00:00, 365.24 examples/s]Tokenizing train dataset:  98%|█████████▊| 6575/6690 [00:17<00:00, 351.70 examples/s]Tokenizing train dataset:  99%|█████████▉| 6611/6690 [00:17<00:00, 352.25 examples/s]Tokenizing train dataset:  99%|█████████▉| 6650/6690 [00:17<00:00, 355.74 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:17<00:00, 365.91 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:17<00:00, 385.44 examples/s]
[rank0]:[W612 20:57:19.881679505 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   8%|▊         | 550/6690 [00:00<00:01, 5422.02 examples/s]Extracting prompt in train dataset:   8%|▊         | 553/6690 [00:00<00:01, 5488.74 examples/s]Extracting prompt in train dataset:   8%|▊         | 550/6690 [00:00<00:01, 5449.40 examples/s]Extracting prompt in eval dataset:  58%|█████▊    | 550/953 [00:00<00:00, 5457.52 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 4844.32 examples/s]
Extracting prompt in train dataset:  19%|█▉        | 1290/6690 [00:00<00:01, 5078.44 examples/s]Extracting prompt in train dataset:  18%|█▊        | 1220/6690 [00:00<00:01, 4723.29 examples/s]Extracting prompt in train dataset:  18%|█▊        | 1214/6690 [00:00<00:01, 4715.82 examples/s]Extracting prompt in train dataset:  27%|██▋       | 1824/6690 [00:00<00:00, 5179.17 examples/s]Extracting prompt in train dataset:  26%|██▋       | 1770/6690 [00:00<00:00, 5021.79 examples/s]Extracting prompt in train dataset:  29%|██▉       | 1941/6690 [00:00<00:00, 4776.67 examples/s]Extracting prompt in train dataset:  34%|███▍      | 2300/6690 [00:00<00:00, 5100.82 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2630/6690 [00:00<00:00, 5251.77 examples/s]Extracting prompt in train dataset:  41%|████      | 2710/6690 [00:00<00:00, 4911.06 examples/s]Extracting prompt in train dataset:  48%|████▊     | 3190/6690 [00:00<00:00, 5341.35 examples/s]Extracting prompt in train dataset:  46%|████▋     | 3100/6690 [00:00<00:00, 5198.06 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  48%|████▊     | 3221/6690 [00:00<00:00, 4965.86 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3740/6690 [00:00<00:00, 5383.35 examples/s]Extracting prompt in train dataset:  55%|█████▍    | 3660/6690 [00:00<00:00, 5299.13 examples/s]Applying chat template to eval dataset:  32%|███▏      | 301/953 [00:00<00:00, 2986.12 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3760/6690 [00:00<00:00, 5072.94 examples/s]Extracting prompt in train dataset:  68%|██████▊   | 4535/6690 [00:00<00:00, 5340.23 examples/s]Extracting prompt in train dataset:  66%|██████▋   | 4447/6690 [00:00<00:00, 5266.06 examples/s]Extracting prompt in train dataset:  64%|██████▍   | 4290/6690 [00:00<00:00, 5138.00 examples/s]Applying chat template to eval dataset:  75%|███████▌  | 719/953 [00:00<00:00, 2845.53 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 5080/6690 [00:00<00:00, 5366.22 examples/s]Extracting prompt in train dataset:  75%|███████▌  | 5022/6690 [00:01<00:00, 4754.02 examples/s]Extracting prompt in train dataset:  72%|███████▏  | 4830/6690 [00:01<00:00, 4414.91 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2288.47 examples/s]
Extracting prompt in train dataset:  87%|████████▋ | 5810/6690 [00:01<00:00, 5167.77 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 5573/6690 [00:01<00:00, 4943.34 examples/s]Extracting prompt in train dataset:  80%|███████▉  | 5340/6690 [00:01<00:00, 4576.02 examples/s]Extracting prompt in train dataset:  95%|█████████▍| 6350/6690 [00:01<00:00, 5219.49 examples/s]Extracting prompt in train dataset:  92%|█████████▏| 6122/6690 [00:01<00:00, 5086.55 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 5860/6690 [00:01<00:00, 4732.29 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5223.55 examples/s]
Extracting prompt in train dataset: 100%|█████████▉| 6680/6690 [00:01<00:00, 5204.64 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5047.85 examples/s]
Extracting prompt in train dataset:  98%|█████████▊| 6580/6690 [00:01<00:00, 4746.15 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 4760.85 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 33/953 [00:00<00:02, 317.58 examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 247.47 examples/s]Applying chat template to train dataset:   4%|▍         | 290/6690 [00:00<00:02, 2859.28 examples/s]Applying chat template to train dataset:   4%|▍         | 286/6690 [00:00<00:02, 2829.03 examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing eval dataset:  10%|▉         | 95/953 [00:00<00:03, 256.40 examples/s]Applying chat template to train dataset:  11%|█         | 703/6690 [00:00<00:02, 2781.79 examples/s]Applying chat template to train dataset:   4%|▍         | 291/6690 [00:00<00:02, 2876.02 examples/s]Applying chat template to train dataset:  11%|█         | 714/6690 [00:00<00:02, 2834.13 examples/s]Tokenizing eval dataset:  14%|█▎        | 131/953 [00:00<00:03, 242.89 examples/s]Applying chat template to train dataset:  15%|█▌        | 1013/6690 [00:00<00:01, 2890.36 examples/s]Applying chat template to train dataset:  16%|█▌        | 1060/6690 [00:00<00:02, 2579.70 examples/s]Applying chat template to train dataset:  11%|█         | 706/6690 [00:00<00:02, 2795.13 examples/s]Tokenizing eval dataset:  17%|█▋        | 166/953 [00:00<00:03, 236.15 examples/s]Applying chat template to train dataset:  15%|█▍        | 989/6690 [00:00<00:02, 2804.58 examples/s]Applying chat template to train dataset:  21%|██        | 1421/6690 [00:00<00:01, 2804.28 examples/s]Applying chat template to train dataset:  22%|██▏       | 1457/6690 [00:00<00:02, 2600.44 examples/s]Applying chat template to train dataset:  19%|█▉        | 1274/6690 [00:00<00:01, 2819.99 examples/s]Tokenizing eval dataset:  21%|██        | 200/953 [00:00<00:03, 226.54 examples/s]Applying chat template to train dataset:  26%|██▌       | 1712/6690 [00:00<00:01, 2833.95 examples/s]Applying chat template to train dataset:  26%|██▌       | 1742/6690 [00:00<00:01, 2667.23 examples/s]Applying chat template to train dataset:  24%|██▎       | 1575/6690 [00:00<00:01, 2881.74 examples/s]Tokenizing eval dataset:  25%|██▌       | 239/953 [00:00<00:02, 265.30 examples/s]Applying chat template to train dataset:  30%|███       | 2011/6690 [00:00<00:01, 2877.98 examples/s]Applying chat template to train dataset:  30%|███       | 2037/6690 [00:00<00:01, 2745.42 examples/s]Applying chat template to train dataset:  28%|██▊       | 1873/6690 [00:00<00:01, 2910.89 examples/s]Tokenizing eval dataset:  32%|███▏      | 303/953 [00:01<00:01, 358.34 examples/s]Applying chat template to train dataset:  34%|███▍      | 2308/6690 [00:00<00:01, 2902.99 examples/s]Applying chat template to train dataset:  36%|███▌      | 2425/6690 [00:00<00:01, 2682.66 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 412.78 examples/s]Applying chat template to train dataset:  35%|███▍      | 2312/6690 [00:00<00:01, 2913.87 examples/s]Applying chat template to train dataset:  41%|████      | 2738/6690 [00:00<00:01, 2884.65 examples/s]Tokenizing eval dataset:  44%|████▎     | 416/953 [00:01<00:01, 451.41 examples/s]Applying chat template to train dataset:  42%|████▏     | 2843/6690 [00:01<00:01, 2716.34 examples/s]Applying chat template to train dataset:  45%|████▌     | 3037/6690 [00:01<00:01, 2909.85 examples/s]Applying chat template to train dataset:  41%|████▏     | 2760/6690 [00:00<00:01, 2934.33 examples/s]Tokenizing eval dataset:  50%|████▉     | 474/953 [00:01<00:00, 482.92 examples/s]Applying chat template to train dataset:  49%|████▊     | 3255/6690 [00:01<00:01, 2725.11 examples/s]Applying chat template to train dataset:  46%|████▌     | 3065/6690 [00:01<00:01, 2962.06 examples/s]Tokenizing eval dataset:  56%|█████▌    | 534/953 [00:01<00:00, 513.99 examples/s]Applying chat template to train dataset:  52%|█████▏    | 3483/6690 [00:01<00:01, 2929.00 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3544/6690 [00:01<00:01, 2763.08 examples/s]Tokenizing eval dataset:  63%|██████▎   | 600/953 [00:01<00:00, 553.23 examples/s]Applying chat template to train dataset:  57%|█████▋    | 3783/6690 [00:01<00:00, 2945.94 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3519/6690 [00:01<00:01, 2981.53 examples/s]Applying chat template to train dataset:  57%|█████▋    | 3836/6690 [00:01<00:01, 2801.60 examples/s]Applying chat template to train dataset:  61%|██████    | 4079/6690 [00:01<00:00, 2949.60 examples/s]Tokenizing eval dataset:  69%|██████▉   | 660/953 [00:01<00:00, 556.10 examples/s]Applying chat template to train dataset:  59%|█████▊    | 3918/6690 [00:01<00:00, 2870.05 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4200/6690 [00:01<00:00, 2665.15 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4488/6690 [00:01<00:00, 2864.45 examples/s]Tokenizing eval dataset:  77%|███████▋  | 735/953 [00:01<00:00, 525.04 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4214/6690 [00:01<00:00, 2890.79 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4481/6690 [00:01<00:00, 2700.86 examples/s]Applying chat template to train dataset:  71%|███████▏  | 4783/6690 [00:01<00:00, 2885.65 examples/s]Tokenizing eval dataset:  85%|████████▍ | 806/953 [00:01<00:00, 499.42 examples/s]Applying chat template to train dataset:  69%|██████▉   | 4642/6690 [00:01<00:00, 2874.63 examples/s]Applying chat template to train dataset:  73%|███████▎  | 4910/6690 [00:01<00:00, 2751.34 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5225/6690 [00:01<00:00, 2904.11 examples/s]Applying chat template to train dataset:  74%|███████▍  | 4942/6690 [00:01<00:00, 2902.26 examples/s]Tokenizing eval dataset:  92%|█████████▏| 879/953 [00:02<00:00, 492.29 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5232/6690 [00:01<00:00, 2500.18 examples/s]Applying chat template to train dataset:  84%|████████▍ | 5610/6690 [00:01<00:00, 2787.20 examples/s]Applying chat template to train dataset:  81%|████████  | 5393/6690 [00:01<00:00, 2935.06 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5531/6690 [00:02<00:00, 2614.05 examples/s]Tokenizing eval dataset: 100%|█████████▉| 950/953 [00:02<00:00, 479.81 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 414.04 examples/s]
Applying chat template to train dataset:  88%|████████▊ | 5908/6690 [00:02<00:00, 2832.67 examples/s]Applying chat template to train dataset:  85%|████████▌ | 5701/6690 [00:01<00:00, 2967.87 examples/s]Applying chat template to train dataset:  87%|████████▋ | 5811/6690 [00:02<00:00, 2660.09 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6208/6690 [00:02<00:00, 2872.58 examples/s]Applying chat template to train dataset:  90%|████████▉ | 6008/6690 [00:02<00:00, 2993.05 examples/s]Applying chat template to train dataset:  91%|█████████▏| 6115/6690 [00:02<00:00, 2759.22 examples/s]Applying chat template to train dataset:  99%|█████████▉| 6630/6690 [00:02<00:00, 2848.18 examples/s]Applying chat template to train dataset:  96%|█████████▌| 6416/6690 [00:02<00:00, 2826.49 examples/s]Applying chat template to train dataset:  96%|█████████▌| 6435/6690 [00:02<00:00, 2937.51 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 2843.62 examples/s]
Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 2713.27 examples/s]
Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 2850.58 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 43/6690 [00:00<00:15, 416.27 examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:15, 417.24 examples/s]Tokenizing train dataset:   1%|          | 35/6690 [00:00<00:20, 331.83 examples/s]Tokenizing train dataset:   1%|▏         | 96/6690 [00:00<00:17, 370.15 examples/s]Tokenizing train dataset:   1%|▏         | 89/6690 [00:00<00:15, 425.00 examples/s]Tokenizing train dataset:   1%|          | 71/6690 [00:00<00:19, 341.18 examples/s]Tokenizing train dataset:   2%|▏         | 159/6690 [00:00<00:16, 386.47 examples/s]Tokenizing train dataset:   2%|▏         | 106/6690 [00:00<00:19, 342.26 examples/s]Tokenizing train dataset:   2%|▏         | 150/6690 [00:00<00:15, 412.82 examples/s]Tokenizing train dataset:   3%|▎         | 199/6690 [00:00<00:16, 387.33 examples/s]Tokenizing train dataset:   2%|▏         | 162/6690 [00:00<00:18, 354.28 examples/s]Tokenizing train dataset:   3%|▎         | 205/6690 [00:00<00:16, 391.55 examples/s]Tokenizing train dataset:   4%|▎         | 239/6690 [00:00<00:16, 387.96 examples/s]Tokenizing train dataset:   3%|▎         | 206/6690 [00:00<00:17, 374.56 examples/s]Tokenizing train dataset:   4%|▎         | 245/6690 [00:00<00:16, 391.94 examples/s]Tokenizing train dataset:   4%|▍         | 290/6690 [00:00<00:17, 363.68 examples/s]Tokenizing train dataset:   4%|▎         | 245/6690 [00:00<00:17, 374.81 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:16, 390.01 examples/s]Tokenizing train dataset:   5%|▍         | 330/6690 [00:00<00:17, 363.03 examples/s]Tokenizing train dataset:   4%|▍         | 288/6690 [00:00<00:18, 339.48 examples/s]Tokenizing train dataset:   5%|▌         | 343/6690 [00:00<00:16, 383.35 examples/s]Tokenizing train dataset:   6%|▌         | 383/6690 [00:01<00:17, 355.87 examples/s]Tokenizing train dataset:   5%|▌         | 342/6690 [00:00<00:18, 343.66 examples/s]Tokenizing train dataset:   6%|▌         | 399/6690 [00:01<00:16, 372.89 examples/s]Tokenizing train dataset:   6%|▋         | 424/6690 [00:01<00:16, 369.06 examples/s]Tokenizing train dataset:   6%|▌         | 392/6690 [00:01<00:16, 381.53 examples/s]Tokenizing train dataset:   7%|▋         | 451/6690 [00:01<00:15, 407.05 examples/s]Tokenizing train dataset:   7%|▋         | 473/6690 [00:01<00:15, 398.28 examples/s]Tokenizing train dataset:   7%|▋         | 500/6690 [00:01<00:14, 423.19 examples/s]Tokenizing train dataset:   7%|▋         | 456/6690 [00:01<00:15, 392.76 examples/s]Tokenizing train dataset:   8%|▊         | 514/6690 [00:01<00:15, 395.81 examples/s]Tokenizing train dataset:   8%|▊         | 509/6690 [00:01<00:16, 377.50 examples/s]Tokenizing train dataset:   8%|▊         | 563/6690 [00:01<00:14, 415.11 examples/s]Tokenizing train dataset:   9%|▊         | 569/6690 [00:01<00:16, 382.05 examples/s]Tokenizing train dataset:   8%|▊         | 550/6690 [00:01<00:15, 384.58 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:14, 423.93 examples/s]Tokenizing train dataset:   9%|▉         | 612/6690 [00:01<00:15, 392.04 examples/s]Tokenizing train dataset:   9%|▉         | 595/6690 [00:01<00:15, 400.38 examples/s]Tokenizing train dataset:  10%|▉         | 656/6690 [00:01<00:14, 404.13 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:13, 436.01 examples/s]Tokenizing train dataset:  10%|▉         | 642/6690 [00:01<00:14, 414.68 examples/s]Tokenizing train dataset:  10%|█         | 699/6690 [00:01<00:14, 410.14 examples/s]Tokenizing train dataset:  11%|█         | 731/6690 [00:01<00:13, 442.81 examples/s]Tokenizing train dataset:  10%|█         | 688/6690 [00:01<00:14, 425.01 examples/s]Tokenizing train dataset:  11%|█         | 747/6690 [00:01<00:13, 428.90 examples/s]Tokenizing train dataset:  12%|█▏        | 780/6690 [00:01<00:13, 450.03 examples/s]Tokenizing train dataset:  12%|█▏        | 792/6690 [00:02<00:13, 432.43 examples/s]Tokenizing train dataset:  11%|█▏        | 756/6690 [00:01<00:13, 431.31 examples/s]Tokenizing train dataset:  12%|█▏        | 827/6690 [00:01<00:12, 454.43 examples/s]Tokenizing train dataset:  12%|█▏        | 805/6690 [00:02<00:13, 444.46 examples/s]Tokenizing train dataset:  13%|█▎        | 855/6690 [00:02<00:13, 423.70 examples/s]Tokenizing train dataset:  13%|█▎        | 892/6690 [00:02<00:13, 443.45 examples/s]Tokenizing train dataset:  13%|█▎        | 868/6690 [00:02<00:13, 433.47 examples/s]Tokenizing train dataset:  14%|█▎        | 910/6690 [00:02<00:14, 398.03 examples/s]Tokenizing train dataset:  14%|█▍        | 958/6690 [00:02<00:13, 439.74 examples/s]Tokenizing train dataset:  14%|█▎        | 919/6690 [00:02<00:14, 395.27 examples/s]Tokenizing train dataset:  14%|█▍        | 970/6690 [00:02<00:14, 396.01 examples/s]Tokenizing train dataset:  15%|█▌        | 1013/6690 [00:02<00:13, 414.11 examples/s]Tokenizing train dataset:  15%|█▍        | 977/6690 [00:02<00:14, 389.47 examples/s]Tokenizing train dataset:  16%|█▌        | 1061/6690 [00:02<00:13, 427.12 examples/s]Tokenizing train dataset:  15%|█▌        | 1026/6690 [00:02<00:14, 381.23 examples/s]Tokenizing train dataset:  15%|█▌        | 1021/6690 [00:02<00:14, 396.50 examples/s]Tokenizing train dataset:  17%|█▋        | 1106/6690 [00:02<00:12, 430.61 examples/s]Tokenizing train dataset:  16%|█▌        | 1079/6690 [00:02<00:15, 366.05 examples/s]Tokenizing train dataset:  16%|█▌        | 1062/6690 [00:02<00:14, 398.10 examples/s]Tokenizing train dataset:  17%|█▋        | 1166/6690 [00:02<00:13, 416.33 examples/s]Tokenizing train dataset:  16%|█▋        | 1103/6690 [00:02<00:14, 396.58 examples/s]Tokenizing train dataset:  17%|█▋        | 1137/6690 [00:02<00:15, 368.29 examples/s]Tokenizing train dataset:  18%|█▊        | 1209/6690 [00:02<00:13, 415.09 examples/s]Tokenizing train dataset:  17%|█▋        | 1145/6690 [00:02<00:13, 398.85 examples/s]Tokenizing train dataset:  18%|█▊        | 1180/6690 [00:03<00:14, 376.00 examples/s]Tokenizing train dataset:  19%|█▉        | 1272/6690 [00:03<00:13, 413.25 examples/s]Tokenizing train dataset:  18%|█▊        | 1203/6690 [00:03<00:13, 392.17 examples/s]Tokenizing train dataset:  18%|█▊        | 1234/6690 [00:03<00:14, 366.44 examples/s]Tokenizing train dataset:  20%|█▉        | 1319/6690 [00:03<00:12, 422.01 examples/s]Tokenizing train dataset:  19%|█▉        | 1275/6690 [00:03<00:14, 374.87 examples/s]Tokenizing train dataset:  19%|█▉        | 1255/6690 [00:03<00:14, 374.27 examples/s]Tokenizing train dataset:  20%|██        | 1365/6690 [00:03<00:12, 430.24 examples/s]Tokenizing train dataset:  20%|█▉        | 1335/6690 [00:03<00:14, 378.70 examples/s]Tokenizing train dataset:  20%|█▉        | 1320/6690 [00:03<00:13, 389.86 examples/s]Tokenizing train dataset:  21%|██        | 1420/6690 [00:03<00:13, 398.42 examples/s]Tokenizing train dataset:  20%|██        | 1362/6690 [00:03<00:13, 391.74 examples/s]Tokenizing train dataset:  21%|██        | 1387/6690 [00:03<00:14, 364.96 examples/s]Tokenizing train dataset:  22%|██▏       | 1481/6690 [00:03<00:13, 396.79 examples/s]Tokenizing train dataset:  21%|██        | 1412/6690 [00:03<00:14, 370.23 examples/s]Tokenizing train dataset:  23%|██▎       | 1533/6690 [00:03<00:12, 420.92 examples/s]Tokenizing train dataset:  22%|██▏       | 1446/6690 [00:03<00:14, 367.33 examples/s]Tokenizing train dataset:  22%|██▏       | 1450/6690 [00:03<00:14, 369.38 examples/s]Tokenizing train dataset:  24%|██▎       | 1581/6690 [00:03<00:13, 385.11 examples/s]Tokenizing train dataset:  22%|██▏       | 1492/6690 [00:03<00:15, 341.38 examples/s]Tokenizing train dataset:  23%|██▎       | 1514/6690 [00:03<00:13, 382.92 examples/s]Tokenizing train dataset:  24%|██▍       | 1624/6690 [00:03<00:12, 392.92 examples/s]Tokenizing train dataset:  23%|██▎       | 1541/6690 [00:04<00:13, 372.60 examples/s]Tokenizing train dataset:  23%|██▎       | 1557/6690 [00:04<00:13, 391.04 examples/s]Tokenizing train dataset:  25%|██▌       | 1690/6690 [00:04<00:12, 405.05 examples/s]Tokenizing train dataset:  24%|██▎       | 1584/6690 [00:04<00:14, 340.89 examples/s]Tokenizing train dataset:  24%|██▍       | 1609/6690 [00:04<00:13, 371.55 examples/s]Tokenizing train dataset:  24%|██▍       | 1621/6690 [00:04<00:14, 341.64 examples/s]Tokenizing train dataset:  26%|██▋       | 1757/6690 [00:04<00:11, 414.29 examples/s]Tokenizing train dataset:  25%|██▍       | 1664/6690 [00:04<00:13, 364.01 examples/s]Tokenizing train dataset:  25%|██▌       | 1679/6690 [00:04<00:14, 352.41 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:04<00:11, 407.94 examples/s]Tokenizing train dataset:  26%|██▌       | 1723/6690 [00:04<00:13, 367.65 examples/s]Tokenizing train dataset:  26%|██▌       | 1728/6690 [00:04<00:14, 340.21 examples/s]Tokenizing train dataset:  28%|██▊       | 1870/6690 [00:04<00:12, 382.42 examples/s]Tokenizing train dataset:  26%|██▋       | 1761/6690 [00:04<00:13, 367.15 examples/s]Tokenizing train dataset:  27%|██▋       | 1780/6690 [00:04<00:14, 339.13 examples/s]Tokenizing train dataset:  29%|██▊       | 1920/6690 [00:04<00:13, 362.39 examples/s]Tokenizing train dataset:  27%|██▋       | 1805/6690 [00:04<00:14, 333.10 examples/s]Tokenizing train dataset:  27%|██▋       | 1832/6690 [00:04<00:14, 338.61 examples/s]Tokenizing train dataset:  28%|██▊       | 1843/6690 [00:04<00:14, 342.50 examples/s]Tokenizing train dataset:  29%|██▉       | 1970/6690 [00:04<00:13, 341.49 examples/s]Tokenizing train dataset:  28%|██▊       | 1886/6690 [00:04<00:13, 360.62 examples/s]Tokenizing train dataset:  28%|██▊       | 1881/6690 [00:05<00:14, 331.56 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:05<00:13, 342.35 examples/s]Tokenizing train dataset:  29%|██▊       | 1919/6690 [00:05<00:14, 335.62 examples/s]Tokenizing train dataset:  29%|██▉       | 1943/6690 [00:05<00:13, 357.28 examples/s]Tokenizing train dataset:  31%|███       | 2071/6690 [00:05<00:15, 292.93 examples/s]Tokenizing train dataset:  29%|██▉       | 1967/6690 [00:05<00:15, 312.93 examples/s]Tokenizing train dataset:  30%|██▉       | 1985/6690 [00:05<00:14, 329.55 examples/s]Tokenizing train dataset:  32%|███▏      | 2109/6690 [00:05<00:14, 307.80 examples/s]Tokenizing train dataset:  30%|██▉       | 2000/6690 [00:05<00:15, 309.60 examples/s]Tokenizing train dataset:  30%|███       | 2025/6690 [00:05<00:13, 341.17 examples/s]Tokenizing train dataset:  32%|███▏      | 2148/6690 [00:05<00:13, 324.60 examples/s]Tokenizing train dataset:  30%|███       | 2040/6690 [00:05<00:14, 328.78 examples/s]Tokenizing train dataset:  31%|███       | 2080/6690 [00:05<00:13, 346.36 examples/s]Tokenizing train dataset:  33%|███▎      | 2200/6690 [00:05<00:15, 293.87 examples/s]Tokenizing train dataset:  31%|███       | 2081/6690 [00:05<00:16, 275.93 examples/s]Tokenizing train dataset:  32%|███▏      | 2126/6690 [00:05<00:13, 327.61 examples/s]Tokenizing train dataset:  34%|███▎      | 2242/6690 [00:05<00:14, 317.52 examples/s]Tokenizing train dataset:  32%|███▏      | 2125/6690 [00:05<00:16, 272.15 examples/s]Tokenizing train dataset:  33%|███▎      | 2178/6690 [00:05<00:14, 319.76 examples/s]Tokenizing train dataset:  34%|███▍      | 2282/6690 [00:05<00:13, 333.25 examples/s]Tokenizing train dataset:  33%|███▎      | 2176/6690 [00:06<00:17, 265.17 examples/s]Tokenizing train dataset:  33%|███▎      | 2228/6690 [00:06<00:16, 277.26 examples/s]Tokenizing train dataset:  35%|███▍      | 2329/6690 [00:06<00:15, 274.94 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:06<00:15, 280.83 examples/s]Tokenizing train dataset:  33%|███▎      | 2222/6690 [00:06<00:16, 264.23 examples/s]Tokenizing train dataset:  34%|███▍      | 2276/6690 [00:06<00:15, 287.07 examples/s]Tokenizing train dataset:  36%|███▌      | 2395/6690 [00:06<00:14, 292.48 examples/s]Tokenizing train dataset:  34%|███▍      | 2267/6690 [00:06<00:16, 270.31 examples/s]Tokenizing train dataset:  35%|███▍      | 2314/6690 [00:06<00:15, 275.74 examples/s]Tokenizing train dataset:  37%|███▋      | 2448/6690 [00:06<00:13, 305.72 examples/s]Tokenizing train dataset:  34%|███▍      | 2308/6690 [00:06<00:17, 255.56 examples/s]Tokenizing train dataset:  35%|███▌      | 2362/6690 [00:06<00:15, 280.87 examples/s]Tokenizing train dataset:  37%|███▋      | 2499/6690 [00:06<00:13, 310.53 examples/s]Tokenizing train dataset:  35%|███▌      | 2354/6690 [00:06<00:17, 248.15 examples/s]Tokenizing train dataset:  36%|███▌      | 2405/6690 [00:06<00:16, 258.59 examples/s]Tokenizing train dataset:  38%|███▊      | 2560/6690 [00:06<00:12, 333.39 examples/s]Tokenizing train dataset:  36%|███▌      | 2396/6690 [00:07<00:17, 244.07 examples/s]Tokenizing train dataset:  37%|███▋      | 2454/6690 [00:06<00:16, 256.35 examples/s]Tokenizing train dataset:  39%|███▉      | 2613/6690 [00:06<00:12, 322.83 examples/s]Tokenizing train dataset:  36%|███▋      | 2439/6690 [00:07<00:17, 240.96 examples/s]Tokenizing train dataset:  37%|███▋      | 2498/6690 [00:07<00:16, 256.18 examples/s]Tokenizing train dataset:  40%|███▉      | 2656/6690 [00:07<00:13, 299.16 examples/s]Tokenizing train dataset:  37%|███▋      | 2478/6690 [00:07<00:15, 269.10 examples/s]Tokenizing train dataset:  38%|███▊      | 2537/6690 [00:07<00:14, 280.96 examples/s]Tokenizing train dataset:  40%|████      | 2702/6690 [00:07<00:14, 268.70 examples/s]Tokenizing train dataset:  38%|███▊      | 2525/6690 [00:07<00:14, 279.54 examples/s]Tokenizing train dataset:  39%|███▊      | 2587/6690 [00:07<00:14, 292.25 examples/s]Tokenizing train dataset:  41%|████      | 2734/6690 [00:07<00:14, 276.58 examples/s]Tokenizing train dataset:  38%|███▊      | 2563/6690 [00:07<00:13, 299.27 examples/s]Tokenizing train dataset:  39%|███▉      | 2627/6690 [00:07<00:13, 311.61 examples/s]Tokenizing train dataset:  42%|████▏     | 2777/6690 [00:07<00:14, 275.93 examples/s]Tokenizing train dataset:  39%|███▉      | 2613/6690 [00:07<00:13, 308.18 examples/s]Tokenizing train dataset:  40%|███▉      | 2669/6690 [00:07<00:13, 299.61 examples/s]Tokenizing train dataset:  42%|████▏     | 2819/6690 [00:07<00:14, 272.47 examples/s]Tokenizing train dataset:  40%|███▉      | 2653/6690 [00:07<00:13, 290.24 examples/s]Tokenizing train dataset:  41%|████      | 2713/6690 [00:07<00:14, 275.35 examples/s]Tokenizing train dataset:  43%|████▎     | 2870/6690 [00:07<00:13, 278.51 examples/s]Tokenizing train dataset:  40%|████      | 2698/6690 [00:08<00:16, 247.38 examples/s]Tokenizing train dataset:  41%|████      | 2753/6690 [00:08<00:17, 226.43 examples/s]Tokenizing train dataset:  44%|████▎     | 2923/6690 [00:08<00:13, 289.29 examples/s]Tokenizing train dataset:  41%|████      | 2729/6690 [00:08<00:15, 259.28 examples/s]Tokenizing train dataset:  42%|████▏     | 2786/6690 [00:08<00:15, 245.45 examples/s]Tokenizing train dataset:  44%|████▍     | 2970/6690 [00:08<00:11, 324.86 examples/s]Tokenizing train dataset:  42%|████▏     | 2815/6690 [00:08<00:15, 252.35 examples/s]Tokenizing train dataset:  41%|████▏     | 2770/6690 [00:08<00:15, 249.87 examples/s]Tokenizing train dataset:  45%|████▌     | 3016/6690 [00:08<00:10, 353.01 examples/s]Tokenizing train dataset:  43%|████▎     | 2864/6690 [00:08<00:15, 251.31 examples/s]Tokenizing train dataset:  42%|████▏     | 2810/6690 [00:08<00:17, 222.34 examples/s]Tokenizing train dataset:  46%|████▌     | 3070/6690 [00:08<00:12, 287.62 examples/s]Tokenizing train dataset:  43%|████▎     | 2903/6690 [00:08<00:13, 278.48 examples/s]Tokenizing train dataset:  43%|████▎     | 2848/6690 [00:08<00:15, 251.51 examples/s]Tokenizing train dataset:  47%|████▋     | 3120/6690 [00:08<00:12, 279.56 examples/s]Tokenizing train dataset:  44%|████▍     | 2956/6690 [00:08<00:13, 282.41 examples/s]Tokenizing train dataset:  43%|████▎     | 2892/6690 [00:08<00:14, 262.45 examples/s]Tokenizing train dataset:  44%|████▎     | 2924/6690 [00:08<00:13, 273.91 examples/s]Tokenizing train dataset:  47%|████▋     | 3173/6690 [00:08<00:11, 296.53 examples/s]Tokenizing train dataset:  45%|████▍     | 3010/6690 [00:08<00:12, 303.65 examples/s]Tokenizing train dataset:  44%|████▍     | 2967/6690 [00:09<00:12, 309.18 examples/s]Tokenizing train dataset:  48%|████▊     | 3212/6690 [00:09<00:11, 313.75 examples/s]Tokenizing train dataset:  46%|████▌     | 3050/6690 [00:09<00:11, 321.61 examples/s]Tokenizing train dataset:  45%|████▍     | 3003/6690 [00:09<00:11, 319.21 examples/s]Tokenizing train dataset:  49%|████▊     | 3258/6690 [00:09<00:10, 341.09 examples/s]Tokenizing train dataset:  46%|████▋     | 3100/6690 [00:09<00:09, 360.08 examples/s]Tokenizing train dataset:  45%|████▌     | 3042/6690 [00:09<00:10, 335.48 examples/s]Tokenizing train dataset:  49%|████▉     | 3298/6690 [00:09<00:09, 353.76 examples/s]Tokenizing train dataset:  47%|████▋     | 3151/6690 [00:09<00:10, 351.12 examples/s]Tokenizing train dataset:  46%|████▌     | 3094/6690 [00:09<00:09, 382.32 examples/s]Tokenizing train dataset:  50%|█████     | 3351/6690 [00:09<00:09, 348.83 examples/s]Tokenizing train dataset:  48%|████▊     | 3212/6690 [00:09<00:09, 363.45 examples/s]Tokenizing train dataset:  47%|████▋     | 3142/6690 [00:09<00:09, 356.01 examples/s]Tokenizing train dataset:  51%|█████     | 3388/6690 [00:09<00:09, 349.19 examples/s]Tokenizing train dataset:  49%|████▉     | 3268/6690 [00:09<00:09, 363.59 examples/s]Tokenizing train dataset:  48%|████▊     | 3184/6690 [00:09<00:10, 327.32 examples/s]Tokenizing train dataset:  51%|█████▏    | 3444/6690 [00:09<00:09, 350.13 examples/s]Tokenizing train dataset:  50%|████▉     | 3319/6690 [00:09<00:09, 353.09 examples/s]Tokenizing train dataset:  48%|████▊     | 3234/6690 [00:09<00:11, 295.05 examples/s]Tokenizing train dataset:  52%|█████▏    | 3492/6690 [00:09<00:10, 307.72 examples/s]Tokenizing train dataset:  50%|█████     | 3363/6690 [00:09<00:09, 364.04 examples/s]Tokenizing train dataset:  53%|█████▎    | 3527/6690 [00:09<00:10, 313.87 examples/s]Tokenizing train dataset:  49%|████▉     | 3285/6690 [00:10<00:11, 305.45 examples/s]Tokenizing train dataset:  51%|█████     | 3411/6690 [00:10<00:09, 346.42 examples/s]Tokenizing train dataset:  53%|█████▎    | 3560/6690 [00:10<00:09, 313.44 examples/s]Tokenizing train dataset:  50%|████▉     | 3318/6690 [00:10<00:12, 274.32 examples/s]Tokenizing train dataset:  52%|█████▏    | 3448/6690 [00:10<00:10, 300.86 examples/s]Tokenizing train dataset:  50%|█████     | 3349/6690 [00:10<00:11, 279.94 examples/s]Tokenizing train dataset:  54%|█████▍    | 3598/6690 [00:10<00:11, 272.42 examples/s]Tokenizing train dataset:  51%|█████     | 3380/6690 [00:10<00:11, 285.19 examples/s]Tokenizing train dataset:  52%|█████▏    | 3498/6690 [00:10<00:10, 308.67 examples/s]Tokenizing train dataset:  54%|█████▍    | 3637/6690 [00:10<00:10, 297.08 examples/s]Tokenizing train dataset:  51%|█████     | 3411/6690 [00:10<00:11, 286.50 examples/s]Tokenizing train dataset:  53%|█████▎    | 3531/6690 [00:10<00:10, 312.65 examples/s]Tokenizing train dataset:  55%|█████▌    | 3690/6690 [00:10<00:09, 309.62 examples/s]Tokenizing train dataset:  52%|█████▏    | 3446/6690 [00:10<00:10, 300.74 examples/s]Tokenizing train dataset:  53%|█████▎    | 3579/6690 [00:10<00:09, 313.04 examples/s]Tokenizing train dataset:  56%|█████▌    | 3725/6690 [00:10<00:09, 317.41 examples/s]Tokenizing train dataset:  52%|█████▏    | 3483/6690 [00:10<00:10, 317.72 examples/s]Tokenizing train dataset:  54%|█████▍    | 3620/6690 [00:10<00:10, 298.18 examples/s]Tokenizing train dataset:  56%|█████▋    | 3775/6690 [00:10<00:09, 318.97 examples/s]Tokenizing train dataset:  53%|█████▎    | 3523/6690 [00:10<00:10, 288.33 examples/s]Tokenizing train dataset:  55%|█████▍    | 3651/6690 [00:10<00:10, 299.52 examples/s]Tokenizing train dataset:  57%|█████▋    | 3808/6690 [00:10<00:09, 319.29 examples/s]Tokenizing train dataset:  53%|█████▎    | 3560/6690 [00:11<00:11, 262.09 examples/s]Tokenizing train dataset:  55%|█████▌    | 3700/6690 [00:11<00:10, 297.68 examples/s]Tokenizing train dataset:  58%|█████▊    | 3858/6690 [00:11<00:08, 319.46 examples/s]Tokenizing train dataset:  56%|█████▌    | 3734/6690 [00:11<00:09, 304.77 examples/s]Tokenizing train dataset:  58%|█████▊    | 3893/6690 [00:11<00:08, 323.65 examples/s]Tokenizing train dataset:  54%|█████▍    | 3603/6690 [00:11<00:11, 266.67 examples/s]Tokenizing train dataset:  59%|█████▉    | 3932/6690 [00:11<00:08, 337.87 examples/s]Tokenizing train dataset:  56%|█████▋    | 3772/6690 [00:11<00:09, 319.09 examples/s]Tokenizing train dataset:  54%|█████▍    | 3636/6690 [00:11<00:10, 280.19 examples/s]Tokenizing train dataset:  59%|█████▉    | 3976/6690 [00:11<00:07, 363.83 examples/s]Tokenizing train dataset:  55%|█████▍    | 3668/6690 [00:11<00:10, 287.62 examples/s]Tokenizing train dataset:  57%|█████▋    | 3810/6690 [00:11<00:08, 327.48 examples/s]Tokenizing train dataset:  60%|██████    | 4022/6690 [00:11<00:06, 388.48 examples/s]Tokenizing train dataset:  55%|█████▌    | 3701/6690 [00:11<00:10, 291.62 examples/s]Tokenizing train dataset:  58%|█████▊    | 3861/6690 [00:11<00:08, 330.25 examples/s]Tokenizing train dataset:  61%|██████    | 4070/6690 [00:11<00:07, 360.65 examples/s]Tokenizing train dataset:  56%|█████▌    | 3740/6690 [00:11<00:10, 276.61 examples/s]Tokenizing train dataset:  59%|█████▊    | 3919/6690 [00:11<00:08, 346.35 examples/s]Tokenizing train dataset:  61%|██████▏   | 4111/6690 [00:11<00:06, 370.61 examples/s]Tokenizing train dataset:  56%|█████▋    | 3775/6690 [00:11<00:09, 293.23 examples/s]Tokenizing train dataset:  59%|█████▉    | 3956/6690 [00:11<00:07, 350.49 examples/s]Tokenizing train dataset:  62%|██████▏   | 4173/6690 [00:11<00:06, 378.80 examples/s]Tokenizing train dataset:  57%|█████▋    | 3820/6690 [00:11<00:10, 276.16 examples/s]Tokenizing train dataset:  60%|█████▉    | 4006/6690 [00:11<00:07, 340.35 examples/s]Tokenizing train dataset:  63%|██████▎   | 4215/6690 [00:11<00:06, 384.54 examples/s]Tokenizing train dataset:  58%|█████▊    | 3866/6690 [00:12<00:10, 280.69 examples/s]Tokenizing train dataset:  61%|██████    | 4057/6690 [00:12<00:07, 336.42 examples/s]Tokenizing train dataset:  64%|██████▍   | 4267/6690 [00:12<00:06, 370.66 examples/s]Tokenizing train dataset:  58%|█████▊    | 3908/6690 [00:12<00:08, 310.94 examples/s]Tokenizing train dataset:  61%|██████    | 4093/6690 [00:12<00:07, 341.08 examples/s]Tokenizing train dataset:  64%|██████▍   | 4309/6690 [00:12<00:06, 381.93 examples/s]Tokenizing train dataset:  59%|█████▉    | 3954/6690 [00:12<00:10, 252.82 examples/s]Tokenizing train dataset:  62%|██████▏   | 4140/6690 [00:12<00:09, 272.93 examples/s]Tokenizing train dataset:  65%|██████▌   | 4354/6690 [00:12<00:08, 285.59 examples/s]Tokenizing train dataset:  60%|█████▉    | 3997/6690 [00:12<00:09, 288.59 examples/s]Tokenizing train dataset:  63%|██████▎   | 4190/6690 [00:12<00:08, 285.54 examples/s]Tokenizing train dataset:  66%|██████▌   | 4407/6690 [00:12<00:07, 302.69 examples/s]Tokenizing train dataset:  63%|██████▎   | 4227/6690 [00:12<00:08, 299.82 examples/s]Tokenizing train dataset:  60%|██████    | 4039/6690 [00:12<00:09, 273.73 examples/s]Tokenizing train dataset:  67%|██████▋   | 4458/6690 [00:12<00:07, 309.99 examples/s]Tokenizing train dataset:  64%|██████▍   | 4270/6690 [00:12<00:08, 285.97 examples/s]Tokenizing train dataset:  61%|██████    | 4080/6690 [00:13<00:10, 240.04 examples/s]Tokenizing train dataset:  67%|██████▋   | 4508/6690 [00:12<00:06, 312.77 examples/s]Tokenizing train dataset:  64%|██████▍   | 4311/6690 [00:12<00:07, 312.08 examples/s]Tokenizing train dataset:  62%|██████▏   | 4124/6690 [00:13<00:09, 279.05 examples/s]Tokenizing train dataset:  68%|██████▊   | 4545/6690 [00:13<00:06, 322.95 examples/s]Tokenizing train dataset:  65%|██████▍   | 4345/6690 [00:13<00:07, 314.92 examples/s]Tokenizing train dataset:  62%|██████▏   | 4158/6690 [00:13<00:08, 289.37 examples/s]Tokenizing train dataset:  63%|██████▎   | 4190/6690 [00:13<00:08, 295.52 examples/s]Tokenizing train dataset:  69%|██████▊   | 4590/6690 [00:13<00:07, 297.93 examples/s]Tokenizing train dataset:  65%|██████▌   | 4381/6690 [00:13<00:08, 281.08 examples/s]Tokenizing train dataset:  63%|██████▎   | 4245/6690 [00:13<00:07, 315.99 examples/s]Tokenizing train dataset:  69%|██████▉   | 4647/6690 [00:13<00:06, 297.24 examples/s]Tokenizing train dataset:  66%|██████▌   | 4426/6690 [00:13<00:08, 264.42 examples/s]Tokenizing train dataset:  70%|██████▉   | 4678/6690 [00:13<00:06, 299.42 examples/s]Tokenizing train dataset:  64%|██████▍   | 4291/6690 [00:13<00:07, 311.97 examples/s]Tokenizing train dataset:  67%|██████▋   | 4468/6690 [00:13<00:08, 254.73 examples/s]Tokenizing train dataset:  71%|███████   | 4728/6690 [00:13<00:06, 302.09 examples/s]Tokenizing train dataset:  65%|██████▍   | 4340/6690 [00:13<00:07, 301.35 examples/s]Tokenizing train dataset:  67%|██████▋   | 4511/6690 [00:13<00:10, 216.69 examples/s]Tokenizing train dataset:  71%|███████▏  | 4771/6690 [00:13<00:07, 272.58 examples/s]Tokenizing train dataset:  65%|██████▌   | 4376/6690 [00:13<00:08, 265.25 examples/s]Tokenizing train dataset:  68%|██████▊   | 4557/6690 [00:13<00:08, 257.85 examples/s]Tokenizing train dataset:  72%|███████▏  | 4808/6690 [00:13<00:06, 290.48 examples/s]Tokenizing train dataset:  66%|██████▌   | 4413/6690 [00:14<00:08, 284.13 examples/s]Tokenizing train dataset:  69%|██████▉   | 4608/6690 [00:14<00:07, 279.11 examples/s]Tokenizing train dataset:  73%|███████▎  | 4855/6690 [00:14<00:06, 275.05 examples/s]Tokenizing train dataset:  67%|██████▋   | 4459/6690 [00:14<00:08, 268.85 examples/s]Tokenizing train dataset:  70%|██████▉   | 4660/6690 [00:14<00:07, 271.40 examples/s]Tokenizing train dataset:  73%|███████▎  | 4901/6690 [00:14<00:06, 267.27 examples/s]Tokenizing train dataset:  67%|██████▋   | 4500/6690 [00:14<00:08, 248.51 examples/s]Tokenizing train dataset:  68%|██████▊   | 4527/6690 [00:14<00:08, 251.07 examples/s]Tokenizing train dataset:  70%|███████   | 4704/6690 [00:14<00:07, 268.61 examples/s]Tokenizing train dataset:  74%|███████▍  | 4952/6690 [00:14<00:06, 285.75 examples/s]Tokenizing train dataset:  68%|██████▊   | 4570/6690 [00:14<00:08, 251.13 examples/s]Tokenizing train dataset:  71%|███████   | 4749/6690 [00:14<00:07, 270.15 examples/s]Tokenizing train dataset:  75%|███████▍  | 4997/6690 [00:14<00:05, 286.87 examples/s]Tokenizing train dataset:  69%|██████▉   | 4619/6690 [00:14<00:07, 266.59 examples/s]Tokenizing train dataset:  72%|███████▏  | 4786/6690 [00:14<00:08, 234.95 examples/s]Tokenizing train dataset:  75%|███████▌  | 5038/6690 [00:14<00:06, 248.29 examples/s]Tokenizing train dataset:  70%|██████▉   | 4650/6690 [00:15<00:07, 274.43 examples/s]Tokenizing train dataset:  72%|███████▏  | 4820/6690 [00:14<00:07, 253.22 examples/s]Tokenizing train dataset:  76%|███████▌  | 5070/6690 [00:14<00:06, 259.09 examples/s]Tokenizing train dataset:  70%|███████   | 4695/6690 [00:15<00:07, 278.89 examples/s]Tokenizing train dataset:  73%|███████▎  | 4862/6690 [00:15<00:06, 286.68 examples/s]Tokenizing train dataset:  76%|███████▋  | 5108/6690 [00:15<00:05, 282.79 examples/s]Tokenizing train dataset:  71%|███████   | 4728/6690 [00:15<00:06, 289.17 examples/s]Tokenizing train dataset:  73%|███████▎  | 4910/6690 [00:15<00:06, 293.89 examples/s]Tokenizing train dataset:  77%|███████▋  | 5154/6690 [00:15<00:05, 286.31 examples/s]Tokenizing train dataset:  71%|███████   | 4764/6690 [00:15<00:06, 302.30 examples/s]Tokenizing train dataset:  74%|███████▍  | 4942/6690 [00:15<00:05, 299.01 examples/s]Tokenizing train dataset:  78%|███████▊  | 5206/6690 [00:15<00:05, 269.80 examples/s]Tokenizing train dataset:  72%|███████▏  | 4805/6690 [00:15<00:07, 259.07 examples/s]Tokenizing train dataset:  74%|███████▍  | 4982/6690 [00:15<00:05, 287.24 examples/s]Tokenizing train dataset:  78%|███████▊  | 5240/6690 [00:15<00:05, 280.52 examples/s]Tokenizing train dataset:  72%|███████▏  | 4841/6690 [00:15<00:06, 281.00 examples/s]Tokenizing train dataset:  75%|███████▍  | 5015/6690 [00:15<00:05, 294.89 examples/s]Tokenizing train dataset:  79%|███████▉  | 5280/6690 [00:15<00:04, 305.65 examples/s]Tokenizing train dataset:  73%|███████▎  | 4877/6690 [00:15<00:06, 296.77 examples/s]Tokenizing train dataset:  75%|███████▌  | 5048/6690 [00:15<00:05, 301.28 examples/s]Tokenizing train dataset:  80%|███████▉  | 5333/6690 [00:15<00:04, 317.30 examples/s]Tokenizing train dataset:  74%|███████▎  | 4926/6690 [00:15<00:05, 306.36 examples/s]Tokenizing train dataset:  76%|███████▌  | 5091/6690 [00:15<00:06, 265.50 examples/s]Tokenizing train dataset:  80%|████████  | 5380/6690 [00:15<00:03, 349.81 examples/s]Tokenizing train dataset:  74%|███████▍  | 4974/6690 [00:16<00:05, 307.83 examples/s]Tokenizing train dataset:  77%|███████▋  | 5124/6690 [00:15<00:05, 278.22 examples/s]Tokenizing train dataset:  81%|████████  | 5428/6690 [00:16<00:03, 336.42 examples/s]Tokenizing train dataset:  77%|███████▋  | 5178/6690 [00:16<00:04, 302.46 examples/s]Tokenizing train dataset:  75%|███████▌  | 5024/6690 [00:16<00:05, 311.14 examples/s]Tokenizing train dataset:  82%|████████▏ | 5465/6690 [00:16<00:03, 343.70 examples/s]Tokenizing train dataset:  78%|███████▊  | 5211/6690 [00:16<00:04, 305.82 examples/s]Tokenizing train dataset:  76%|███████▌  | 5065/6690 [00:16<00:05, 296.53 examples/s]Tokenizing train dataset:  83%|████████▎ | 5520/6690 [00:16<00:03, 345.65 examples/s]Tokenizing train dataset:  78%|███████▊  | 5247/6690 [00:16<00:04, 316.14 examples/s]Tokenizing train dataset:  76%|███████▌  | 5101/6690 [00:16<00:05, 306.02 examples/s]Tokenizing train dataset:  83%|████████▎ | 5558/6690 [00:16<00:03, 353.27 examples/s]Tokenizing train dataset:  79%|███████▉  | 5282/6690 [00:16<00:04, 319.24 examples/s]Tokenizing train dataset:  77%|███████▋  | 5134/6690 [00:16<00:05, 307.82 examples/s]Tokenizing train dataset:  84%|████████▎ | 5600/6690 [00:16<00:03, 318.78 examples/s]Tokenizing train dataset:  80%|███████▉  | 5321/6690 [00:16<00:04, 295.47 examples/s]Tokenizing train dataset:  78%|███████▊  | 5186/6690 [00:16<00:04, 316.65 examples/s]Tokenizing train dataset:  84%|████████▍ | 5643/6690 [00:16<00:03, 343.32 examples/s]Tokenizing train dataset:  80%|████████  | 5354/6690 [00:16<00:04, 301.86 examples/s]Tokenizing train dataset:  78%|███████▊  | 5229/6690 [00:16<00:04, 298.39 examples/s]Tokenizing train dataset:  85%|████████▌ | 5688/6690 [00:16<00:03, 265.96 examples/s]Tokenizing train dataset:  81%|████████  | 5405/6690 [00:16<00:04, 257.02 examples/s]Tokenizing train dataset:  79%|███████▉  | 5275/6690 [00:17<00:05, 277.71 examples/s]Tokenizing train dataset:  86%|████████▌ | 5737/6690 [00:17<00:03, 266.88 examples/s]Tokenizing train dataset:  81%|████████▏ | 5444/6690 [00:17<00:05, 236.53 examples/s]Tokenizing train dataset:  79%|███████▉  | 5314/6690 [00:17<00:05, 266.09 examples/s]Tokenizing train dataset:  86%|████████▋ | 5774/6690 [00:17<00:03, 286.26 examples/s]Tokenizing train dataset:  82%|████████▏ | 5485/6690 [00:17<00:04, 269.81 examples/s]Tokenizing train dataset:  87%|████████▋ | 5820/6690 [00:17<00:02, 322.63 examples/s]Tokenizing train dataset:  80%|████████  | 5363/6690 [00:17<00:04, 277.51 examples/s]Tokenizing train dataset:  83%|████████▎ | 5531/6690 [00:17<00:04, 278.12 examples/s]Tokenizing train dataset:  88%|████████▊ | 5860/6690 [00:17<00:02, 289.82 examples/s]Tokenizing train dataset:  81%|████████  | 5411/6690 [00:17<00:05, 244.29 examples/s]Tokenizing train dataset:  83%|████████▎ | 5573/6690 [00:17<00:04, 258.67 examples/s]Tokenizing train dataset:  88%|████████▊ | 5904/6690 [00:17<00:02, 320.95 examples/s]Tokenizing train dataset:  82%|████████▏ | 5453/6690 [00:17<00:04, 250.97 examples/s]Tokenizing train dataset:  84%|████████▍ | 5614/6690 [00:17<00:04, 261.10 examples/s]Tokenizing train dataset:  89%|████████▉ | 5957/6690 [00:17<00:02, 325.42 examples/s]Tokenizing train dataset:  82%|████████▏ | 5486/6690 [00:17<00:04, 265.12 examples/s]Tokenizing train dataset:  85%|████████▍ | 5656/6690 [00:17<00:03, 261.95 examples/s]Tokenizing train dataset:  83%|████████▎ | 5520/6690 [00:18<00:04, 277.65 examples/s]Tokenizing train dataset:  90%|████████▉ | 6003/6690 [00:17<00:02, 297.55 examples/s]Tokenizing train dataset:  85%|████████▌ | 5702/6690 [00:18<00:03, 254.18 examples/s]Tokenizing train dataset:  83%|████████▎ | 5560/6690 [00:18<00:04, 267.15 examples/s]Tokenizing train dataset:  90%|█████████ | 6049/6690 [00:18<00:02, 261.41 examples/s]Tokenizing train dataset:  86%|████████▌ | 5750/6690 [00:18<00:03, 266.36 examples/s]Tokenizing train dataset:  84%|████████▎ | 5600/6690 [00:18<00:04, 254.52 examples/s]Tokenizing train dataset:  91%|█████████ | 6103/6690 [00:18<00:02, 281.98 examples/s]Tokenizing train dataset:  86%|████████▋ | 5783/6690 [00:18<00:03, 277.77 examples/s]Tokenizing train dataset:  92%|█████████▏| 6148/6690 [00:18<00:01, 314.18 examples/s]Tokenizing train dataset:  84%|████████▍ | 5646/6690 [00:18<00:04, 258.50 examples/s]Tokenizing train dataset:  87%|████████▋ | 5814/6690 [00:18<00:03, 281.82 examples/s]Tokenizing train dataset:  93%|█████████▎| 6190/6690 [00:18<00:01, 335.69 examples/s]Tokenizing train dataset:  85%|████████▍ | 5679/6690 [00:18<00:03, 270.90 examples/s]Tokenizing train dataset:  88%|████████▊ | 5861/6690 [00:18<00:02, 287.46 examples/s]Tokenizing train dataset:  85%|████████▌ | 5713/6690 [00:18<00:03, 283.71 examples/s]Tokenizing train dataset:  93%|█████████▎| 6239/6690 [00:18<00:01, 327.75 examples/s]Tokenizing train dataset:  88%|████████▊ | 5909/6690 [00:18<00:02, 329.81 examples/s]Tokenizing train dataset:  86%|████████▌ | 5748/6690 [00:18<00:03, 298.70 examples/s]Tokenizing train dataset:  94%|█████████▍| 6278/6690 [00:18<00:01, 341.01 examples/s]Tokenizing train dataset:  86%|████████▋ | 5782/6690 [00:18<00:02, 308.00 examples/s]Tokenizing train dataset:  89%|████████▉ | 5963/6690 [00:18<00:02, 336.13 examples/s]Tokenizing train dataset:  95%|█████████▍| 6331/6690 [00:18<00:01, 340.22 examples/s]Tokenizing train dataset:  87%|████████▋ | 5823/6690 [00:19<00:03, 279.08 examples/s]Tokenizing train dataset:  90%|████████▉ | 6009/6690 [00:19<00:02, 258.40 examples/s]Tokenizing train dataset:  95%|█████████▌| 6375/6690 [00:19<00:01, 281.15 examples/s]Tokenizing train dataset:  88%|████████▊ | 5867/6690 [00:19<00:02, 280.59 examples/s]Tokenizing train dataset:  90%|█████████ | 6041/6690 [00:19<00:02, 268.29 examples/s]Tokenizing train dataset:  96%|█████████▌| 6420/6690 [00:19<00:00, 313.22 examples/s]Tokenizing train dataset:  88%|████████▊ | 5909/6690 [00:19<00:02, 310.89 examples/s]Tokenizing train dataset:  91%|█████████ | 6085/6690 [00:19<00:01, 304.16 examples/s]Tokenizing train dataset:  97%|█████████▋| 6478/6690 [00:19<00:00, 329.27 examples/s]Tokenizing train dataset:  92%|█████████▏| 6127/6690 [00:19<00:01, 329.51 examples/s]Tokenizing train dataset:  89%|████████▉ | 5950/6690 [00:19<00:02, 286.80 examples/s]Tokenizing train dataset:  92%|█████████▏| 6163/6690 [00:19<00:01, 335.79 examples/s]Tokenizing train dataset:  89%|████████▉ | 5981/6690 [00:19<00:02, 289.36 examples/s]Tokenizing train dataset:  98%|█████████▊| 6535/6690 [00:19<00:00, 343.61 examples/s]Tokenizing train dataset:  93%|█████████▎| 6200/6690 [00:19<00:01, 342.33 examples/s]Tokenizing train dataset:  90%|█████████ | 6025/6690 [00:19<00:02, 276.72 examples/s]Tokenizing train dataset:  98%|█████████▊| 6578/6690 [00:19<00:00, 300.13 examples/s]Tokenizing train dataset:  93%|█████████▎| 6237/6690 [00:19<00:01, 302.89 examples/s]Tokenizing train dataset:  90%|█████████ | 6054/6690 [00:19<00:02, 277.23 examples/s]Tokenizing train dataset:  91%|█████████ | 6100/6690 [00:20<00:01, 320.64 examples/s]Tokenizing train dataset:  99%|█████████▉| 6620/6690 [00:19<00:00, 291.10 examples/s]Tokenizing train dataset:  94%|█████████▍| 6289/6690 [00:19<00:01, 316.48 examples/s]Tokenizing train dataset:  92%|█████████▏| 6145/6690 [00:20<00:01, 351.56 examples/s]Tokenizing train dataset:  99%|█████████▉| 6655/6690 [00:20<00:00, 301.02 examples/s]Tokenizing train dataset:  95%|█████████▍| 6327/6690 [00:20<00:01, 329.97 examples/s]Tokenizing train dataset:  92%|█████████▏| 6183/6690 [00:20<00:01, 355.79 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:20<00:00, 294.67 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:20<00:00, 330.48 examples/s]
Tokenizing train dataset:  95%|█████████▌| 6369/6690 [00:20<00:01, 309.22 examples/s]Tokenizing train dataset:  93%|█████████▎| 6221/6690 [00:20<00:01, 316.76 examples/s]Tokenizing train dataset:  96%|█████████▌| 6413/6690 [00:20<00:00, 281.45 examples/s]Tokenizing train dataset:  94%|█████████▎| 6261/6690 [00:20<00:01, 295.77 examples/s]Tokenizing train dataset:  96%|█████████▋| 6455/6690 [00:20<00:00, 307.88 examples/s]Tokenizing train dataset:  97%|█████████▋| 6488/6690 [00:20<00:00, 311.47 examples/s]Tokenizing train dataset:  94%|█████████▍| 6310/6690 [00:20<00:01, 280.98 examples/s]Tokenizing train dataset:  98%|█████████▊| 6531/6690 [00:20<00:00, 295.81 examples/s]Tokenizing train dataset:  95%|█████████▍| 6347/6690 [00:20<00:01, 257.19 examples/s]Tokenizing train dataset:  98%|█████████▊| 6577/6690 [00:20<00:00, 296.83 examples/s]Tokenizing train dataset:  95%|█████████▌| 6385/6690 [00:21<00:01, 250.55 examples/s]Tokenizing train dataset:  96%|█████████▌| 6422/6690 [00:21<00:00, 273.83 examples/s]Tokenizing train dataset:  99%|█████████▉| 6617/6690 [00:21<00:00, 279.30 examples/s]Tokenizing train dataset:  97%|█████████▋| 6465/6690 [00:21<00:00, 271.17 examples/s]Tokenizing train dataset: 100%|█████████▉| 6657/6690 [00:21<00:00, 274.58 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:21<00:00, 312.98 examples/s]
Tokenizing train dataset:  97%|█████████▋| 6500/6690 [00:21<00:00, 286.23 examples/s]Tokenizing train dataset:  98%|█████████▊| 6547/6690 [00:21<00:00, 291.98 examples/s]Tokenizing train dataset:  98%|█████████▊| 6577/6690 [00:21<00:00, 290.37 examples/s]Tokenizing train dataset:  99%|█████████▉| 6611/6690 [00:21<00:00, 299.78 examples/s]Tokenizing train dataset: 100%|█████████▉| 6657/6690 [00:22<00:00, 296.18 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:22<00:00, 302.68 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  58%|█████▊    | 557/953 [00:00<00:00, 5534.56 examples/s]Extracting prompt in eval dataset:  58%|█████▊    | 550/953 [00:00<00:00, 5377.61 examples/s]Extracting prompt in eval dataset:  56%|█████▌    | 532/953 [00:00<00:00, 5224.87 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3848.43 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2840.56 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2945.08 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  31%|███▏      | 299/953 [00:00<00:00, 2955.25 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  69%|██████▊   | 653/953 [00:00<00:00, 2546.47 examples/s]Applying chat template to eval dataset:  27%|██▋       | 256/953 [00:00<00:00, 2515.58 examples/s]Applying chat template to eval dataset:  27%|██▋       | 260/953 [00:00<00:00, 2555.52 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2519.83 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2235.82 examples/s]
Applying chat template to eval dataset:  61%|██████    | 579/953 [00:00<00:00, 2266.92 examples/s]Applying chat template to eval dataset:  65%|██████▌   | 623/953 [00:00<00:00, 1940.31 examples/s]Applying chat template to eval dataset:  86%|████████▌ | 821/953 [00:00<00:00, 1867.87 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 1959.59 examples/s]
Applying chat template to eval dataset:  97%|█████████▋| 929/953 [00:00<00:00, 1977.83 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 1468.78 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 284.77 examples/s]Tokenizing eval dataset:   7%|▋         | 68/953 [00:00<00:05, 160.46 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:  10%|▉         | 95/953 [00:00<00:05, 161.21 examples/s]Tokenizing eval dataset:   3%|▎         | 24/953 [00:00<00:04, 229.75 examples/s]Tokenizing eval dataset:  12%|█▏        | 114/953 [00:00<00:05, 166.81 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   6%|▌         | 54/953 [00:00<00:04, 204.45 examples/s]Tokenizing eval dataset:  14%|█▍        | 138/953 [00:00<00:06, 134.40 examples/s]Tokenizing eval dataset:   3%|▎         | 24/953 [00:00<00:04, 228.67 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:06, 131.48 examples/s]Tokenizing eval dataset:  16%|█▋        | 156/953 [00:01<00:05, 143.38 examples/s]Tokenizing eval dataset:   5%|▍         | 47/953 [00:00<00:04, 215.70 examples/s]Tokenizing eval dataset:  10%|█         | 96/953 [00:00<00:06, 138.80 examples/s]Tokenizing eval dataset:  18%|█▊        | 173/953 [00:01<00:05, 148.18 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:04, 209.06 examples/s]Tokenizing eval dataset:  12%|█▏        | 113/953 [00:00<00:05, 142.70 examples/s]Tokenizing eval dataset:  21%|██        | 200/953 [00:01<00:04, 153.69 examples/s]Tokenizing eval dataset:  14%|█▎        | 131/953 [00:00<00:05, 147.32 examples/s]Tokenizing eval dataset:  11%|█         | 105/953 [00:00<00:04, 190.31 examples/s]Tokenizing eval dataset:  23%|██▎       | 218/953 [00:01<00:04, 157.52 examples/s]Tokenizing eval dataset:  16%|█▌        | 150/953 [00:00<00:05, 154.84 examples/s]Tokenizing eval dataset:  26%|██▌       | 247/953 [00:01<00:03, 188.17 examples/s]Tokenizing eval dataset:  14%|█▍        | 135/953 [00:00<00:04, 191.11 examples/s]Tokenizing eval dataset:  18%|█▊        | 167/953 [00:01<00:05, 156.46 examples/s]Tokenizing eval dataset:  29%|██▉       | 277/953 [00:01<00:03, 215.88 examples/s]Tokenizing eval dataset:  19%|█▉        | 184/953 [00:01<00:04, 156.17 examples/s]Tokenizing eval dataset:  16%|█▋        | 156/953 [00:00<00:04, 170.02 examples/s]Tokenizing eval dataset:  33%|███▎      | 319/953 [00:01<00:02, 270.07 examples/s]Tokenizing eval dataset:  37%|███▋      | 351/953 [00:01<00:02, 281.21 examples/s]Tokenizing eval dataset:  22%|██▏       | 207/953 [00:01<00:04, 153.35 examples/s]Tokenizing eval dataset:  18%|█▊        | 175/953 [00:01<00:05, 150.96 examples/s]Tokenizing eval dataset:  42%|████▏     | 396/953 [00:01<00:01, 324.21 examples/s]Tokenizing eval dataset:  45%|████▌     | 430/953 [00:02<00:01, 315.75 examples/s]Tokenizing eval dataset:  25%|██▌       | 243/953 [00:01<00:04, 161.37 examples/s]Tokenizing eval dataset:  20%|██        | 195/953 [00:01<00:05, 128.12 examples/s]Tokenizing eval dataset:  49%|████▊     | 464/953 [00:02<00:01, 313.60 examples/s]Tokenizing eval dataset:  29%|██▉       | 274/953 [00:01<00:04, 160.50 examples/s]Tokenizing eval dataset:  22%|██▏       | 209/953 [00:01<00:06, 109.34 examples/s]Tokenizing eval dataset:  55%|█████▍    | 522/953 [00:02<00:01, 331.64 examples/s]Tokenizing eval dataset:  32%|███▏      | 309/953 [00:01<00:03, 198.96 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:01<00:06, 120.19 examples/s]Tokenizing eval dataset:  59%|█████▊    | 558/953 [00:02<00:01, 274.38 examples/s]Tokenizing eval dataset:  36%|███▌      | 341/953 [00:02<00:03, 175.92 examples/s]Tokenizing eval dataset:  27%|██▋       | 260/953 [00:01<00:05, 132.96 examples/s]Tokenizing eval dataset:  62%|██████▏   | 593/953 [00:02<00:01, 290.06 examples/s]Tokenizing eval dataset:  40%|████      | 385/953 [00:02<00:02, 228.56 examples/s]Tokenizing eval dataset:  32%|███▏      | 301/953 [00:01<00:03, 188.16 examples/s]Tokenizing eval dataset:  68%|██████▊   | 644/953 [00:02<00:00, 342.16 examples/s]Tokenizing eval dataset:  45%|████▌     | 430/953 [00:02<00:01, 274.68 examples/s]Tokenizing eval dataset:  35%|███▍      | 330/953 [00:01<00:03, 205.50 examples/s]Tokenizing eval dataset:  74%|███████▎  | 702/953 [00:02<00:00, 353.72 examples/s]Tokenizing eval dataset:  49%|████▉     | 467/953 [00:02<00:01, 274.60 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:02<00:02, 215.11 examples/s]Tokenizing eval dataset:  53%|█████▎    | 508/953 [00:02<00:01, 306.09 examples/s]Tokenizing eval dataset:  81%|████████  | 769/953 [00:03<00:00, 379.63 examples/s]Tokenizing eval dataset:  42%|████▏     | 400/953 [00:02<00:02, 257.46 examples/s]Tokenizing eval dataset:  58%|█████▊    | 550/953 [00:02<00:01, 331.68 examples/s]Tokenizing eval dataset:  46%|████▌     | 436/953 [00:02<00:01, 262.55 examples/s]Tokenizing eval dataset:  86%|████████▌ | 817/953 [00:03<00:00, 335.56 examples/s]Tokenizing eval dataset:  50%|█████     | 477/953 [00:02<00:01, 298.54 examples/s]Tokenizing eval dataset:  64%|██████▍   | 611/953 [00:02<00:00, 351.11 examples/s]Tokenizing eval dataset:  53%|█████▎    | 509/953 [00:02<00:01, 297.96 examples/s]Tokenizing eval dataset:  91%|█████████ | 866/953 [00:03<00:00, 316.58 examples/s]Tokenizing eval dataset:  71%|███████   | 672/953 [00:02<00:00, 366.23 examples/s]Tokenizing eval dataset:  57%|█████▋    | 540/953 [00:02<00:01, 271.57 examples/s]Tokenizing eval dataset:  96%|█████████▌| 915/953 [00:03<00:00, 294.86 examples/s]Tokenizing eval dataset:  61%|██████    | 581/953 [00:02<00:01, 306.19 examples/s]Tokenizing eval dataset:  76%|███████▌  | 726/953 [00:03<00:00, 338.83 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:03<00:00, 261.09 examples/s]
Tokenizing eval dataset:  67%|██████▋   | 640/953 [00:02<00:00, 381.06 examples/s]Tokenizing eval dataset:  82%|████████▏ | 783/953 [00:03<00:00, 347.62 examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  73%|███████▎  | 696/953 [00:03<00:01, 216.18 examples/s]Tokenizing eval dataset:  87%|████████▋ | 830/953 [00:03<00:00, 223.96 examples/s]Tokenizing eval dataset:  77%|███████▋  | 731/953 [00:03<00:00, 236.89 examples/s]Tokenizing eval dataset:  90%|█████████ | 860/953 [00:03<00:00, 233.79 examples/s]Tokenizing eval dataset:  80%|████████  | 765/953 [00:03<00:00, 253.32 examples/s]Tokenizing eval dataset:  94%|█████████▎| 892/953 [00:03<00:00, 248.67 examples/s]Tokenizing eval dataset:  85%|████████▍ | 809/953 [00:03<00:00, 193.89 examples/s]Tokenizing eval dataset:  99%|█████████▊| 940/953 [00:04<00:00, 195.73 examples/s]Tokenizing eval dataset:  89%|████████▉ | 850/953 [00:03<00:00, 228.02 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:04<00:00, 216.52 examples/s]
Tokenizing eval dataset:  92%|█████████▏| 881/953 [00:04<00:00, 242.78 examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  98%|█████████▊| 932/953 [00:04<00:00, 268.97 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:04<00:00, 218.36 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.818056583404541 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.660285472869873 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.8173229694366455 seconds
Time to load cpu_adam op: 2.895944833755493 seconds
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: vajdadario (slolama) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: creating run
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
wandb: ERROR Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.
Traceback (most recent call last):
  File "/usr/lib/python3.10/asyncio/locks.py", line 214, in wait
    await fut
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/handles.py", line 141, in wait_async
    await asyncio.wait_for(evt.wait(), timeout=timeout)
  File "/usr/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py", line 946, in init
    result = wait_with_progress(
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/wait_with_progress.py", line 23, in wait_with_progress
    return wait_all_with_progress(
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/wait_with_progress.py", line 86, in wait_all_with_progress
    return asyncio_compat.run(progress_loop_with_timeout)
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/asyncio_compat.py", line 30, in run
    return future.result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
    return self.__get_result()
  File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/usr/lib/python3.10/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/asyncio_compat.py", line 74, in run
    return asyncio.run(self._run_or_cancel(fn))
  File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/asyncio_compat.py", line 98, in _run_or_cancel
    return fn_task.result()
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/wait_with_progress.py", line 81, in progress_loop_with_timeout
    return await _wait_handles_async(
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/wait_with_progress.py", line 129, in _wait_handles_async
    async with asyncio_compat.open_task_group() as task_group:
  File "/usr/lib/python3.10/contextlib.py", line 206, in __aexit__
    await anext(self.gen)
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/asyncio_compat.py", line 190, in open_task_group
    await task_group._wait_all()
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/asyncio_compat.py", line 159, in _wait_all
    raise exc
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/wait_with_progress.py", line 127, in wait_single
    results[index] = await handle.wait_async(timeout=timeout)
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/handles.py", line 150, in wait_async
    raise TimeoutError(
TimeoutError: Timed out waiting for response on 3gumv8uukewc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 235, in <module>
    main(get_train_data(CURRICULUM_STAGE), val_data, args.rank, args.learning_rate, args.total_epochs, args.beta, CURRICULUM_STAGE)
  File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 211, in main
    dpo_trainer.train()
  File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2469, in _inner_training_loop
    self.control = self.callback_handler.on_train_begin(args, self.state, self.control)
  File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer_callback.py", line 506, in on_train_begin
    return self.call_event("on_train_begin", args, state, control)
  File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer_callback.py", line 556, in call_event
    result = getattr(callback, event)(
  File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/integrations/integration_utils.py", line 930, in on_train_begin
    self.setup(args, state, model, **kwargs)
  File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/integrations/integration_utils.py", line 857, in setup
    self._wandb.init(
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py", line 1485, in init
    wandb._sentry.reraise(e)
  File "/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py", line 156, in reraise
    raise exc.with_traceback(sys.exc_info()[2])
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py", line 1471, in init
    return wi.init(run_settings, run_config)
  File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py", line 959, in init
    raise CommError(
wandb.errors.errors.CommError: Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/usr/lib/python3.10/asyncio/locks.py", line 214, in wait
[rank0]:     await fut
[rank0]: asyncio.exceptions.CancelledError

[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/usr/lib/python3.10/asyncio/tasks.py", line 456, in wait_for
[rank0]:     return fut.result()
[rank0]: asyncio.exceptions.CancelledError

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/handles.py", line 141, in wait_async
[rank0]:     await asyncio.wait_for(evt.wait(), timeout=timeout)
[rank0]:   File "/usr/lib/python3.10/asyncio/tasks.py", line 458, in wait_for
[rank0]:     raise exceptions.TimeoutError() from exc
[rank0]: asyncio.exceptions.TimeoutError

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py", line 946, in init
[rank0]:     result = wait_with_progress(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/wait_with_progress.py", line 23, in wait_with_progress
[rank0]:     return wait_all_with_progress(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/wait_with_progress.py", line 86, in wait_all_with_progress
[rank0]:     return asyncio_compat.run(progress_loop_with_timeout)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/asyncio_compat.py", line 30, in run
[rank0]:     return future.result()
[rank0]:   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 458, in result
[rank0]:     return self.__get_result()
[rank0]:   File "/usr/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
[rank0]:     raise self._exception
[rank0]:   File "/usr/lib/python3.10/concurrent/futures/thread.py", line 58, in run
[rank0]:     result = self.fn(*self.args, **self.kwargs)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/asyncio_compat.py", line 74, in run
[rank0]:     return asyncio.run(self._run_or_cancel(fn))
[rank0]:   File "/usr/lib/python3.10/asyncio/runners.py", line 44, in run
[rank0]:     return loop.run_until_complete(main)
[rank0]:   File "/usr/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
[rank0]:     return future.result()
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/asyncio_compat.py", line 98, in _run_or_cancel
[rank0]:     return fn_task.result()
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/wait_with_progress.py", line 81, in progress_loop_with_timeout
[rank0]:     return await _wait_handles_async(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/wait_with_progress.py", line 129, in _wait_handles_async
[rank0]:     async with asyncio_compat.open_task_group() as task_group:
[rank0]:   File "/usr/lib/python3.10/contextlib.py", line 206, in __aexit__
[rank0]:     await anext(self.gen)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/asyncio_compat.py", line 190, in open_task_group
[rank0]:     await task_group._wait_all()
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/asyncio_compat.py", line 159, in _wait_all
[rank0]:     raise exc
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/wait_with_progress.py", line 127, in wait_single
[rank0]:     results[index] = await handle.wait_async(timeout=timeout)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/mailbox/handles.py", line 150, in wait_async
[rank0]:     raise TimeoutError(
[rank0]: TimeoutError: Timed out waiting for response on 3gumv8uukewc

[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 235, in <module>
[rank0]:     main(get_train_data(CURRICULUM_STAGE), val_data, args.rank, args.learning_rate, args.total_epochs, args.beta, CURRICULUM_STAGE)
[rank0]:   File "/ceph/hpc/data/s24o01-42-users/translation_optimization/trl/train_curriculum.py", line 211, in main
[rank0]:     dpo_trainer.train()
[rank0]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer.py", line 2469, in _inner_training_loop
[rank0]:     self.control = self.callback_handler.on_train_begin(args, self.state, self.control)
[rank0]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer_callback.py", line 506, in on_train_begin
[rank0]:     return self.call_event("on_train_begin", args, state, control)
[rank0]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/trainer_callback.py", line 556, in call_event
[rank0]:     result = getattr(callback, event)(
[rank0]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/integrations/integration_utils.py", line 930, in on_train_begin
[rank0]:     self.setup(args, state, model, **kwargs)
[rank0]:   File "/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/integrations/integration_utils.py", line 857, in setup
[rank0]:     self._wandb.init(
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py", line 1485, in init
[rank0]:     wandb._sentry.reraise(e)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/analytics/sentry.py", line 156, in reraise
[rank0]:     raise exc.with_traceback(sys.exc_info()[2])
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py", line 1471, in init
[rank0]:     return wi.init(run_settings, run_config)
[rank0]:   File "/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py", line 959, in init
[rank0]:     raise CommError(
[rank0]: wandb.errors.errors.CommError: Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33mCurriculum-2-DPO_r-64_lr-1e-06_e-3_b-0.1[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250612_205823-5vjkailr/logs[0m
[rank0]:[W612 20:59:54.057563973 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
W0612 20:59:56.152000 3243317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3243514 closing signal SIGTERM
W0612 20:59:56.158000 3243317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3243515 closing signal SIGTERM
W0612 20:59:56.212000 3243317 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3243516 closing signal SIGTERM
E0612 20:59:56.911000 3243317 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 3243513) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1182, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 861, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_curriculum.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-12_20:59:56
  host      : pm5-nod00.vega.pri
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3243513)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
