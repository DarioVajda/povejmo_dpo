cpu-bind=MASK - gn37, task  3  0 [537155]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 3 ---
Total Nodes: 4
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn34
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 4     --machine_rank 3     --main_process_ip gn34     --main_process_port 29500     --num_processes 16     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62777380     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.2 --curriculum_stage=0
-------------------------------------------
[2025-06-08 19:35:20,769] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0608 19:35:22.501000 537205 torch/distributed/run.py:792] 
W0608 19:35:22.501000 537205 torch/distributed/run.py:792] *****************************************
W0608 19:35:22.501000 537205 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0608 19:35:22.501000 537205 torch/distributed/run.py:792] *****************************************
[2025-06-08 19:35:38,976] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-08 19:35:38,991] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-08 19:35:39,029] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-08 19:35:39,037] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Training data of type 'bad_lang_examples':    3489
Training data of type 'short_examples':       699
Training data of type 'choose_examples':      13379
Training data of type 'bad_format_examples':  3148
**************************************************
Training data of type 'bad_lang_examples':   Training data of type 'bad_lang_examples':     34893489

Training data of type 'short_examples':      Training data of type 'short_examples':        699699

Training data of type 'choose_examples':     Training data of type 'choose_examples':       1337913379

Training data of type 'bad_format_examples': Training data of type 'bad_format_examples':   31483148

****************************************************************************************************

Training data of type 'bad_lang_examples':    3489
Training data of type 'short_examples':       699
Training data of type 'choose_examples':      13379
Training data of type 'bad_format_examples':  3148
**************************************************
Evaluation data size: 953
Evaluation data size: 953
Evaluation data size: 953
Evaluation data size: 953
Curriculum stage 0 training data size: 7336
Curriculum stage 0 training data size:Curriculum stage 0 training data size:  73367336

Curriculum stage 0 training data size: 7336
Curriculum stage 1 training data size: 6689
Curriculum stage 1 training data size: 6689
Curriculum stage 1 training data size: 6689
Curriculum stage 1 training data size: 6689
Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
Curriculum stage 2 training data size: 6690
Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
Curriculum stage 2 training data size: 6690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
World size: 16
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=0)
4e-07
[2025-06-08 19:35:42,929] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 7336
Validation dataset size: 953
Steps per epoch: 458
Evaluate each 229 steps
[2025-06-08 19:35:42,932] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-08 19:35:42,932] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-08 19:35:42,933] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: cjvt/GaMS-9B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:12, 24.08s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.41s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.41s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:45<00:44, 22.28s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:45<00:44, 22.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:45<00:44, 22.50s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:45<00:45, 22.56s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:10<00:23, 23.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:10<00:23, 23.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:10<00:23, 23.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:10<00:23, 23.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 20.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.74s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 20.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 21.80s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 20.90s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 20.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 21.80s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:27<00:00, 21.80s/it]
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loaded model
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/ceph/hpc/home/dv70648/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank15]:[W608 19:37:15.901855468 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 15]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s][rank13]:[W608 19:37:15.975180683 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 13]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   8%|▊         | 560/7336 [00:00<00:01, 5511.63 examples/s][rank14]:[W608 19:37:15.156660511 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 14]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  15%|█▌        | 1130/7336 [00:00<00:01, 5601.19 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1706/7336 [00:00<00:01, 5608.61 examples/s]Extracting prompt in train dataset:  31%|███       | 2279/7336 [00:00<00:00, 5632.97 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3098/7336 [00:00<00:00, 5539.66 examples/s]Extracting prompt in train dataset:  50%|████▉     | 3661/7336 [00:00<00:00, 5553.29 examples/s]Extracting prompt in train dataset:  58%|█████▊    | 4229/7336 [00:00<00:00, 5590.58 examples/s]Extracting prompt in train dataset:  66%|██████▌   | 4830/7336 [00:00<00:00, 5698.79 examples/s]Extracting prompt in train dataset:  74%|███████▍  | 5430/7336 [00:00<00:00, 5782.15 examples/s]Extracting prompt in train dataset:  82%|████████▏ | 6030/7336 [00:01<00:00, 5844.04 examples/s]Extracting prompt in train dataset:  90%|█████████ | 6630/7336 [00:01<00:00, 5887.50 examples/s]Extracting prompt in train dataset:  99%|█████████▊| 7230/7336 [00:01<00:00, 5911.21 examples/s]Extracting prompt in train dataset: 100%|██████████| 7336/7336 [00:01<00:00, 5679.75 examples/s]
Applying chat template to train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 290/7336 [00:00<00:02, 2873.19 examples/s]Applying chat template to train dataset:   8%|▊         | 606/7336 [00:00<00:02, 3034.27 examples/s]Applying chat template to train dataset:  13%|█▎        | 921/7336 [00:00<00:02, 3083.74 examples/s]Applying chat template to train dataset:  17%|█▋        | 1240/7336 [00:00<00:01, 3113.81 examples/s]Applying chat template to train dataset:  21%|██        | 1555/7336 [00:00<00:01, 3119.88 examples/s]Applying chat template to train dataset:  26%|██▌       | 1874/7336 [00:00<00:01, 3138.86 examples/s]Applying chat template to train dataset:  30%|██▉       | 2192/7336 [00:00<00:01, 3148.80 examples/s]Applying chat template to train dataset:  34%|███▍      | 2510/7336 [00:00<00:01, 3153.91 examples/s]Applying chat template to train dataset:  40%|████      | 2970/7336 [00:00<00:01, 3111.85 examples/s]Applying chat template to train dataset:  45%|████▍     | 3287/7336 [00:01<00:01, 3126.36 examples/s]Applying chat template to train dataset:  49%|████▉     | 3602/7336 [00:01<00:01, 3129.74 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3920/7336 [00:01<00:01, 3139.58 examples/s]Applying chat template to train dataset:  58%|█████▊    | 4241/7336 [00:01<00:00, 3152.52 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4572/7336 [00:01<00:00, 3197.71 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4904/7336 [00:01<00:00, 3231.40 examples/s]Applying chat template to train dataset:  71%|███████▏  | 5237/7336 [00:01<00:00, 3256.57 examples/s]Applying chat template to train dataset:  76%|███████▌  | 5569/7336 [00:01<00:00, 3272.69 examples/s]Applying chat template to train dataset:  80%|████████  | 5900/7336 [00:01<00:00, 3282.10 examples/s]Applying chat template to train dataset:  85%|████████▍ | 6232/7336 [00:01<00:00, 3290.89 examples/s]Applying chat template to train dataset:  89%|████████▉ | 6565/7336 [00:02<00:00, 3299.82 examples/s]Applying chat template to train dataset:  94%|█████████▍| 6897/7336 [00:02<00:00, 3303.46 examples/s]Applying chat template to train dataset:  99%|█████████▊| 7228/7336 [00:02<00:00, 3303.04 examples/s]Applying chat template to train dataset: 100%|██████████| 7336/7336 [00:02<00:00, 3193.61 examples/s]
Tokenizing train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 42/7336 [00:00<00:18, 401.39 examples/s]Tokenizing train dataset:   1%|          | 90/7336 [00:00<00:21, 337.48 examples/s]Tokenizing train dataset:   2%|▏         | 139/7336 [00:00<00:22, 320.50 examples/s]Tokenizing train dataset:   3%|▎         | 184/7336 [00:00<00:23, 308.49 examples/s]Tokenizing train dataset:   3%|▎         | 218/7336 [00:00<00:22, 314.02 examples/s]Tokenizing train dataset:   3%|▎         | 251/7336 [00:00<00:22, 316.81 examples/s]Tokenizing train dataset:   4%|▍         | 289/7336 [00:00<00:21, 332.70 examples/s]Tokenizing train dataset:   5%|▍         | 337/7336 [00:01<00:21, 324.88 examples/s]Tokenizing train dataset:   5%|▌         | 387/7336 [00:01<00:21, 323.06 examples/s]Tokenizing train dataset:   6%|▌         | 434/7336 [00:01<00:21, 314.67 examples/s]Tokenizing train dataset:   7%|▋         | 480/7336 [00:01<00:22, 308.31 examples/s]Tokenizing train dataset:   7%|▋         | 527/7336 [00:01<00:22, 307.60 examples/s]Tokenizing train dataset:   8%|▊         | 561/7336 [00:01<00:21, 312.38 examples/s]Tokenizing train dataset:   8%|▊         | 605/7336 [00:01<00:22, 301.92 examples/s]Tokenizing train dataset:   9%|▉         | 644/7336 [00:02<00:20, 319.24 examples/s]Tokenizing train dataset:   9%|▉         | 690/7336 [00:02<00:21, 308.67 examples/s]Tokenizing train dataset:  10%|▉         | 722/7336 [00:02<00:21, 309.60 examples/s]Tokenizing train dataset:  10%|█         | 759/7336 [00:02<00:20, 318.80 examples/s]Tokenizing train dataset:  11%|█         | 802/7336 [00:02<00:21, 300.99 examples/s]Tokenizing train dataset:  12%|█▏        | 847/7336 [00:02<00:21, 298.25 examples/s]Tokenizing train dataset:  12%|█▏        | 899/7336 [00:02<00:20, 308.09 examples/s]Tokenizing train dataset:  13%|█▎        | 942/7336 [00:03<00:21, 299.69 examples/s]Tokenizing train dataset:  13%|█▎        | 973/7336 [00:03<00:21, 300.47 examples/s]Tokenizing train dataset:  14%|█▍        | 1017/7336 [00:03<00:21, 291.21 examples/s]Tokenizing train dataset:  14%|█▍        | 1062/7336 [00:03<00:21, 288.59 examples/s]Tokenizing train dataset:  15%|█▍        | 1098/7336 [00:03<00:20, 302.40 examples/s]Tokenizing train dataset:  16%|█▌        | 1141/7336 [00:03<00:21, 294.01 examples/s]Tokenizing train dataset:  16%|█▌        | 1190/7336 [00:03<00:20, 296.62 examples/s]Tokenizing train dataset:  17%|█▋        | 1222/7336 [00:03<00:20, 298.56 examples/s]Tokenizing train dataset:  17%|█▋        | 1258/7336 [00:04<00:19, 312.62 examples/s]Tokenizing train dataset:  18%|█▊        | 1309/7336 [00:04<00:19, 315.91 examples/s]Tokenizing train dataset:  18%|█▊        | 1341/7336 [00:04<00:19, 313.92 examples/s]Tokenizing train dataset:  19%|█▉        | 1383/7336 [00:04<00:19, 300.23 examples/s]Tokenizing train dataset:  19%|█▉        | 1429/7336 [00:04<00:19, 299.26 examples/s]Tokenizing train dataset:  20%|██        | 1473/7336 [00:04<00:19, 293.49 examples/s]Tokenizing train dataset:  21%|██        | 1515/7336 [00:04<00:20, 286.73 examples/s]Tokenizing train dataset:  21%|██        | 1550/7336 [00:05<00:19, 294.90 examples/s]Tokenizing train dataset:  22%|██▏       | 1581/7336 [00:05<00:19, 294.45 examples/s]Tokenizing train dataset:  22%|██▏       | 1613/7336 [00:05<00:19, 297.80 examples/s]Tokenizing train dataset:  22%|██▏       | 1650/7336 [00:05<00:18, 311.97 examples/s]Tokenizing train dataset:  23%|██▎       | 1685/7336 [00:05<00:17, 317.47 examples/s]Tokenizing train dataset:  23%|██▎       | 1721/7336 [00:05<00:17, 328.24 examples/s]Tokenizing train dataset:  24%|██▍       | 1755/7336 [00:05<00:17, 326.85 examples/s]Tokenizing train dataset:  24%|██▍       | 1790/7336 [00:05<00:16, 331.96 examples/s]Tokenizing train dataset:  25%|██▍       | 1830/7336 [00:05<00:17, 306.59 examples/s]Tokenizing train dataset:  25%|██▌       | 1863/7336 [00:06<00:17, 308.34 examples/s]Tokenizing train dataset:  26%|██▌       | 1900/7336 [00:06<00:19, 285.38 examples/s]Tokenizing train dataset:  26%|██▋       | 1940/7336 [00:06<00:19, 273.91 examples/s]Tokenizing train dataset:  27%|██▋       | 1969/7336 [00:06<00:19, 274.96 examples/s]Tokenizing train dataset:  27%|██▋       | 2000/7336 [00:06<00:18, 282.64 examples/s]Tokenizing train dataset:  28%|██▊       | 2033/7336 [00:06<00:18, 294.09 examples/s]Tokenizing train dataset:  28%|██▊       | 2071/7336 [00:06<00:16, 314.25 examples/s]Tokenizing train dataset:  29%|██▊       | 2106/7336 [00:06<00:16, 318.14 examples/s]Tokenizing train dataset:  29%|██▉       | 2151/7336 [00:07<00:16, 305.02 examples/s]Tokenizing train dataset:  30%|██▉       | 2196/7336 [00:07<00:17, 301.50 examples/s]Tokenizing train dataset:  31%|███       | 2246/7336 [00:07<00:16, 307.67 examples/s]Tokenizing train dataset:  31%|███       | 2277/7336 [00:07<00:16, 306.75 examples/s]Tokenizing train dataset:  32%|███▏      | 2324/7336 [00:07<00:16, 306.54 examples/s]Tokenizing train dataset:  32%|███▏      | 2367/7336 [00:07<00:16, 296.11 examples/s]Tokenizing train dataset:  33%|███▎      | 2412/7336 [00:07<00:16, 292.21 examples/s]Tokenizing train dataset:  33%|███▎      | 2445/7336 [00:08<00:16, 295.98 examples/s]Tokenizing train dataset:  34%|███▍      | 2491/7336 [00:08<00:16, 297.93 examples/s]Tokenizing train dataset:  34%|███▍      | 2525/7336 [00:08<00:17, 271.69 examples/s]Tokenizing train dataset:  35%|███▍      | 2558/7336 [00:08<00:16, 282.21 examples/s]Tokenizing train dataset:  35%|███▌      | 2592/7336 [00:08<00:16, 295.05 examples/s]Tokenizing train dataset:  36%|███▌      | 2626/7336 [00:08<00:15, 305.93 examples/s]Tokenizing train dataset:  36%|███▌      | 2659/7336 [00:08<00:15, 309.59 examples/s]Tokenizing train dataset:  37%|███▋      | 2693/7336 [00:08<00:14, 316.56 examples/s]Tokenizing train dataset:  37%|███▋      | 2739/7336 [00:08<00:14, 311.01 examples/s]Tokenizing train dataset:  38%|███▊      | 2782/7336 [00:09<00:15, 300.91 examples/s]Tokenizing train dataset:  38%|███▊      | 2815/7336 [00:09<00:14, 306.33 examples/s]Tokenizing train dataset:  39%|███▉      | 2861/7336 [00:09<00:14, 299.45 examples/s]Tokenizing train dataset:  39%|███▉      | 2892/7336 [00:09<00:14, 299.28 examples/s]Tokenizing train dataset:  40%|████      | 2939/7336 [00:09<00:14, 300.10 examples/s]Tokenizing train dataset:  41%|████      | 2973/7336 [00:09<00:14, 303.79 examples/s]Tokenizing train dataset:  41%|████      | 3020/7336 [00:09<00:14, 302.44 examples/s]Tokenizing train dataset:  42%|████▏     | 3062/7336 [00:10<00:14, 289.68 examples/s]Tokenizing train dataset:  42%|████▏     | 3094/7336 [00:10<00:14, 294.84 examples/s]Tokenizing train dataset:  43%|████▎     | 3126/7336 [00:10<00:14, 296.30 examples/s]Tokenizing train dataset:  43%|████▎     | 3170/7336 [00:10<00:14, 290.98 examples/s]Tokenizing train dataset:  44%|████▎     | 3204/7336 [00:10<00:13, 299.62 examples/s]Tokenizing train dataset:  44%|████▍     | 3235/7336 [00:10<00:13, 298.14 examples/s]Tokenizing train dataset:  45%|████▍     | 3269/7336 [00:10<00:13, 306.74 examples/s]Tokenizing train dataset:  45%|████▍     | 3300/7336 [00:10<00:13, 303.59 examples/s]Tokenizing train dataset:  46%|████▌     | 3347/7336 [00:11<00:13, 303.45 examples/s]Tokenizing train dataset:  46%|████▌     | 3387/7336 [00:11<00:13, 286.00 examples/s]Tokenizing train dataset:  47%|████▋     | 3428/7336 [00:11<00:13, 279.92 examples/s]Tokenizing train dataset:  47%|████▋     | 3465/7336 [00:11<00:14, 263.78 examples/s]Tokenizing train dataset:  48%|████▊     | 3506/7336 [00:11<00:14, 261.41 examples/s]Tokenizing train dataset:  48%|████▊     | 3545/7336 [00:11<00:13, 284.36 examples/s]Tokenizing train dataset:  49%|████▉     | 3580/7336 [00:11<00:12, 298.66 examples/s]Tokenizing train dataset:  49%|████▉     | 3627/7336 [00:12<00:12, 298.44 examples/s]Tokenizing train dataset:  50%|████▉     | 3661/7336 [00:12<00:12, 305.74 examples/s]Tokenizing train dataset:  51%|█████     | 3707/7336 [00:12<00:12, 298.29 examples/s]Tokenizing train dataset:  51%|█████     | 3750/7336 [00:12<00:10, 328.43 examples/s]Tokenizing train dataset:  52%|█████▏    | 3800/7336 [00:12<00:10, 325.17 examples/s]Tokenizing train dataset:  52%|█████▏    | 3838/7336 [00:12<00:10, 335.36 examples/s]Tokenizing train dataset:  53%|█████▎    | 3884/7336 [00:12<00:10, 323.07 examples/s]Tokenizing train dataset:  54%|█████▎    | 3925/7336 [00:12<00:11, 302.04 examples/s]Tokenizing train dataset:  54%|█████▍    | 3977/7336 [00:13<00:10, 311.73 examples/s]Tokenizing train dataset:  55%|█████▍    | 4016/7336 [00:13<00:10, 327.72 examples/s]Tokenizing train dataset:  55%|█████▌    | 4060/7336 [00:13<00:10, 310.14 examples/s]Tokenizing train dataset:  56%|█████▌    | 4095/7336 [00:13<00:10, 313.14 examples/s]Tokenizing train dataset:  56%|█████▋    | 4140/7336 [00:13<00:10, 302.83 examples/s]Tokenizing train dataset:  57%|█████▋    | 4186/7336 [00:13<00:10, 298.88 examples/s]Tokenizing train dataset:  59%|█████▊    | 4304/7336 [00:13<00:06, 497.82 examples/s]Tokenizing train dataset:  60%|██████    | 4435/7336 [00:14<00:04, 695.27 examples/s]Tokenizing train dataset:  62%|██████▏   | 4561/7336 [00:14<00:03, 839.03 examples/s]Tokenizing train dataset:  64%|██████▍   | 4682/7336 [00:14<00:02, 936.26 examples/s]Tokenizing train dataset:  66%|██████▌   | 4811/7336 [00:14<00:02, 1032.87 examples/s]Tokenizing train dataset:  67%|██████▋   | 4938/7336 [00:14<00:02, 1096.99 examples/s]Tokenizing train dataset:  69%|██████▉   | 5060/7336 [00:14<00:02, 1124.44 examples/s]Tokenizing train dataset:  71%|███████   | 5184/7336 [00:14<00:01, 1153.36 examples/s]Tokenizing train dataset:  73%|███████▎  | 5359/7336 [00:14<00:01, 1156.18 examples/s]Tokenizing train dataset:  75%|███████▍  | 5481/7336 [00:14<00:01, 1170.96 examples/s]Tokenizing train dataset:  77%|███████▋  | 5660/7336 [00:15<00:01, 1171.52 examples/s]Tokenizing train dataset:  79%|███████▉  | 5781/7336 [00:15<00:01, 1178.24 examples/s]Tokenizing train dataset:  80%|████████  | 5901/7336 [00:15<00:01, 1183.03 examples/s]Tokenizing train dataset:  82%|████████▏ | 6028/7336 [00:15<00:01, 1204.89 examples/s]Tokenizing train dataset:  84%|████████▍ | 6150/7336 [00:15<00:00, 1205.72 examples/s]Tokenizing train dataset:  86%|████████▌ | 6273/7336 [00:15<00:00, 1209.64 examples/s]Tokenizing train dataset:  88%|████████▊ | 6448/7336 [00:15<00:00, 1189.56 examples/s]Tokenizing train dataset:  90%|████████▉ | 6575/7336 [00:15<00:00, 1204.53 examples/s]Tokenizing train dataset:  91%|█████████▏| 6698/7336 [00:15<00:00, 1208.05 examples/s]Tokenizing train dataset:  94%|█████████▎| 6877/7336 [00:16<00:00, 1200.52 examples/s]Tokenizing train dataset:  96%|█████████▌| 7055/7336 [00:16<00:00, 1192.91 examples/s]Tokenizing train dataset:  98%|█████████▊| 7175/7336 [00:16<00:00, 1192.66 examples/s]Tokenizing train dataset: 100%|██████████| 7336/7336 [00:16<00:00, 1188.42 examples/s]Tokenizing train dataset: 100%|██████████| 7336/7336 [00:16<00:00, 446.57 examples/s] 
[rank12]:[W608 19:37:36.928881089 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 12]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   7%|▋         | 536/7336 [00:00<00:01, 5316.48 examples/s]Extracting prompt in train dataset:   8%|▊         | 568/7336 [00:00<00:01, 5584.83 examples/s]Extracting prompt in train dataset:   8%|▊         | 560/7336 [00:00<00:01, 5501.23 examples/s]Extracting prompt in eval dataset:  61%|██████    | 580/953 [00:00<00:00, 5701.48 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5676.28 examples/s]
Extracting prompt in train dataset:  15%|█▍        | 1072/7336 [00:00<00:01, 5341.50 examples/s]Extracting prompt in train dataset:  15%|█▌        | 1120/7336 [00:00<00:01, 5543.08 examples/s]Extracting prompt in train dataset:  16%|█▌        | 1140/7336 [00:00<00:01, 5614.14 examples/s]Extracting prompt in train dataset:  22%|██▏       | 1610/7336 [00:00<00:01, 5327.84 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1690/7336 [00:00<00:01, 5567.01 examples/s]Extracting prompt in train dataset:  23%|██▎       | 1710/7336 [00:00<00:01, 5616.61 examples/s]Extracting prompt in train dataset:  29%|██▉       | 2160/7336 [00:00<00:00, 5379.07 examples/s]Extracting prompt in train dataset:  31%|███       | 2280/7336 [00:00<00:00, 5646.01 examples/s]Extracting prompt in train dataset:  31%|███       | 2260/7336 [00:00<00:00, 5602.84 examples/s]Extracting prompt in train dataset:  37%|███▋      | 2730/7336 [00:00<00:00, 5488.38 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  42%|████▏     | 3100/7336 [00:00<00:00, 5554.28 examples/s]Extracting prompt in train dataset:  42%|████▏     | 3092/7336 [00:00<00:00, 5516.57 examples/s]Applying chat template to eval dataset:  33%|███▎      | 313/953 [00:00<00:00, 3106.99 examples/s]Extracting prompt in train dataset:  49%|████▊     | 3558/7336 [00:00<00:00, 5496.62 examples/s]Extracting prompt in train dataset:  50%|█████     | 3670/7336 [00:00<00:00, 5580.94 examples/s]Extracting prompt in train dataset:  50%|████▉     | 3659/7336 [00:00<00:00, 5561.33 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 640/953 [00:00<00:00, 3191.73 examples/s]Extracting prompt in train dataset:  56%|█████▋    | 4130/7336 [00:00<00:00, 5541.12 examples/s]Extracting prompt in train dataset:  58%|█████▊    | 4240/7336 [00:00<00:00, 5609.88 examples/s]Extracting prompt in train dataset:  58%|█████▊    | 4230/7336 [00:00<00:00, 5575.63 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3180.76 examples/s]
Extracting prompt in train dataset:  65%|██████▍   | 4740/7336 [00:00<00:00, 5684.93 examples/s]Extracting prompt in train dataset:  66%|██████▌   | 4846/7336 [00:00<00:00, 5743.76 examples/s]Extracting prompt in train dataset:  66%|██████▌   | 4832/7336 [00:00<00:00, 5706.51 examples/s]Extracting prompt in train dataset:  73%|███████▎  | 5350/7336 [00:00<00:00, 5795.67 examples/s]Extracting prompt in train dataset:  74%|███████▍  | 5447/7336 [00:00<00:00, 5822.62 examples/s]Extracting prompt in train dataset:  74%|███████▍  | 5430/7336 [00:00<00:00, 5785.09 examples/s]Extracting prompt in train dataset:  82%|████████▏ | 6046/7336 [00:01<00:00, 5871.70 examples/s]Extracting prompt in train dataset:  81%|████████  | 5960/7336 [00:01<00:00, 5879.57 examples/s]Extracting prompt in train dataset:  82%|████████▏ | 6030/7336 [00:01<00:00, 5845.23 examples/s]Extracting prompt in train dataset:  91%|█████████ | 6640/7336 [00:01<00:00, 5884.19 examples/s]Extracting prompt in train dataset:  90%|████████▉ | 6570/7336 [00:01<00:00, 5934.61 examples/s]Extracting prompt in train dataset:  90%|█████████ | 6630/7336 [00:01<00:00, 5890.33 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  98%|█████████▊| 7180/7336 [00:01<00:00, 5976.94 examples/s]Extracting prompt in train dataset:  99%|█████████▊| 7230/7336 [00:01<00:00, 5871.17 examples/s]Extracting prompt in train dataset:  99%|█████████▊| 7231/7336 [00:01<00:00, 5924.24 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 325.45 examples/s]Extracting prompt in train dataset: 100%|██████████| 7336/7336 [00:01<00:00, 5709.64 examples/s]
Extracting prompt in train dataset: 100%|██████████| 7336/7336 [00:01<00:00, 5700.78 examples/s]
Extracting prompt in train dataset: 100%|██████████| 7336/7336 [00:01<00:00, 5675.90 examples/s]
Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 293.35 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:03, 275.92 examples/s]Applying chat template to train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 290/7336 [00:00<00:02, 2872.26 examples/s]Applying chat template to train dataset:   4%|▍         | 289/7336 [00:00<00:02, 2865.58 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 266.34 examples/s]Applying chat template to train dataset:   4%|▍         | 292/7336 [00:00<00:02, 2894.33 examples/s]Applying chat template to train dataset:   8%|▊         | 607/7336 [00:00<00:02, 3040.53 examples/s]Applying chat template to train dataset:   8%|▊         | 603/7336 [00:00<00:02, 3020.55 examples/s]Applying chat template to train dataset:   8%|▊         | 611/7336 [00:00<00:02, 3064.21 examples/s]Tokenizing eval dataset:  21%|██        | 198/953 [00:00<00:02, 258.23 examples/s]Applying chat template to train dataset:  13%|█▎        | 921/7336 [00:00<00:02, 3083.02 examples/s]Applying chat template to train dataset:  13%|█▎        | 919/7336 [00:00<00:02, 3078.43 examples/s]Applying chat template to train dataset:  13%|█▎        | 930/7336 [00:00<00:02, 3119.00 examples/s]Tokenizing eval dataset:  25%|██▍       | 238/953 [00:00<00:02, 289.84 examples/s]Applying chat template to train dataset:  17%|█▋        | 1239/7336 [00:00<00:01, 3112.76 examples/s]Applying chat template to train dataset:  17%|█▋        | 1232/7336 [00:00<00:01, 3096.39 examples/s]Applying chat template to train dataset:  17%|█▋        | 1250/7336 [00:00<00:01, 3142.47 examples/s]Tokenizing eval dataset:  32%|███▏      | 304/953 [00:00<00:01, 383.35 examples/s]Applying chat template to train dataset:  21%|██        | 1543/7336 [00:00<00:01, 3098.04 examples/s]Applying chat template to train dataset:  21%|██▏       | 1566/7336 [00:00<00:01, 3143.40 examples/s]Applying chat template to train dataset:  23%|██▎       | 1710/7336 [00:00<00:01, 3120.56 examples/s]Tokenizing eval dataset:  39%|███▊      | 368/953 [00:01<00:01, 448.86 examples/s]Applying chat template to train dataset:  25%|██▌       | 1860/7336 [00:00<00:01, 3114.89 examples/s]Applying chat template to train dataset:  26%|██▌       | 1886/7336 [00:00<00:01, 3160.35 examples/s]Applying chat template to train dataset:  28%|██▊       | 2027/7336 [00:00<00:01, 3134.03 examples/s]Tokenizing eval dataset:  46%|████▌     | 436/953 [00:01<00:01, 510.77 examples/s]Applying chat template to train dataset:  30%|██▉       | 2176/7336 [00:00<00:01, 3126.15 examples/s]Applying chat template to train dataset:  30%|███       | 2205/7336 [00:00<00:01, 3167.32 examples/s]Applying chat template to train dataset:  32%|███▏      | 2343/7336 [00:00<00:01, 3140.64 examples/s]Tokenizing eval dataset:  53%|█████▎    | 504/953 [00:01<00:00, 553.25 examples/s]Applying chat template to train dataset:  34%|███▍      | 2491/7336 [00:00<00:01, 3129.90 examples/s]Applying chat template to train dataset:  34%|███▍      | 2525/7336 [00:00<00:01, 3173.46 examples/s]Applying chat template to train dataset:  36%|███▋      | 2668/7336 [00:00<00:01, 3171.36 examples/s]Tokenizing eval dataset:  60%|█████▉    | 568/953 [00:01<00:00, 574.25 examples/s]Applying chat template to train dataset:  38%|███▊      | 2810/7336 [00:00<00:01, 3077.78 examples/s]Applying chat template to train dataset:  41%|████      | 2983/7336 [00:00<00:01, 3122.86 examples/s]Tokenizing eval dataset:  67%|██████▋   | 637/953 [00:01<00:00, 603.51 examples/s]Applying chat template to train dataset:  43%|████▎     | 3129/7336 [00:01<00:01, 3126.69 examples/s]Applying chat template to train dataset:  43%|████▎     | 3124/7336 [00:01<00:01, 3093.17 examples/s]Applying chat template to train dataset:  45%|████▍     | 3301/7336 [00:01<00:01, 3137.51 examples/s]Applying chat template to train dataset:  47%|████▋     | 3438/7336 [00:01<00:01, 3105.88 examples/s]Tokenizing eval dataset:  76%|███████▌  | 726/953 [00:01<00:00, 589.76 examples/s]Applying chat template to train dataset:  49%|████▉     | 3600/7336 [00:01<00:01, 3127.78 examples/s]Applying chat template to train dataset:  49%|████▉     | 3620/7336 [00:01<00:01, 3146.26 examples/s]Applying chat template to train dataset:  51%|█████     | 3752/7336 [00:01<00:01, 3112.84 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3916/7336 [00:01<00:01, 3134.49 examples/s]Applying chat template to train dataset:  54%|█████▎    | 3940/7336 [00:01<00:01, 3156.61 examples/s]Tokenizing eval dataset:  84%|████████▍ | 803/953 [00:01<00:00, 558.12 examples/s]Applying chat template to train dataset:  55%|█████▌    | 4069/7336 [00:01<00:01, 3124.13 examples/s]Applying chat template to train dataset:  58%|█████▊    | 4240/7336 [00:01<00:00, 3160.13 examples/s]Applying chat template to train dataset:  58%|█████▊    | 4262/7336 [00:01<00:00, 3172.52 examples/s]Applying chat template to train dataset:  60%|█████▉    | 4391/7336 [00:01<00:00, 3149.38 examples/s]Tokenizing eval dataset:  92%|█████████▏| 881/953 [00:01<00:00, 536.38 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4580/7336 [00:01<00:00, 3220.52 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4598/7336 [00:01<00:00, 3222.41 examples/s]Applying chat template to train dataset:  64%|██████▍   | 4722/7336 [00:01<00:00, 3193.16 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4920/7336 [00:01<00:00, 3269.24 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4932/7336 [00:01<00:00, 3254.27 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 523.36 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 458.98 examples/s]
Applying chat template to train dataset:  69%|██████▉   | 5056/7336 [00:01<00:00, 3235.35 examples/s]Applying chat template to train dataset:  72%|███████▏  | 5261/7336 [00:01<00:00, 3303.34 examples/s]Applying chat template to train dataset:  72%|███████▏  | 5266/7336 [00:01<00:00, 3276.16 examples/s]Applying chat template to train dataset:  74%|███████▎  | 5396/7336 [00:01<00:00, 3280.46 examples/s]Applying chat template to train dataset:  76%|███████▋  | 5601/7336 [00:01<00:00, 3330.07 examples/s]Applying chat template to train dataset:  76%|███████▋  | 5600/7336 [00:01<00:00, 3289.34 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5734/7336 [00:01<00:00, 3308.52 examples/s]Applying chat template to train dataset:  81%|████████  | 5941/7336 [00:01<00:00, 3346.50 examples/s]Applying chat template to train dataset:  81%|████████  | 5934/7336 [00:01<00:00, 3300.44 examples/s]Applying chat template to train dataset:  83%|████████▎ | 6073/7336 [00:01<00:00, 3332.28 examples/s]Applying chat template to train dataset:  86%|████████▌ | 6282/7336 [00:01<00:00, 3362.31 examples/s]Applying chat template to train dataset:  85%|████████▌ | 6270/7336 [00:01<00:00, 3311.18 examples/s]Applying chat template to train dataset:  90%|█████████ | 6623/7336 [00:02<00:00, 3373.10 examples/s]Applying chat template to train dataset:  90%|████████▉ | 6569/7336 [00:02<00:00, 3320.61 examples/s]Applying chat template to train dataset:  90%|█████████ | 6606/7336 [00:02<00:00, 3320.98 examples/s]Applying chat template to train dataset:  95%|█████████▍| 6963/7336 [00:02<00:00, 3378.80 examples/s]Applying chat template to train dataset:  95%|█████████▍| 6940/7336 [00:02<00:00, 3325.39 examples/s]Applying chat template to train dataset:  96%|█████████▋| 7064/7336 [00:02<00:00, 3309.22 examples/s]Applying chat template to train dataset: 100%|██████████| 7336/7336 [00:02<00:00, 3351.62 examples/s]Applying chat template to train dataset: 100%|██████████| 7336/7336 [00:02<00:00, 3223.53 examples/s]
Applying chat template to train dataset:  99%|█████████▉| 7276/7336 [00:02<00:00, 3333.45 examples/s]Applying chat template to train dataset: 100%|██████████| 7336/7336 [00:02<00:00, 3214.77 examples/s]
Applying chat template to train dataset: 100%|██████████| 7336/7336 [00:02<00:00, 3185.92 examples/s]
Tokenizing train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/7336 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 42/7336 [00:00<00:17, 409.22 examples/s]Tokenizing train dataset:   1%|          | 42/7336 [00:00<00:17, 409.38 examples/s]Tokenizing train dataset:   1%|          | 42/7336 [00:00<00:17, 408.10 examples/s]Tokenizing train dataset:   1%|          | 90/7336 [00:00<00:21, 339.50 examples/s]Tokenizing train dataset:   1%|          | 90/7336 [00:00<00:21, 340.54 examples/s]Tokenizing train dataset:   1%|          | 90/7336 [00:00<00:21, 337.29 examples/s]Tokenizing train dataset:   2%|▏         | 139/7336 [00:00<00:22, 321.37 examples/s]Tokenizing train dataset:   2%|▏         | 139/7336 [00:00<00:22, 323.32 examples/s]Tokenizing train dataset:   2%|▏         | 139/7336 [00:00<00:22, 319.19 examples/s]Tokenizing train dataset:   3%|▎         | 184/7336 [00:00<00:23, 309.59 examples/s]Tokenizing train dataset:   3%|▎         | 184/7336 [00:00<00:22, 311.50 examples/s]Tokenizing train dataset:   2%|▏         | 183/7336 [00:00<00:23, 304.78 examples/s]Tokenizing train dataset:   3%|▎         | 218/7336 [00:00<00:22, 314.89 examples/s]Tokenizing train dataset:   3%|▎         | 218/7336 [00:00<00:22, 316.93 examples/s]Tokenizing train dataset:   3%|▎         | 218/7336 [00:00<00:22, 312.01 examples/s]Tokenizing train dataset:   3%|▎         | 252/7336 [00:00<00:22, 319.50 examples/s]Tokenizing train dataset:   3%|▎         | 251/7336 [00:00<00:22, 318.43 examples/s]Tokenizing train dataset:   3%|▎         | 251/7336 [00:00<00:22, 314.66 examples/s]Tokenizing train dataset:   4%|▍         | 290/7336 [00:00<00:21, 332.61 examples/s]Tokenizing train dataset:   4%|▍         | 289/7336 [00:00<00:21, 334.17 examples/s]Tokenizing train dataset:   4%|▍         | 289/7336 [00:00<00:21, 330.42 examples/s]Tokenizing train dataset:   4%|▍         | 324/7336 [00:00<00:21, 331.24 examples/s]Tokenizing train dataset:   5%|▍         | 338/7336 [00:01<00:21, 326.99 examples/s]Tokenizing train dataset:   5%|▍         | 337/7336 [00:01<00:21, 322.55 examples/s]Tokenizing train dataset:   5%|▌         | 370/7336 [00:01<00:21, 317.21 examples/s]Tokenizing train dataset:   5%|▌         | 389/7336 [00:01<00:21, 325.20 examples/s]Tokenizing train dataset:   5%|▌         | 387/7336 [00:01<00:21, 320.17 examples/s]Tokenizing train dataset:   6%|▌         | 420/7336 [00:01<00:21, 315.66 examples/s]Tokenizing train dataset:   6%|▌         | 437/7336 [00:01<00:21, 315.60 examples/s]Tokenizing train dataset:   6%|▌         | 434/7336 [00:01<00:22, 312.13 examples/s]Tokenizing train dataset:   6%|▋         | 469/7336 [00:01<00:21, 313.93 examples/s]Tokenizing train dataset:   6%|▋         | 469/7336 [00:01<00:21, 315.31 examples/s]Tokenizing train dataset:   7%|▋         | 480/7336 [00:01<00:22, 305.86 examples/s]Tokenizing train dataset:   7%|▋         | 512/7336 [00:01<00:22, 303.44 examples/s]Tokenizing train dataset:   7%|▋         | 516/7336 [00:01<00:22, 308.84 examples/s]Tokenizing train dataset:   7%|▋         | 527/7336 [00:01<00:22, 304.83 examples/s]Tokenizing train dataset:   7%|▋         | 549/7336 [00:01<00:21, 316.44 examples/s]Tokenizing train dataset:   7%|▋         | 550/7336 [00:01<00:21, 314.26 examples/s]Tokenizing train dataset:   8%|▊         | 561/7336 [00:01<00:21, 309.73 examples/s]Tokenizing train dataset:   8%|▊         | 594/7336 [00:01<00:21, 307.32 examples/s]Tokenizing train dataset:   8%|▊         | 596/7336 [00:01<00:21, 309.76 examples/s]Tokenizing train dataset:   8%|▊         | 605/7336 [00:01<00:22, 299.31 examples/s]Tokenizing train dataset:   9%|▊         | 630/7336 [00:01<00:21, 317.66 examples/s]Tokenizing train dataset:   9%|▊         | 634/7336 [00:01<00:20, 324.45 examples/s]Tokenizing train dataset:   9%|▉         | 644/7336 [00:02<00:21, 316.50 examples/s]Tokenizing train dataset:   9%|▉         | 678/7336 [00:02<00:21, 314.01 examples/s]Tokenizing train dataset:   9%|▉         | 681/7336 [00:02<00:21, 315.43 examples/s]Tokenizing train dataset:   9%|▉         | 689/7336 [00:02<00:21, 307.27 examples/s]Tokenizing train dataset:  10%|▉         | 725/7336 [00:02<00:21, 310.17 examples/s]Tokenizing train dataset:  10%|▉         | 729/7336 [00:02<00:21, 313.82 examples/s]Tokenizing train dataset:  10%|█         | 740/7336 [00:02<00:20, 314.47 examples/s]Tokenizing train dataset:  10%|█         | 759/7336 [00:02<00:20, 313.48 examples/s]Tokenizing train dataset:  10%|█         | 763/7336 [00:02<00:20, 315.29 examples/s]Tokenizing train dataset:  11%|█         | 785/7336 [00:02<00:21, 305.48 examples/s]Tokenizing train dataset:  11%|█         | 802/7336 [00:02<00:21, 298.12 examples/s]Tokenizing train dataset:  11%|█         | 808/7336 [00:02<00:21, 303.56 examples/s]Tokenizing train dataset:  11%|█▏        | 839/7336 [00:02<00:21, 300.82 examples/s]Tokenizing train dataset:  11%|█▏        | 827/7336 [00:02<00:22, 292.25 examples/s]Tokenizing train dataset:  12%|█▏        | 847/7336 [00:02<00:21, 296.05 examples/s]Tokenizing train dataset:  12%|█▏        | 860/7336 [00:02<00:21, 297.38 examples/s]Tokenizing train dataset:  12%|█▏        | 879/7336 [00:02<00:21, 296.84 examples/s]Tokenizing train dataset:  12%|█▏        | 887/7336 [00:02<00:21, 305.98 examples/s]Tokenizing train dataset:  12%|█▏        | 897/7336 [00:02<00:20, 313.20 examples/s]Tokenizing train dataset:  12%|█▏        | 916/7336 [00:02<00:20, 313.72 examples/s]Tokenizing train dataset:  13%|█▎        | 920/7336 [00:02<00:20, 307.16 examples/s]Tokenizing train dataset:  13%|█▎        | 952/7336 [00:03<00:20, 305.86 examples/s]Tokenizing train dataset:  13%|█▎        | 940/7336 [00:03<00:21, 300.49 examples/s]Tokenizing train dataset:  13%|█▎        | 962/7336 [00:03<00:20, 304.50 examples/s]Tokenizing train dataset:  13%|█▎        | 983/7336 [00:03<00:21, 302.30 examples/s]Tokenizing train dataset:  13%|█▎        | 972/7336 [00:03<00:20, 303.92 examples/s]Tokenizing train dataset:  14%|█▎        | 1007/7336 [00:03<00:21, 296.92 examples/s]Tokenizing train dataset:  14%|█▍        | 1027/7336 [00:03<00:21, 293.57 examples/s]Tokenizing train dataset:  14%|█▍        | 1013/7336 [00:03<00:21, 287.98 examples/s]Tokenizing train dataset:  14%|█▍        | 1049/7336 [00:03<00:21, 290.36 examples/s]Tokenizing train dataset:  14%|█▍        | 1057/7336 [00:03<00:21, 293.09 examples/s]Tokenizing train dataset:  14%|█▍        | 1060/7336 [00:03<00:21, 289.41 examples/s]Tokenizing train dataset:  15%|█▍        | 1082/7336 [00:03<00:20, 298.91 examples/s]Tokenizing train dataset:  15%|█▍        | 1093/7336 [00:03<00:20, 308.66 examples/s]Tokenizing train dataset:  15%|█▍        | 1095/7336 [00:03<00:20, 301.95 examples/s]Tokenizing train dataset:  15%|█▌        | 1117/7336 [00:03<00:20, 306.10 examples/s]Tokenizing train dataset:  15%|█▌        | 1135/7336 [00:03<00:21, 295.20 examples/s]Tokenizing train dataset:  15%|█▌        | 1137/7336 [00:03<00:21, 289.21 examples/s]Tokenizing train dataset:  16%|█▌        | 1159/7336 [00:03<00:21, 292.45 examples/s]Tokenizing train dataset:  16%|█▌        | 1166/7336 [00:03<00:20, 296.07 examples/s]Tokenizing train dataset:  16%|█▌        | 1167/7336 [00:03<00:21, 289.62 examples/s]Tokenizing train dataset:  16%|█▌        | 1190/7336 [00:03<00:20, 296.17 examples/s]Tokenizing train dataset:  16%|█▋        | 1202/7336 [00:03<00:19, 306.75 examples/s]Tokenizing train dataset:  16%|█▋        | 1202/7336 [00:03<00:20, 300.62 examples/s]Tokenizing train dataset:  17%|█▋        | 1222/7336 [00:03<00:20, 298.56 examples/s]Tokenizing train dataset:  17%|█▋        | 1236/7336 [00:03<00:19, 312.59 examples/s]Tokenizing train dataset:  17%|█▋        | 1258/7336 [00:04<00:19, 313.89 examples/s]Tokenizing train dataset:  17%|█▋        | 1236/7336 [00:04<00:19, 306.56 examples/s]Tokenizing train dataset:  17%|█▋        | 1270/7336 [00:04<00:19, 314.29 examples/s]Tokenizing train dataset:  17%|█▋        | 1270/7336 [00:04<00:19, 308.43 examples/s]Tokenizing train dataset:  18%|█▊        | 1305/7336 [00:04<00:18, 319.12 examples/s]Tokenizing train dataset:  18%|█▊        | 1309/7336 [00:04<00:18, 317.64 examples/s]Tokenizing train dataset:  18%|█▊        | 1304/7336 [00:04<00:19, 311.31 examples/s]Tokenizing train dataset:  18%|█▊        | 1339/7336 [00:04<00:18, 321.43 examples/s]Tokenizing train dataset:  18%|█▊        | 1342/7336 [00:04<00:19, 315.31 examples/s]Tokenizing train dataset:  18%|█▊        | 1339/7336 [00:04<00:19, 314.90 examples/s]Tokenizing train dataset:  19%|█▉        | 1381/7336 [00:04<00:19, 301.69 examples/s]Tokenizing train dataset:  19%|█▉        | 1386/7336 [00:04<00:19, 300.76 examples/s]Tokenizing train dataset:  19%|█▉        | 1379/7336 [00:04<00:20, 294.98 examples/s]Tokenizing train dataset:  19%|█▉        | 1412/7336 [00:04<00:19, 302.36 examples/s]Tokenizing train dataset:  19%|█▉        | 1418/7336 [00:04<00:19, 301.11 examples/s]Tokenizing train dataset:  19%|█▉        | 1410/7336 [00:04<00:20, 294.55 examples/s]Tokenizing train dataset:  20%|█▉        | 1457/7336 [00:04<00:19, 299.89 examples/s]Tokenizing train dataset:  20%|█▉        | 1462/7336 [00:04<00:19, 293.89 examples/s]Tokenizing train dataset:  20%|█▉        | 1440/7336 [00:04<00:20, 292.50 examples/s]Tokenizing train dataset:  20%|██        | 1488/7336 [00:04<00:19, 299.14 examples/s]Tokenizing train dataset:  20%|██        | 1492/7336 [00:04<00:19, 293.68 examples/s]Tokenizing train dataset:  20%|██        | 1472/7336 [00:04<00:19, 297.57 examples/s]Tokenizing train dataset:  21%|██        | 1535/7336 [00:04<00:19, 300.71 examples/s]Tokenizing train dataset:  21%|██        | 1539/7336 [00:05<00:19, 296.24 examples/s]Tokenizing train dataset:  21%|██        | 1514/7336 [00:04<00:20, 285.61 examples/s]Tokenizing train dataset:  21%|██▏       | 1567/7336 [00:05<00:19, 300.14 examples/s]Tokenizing train dataset:  21%|██        | 1548/7336 [00:05<00:19, 297.42 examples/s]Tokenizing train dataset:  21%|██▏       | 1570/7336 [00:05<00:19, 293.85 examples/s]Tokenizing train dataset:  22%|██▏       | 1598/7336 [00:05<00:19, 298.35 examples/s]Tokenizing train dataset:  22%|██▏       | 1601/7336 [00:05<00:19, 293.37 examples/s]Tokenizing train dataset:  22%|██▏       | 1591/7336 [00:05<00:19, 291.18 examples/s]Tokenizing train dataset:  22%|██▏       | 1631/7336 [00:05<00:18, 304.92 examples/s]Tokenizing train dataset:  22%|██▏       | 1637/7336 [00:05<00:18, 305.80 examples/s]Tokenizing train dataset:  22%|██▏       | 1627/7336 [00:05<00:18, 301.16 examples/s]Tokenizing train dataset:  23%|██▎       | 1667/7336 [00:05<00:17, 315.68 examples/s]Tokenizing train dataset:  23%|██▎       | 1670/7336 [00:05<00:18, 309.85 examples/s]Tokenizing train dataset:  23%|██▎       | 1661/7336 [00:05<00:18, 309.00 examples/s]Tokenizing train dataset:  23%|██▎       | 1704/7336 [00:05<00:17, 330.10 examples/s]Tokenizing train dataset:  23%|██▎       | 1710/7336 [00:05<00:17, 328.11 examples/s]Tokenizing train dataset:  23%|██▎       | 1701/7336 [00:05<00:17, 328.33 examples/s]Tokenizing train dataset:  24%|██▎       | 1739/7336 [00:05<00:16, 332.32 examples/s]Tokenizing train dataset:  24%|██▍       | 1746/7336 [00:05<00:16, 334.28 examples/s]Tokenizing train dataset:  24%|██▍       | 1775/7336 [00:05<00:16, 337.23 examples/s]Tokenizing train dataset:  24%|██▍       | 1751/7336 [00:05<00:17, 325.85 examples/s]Tokenizing train dataset:  24%|██▍       | 1781/7336 [00:05<00:16, 334.76 examples/s]Tokenizing train dataset:  24%|██▍       | 1789/7336 [00:05<00:16, 334.31 examples/s]Tokenizing train dataset:  25%|██▍       | 1822/7336 [00:05<00:17, 320.78 examples/s]Tokenizing train dataset:  25%|██▍       | 1826/7336 [00:05<00:17, 315.10 examples/s]Tokenizing train dataset:  25%|██▍       | 1830/7336 [00:05<00:17, 307.67 examples/s]Tokenizing train dataset:  25%|██▌       | 1868/7336 [00:05<00:17, 311.97 examples/s]Tokenizing train dataset:  26%|██▌       | 1872/7336 [00:06<00:17, 306.76 examples/s]Tokenizing train dataset:  25%|██▌       | 1863/7336 [00:06<00:17, 308.70 examples/s]Tokenizing train dataset:  26%|██▌       | 1907/7336 [00:06<00:18, 289.64 examples/s]Tokenizing train dataset:  26%|██▌       | 1900/7336 [00:06<00:18, 286.17 examples/s]Tokenizing train dataset:  26%|██▌       | 1910/7336 [00:06<00:21, 257.05 examples/s]Tokenizing train dataset:  27%|██▋       | 1951/7336 [00:06<00:18, 283.83 examples/s]Tokenizing train dataset:  26%|██▋       | 1940/7336 [00:06<00:19, 274.62 examples/s]Tokenizing train dataset:  27%|██▋       | 1953/7336 [00:06<00:20, 263.44 examples/s]Tokenizing train dataset:  27%|██▋       | 1980/7336 [00:06<00:19, 279.87 examples/s]Tokenizing train dataset:  27%|██▋       | 1982/7336 [00:06<00:20, 267.62 examples/s]Tokenizing train dataset:  27%|██▋       | 1969/7336 [00:06<00:19, 275.24 examples/s]Tokenizing train dataset:  27%|██▋       | 2013/7336 [00:06<00:18, 288.60 examples/s]Tokenizing train dataset:  27%|██▋       | 2013/7336 [00:06<00:19, 276.96 examples/s]Tokenizing train dataset:  27%|██▋       | 2000/7336 [00:06<00:18, 282.37 examples/s]Tokenizing train dataset:  28%|██▊       | 2047/7336 [00:06<00:17, 300.43 examples/s]Tokenizing train dataset:  28%|██▊       | 2047/7336 [00:06<00:18, 291.29 examples/s]Tokenizing train dataset:  28%|██▊       | 2033/7336 [00:06<00:18, 293.61 examples/s]Tokenizing train dataset:  28%|██▊       | 2083/7336 [00:06<00:16, 314.04 examples/s]Tokenizing train dataset:  28%|██▊       | 2083/7336 [00:06<00:17, 306.44 examples/s]Tokenizing train dataset:  28%|██▊       | 2071/7336 [00:06<00:16, 313.59 examples/s]Tokenizing train dataset:  29%|██▉       | 2116/7336 [00:06<00:16, 316.11 examples/s]Tokenizing train dataset:  29%|██▉       | 2116/7336 [00:06<00:16, 309.73 examples/s]Tokenizing train dataset:  29%|██▊       | 2106/7336 [00:06<00:16, 317.33 examples/s]Tokenizing train dataset:  29%|██▉       | 2162/7336 [00:06<00:16, 309.30 examples/s]Tokenizing train dataset:  29%|██▉       | 2162/7336 [00:07<00:17, 304.30 examples/s]Tokenizing train dataset:  29%|██▉       | 2151/7336 [00:07<00:17, 304.25 examples/s]Tokenizing train dataset:  30%|███       | 2205/7336 [00:07<00:17, 297.02 examples/s]Tokenizing train dataset:  30%|███       | 2204/7336 [00:07<00:17, 293.23 examples/s]Tokenizing train dataset:  30%|██▉       | 2196/7336 [00:07<00:17, 300.84 examples/s]Tokenizing train dataset:  31%|███       | 2243/7336 [00:07<00:16, 313.22 examples/s]Tokenizing train dataset:  31%|███       | 2241/7336 [00:07<00:16, 309.70 examples/s]Tokenizing train dataset:  31%|███       | 2276/7336 [00:07<00:16, 313.17 examples/s]Tokenizing train dataset:  31%|███       | 2246/7336 [00:07<00:16, 307.37 examples/s]Tokenizing train dataset:  31%|███       | 2275/7336 [00:07<00:16, 312.13 examples/s]Tokenizing train dataset:  31%|███▏      | 2308/7336 [00:07<00:16, 307.43 examples/s]Tokenizing train dataset:  31%|███       | 2277/7336 [00:07<00:16, 306.67 examples/s]Tokenizing train dataset:  32%|███▏      | 2321/7336 [00:07<00:16, 305.95 examples/s]Tokenizing train dataset:  32%|███▏      | 2353/7336 [00:07<00:16, 298.24 examples/s]Tokenizing train dataset:  32%|███▏      | 2324/7336 [00:07<00:16, 306.86 examples/s]Tokenizing train dataset:  32%|███▏      | 2384/7336 [00:07<00:16, 295.82 examples/s]Tokenizing train dataset:  32%|███▏      | 2365/7336 [00:07<00:16, 297.29 examples/s]Tokenizing train dataset:  32%|███▏      | 2368/7336 [00:07<00:16, 297.11 examples/s]Tokenizing train dataset:  33%|███▎      | 2416/7336 [00:07<00:16, 300.78 examples/s]Tokenizing train dataset:  33%|███▎      | 2410/7336 [00:07<00:16, 294.07 examples/s]Tokenizing train dataset:  33%|███▎      | 2447/7336 [00:07<00:16, 299.32 examples/s]Tokenizing train dataset:  33%|███▎      | 2412/7336 [00:07<00:16, 292.40 examples/s]Tokenizing train dataset:  33%|███▎      | 2443/7336 [00:08<00:16, 297.87 examples/s]Tokenizing train dataset:  34%|███▍      | 2478/7336 [00:08<00:16, 300.88 examples/s]Tokenizing train dataset:  33%|███▎      | 2445/7336 [00:08<00:16, 296.23 examples/s]Tokenizing train dataset:  34%|███▍      | 2491/7336 [00:08<00:16, 299.88 examples/s]Tokenizing train dataset:  34%|███▍      | 2517/7336 [00:08<00:17, 278.41 examples/s]Tokenizing train dataset:  34%|███▍      | 2491/7336 [00:08<00:16, 297.88 examples/s]Tokenizing train dataset:  35%|███▍      | 2549/7336 [00:08<00:16, 283.23 examples/s]Tokenizing train dataset:  34%|███▍      | 2525/7336 [00:08<00:17, 273.78 examples/s]Tokenizing train dataset:  34%|███▍      | 2524/7336 [00:08<00:17, 268.97 examples/s]Tokenizing train dataset:  35%|███▌      | 2583/7336 [00:08<00:16, 296.36 examples/s]Tokenizing train dataset:  35%|███▍      | 2558/7336 [00:08<00:16, 283.94 examples/s]Tokenizing train dataset:  35%|███▍      | 2558/7336 [00:08<00:17, 280.71 examples/s]Tokenizing train dataset:  36%|███▌      | 2619/7336 [00:08<00:15, 308.99 examples/s]Tokenizing train dataset:  35%|███▌      | 2593/7336 [00:08<00:15, 298.22 examples/s]Tokenizing train dataset:  35%|███▌      | 2593/7336 [00:08<00:16, 295.66 examples/s]Tokenizing train dataset:  36%|███▌      | 2653/7336 [00:08<00:14, 312.59 examples/s]Tokenizing train dataset:  36%|███▌      | 2627/7336 [00:08<00:15, 305.97 examples/s]Tokenizing train dataset:  36%|███▌      | 2627/7336 [00:08<00:15, 303.65 examples/s]Tokenizing train dataset:  37%|███▋      | 2688/7336 [00:08<00:14, 317.48 examples/s]Tokenizing train dataset:  36%|███▋      | 2660/7336 [00:08<00:15, 310.39 examples/s]Tokenizing train dataset:  36%|███▋      | 2660/7336 [00:08<00:15, 308.54 examples/s]Tokenizing train dataset:  37%|███▋      | 2724/7336 [00:08<00:14, 323.16 examples/s]Tokenizing train dataset:  37%|███▋      | 2695/7336 [00:08<00:14, 317.46 examples/s]Tokenizing train dataset:  37%|███▋      | 2695/7336 [00:08<00:14, 315.42 examples/s]Tokenizing train dataset:  38%|███▊      | 2767/7336 [00:08<00:14, 306.48 examples/s]Tokenizing train dataset:  37%|███▋      | 2740/7336 [00:09<00:14, 308.43 examples/s]Tokenizing train dataset:  37%|███▋      | 2727/7336 [00:08<00:14, 313.56 examples/s]Tokenizing train dataset:  38%|███▊      | 2800/7336 [00:09<00:14, 310.86 examples/s]Tokenizing train dataset:  38%|███▊      | 2788/7336 [00:09<00:14, 306.27 examples/s]Tokenizing train dataset:  38%|███▊      | 2770/7336 [00:09<00:15, 299.78 examples/s]Tokenizing train dataset:  39%|███▉      | 2845/7336 [00:09<00:14, 302.91 examples/s]Tokenizing train dataset:  38%|███▊      | 2820/7336 [00:09<00:14, 306.20 examples/s]Tokenizing train dataset:  38%|███▊      | 2804/7336 [00:09<00:14, 307.25 examples/s]Tokenizing train dataset:  39%|███▉      | 2877/7336 [00:09<00:14, 303.00 examples/s]Tokenizing train dataset:  39%|███▉      | 2863/7336 [00:09<00:15, 297.58 examples/s]Tokenizing train dataset:  39%|███▉      | 2848/7336 [00:09<00:15, 297.72 examples/s]Tokenizing train dataset:  40%|███▉      | 2909/7336 [00:09<00:14, 304.62 examples/s]Tokenizing train dataset:  39%|███▉      | 2895/7336 [00:09<00:14, 301.02 examples/s]Tokenizing train dataset:  39%|███▉      | 2879/7336 [00:09<00:15, 296.78 examples/s]Tokenizing train dataset:  40%|████      | 2940/7336 [00:09<00:14, 303.03 examples/s]Tokenizing train dataset:  40%|███▉      | 2910/7336 [00:09<00:14, 298.64 examples/s]Tokenizing train dataset:  41%|████      | 2973/7336 [00:09<00:14, 306.97 examples/s]Tokenizing train dataset:  40%|████      | 2942/7336 [00:09<00:14, 301.86 examples/s]Tokenizing train dataset:  40%|████      | 2942/7336 [00:09<00:14, 300.59 examples/s]Tokenizing train dataset:  41%|████      | 2974/7336 [00:09<00:14, 303.40 examples/s]Tokenizing train dataset:  41%|████      | 3020/7336 [00:09<00:14, 305.54 examples/s]Tokenizing train dataset:  41%|████      | 2974/7336 [00:09<00:14, 302.56 examples/s]Tokenizing train dataset:  41%|████      | 3005/7336 [00:09<00:14, 300.34 examples/s]Tokenizing train dataset:  41%|████      | 3005/7336 [00:09<00:14, 298.75 examples/s]Tokenizing train dataset:  42%|████▏     | 3062/7336 [00:09<00:14, 291.91 examples/s]Tokenizing train dataset:  42%|████▏     | 3051/7336 [00:10<00:14, 300.43 examples/s]Tokenizing train dataset:  42%|████▏     | 3094/7336 [00:10<00:14, 297.47 examples/s]Tokenizing train dataset:  42%|████▏     | 3051/7336 [00:10<00:14, 298.56 examples/s]Tokenizing train dataset:  43%|████▎     | 3126/7336 [00:10<00:14, 299.38 examples/s]Tokenizing train dataset:  42%|████▏     | 3096/7336 [00:10<00:14, 296.01 examples/s]Tokenizing train dataset:  42%|████▏     | 3094/7336 [00:10<00:14, 292.76 examples/s]Tokenizing train dataset:  43%|████▎     | 3127/7336 [00:10<00:14, 294.74 examples/s]Tokenizing train dataset:  43%|████▎     | 3171/7336 [00:10<00:14, 293.72 examples/s]Tokenizing train dataset:  43%|████▎     | 3126/7336 [00:10<00:14, 294.84 examples/s]Tokenizing train dataset:  43%|████▎     | 3157/7336 [00:10<00:14, 289.18 examples/s]Tokenizing train dataset:  44%|████▎     | 3205/7336 [00:10<00:13, 302.50 examples/s]Tokenizing train dataset:  44%|████▎     | 3192/7336 [00:10<00:13, 303.34 examples/s]Tokenizing train dataset:  43%|████▎     | 3170/7336 [00:10<00:14, 289.71 examples/s]Tokenizing train dataset:  44%|████▍     | 3237/7336 [00:10<00:13, 302.71 examples/s]Tokenizing train dataset:  44%|████▎     | 3204/7336 [00:10<00:13, 298.68 examples/s]Tokenizing train dataset:  45%|████▍     | 3270/7336 [00:10<00:13, 308.31 examples/s]Tokenizing train dataset:  44%|████▍     | 3238/7336 [00:10<00:13, 300.19 examples/s]Tokenizing train dataset:  44%|████▍     | 3235/7336 [00:10<00:13, 297.12 examples/s]Tokenizing train dataset:  45%|████▌     | 3302/7336 [00:10<00:13, 304.96 examples/s]Tokenizing train dataset:  45%|████▍     | 3271/7336 [00:10<00:13, 304.84 examples/s]Tokenizing train dataset:  45%|████▍     | 3269/7336 [00:10<00:13, 305.79 examples/s]Tokenizing train dataset:  45%|████▌     | 3302/7336 [00:10<00:13, 303.99 examples/s]Tokenizing train dataset:  46%|████▌     | 3350/7336 [00:10<00:12, 307.64 examples/s]Tokenizing train dataset:  45%|████▍     | 3300/7336 [00:10<00:13, 302.57 examples/s]Tokenizing train dataset:  46%|████▌     | 3350/7336 [00:11<00:13, 306.31 examples/s]Tokenizing train dataset:  46%|████▌     | 3389/7336 [00:11<00:13, 287.38 examples/s]Tokenizing train dataset:  46%|████▌     | 3347/7336 [00:11<00:13, 302.46 examples/s]Tokenizing train dataset:  46%|████▌     | 3389/7336 [00:11<00:13, 286.39 examples/s]Tokenizing train dataset:  47%|████▋     | 3430/7336 [00:11<00:13, 279.87 examples/s]Tokenizing train dataset:  46%|████▌     | 3387/7336 [00:11<00:13, 284.70 examples/s]Tokenizing train dataset:  47%|████▋     | 3421/7336 [00:11<00:16, 236.46 examples/s]Tokenizing train dataset:  47%|████▋     | 3424/7336 [00:11<00:14, 265.05 examples/s]Tokenizing train dataset:  47%|████▋     | 3470/7336 [00:11<00:15, 246.41 examples/s]Tokenizing train dataset:  47%|████▋     | 3447/7336 [00:11<00:16, 238.89 examples/s]Tokenizing train dataset:  47%|████▋     | 3460/7336 [00:11<00:15, 254.70 examples/s]Tokenizing train dataset:  48%|████▊     | 3517/7336 [00:11<00:14, 264.23 examples/s]Tokenizing train dataset:  47%|████▋     | 3481/7336 [00:11<00:16, 232.58 examples/s]Tokenizing train dataset:  48%|████▊     | 3549/7336 [00:11<00:13, 274.33 examples/s]Tokenizing train dataset:  48%|████▊     | 3497/7336 [00:11<00:15, 247.56 examples/s]Tokenizing train dataset:  48%|████▊     | 3517/7336 [00:11<00:14, 259.94 examples/s]Tokenizing train dataset:  49%|████▉     | 3586/7336 [00:11<00:12, 293.86 examples/s]Tokenizing train dataset:  48%|████▊     | 3538/7336 [00:11<00:13, 282.34 examples/s]Tokenizing train dataset:  48%|████▊     | 3549/7336 [00:11<00:13, 271.98 examples/s]Tokenizing train dataset:  49%|████▉     | 3617/7336 [00:11<00:12, 293.67 examples/s]Tokenizing train dataset:  49%|████▊     | 3570/7336 [00:11<00:13, 286.61 examples/s]Tokenizing train dataset:  49%|████▉     | 3585/7336 [00:11<00:12, 294.40 examples/s]Tokenizing train dataset:  50%|████▉     | 3650/7336 [00:11<00:12, 296.76 examples/s]Tokenizing train dataset:  49%|████▉     | 3603/7336 [00:12<00:12, 293.89 examples/s]Tokenizing train dataset:  49%|████▉     | 3616/7336 [00:12<00:12, 292.66 examples/s]Tokenizing train dataset:  50%|█████     | 3681/7336 [00:12<00:12, 295.57 examples/s]Tokenizing train dataset:  50%|████▉     | 3635/7336 [00:12<00:12, 298.75 examples/s]Tokenizing train dataset:  50%|████▉     | 3648/7336 [00:12<00:12, 296.35 examples/s]Tokenizing train dataset:  51%|█████     | 3713/7336 [00:12<00:12, 298.14 examples/s]Tokenizing train dataset:  50%|█████     | 3668/7336 [00:12<00:12, 302.86 examples/s]Tokenizing train dataset:  50%|█████     | 3679/7336 [00:12<00:12, 294.43 examples/s]Tokenizing train dataset:  51%|█████     | 3757/7336 [00:12<00:10, 334.15 examples/s]Tokenizing train dataset:  51%|█████     | 3710/7336 [00:12<00:12, 295.03 examples/s]Tokenizing train dataset:  51%|█████     | 3713/7336 [00:12<00:12, 295.59 examples/s]Tokenizing train dataset:  52%|█████▏    | 3793/7336 [00:12<00:10, 334.81 examples/s]Tokenizing train dataset:  51%|█████     | 3755/7336 [00:12<00:10, 336.84 examples/s]Tokenizing train dataset:  51%|█████     | 3757/7336 [00:12<00:10, 328.46 examples/s]Tokenizing train dataset:  52%|█████▏    | 3830/7336 [00:12<00:10, 341.88 examples/s]Tokenizing train dataset:  52%|█████▏    | 3805/7336 [00:12<00:10, 333.30 examples/s]Tokenizing train dataset:  52%|█████▏    | 3809/7336 [00:12<00:10, 332.69 examples/s]Tokenizing train dataset:  53%|█████▎    | 3878/7336 [00:12<00:10, 330.66 examples/s]Tokenizing train dataset:  52%|█████▏    | 3841/7336 [00:12<00:10, 336.23 examples/s]Tokenizing train dataset:  52%|█████▏    | 3844/7336 [00:12<00:10, 332.76 examples/s]Tokenizing train dataset:  53%|█████▎    | 3919/7336 [00:12<00:11, 307.56 examples/s]Tokenizing train dataset:  53%|█████▎    | 3889/7336 [00:12<00:10, 325.53 examples/s]Tokenizing train dataset:  53%|█████▎    | 3892/7336 [00:12<00:10, 322.95 examples/s]Tokenizing train dataset:  54%|█████▍    | 3972/7336 [00:12<00:10, 314.74 examples/s]Tokenizing train dataset:  54%|█████▎    | 3930/7336 [00:13<00:11, 304.93 examples/s]Tokenizing train dataset:  54%|█████▎    | 3933/7336 [00:13<00:11, 303.52 examples/s]Tokenizing train dataset:  55%|█████▍    | 4013/7336 [00:13<00:10, 331.41 examples/s]Tokenizing train dataset:  54%|█████▍    | 3967/7336 [00:13<00:10, 310.76 examples/s]Tokenizing train dataset:  54%|█████▍    | 3980/7336 [00:13<00:10, 311.55 examples/s]Tokenizing train dataset:  55%|█████▌    | 4057/7336 [00:13<00:10, 315.01 examples/s]Tokenizing train dataset:  55%|█████▍    | 4005/7336 [00:13<00:10, 325.23 examples/s]Tokenizing train dataset:  55%|█████▍    | 4020/7336 [00:13<00:10, 327.32 examples/s]Tokenizing train dataset:  56%|█████▌    | 4091/7336 [00:13<00:10, 318.43 examples/s]Tokenizing train dataset:  55%|█████▌    | 4039/7336 [00:13<00:10, 324.88 examples/s]Tokenizing train dataset:  55%|█████▌    | 4067/7336 [00:13<00:10, 315.18 examples/s]Tokenizing train dataset:  56%|█████▋    | 4136/7336 [00:13<00:10, 308.24 examples/s]Tokenizing train dataset:  56%|█████▌    | 4086/7336 [00:13<00:10, 313.58 examples/s]Tokenizing train dataset:  56%|█████▌    | 4100/7336 [00:13<00:10, 314.49 examples/s]Tokenizing train dataset:  57%|█████▋    | 4181/7336 [00:13<00:10, 303.60 examples/s]Tokenizing train dataset:  56%|█████▋    | 4130/7336 [00:13<00:10, 300.88 examples/s]Tokenizing train dataset:  56%|█████▋    | 4142/7336 [00:13<00:10, 299.56 examples/s]Tokenizing train dataset:  58%|█████▊    | 4277/7336 [00:13<00:06, 455.77 examples/s]Tokenizing train dataset:  57%|█████▋    | 4174/7336 [00:13<00:10, 299.62 examples/s]Tokenizing train dataset:  57%|█████▋    | 4175/7336 [00:13<00:10, 295.42 examples/s]Tokenizing train dataset:  60%|██████    | 4409/7336 [00:13<00:04, 669.91 examples/s]Tokenizing train dataset:  58%|█████▊    | 4251/7336 [00:13<00:07, 416.04 examples/s]Tokenizing train dataset:  58%|█████▊    | 4256/7336 [00:13<00:07, 411.08 examples/s]Tokenizing train dataset:  62%|██████▏   | 4536/7336 [00:13<00:03, 824.89 examples/s]Tokenizing train dataset:  60%|█████▉    | 4382/7336 [00:14<00:04, 649.82 examples/s]Tokenizing train dataset:  60%|█████▉    | 4388/7336 [00:14<00:04, 632.78 examples/s]Tokenizing train dataset:  64%|██████▎   | 4659/7336 [00:14<00:02, 933.14 examples/s]Tokenizing train dataset:  61%|██████▏   | 4510/7336 [00:14<00:03, 819.30 examples/s]Tokenizing train dataset:  62%|██████▏   | 4517/7336 [00:14<00:03, 800.32 examples/s]Tokenizing train dataset:  65%|██████▌   | 4790/7336 [00:14<00:02, 1034.27 examples/s]Tokenizing train dataset:  63%|██████▎   | 4636/7336 [00:14<00:02, 941.95 examples/s]Tokenizing train dataset:  63%|██████▎   | 4644/7336 [00:14<00:02, 924.95 examples/s]Tokenizing train dataset:  67%|██████▋   | 4918/7336 [00:14<00:02, 1102.32 examples/s]Tokenizing train dataset:  65%|██████▍   | 4763/7336 [00:14<00:02, 1032.01 examples/s]Tokenizing train dataset:  65%|██████▌   | 4773/7336 [00:14<00:02, 1024.99 examples/s]Tokenizing train dataset:  69%|██████▊   | 5041/7336 [00:14<00:02, 1137.93 examples/s]Tokenizing train dataset:  67%|██████▋   | 4892/7336 [00:14<00:02, 1103.96 examples/s]Tokenizing train dataset:  67%|██████▋   | 4902/7336 [00:14<00:02, 1096.91 examples/s]Tokenizing train dataset:  70%|███████   | 5166/7336 [00:14<00:01, 1166.86 examples/s]Tokenizing train dataset:  68%|██████▊   | 5018/7336 [00:14<00:02, 1146.53 examples/s]Tokenizing train dataset:  69%|██████▊   | 5029/7336 [00:14<00:02, 1145.41 examples/s]Tokenizing train dataset:  72%|███████▏  | 5287/7336 [00:14<00:01, 1174.22 examples/s]Tokenizing train dataset:  70%|███████   | 5140/7336 [00:14<00:01, 1167.88 examples/s]Tokenizing train dataset:  70%|███████   | 5154/7336 [00:14<00:01, 1173.60 examples/s]Tokenizing train dataset:  75%|███████▍  | 5468/7336 [00:14<00:01, 1182.64 examples/s]Tokenizing train dataset:  72%|███████▏  | 5260/7336 [00:14<00:01, 1174.34 examples/s]Tokenizing train dataset:  72%|███████▏  | 5275/7336 [00:14<00:01, 1179.61 examples/s]Tokenizing train dataset:  77%|███████▋  | 5647/7336 [00:14<00:01, 1184.74 examples/s]Tokenizing train dataset:  74%|███████▍  | 5440/7336 [00:14<00:01, 1175.57 examples/s]Tokenizing train dataset:  74%|███████▍  | 5456/7336 [00:14<00:01, 1185.83 examples/s]Tokenizing train dataset:  79%|███████▊  | 5767/7336 [00:14<00:01, 1186.49 examples/s]Tokenizing train dataset:  76%|███████▌  | 5562/7336 [00:15<00:01, 1184.17 examples/s]Tokenizing train dataset:  76%|███████▌  | 5577/7336 [00:15<00:01, 1189.89 examples/s]Tokenizing train dataset:  80%|████████  | 5890/7336 [00:15<00:01, 1195.39 examples/s]Tokenizing train dataset:  77%|███████▋  | 5682/7336 [00:15<00:01, 1186.83 examples/s]Tokenizing train dataset:  78%|███████▊  | 5702/7336 [00:15<00:01, 1198.68 examples/s]Tokenizing train dataset:  82%|████████▏ | 6018/7336 [00:15<00:01, 1215.02 examples/s]Tokenizing train dataset:  79%|███████▉  | 5805/7336 [00:15<00:01, 1196.67 examples/s]Tokenizing train dataset:  79%|███████▉  | 5826/7336 [00:15<00:01, 1206.44 examples/s]Tokenizing train dataset:  84%|████████▍ | 6145/7336 [00:15<00:00, 1222.23 examples/s]Tokenizing train dataset:  81%|████████  | 5927/7336 [00:15<00:01, 1200.96 examples/s]Tokenizing train dataset:  81%|████████  | 5949/7336 [00:15<00:01, 1210.85 examples/s]Tokenizing train dataset:  85%|████████▌ | 6269/7336 [00:15<00:00, 1225.19 examples/s]Tokenizing train dataset:  83%|████████▎ | 6057/7336 [00:15<00:01, 1227.13 examples/s]Tokenizing train dataset:  83%|████████▎ | 6077/7336 [00:15<00:01, 1229.04 examples/s]Tokenizing train dataset:  85%|████████▍ | 6204/7336 [00:15<00:00, 1236.81 examples/s]Tokenizing train dataset:  88%|████████▊ | 6446/7336 [00:15<00:00, 1202.91 examples/s]Tokenizing train dataset:  85%|████████▌ | 6240/7336 [00:15<00:00, 1221.51 examples/s]Tokenizing train dataset:  86%|████████▋ | 6328/7336 [00:15<00:00, 1235.46 examples/s]Tokenizing train dataset:  90%|████████▉ | 6574/7336 [00:15<00:00, 1219.62 examples/s]Tokenizing train dataset:  87%|████████▋ | 6363/7336 [00:15<00:00, 1221.79 examples/s]Tokenizing train dataset:  91%|█████████▏| 6698/7336 [00:15<00:00, 1220.39 examples/s]Tokenizing train dataset:  89%|████████▊ | 6509/7336 [00:15<00:00, 1219.62 examples/s]Tokenizing train dataset:  89%|████████▉ | 6543/7336 [00:15<00:00, 1206.65 examples/s]Tokenizing train dataset:  90%|█████████ | 6636/7336 [00:15<00:00, 1228.37 examples/s]Tokenizing train dataset:  94%|█████████▍| 6880/7336 [00:15<00:00, 1211.10 examples/s]Tokenizing train dataset:  91%|█████████ | 6670/7336 [00:15<00:00, 1220.87 examples/s]Tokenizing train dataset:  93%|█████████▎| 6820/7336 [00:16<00:00, 1221.33 examples/s]Tokenizing train dataset:  96%|█████████▌| 7060/7336 [00:16<00:00, 1206.33 examples/s]Tokenizing train dataset:  93%|█████████▎| 6854/7336 [00:16<00:00, 1218.55 examples/s]Tokenizing train dataset:  95%|█████████▍| 6945/7336 [00:16<00:00, 1225.74 examples/s]Tokenizing train dataset:  98%|█████████▊| 7182/7336 [00:16<00:00, 1205.82 examples/s]Tokenizing train dataset:  96%|█████████▌| 7033/7336 [00:16<00:00, 1209.51 examples/s]Tokenizing train dataset: 100%|█████████▉| 7304/7336 [00:16<00:00, 1205.41 examples/s]Tokenizing train dataset:  97%|█████████▋| 7123/7336 [00:16<00:00, 1210.14 examples/s]Tokenizing train dataset: 100%|██████████| 7336/7336 [00:16<00:00, 450.57 examples/s] 
Tokenizing train dataset:  98%|█████████▊| 7214/7336 [00:16<00:00, 1205.38 examples/s]Tokenizing train dataset:  99%|█████████▉| 7245/7336 [00:16<00:00, 1209.21 examples/s]Tokenizing train dataset: 100%|██████████| 7336/7336 [00:16<00:00, 1200.07 examples/s]Tokenizing train dataset: 100%|██████████| 7336/7336 [00:16<00:00, 445.09 examples/s] 
Tokenizing train dataset: 100%|██████████| 7336/7336 [00:16<00:00, 445.79 examples/s] 
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  60%|██████    | 574/953 [00:00<00:00, 5673.47 examples/s]Extracting prompt in eval dataset:  61%|██████    | 579/953 [00:00<00:00, 5727.76 examples/s]Extracting prompt in eval dataset:  60%|██████    | 574/953 [00:00<00:00, 5649.12 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5665.38 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5645.71 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5619.14 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  28%|██▊       | 270/953 [00:00<00:00, 2675.34 examples/s]Applying chat template to eval dataset:  29%|██▉       | 280/953 [00:00<00:00, 2739.25 examples/s]Applying chat template to eval dataset:  30%|███       | 290/953 [00:00<00:00, 2858.87 examples/s]Applying chat template to eval dataset:  60%|██████    | 572/953 [00:00<00:00, 2839.87 examples/s]Applying chat template to eval dataset:  58%|█████▊    | 550/953 [00:00<00:00, 2737.13 examples/s]Applying chat template to eval dataset:  62%|██████▏   | 591/953 [00:00<00:00, 2937.49 examples/s]Applying chat template to eval dataset:  90%|█████████ | 860/953 [00:00<00:00, 2852.78 examples/s]Applying chat template to eval dataset:  87%|████████▋ | 829/953 [00:00<00:00, 2754.20 examples/s]Applying chat template to eval dataset:  93%|█████████▎| 886/953 [00:00<00:00, 2939.05 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2825.50 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2728.46 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2907.62 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 283.93 examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 282.34 examples/s]Tokenizing eval dataset:   3%|▎         | 29/953 [00:00<00:03, 280.64 examples/s]Tokenizing eval dataset:   7%|▋         | 66/953 [00:00<00:03, 247.70 examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 244.94 examples/s]Tokenizing eval dataset:   7%|▋         | 64/953 [00:00<00:03, 242.93 examples/s]Tokenizing eval dataset:  10%|▉         | 92/953 [00:00<00:03, 245.17 examples/s]Tokenizing eval dataset:  10%|▉         | 93/953 [00:00<00:03, 244.87 examples/s]Tokenizing eval dataset:   9%|▉         | 90/953 [00:00<00:03, 244.08 examples/s]Tokenizing eval dataset:  13%|█▎        | 124/953 [00:00<00:03, 224.92 examples/s]Tokenizing eval dataset:  13%|█▎        | 124/953 [00:00<00:03, 223.43 examples/s]Tokenizing eval dataset:  13%|█▎        | 123/953 [00:00<00:03, 225.85 examples/s]Tokenizing eval dataset:  15%|█▌        | 147/953 [00:00<00:03, 222.37 examples/s]Tokenizing eval dataset:  15%|█▌        | 147/953 [00:00<00:03, 221.75 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 221.22 examples/s]Tokenizing eval dataset:  18%|█▊        | 176/953 [00:00<00:03, 208.71 examples/s]Tokenizing eval dataset:  18%|█▊        | 176/953 [00:00<00:03, 210.19 examples/s]Tokenizing eval dataset:  20%|█▉        | 188/953 [00:00<00:03, 210.15 examples/s]Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:03, 210.73 examples/s]Tokenizing eval dataset:  21%|██        | 199/953 [00:00<00:03, 212.83 examples/s]Tokenizing eval dataset:  22%|██▏       | 213/953 [00:00<00:03, 217.04 examples/s]Tokenizing eval dataset:  24%|██▍       | 232/953 [00:00<00:02, 242.19 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:03, 239.84 examples/s]Tokenizing eval dataset:  28%|██▊       | 268/953 [00:01<00:02, 298.63 examples/s]Tokenizing eval dataset:  31%|███       | 291/953 [00:01<00:01, 336.54 examples/s]Tokenizing eval dataset:  31%|███       | 291/953 [00:01<00:01, 337.24 examples/s]Tokenizing eval dataset:  34%|███▍      | 327/953 [00:01<00:01, 375.51 examples/s]Tokenizing eval dataset:  37%|███▋      | 350/953 [00:01<00:01, 406.22 examples/s]Tokenizing eval dataset:  37%|███▋      | 350/953 [00:01<00:01, 406.79 examples/s]Tokenizing eval dataset:  40%|████      | 382/953 [00:01<00:01, 422.08 examples/s]Tokenizing eval dataset:  42%|████▏     | 403/953 [00:01<00:01, 439.17 examples/s]Tokenizing eval dataset:  42%|████▏     | 404/953 [00:01<00:01, 443.62 examples/s]Tokenizing eval dataset:  46%|████▋     | 442/953 [00:01<00:01, 470.68 examples/s]Tokenizing eval dataset:  49%|████▉     | 467/953 [00:01<00:00, 495.41 examples/s]Tokenizing eval dataset:  49%|████▉     | 468/953 [00:01<00:00, 499.47 examples/s]Tokenizing eval dataset:  52%|█████▏    | 500/953 [00:01<00:00, 500.66 examples/s]Tokenizing eval dataset:  55%|█████▍    | 520/953 [00:01<00:00, 502.08 examples/s]Tokenizing eval dataset:  55%|█████▍    | 524/953 [00:01<00:00, 508.30 examples/s]Tokenizing eval dataset:  59%|█████▉    | 560/953 [00:01<00:00, 525.23 examples/s]Tokenizing eval dataset:  61%|██████    | 578/953 [00:01<00:00, 524.14 examples/s]Tokenizing eval dataset:  61%|██████    | 582/953 [00:01<00:00, 526.12 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 533.75 examples/s]Tokenizing eval dataset:  67%|██████▋   | 637/953 [00:01<00:00, 542.15 examples/s]Tokenizing eval dataset:  67%|██████▋   | 640/953 [00:01<00:00, 539.88 examples/s]Tokenizing eval dataset:  73%|███████▎  | 696/953 [00:01<00:00, 531.53 examples/s]Tokenizing eval dataset:  73%|███████▎  | 697/953 [00:01<00:00, 533.42 examples/s]Tokenizing eval dataset:  75%|███████▍  | 714/953 [00:01<00:00, 525.35 examples/s]Tokenizing eval dataset:  81%|████████  | 768/953 [00:01<00:00, 510.94 examples/s]Tokenizing eval dataset:  81%|████████  | 774/953 [00:01<00:00, 517.13 examples/s]Tokenizing eval dataset:  82%|████████▏ | 783/953 [00:02<00:00, 497.08 examples/s]Tokenizing eval dataset:  87%|████████▋ | 830/953 [00:02<00:00, 471.91 examples/s]Tokenizing eval dataset:  88%|████████▊ | 838/953 [00:02<00:00, 484.52 examples/s]Tokenizing eval dataset:  89%|████████▉ | 850/953 [00:02<00:00, 474.55 examples/s]Tokenizing eval dataset:  94%|█████████▍| 896/953 [00:02<00:00, 458.18 examples/s]Tokenizing eval dataset:  95%|█████████▍| 904/953 [00:02<00:00, 463.67 examples/s]Tokenizing eval dataset:  96%|█████████▌| 912/953 [00:02<00:00, 453.11 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 393.54 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 444.06 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 393.65 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 451.98 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 396.67 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.477778196334839 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3548154830932617 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.331465244293213 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.460651397705078 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank12]:[W608 21:47:46.759920041 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 3 ---
