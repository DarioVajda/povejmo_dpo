cpu-bind=MASK - gn02, task  1  0 [2366941]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 1 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 1     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63117581     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=1e-6 --total_epochs=3 --beta=0.1 --curriculum_stage=2
-------------------------------------------
[2025-06-12 20:54:46,333] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 20:54:48.229000 2366996 torch/distributed/run.py:792] 
W0612 20:54:48.229000 2366996 torch/distributed/run.py:792] *****************************************
W0612 20:54:48.229000 2366996 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 20:54:48.229000 2366996 torch/distributed/run.py:792] *****************************************
[2025-06-12 20:55:17,270] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 20:55:17,293] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 20:55:17,294] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 20:55:17,305] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
World size: 12
Setting gradient accumulation steps to: 1
[2025-06-12 20:55:23,170] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 20:55:23,203] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 20:55:23,210] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 6690
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-12 20:55:23,214] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curriculum-1
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:09, 23.25s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:10, 23.43s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:10, 23.43s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:10, 23.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.35s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.24s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.24s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.18s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.25s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 21.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.25s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:28<00:00, 22.25s/it]
Loaded model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank5]:[W612 20:56:56.161143966 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W612 20:56:56.213317582 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s][rank7]:[W612 20:56:56.297074032 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   8%|▊         | 556/6690 [00:00<00:01, 5519.57 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1120/6690 [00:00<00:01, 5568.56 examples/s]Extracting prompt in train dataset:  25%|██▌       | 1692/6690 [00:00<00:00, 5637.26 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2540/6690 [00:00<00:00, 5628.08 examples/s]Extracting prompt in train dataset:  47%|████▋     | 3120/6690 [00:00<00:00, 5656.70 examples/s]Extracting prompt in train dataset:  55%|█████▌    | 3700/6690 [00:00<00:00, 5668.38 examples/s]Extracting prompt in train dataset:  64%|██████▍   | 4280/6690 [00:00<00:00, 5682.49 examples/s]Extracting prompt in train dataset:  76%|███████▋  | 5110/6690 [00:00<00:00, 5603.06 examples/s]Extracting prompt in train dataset:  85%|████████▌ | 5690/6690 [00:01<00:00, 5635.31 examples/s]Extracting prompt in train dataset:  94%|█████████▎| 6270/6690 [00:01<00:00, 5657.78 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5577.17 examples/s]
Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 292/6690 [00:00<00:02, 2897.19 examples/s]Applying chat template to train dataset:   9%|▉         | 613/6690 [00:00<00:01, 3074.98 examples/s]Applying chat template to train dataset:  16%|█▌        | 1071/6690 [00:00<00:01, 3054.75 examples/s]Applying chat template to train dataset:  23%|██▎       | 1522/6690 [00:00<00:01, 3025.17 examples/s]Applying chat template to train dataset:  30%|██▉       | 1975/6690 [00:00<00:01, 3018.43 examples/s]Applying chat template to train dataset:  36%|███▋      | 2434/6690 [00:00<00:01, 3030.65 examples/s]Applying chat template to train dataset:  41%|████      | 2755/6690 [00:00<00:01, 3073.58 examples/s]Applying chat template to train dataset:  46%|████▌     | 3077/6690 [00:01<00:01, 3110.31 examples/s]Applying chat template to train dataset:  51%|█████     | 3398/6690 [00:01<00:01, 3135.26 examples/s]Applying chat template to train dataset:  56%|█████▌    | 3719/6690 [00:01<00:00, 3153.47 examples/s]Applying chat template to train dataset:  60%|██████    | 4040/6690 [00:01<00:00, 3166.85 examples/s]Applying chat template to train dataset:  65%|██████▌   | 4361/6690 [00:01<00:00, 3177.20 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4823/6690 [00:01<00:00, 3135.97 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5143/6690 [00:01<00:00, 3150.93 examples/s]Applying chat template to train dataset:  82%|████████▏ | 5464/6690 [00:01<00:00, 3163.52 examples/s]Applying chat template to train dataset:  86%|████████▋ | 5784/6690 [00:01<00:00, 3171.30 examples/s]Applying chat template to train dataset:  91%|█████████ | 6104/6690 [00:01<00:00, 3178.51 examples/s]Applying chat template to train dataset:  96%|█████████▌| 6424/6690 [00:02<00:00, 3181.67 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3117.84 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 40/6690 [00:00<00:17, 382.85 examples/s]Tokenizing train dataset:   1%|▏         | 89/6690 [00:00<00:15, 439.48 examples/s]Tokenizing train dataset:   2%|▏         | 157/6690 [00:00<00:14, 442.89 examples/s]Tokenizing train dataset:   3%|▎         | 204/6690 [00:00<00:14, 451.04 examples/s]Tokenizing train dataset:   4%|▍         | 254/6690 [00:00<00:13, 462.53 examples/s]Tokenizing train dataset:   5%|▍         | 319/6690 [00:00<00:14, 446.44 examples/s]Tokenizing train dataset:   6%|▌         | 370/6690 [00:00<00:13, 461.34 examples/s]Tokenizing train dataset:   7%|▋         | 442/6690 [00:00<00:13, 467.51 examples/s]Tokenizing train dataset:   7%|▋         | 492/6690 [00:01<00:13, 468.95 examples/s]Tokenizing train dataset:   8%|▊         | 541/6690 [00:01<00:12, 473.51 examples/s]Tokenizing train dataset:   9%|▉         | 589/6690 [00:01<00:12, 473.84 examples/s]Tokenizing train dataset:  10%|▉         | 641/6690 [00:01<00:12, 480.13 examples/s]Tokenizing train dataset:  10%|█         | 690/6690 [00:01<00:12, 477.46 examples/s]Tokenizing train dataset:  11%|█         | 743/6690 [00:01<00:12, 489.46 examples/s]Tokenizing train dataset:  12%|█▏        | 793/6690 [00:01<00:12, 488.07 examples/s]Tokenizing train dataset:  13%|█▎        | 866/6690 [00:01<00:12, 477.91 examples/s]Tokenizing train dataset:  14%|█▍        | 927/6690 [00:02<00:12, 449.92 examples/s]Tokenizing train dataset:  15%|█▍        | 974/6690 [00:02<00:12, 451.30 examples/s]Tokenizing train dataset:  15%|█▌        | 1021/6690 [00:02<00:12, 449.52 examples/s]Tokenizing train dataset:  16%|█▌        | 1071/6690 [00:02<00:12, 462.31 examples/s]Tokenizing train dataset:  17%|█▋        | 1136/6690 [00:02<00:12, 448.58 examples/s]Tokenizing train dataset:  18%|█▊        | 1183/6690 [00:02<00:12, 452.18 examples/s]Tokenizing train dataset:  19%|█▊        | 1246/6690 [00:02<00:12, 436.35 examples/s]Tokenizing train dataset:  20%|█▉        | 1319/6690 [00:02<00:11, 447.63 examples/s]Tokenizing train dataset:  20%|██        | 1368/6690 [00:02<00:11, 452.76 examples/s]Tokenizing train dataset:  21%|██▏       | 1430/6690 [00:03<00:12, 434.01 examples/s]Tokenizing train dataset:  22%|██▏       | 1493/6690 [00:03<00:12, 423.93 examples/s]Tokenizing train dataset:  23%|██▎       | 1548/6690 [00:03<00:11, 448.68 examples/s]Tokenizing train dataset:  24%|██▍       | 1610/6690 [00:03<00:11, 432.06 examples/s]Tokenizing train dataset:  25%|██▍       | 1655/6690 [00:03<00:11, 430.67 examples/s]Tokenizing train dataset:  25%|██▌       | 1704/6690 [00:03<00:11, 443.47 examples/s]Tokenizing train dataset:  26%|██▌       | 1753/6690 [00:03<00:10, 453.42 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:04<00:11, 441.09 examples/s]Tokenizing train dataset:  28%|██▊       | 1865/6690 [00:04<00:10, 442.49 examples/s]Tokenizing train dataset:  29%|██▊       | 1910/6690 [00:04<00:10, 442.15 examples/s]Tokenizing train dataset:  29%|██▉       | 1956/6690 [00:04<00:10, 441.51 examples/s]Tokenizing train dataset:  30%|███       | 2013/6690 [00:04<00:11, 416.45 examples/s]Tokenizing train dataset:  31%|███       | 2057/6690 [00:04<00:10, 421.49 examples/s]Tokenizing train dataset:  31%|███▏      | 2105/6690 [00:04<00:10, 431.94 examples/s]Tokenizing train dataset:  32%|███▏      | 2155/6690 [00:04<00:10, 444.61 examples/s]Tokenizing train dataset:  33%|███▎      | 2206/6690 [00:04<00:09, 459.98 examples/s]Tokenizing train dataset:  34%|███▎      | 2253/6690 [00:04<00:09, 457.73 examples/s]Tokenizing train dataset:  35%|███▍      | 2317/6690 [00:05<00:09, 438.83 examples/s]Tokenizing train dataset:  35%|███▌      | 2368/6690 [00:05<00:09, 454.00 examples/s]Tokenizing train dataset:  36%|███▋      | 2435/6690 [00:05<00:09, 444.86 examples/s]Tokenizing train dataset:  37%|███▋      | 2481/6690 [00:05<00:09, 447.65 examples/s]Tokenizing train dataset:  38%|███▊      | 2530/6690 [00:05<00:09, 452.20 examples/s]Tokenizing train dataset:  39%|███▊      | 2584/6690 [00:05<00:08, 471.77 examples/s]Tokenizing train dataset:  40%|███▉      | 2651/6690 [00:05<00:08, 454.37 examples/s]Tokenizing train dataset:  41%|████      | 2715/6690 [00:06<00:09, 439.95 examples/s]Tokenizing train dataset:  41%|████▏     | 2775/6690 [00:06<00:09, 425.21 examples/s]Tokenizing train dataset:  42%|████▏     | 2838/6690 [00:06<00:09, 421.97 examples/s]Tokenizing train dataset:  43%|████▎     | 2882/6690 [00:06<00:08, 425.69 examples/s]Tokenizing train dataset:  44%|████▍     | 2933/6690 [00:06<00:08, 442.53 examples/s]Tokenizing train dataset:  45%|████▍     | 2986/6690 [00:06<00:08, 459.23 examples/s]Tokenizing train dataset:  45%|████▌     | 3035/6690 [00:06<00:07, 462.65 examples/s]Tokenizing train dataset:  46%|████▌     | 3090/6690 [00:06<00:07, 484.93 examples/s]Tokenizing train dataset:  47%|████▋     | 3153/6690 [00:07<00:07, 457.93 examples/s]Tokenizing train dataset:  48%|████▊     | 3200/6690 [00:07<00:07, 457.56 examples/s]Tokenizing train dataset:  49%|████▊     | 3249/6690 [00:07<00:07, 463.08 examples/s]Tokenizing train dataset:  49%|████▉     | 3310/6690 [00:07<00:07, 432.32 examples/s]Tokenizing train dataset:  50%|█████     | 3373/6690 [00:07<00:07, 426.20 examples/s]Tokenizing train dataset:  51%|█████▏    | 3433/6690 [00:07<00:07, 415.10 examples/s]Tokenizing train dataset:  52%|█████▏    | 3498/6690 [00:07<00:07, 415.71 examples/s]Tokenizing train dataset:  53%|█████▎    | 3559/6690 [00:07<00:07, 407.56 examples/s]Tokenizing train dataset:  54%|█████▍    | 3614/6690 [00:08<00:07, 393.04 examples/s]Tokenizing train dataset:  55%|█████▍    | 3663/6690 [00:08<00:07, 410.96 examples/s]Tokenizing train dataset:  56%|█████▌    | 3720/6690 [00:08<00:07, 392.83 examples/s]Tokenizing train dataset:  56%|█████▋    | 3766/6690 [00:08<00:07, 404.54 examples/s]Tokenizing train dataset:  57%|█████▋    | 3810/6690 [00:08<00:07, 366.66 examples/s]Tokenizing train dataset:  58%|█████▊    | 3856/6690 [00:08<00:07, 385.46 examples/s]Tokenizing train dataset:  58%|█████▊    | 3907/6690 [00:08<00:06, 415.35 examples/s]Tokenizing train dataset:  59%|█████▉    | 3954/6690 [00:08<00:06, 425.49 examples/s]Tokenizing train dataset:  60%|█████▉    | 4003/6690 [00:09<00:06, 442.00 examples/s]Tokenizing train dataset:  61%|██████    | 4067/6690 [00:09<00:06, 433.26 examples/s]Tokenizing train dataset:  62%|██████▏   | 4135/6690 [00:09<00:05, 435.56 examples/s]Tokenizing train dataset:  63%|██████▎   | 4182/6690 [00:09<00:05, 443.63 examples/s]Tokenizing train dataset:  63%|██████▎   | 4227/6690 [00:09<00:05, 440.65 examples/s]Tokenizing train dataset:  64%|██████▍   | 4293/6690 [00:09<00:05, 437.04 examples/s]Tokenizing train dataset:  65%|██████▍   | 4341/6690 [00:09<00:05, 446.04 examples/s]Tokenizing train dataset:  66%|██████▌   | 4400/6690 [00:09<00:05, 422.37 examples/s]Tokenizing train dataset:  66%|██████▋   | 4448/6690 [00:10<00:05, 436.01 examples/s]Tokenizing train dataset:  67%|██████▋   | 4510/6690 [00:10<00:05, 422.52 examples/s]Tokenizing train dataset:  68%|██████▊   | 4557/6690 [00:10<00:04, 431.26 examples/s]Tokenizing train dataset:  69%|██████▉   | 4602/6690 [00:10<00:04, 434.34 examples/s]Tokenizing train dataset:  70%|██████▉   | 4653/6690 [00:10<00:04, 448.87 examples/s]Tokenizing train dataset:  70%|███████   | 4699/6690 [00:10<00:04, 448.35 examples/s]Tokenizing train dataset:  71%|███████   | 4747/6690 [00:10<00:04, 454.22 examples/s]Tokenizing train dataset:  72%|███████▏  | 4808/6690 [00:10<00:04, 429.34 examples/s]Tokenizing train dataset:  73%|███████▎  | 4855/6690 [00:11<00:04, 434.31 examples/s]Tokenizing train dataset:  73%|███████▎  | 4916/6690 [00:11<00:04, 423.89 examples/s]Tokenizing train dataset:  74%|███████▍  | 4963/6690 [00:11<00:04, 429.74 examples/s]Tokenizing train dataset:  75%|███████▌  | 5026/6690 [00:11<00:03, 423.46 examples/s]Tokenizing train dataset:  76%|███████▌  | 5069/6690 [00:11<00:03, 422.27 examples/s]Tokenizing train dataset:  77%|███████▋  | 5130/6690 [00:11<00:03, 411.72 examples/s]Tokenizing train dataset:  77%|███████▋  | 5180/6690 [00:11<00:03, 431.77 examples/s]Tokenizing train dataset:  78%|███████▊  | 5245/6690 [00:11<00:03, 429.06 examples/s]Tokenizing train dataset:  79%|███████▉  | 5289/6690 [00:12<00:03, 429.90 examples/s]Tokenizing train dataset:  80%|████████  | 5355/6690 [00:12<00:03, 426.60 examples/s]Tokenizing train dataset:  81%|████████  | 5407/6690 [00:12<00:02, 447.44 examples/s]Tokenizing train dataset:  82%|████████▏ | 5469/6690 [00:12<00:02, 433.96 examples/s]Tokenizing train dataset:  82%|████████▏ | 5514/6690 [00:12<00:02, 436.13 examples/s]Tokenizing train dataset:  83%|████████▎ | 5577/6690 [00:12<00:02, 428.35 examples/s]Tokenizing train dataset:  84%|████████▍ | 5642/6690 [00:12<00:02, 426.70 examples/s]Tokenizing train dataset:  85%|████████▌ | 5707/6690 [00:13<00:02, 424.84 examples/s]Tokenizing train dataset:  86%|████████▌ | 5757/6690 [00:13<00:02, 438.57 examples/s]Tokenizing train dataset:  87%|████████▋ | 5821/6690 [00:13<00:02, 430.70 examples/s]Tokenizing train dataset:  88%|████████▊ | 5881/6690 [00:13<00:01, 416.21 examples/s]Tokenizing train dataset:  89%|████████▊ | 5932/6690 [00:13<00:01, 434.70 examples/s]Tokenizing train dataset:  90%|████████▉ | 6001/6690 [00:13<00:01, 434.98 examples/s]Tokenizing train dataset:  90%|█████████ | 6046/6690 [00:13<00:01, 432.57 examples/s]Tokenizing train dataset:  91%|█████████ | 6098/6690 [00:13<00:01, 453.07 examples/s]Tokenizing train dataset:  92%|█████████▏| 6148/6690 [00:14<00:01, 462.51 examples/s]Tokenizing train dataset:  93%|█████████▎| 6211/6690 [00:14<00:01, 442.02 examples/s]Tokenizing train dataset:  94%|█████████▎| 6262/6690 [00:14<00:01, 402.38 examples/s]Tokenizing train dataset:  94%|█████████▍| 6311/6690 [00:14<00:00, 418.24 examples/s]Tokenizing train dataset:  95%|█████████▌| 6370/6690 [00:14<00:00, 404.09 examples/s]Tokenizing train dataset:  96%|█████████▌| 6418/6690 [00:14<00:00, 419.45 examples/s]Tokenizing train dataset:  97%|█████████▋| 6461/6690 [00:14<00:00, 417.43 examples/s]Tokenizing train dataset:  97%|█████████▋| 6507/6690 [00:14<00:00, 426.51 examples/s]Tokenizing train dataset:  98%|█████████▊| 6570/6690 [00:15<00:00, 417.61 examples/s]Tokenizing train dataset:  99%|█████████▉| 6631/6690 [00:15<00:00, 411.48 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 416.51 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 436.11 examples/s]
[rank4]:[W612 20:57:16.894551453 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▊         | 576/6690 [00:00<00:01, 5718.22 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5648.33 examples/s]Extracting prompt in eval dataset:  61%|██████    | 580/953 [00:00<00:00, 5707.86 examples/s]Extracting prompt in train dataset:   9%|▉         | 590/6690 [00:00<00:01, 5738.40 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5653.00 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1140/6690 [00:00<00:00, 5668.03 examples/s]Extracting prompt in train dataset:  18%|█▊        | 1173/6690 [00:00<00:00, 5789.64 examples/s]Extracting prompt in train dataset:  21%|██▏       | 1432/6690 [00:00<00:00, 5705.39 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1717/6690 [00:00<00:00, 5713.12 examples/s]Extracting prompt in train dataset:  26%|██▋       | 1770/6690 [00:00<00:00, 5824.56 examples/s]Extracting prompt in train dataset:  30%|███       | 2020/6690 [00:00<00:00, 5751.90 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2560/6690 [00:00<00:00, 5665.25 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2600/6690 [00:00<00:00, 5731.27 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2640/6690 [00:00<00:00, 5796.60 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  47%|████▋     | 3140/6690 [00:00<00:00, 5685.35 examples/s]Extracting prompt in train dataset:  48%|████▊     | 3189/6690 [00:00<00:00, 5782.55 examples/s]Extracting prompt in train dataset:  48%|████▊     | 3229/6690 [00:00<00:00, 5823.39 examples/s]Applying chat template to eval dataset:  32%|███▏      | 309/953 [00:00<00:00, 3056.16 examples/s]Extracting prompt in train dataset:  56%|█████▋    | 3769/6690 [00:00<00:00, 5787.97 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3720/6690 [00:00<00:00, 5698.92 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 3814/6690 [00:00<00:00, 5814.63 examples/s]Applying chat template to eval dataset:  66%|██████▌   | 631/953 [00:00<00:00, 3129.40 examples/s]Extracting prompt in train dataset:  64%|██████▍   | 4300/6690 [00:00<00:00, 5717.20 examples/s]Extracting prompt in train dataset:  65%|██████▌   | 4356/6690 [00:00<00:00, 5799.30 examples/s]Extracting prompt in train dataset:  70%|██████▉   | 4677/6690 [00:00<00:00, 5746.75 examples/s]Applying chat template to eval dataset: 100%|█████████▉| 950/953 [00:00<00:00, 3153.99 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3122.23 examples/s]
Extracting prompt in train dataset:  77%|███████▋  | 5130/6690 [00:00<00:00, 5637.85 examples/s]Extracting prompt in train dataset:  78%|███████▊  | 5200/6690 [00:00<00:00, 5708.82 examples/s]Extracting prompt in train dataset:  79%|███████▊  | 5265/6690 [00:00<00:00, 5766.27 examples/s]Extracting prompt in train dataset:  86%|████████▋ | 5786/6690 [00:01<00:00, 5749.95 examples/s]Extracting prompt in train dataset:  85%|████████▌ | 5710/6690 [00:01<00:00, 5664.98 examples/s]Extracting prompt in train dataset:  87%|████████▋ | 5850/6690 [00:01<00:00, 5779.35 examples/s]Extracting prompt in train dataset:  95%|█████████▌| 6370/6690 [00:01<00:00, 5761.38 examples/s]Extracting prompt in train dataset:  94%|█████████▍| 6300/6690 [00:01<00:00, 5704.69 examples/s]Extracting prompt in train dataset:  96%|█████████▋| 6440/6690 [00:01<00:00, 5798.83 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5753.52 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5718.68 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5656.28 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 32/953 [00:00<00:02, 311.86 examples/s]Tokenizing eval dataset:   8%|▊         | 73/953 [00:00<00:03, 274.25 examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing eval dataset:  11%|█         | 103/953 [00:00<00:03, 279.00 examples/s]Applying chat template to train dataset:   4%|▍         | 300/6690 [00:00<00:02, 2967.39 examples/s]Applying chat template to train dataset:   4%|▍         | 300/6690 [00:00<00:02, 2971.96 examples/s]Applying chat template to train dataset:   4%|▍         | 295/6690 [00:00<00:02, 2926.26 examples/s]Tokenizing eval dataset:  15%|█▌        | 144/953 [00:00<00:02, 270.09 examples/s]Applying chat template to train dataset:   9%|▉         | 630/6690 [00:00<00:01, 3148.40 examples/s]Applying chat template to train dataset:   9%|▉         | 630/6690 [00:00<00:01, 3150.36 examples/s]Applying chat template to train dataset:   9%|▉         | 619/6690 [00:00<00:01, 3104.55 examples/s]Applying chat template to train dataset:  14%|█▍        | 960/6690 [00:00<00:01, 3209.32 examples/s]Applying chat template to train dataset:  14%|█▍        | 960/6690 [00:00<00:01, 3208.07 examples/s]Applying chat template to train dataset:  14%|█▍        | 944/6690 [00:00<00:01, 3162.78 examples/s]Tokenizing eval dataset:  19%|█▉        | 180/953 [00:00<00:03, 251.48 examples/s]Applying chat template to train dataset:  19%|█▉        | 1288/6690 [00:00<00:01, 3236.26 examples/s]Applying chat template to train dataset:  19%|█▉        | 1288/6690 [00:00<00:01, 3233.08 examples/s]Applying chat template to train dataset:  19%|█▉        | 1266/6690 [00:00<00:01, 3181.16 examples/s]Tokenizing eval dataset:  22%|██▏       | 209/953 [00:00<00:02, 256.90 examples/s]Applying chat template to train dataset:  24%|██▍       | 1617/6690 [00:00<00:01, 3250.33 examples/s]Applying chat template to train dataset:  24%|██▍       | 1616/6690 [00:00<00:01, 3245.27 examples/s]Applying chat template to train dataset:  24%|██▍       | 1590/6690 [00:00<00:01, 3193.66 examples/s]Tokenizing eval dataset:  27%|██▋       | 261/953 [00:00<00:02, 326.29 examples/s]Applying chat template to train dataset:  29%|██▉       | 1946/6690 [00:00<00:01, 3261.78 examples/s]Applying chat template to train dataset:  29%|██▉       | 1945/6690 [00:00<00:01, 3257.77 examples/s]Applying chat template to train dataset:  29%|██▊       | 1915/6690 [00:00<00:01, 3209.15 examples/s]Tokenizing eval dataset:  34%|███▍      | 327/953 [00:00<00:01, 416.69 examples/s]Applying chat template to train dataset:  33%|███▎      | 2239/6690 [00:00<00:01, 3217.92 examples/s]Applying chat template to train dataset:  36%|███▋      | 2435/6690 [00:00<00:01, 3258.74 examples/s]Applying chat template to train dataset:  36%|███▋      | 2435/6690 [00:00<00:01, 3254.53 examples/s]Tokenizing eval dataset:  41%|████      | 388/953 [00:01<00:01, 469.28 examples/s]Applying chat template to train dataset:  41%|████▏     | 2765/6690 [00:00<00:01, 3265.11 examples/s]Applying chat template to train dataset:  41%|████▏     | 2765/6690 [00:00<00:01, 3260.82 examples/s]Tokenizing eval dataset:  48%|████▊     | 462/953 [00:01<00:00, 544.05 examples/s]Applying chat template to train dataset:  41%|████      | 2720/6690 [00:00<00:01, 3205.63 examples/s]Applying chat template to train dataset:  46%|████▋     | 3096/6690 [00:00<00:01, 3271.86 examples/s]Applying chat template to train dataset:  46%|████▋     | 3096/6690 [00:00<00:01, 3268.05 examples/s]Tokenizing eval dataset:  55%|█████▍    | 520/953 [00:01<00:00, 550.57 examples/s]Applying chat template to train dataset:  46%|████▌     | 3044/6690 [00:00<00:01, 3213.18 examples/s]Applying chat template to train dataset:  51%|█████     | 3425/6690 [00:01<00:00, 3273.52 examples/s]Applying chat template to train dataset:  51%|█████     | 3424/6690 [00:01<00:00, 3267.72 examples/s]Tokenizing eval dataset:  61%|██████▏   | 586/953 [00:01<00:00, 579.81 examples/s]Applying chat template to train dataset:  50%|█████     | 3369/6690 [00:01<00:01, 3221.35 examples/s]Applying chat template to train dataset:  56%|█████▌    | 3751/6690 [00:01<00:00, 3266.37 examples/s]Tokenizing eval dataset:  68%|██████▊   | 650/953 [00:01<00:00, 592.84 examples/s]Applying chat template to train dataset:  59%|█████▊    | 3918/6690 [00:01<00:00, 3274.92 examples/s]Applying chat template to train dataset:  58%|█████▊    | 3853/6690 [00:01<00:00, 3219.10 examples/s]Applying chat template to train dataset:  61%|██████    | 4080/6690 [00:01<00:00, 3270.16 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4247/6690 [00:01<00:00, 3276.74 examples/s]Tokenizing eval dataset:  77%|███████▋  | 735/953 [00:01<00:00, 575.37 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4179/6690 [00:01<00:00, 3226.66 examples/s]Applying chat template to train dataset:  68%|██████▊   | 4554/6690 [00:01<00:00, 3225.17 examples/s]Applying chat template to train dataset:  71%|███████   | 4720/6690 [00:01<00:00, 3226.71 examples/s]Tokenizing eval dataset:  85%|████████▌ | 811/953 [00:01<00:00, 548.82 examples/s]Applying chat template to train dataset:  69%|██████▉   | 4644/6690 [00:01<00:00, 3178.04 examples/s]Applying chat template to train dataset:  73%|███████▎  | 4881/6690 [00:01<00:00, 3234.81 examples/s]Applying chat template to train dataset:  75%|███████▌  | 5048/6690 [00:01<00:00, 3238.20 examples/s]Applying chat template to train dataset:  74%|███████▍  | 4967/6690 [00:01<00:00, 3189.04 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5210/6690 [00:01<00:00, 3243.88 examples/s]Tokenizing eval dataset:  93%|█████████▎| 888/953 [00:01<00:00, 533.48 examples/s]Applying chat template to train dataset:  80%|████████  | 5375/6690 [00:01<00:00, 3245.26 examples/s]Applying chat template to train dataset:  79%|███████▉  | 5290/6690 [00:01<00:00, 3195.50 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5539/6690 [00:01<00:00, 3254.99 examples/s]Applying chat template to train dataset:  85%|████████▌ | 5702/6690 [00:01<00:00, 3250.09 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 518.77 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 453.36 examples/s]
Applying chat template to train dataset:  84%|████████▍ | 5614/6690 [00:01<00:00, 3204.84 examples/s]Applying chat template to train dataset:  88%|████████▊ | 5866/6690 [00:01<00:00, 3257.44 examples/s]Applying chat template to train dataset:  90%|█████████ | 6030/6690 [00:01<00:00, 3253.25 examples/s]Applying chat template to train dataset:  89%|████████▉ | 5938/6690 [00:01<00:00, 3211.20 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6195/6690 [00:01<00:00, 3260.98 examples/s]Applying chat template to train dataset:  95%|█████████▌| 6358/6690 [00:01<00:00, 3258.83 examples/s]Applying chat template to train dataset:  94%|█████████▎| 6262/6690 [00:01<00:00, 3211.65 examples/s]Applying chat template to train dataset: 100%|█████████▉| 6685/6690 [00:02<00:00, 3260.11 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3241.22 examples/s]
Applying chat template to train dataset: 100%|█████████▉| 6685/6690 [00:02<00:00, 3260.92 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3239.36 examples/s]
Applying chat template to train dataset:  98%|█████████▊| 6585/6690 [00:02<00:00, 3214.94 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3192.40 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:15, 423.70 examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:15, 425.46 examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:15, 422.80 examples/s]Tokenizing train dataset:   1%|▏         | 91/6690 [00:00<00:14, 444.89 examples/s]Tokenizing train dataset:   1%|▏         | 91/6690 [00:00<00:14, 446.51 examples/s]Tokenizing train dataset:   1%|▏         | 91/6690 [00:00<00:14, 442.76 examples/s]Tokenizing train dataset:   2%|▏         | 137/6690 [00:00<00:14, 451.01 examples/s]Tokenizing train dataset:   2%|▏         | 137/6690 [00:00<00:14, 452.31 examples/s]Tokenizing train dataset:   2%|▏         | 137/6690 [00:00<00:14, 449.29 examples/s]Tokenizing train dataset:   3%|▎         | 190/6690 [00:00<00:13, 478.05 examples/s]Tokenizing train dataset:   3%|▎         | 190/6690 [00:00<00:13, 479.16 examples/s]Tokenizing train dataset:   3%|▎         | 190/6690 [00:00<00:13, 475.93 examples/s]Tokenizing train dataset:   4%|▎         | 239/6690 [00:00<00:13, 478.98 examples/s]Tokenizing train dataset:   4%|▎         | 239/6690 [00:00<00:13, 479.49 examples/s]Tokenizing train dataset:   4%|▎         | 239/6690 [00:00<00:13, 476.65 examples/s]Tokenizing train dataset:   5%|▍         | 307/6690 [00:00<00:13, 465.49 examples/s]Tokenizing train dataset:   5%|▍         | 307/6690 [00:00<00:13, 466.06 examples/s]Tokenizing train dataset:   5%|▍         | 307/6690 [00:00<00:13, 463.30 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 467.91 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 468.62 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 465.85 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 481.98 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 482.90 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 480.03 examples/s]Tokenizing train dataset:   7%|▋         | 460/6690 [00:00<00:12, 486.79 examples/s]Tokenizing train dataset:   7%|▋         | 460/6690 [00:00<00:12, 487.84 examples/s]Tokenizing train dataset:   7%|▋         | 460/6690 [00:00<00:12, 484.91 examples/s]Tokenizing train dataset:   8%|▊         | 511/6690 [00:01<00:12, 491.79 examples/s]Tokenizing train dataset:   8%|▊         | 511/6690 [00:01<00:12, 492.96 examples/s]Tokenizing train dataset:   8%|▊         | 511/6690 [00:01<00:12, 490.09 examples/s]Tokenizing train dataset:   8%|▊         | 562/6690 [00:01<00:12, 490.41 examples/s]Tokenizing train dataset:   8%|▊         | 562/6690 [00:01<00:12, 491.35 examples/s]Tokenizing train dataset:   8%|▊         | 561/6690 [00:01<00:12, 490.72 examples/s]Tokenizing train dataset:   9%|▉         | 614/6690 [00:01<00:12, 496.74 examples/s]Tokenizing train dataset:   9%|▉         | 614/6690 [00:01<00:12, 497.46 examples/s]Tokenizing train dataset:  10%|▉         | 640/6690 [00:01<00:12, 500.92 examples/s]Tokenizing train dataset:  10%|█         | 690/6690 [00:01<00:12, 499.08 examples/s]Tokenizing train dataset:  10%|█         | 690/6690 [00:01<00:12, 499.38 examples/s]Tokenizing train dataset:  11%|█         | 745/6690 [00:01<00:11, 508.35 examples/s]Tokenizing train dataset:  11%|█         | 745/6690 [00:01<00:11, 508.96 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 490.80 examples/s]Tokenizing train dataset:  12%|█▏        | 800/6690 [00:01<00:11, 512.28 examples/s]Tokenizing train dataset:  12%|█▏        | 800/6690 [00:01<00:11, 512.95 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 501.76 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 507.68 examples/s]Tokenizing train dataset:  13%|█▎        | 870/6690 [00:01<00:11, 492.41 examples/s]Tokenizing train dataset:  13%|█▎        | 870/6690 [00:01<00:11, 493.32 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:11, 483.95 examples/s]Tokenizing train dataset:  14%|█▍        | 934/6690 [00:01<00:12, 463.39 examples/s]Tokenizing train dataset:  14%|█▍        | 934/6690 [00:01<00:12, 464.09 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:01<00:12, 465.66 examples/s]Tokenizing train dataset:  15%|█▌        | 1009/6690 [00:02<00:12, 468.61 examples/s]Tokenizing train dataset:  15%|█▌        | 1009/6690 [00:02<00:12, 469.48 examples/s]Tokenizing train dataset:  16%|█▌        | 1057/6690 [00:02<00:11, 469.94 examples/s]Tokenizing train dataset:  16%|█▌        | 1058/6690 [00:02<00:11, 471.25 examples/s]Tokenizing train dataset:  15%|█▌        | 1026/6690 [00:02<00:12, 463.27 examples/s]Tokenizing train dataset:  17%|█▋        | 1105/6690 [00:02<00:11, 469.78 examples/s]Tokenizing train dataset:  17%|█▋        | 1106/6690 [00:02<00:11, 471.66 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 471.38 examples/s]Tokenizing train dataset:  18%|█▊        | 1176/6690 [00:02<00:11, 466.51 examples/s]Tokenizing train dataset:  18%|█▊        | 1176/6690 [00:02<00:11, 466.33 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:11, 462.93 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 462.13 examples/s]Tokenizing train dataset:  19%|█▊        | 1242/6690 [00:02<00:12, 452.95 examples/s]Tokenizing train dataset:  19%|█▊        | 1242/6690 [00:02<00:12, 452.74 examples/s]Tokenizing train dataset:  19%|█▉        | 1289/6690 [00:02<00:11, 453.07 examples/s]Tokenizing train dataset:  19%|█▉        | 1289/6690 [00:02<00:11, 453.20 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 455.13 examples/s]Tokenizing train dataset:  20%|██        | 1340/6690 [00:02<00:11, 461.46 examples/s]Tokenizing train dataset:  20%|██        | 1340/6690 [00:02<00:11, 461.81 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 455.30 examples/s]Tokenizing train dataset:  21%|██        | 1387/6690 [00:02<00:11, 459.73 examples/s]Tokenizing train dataset:  21%|██        | 1387/6690 [00:02<00:11, 459.98 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 469.40 examples/s]Tokenizing train dataset:  22%|██▏       | 1451/6690 [00:03<00:11, 443.37 examples/s]Tokenizing train dataset:  22%|██▏       | 1451/6690 [00:03<00:11, 444.47 examples/s]Tokenizing train dataset:  21%|██        | 1421/6690 [00:03<00:11, 440.38 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 439.80 examples/s]Tokenizing train dataset:  23%|██▎       | 1528/6690 [00:03<00:11, 462.61 examples/s]Tokenizing train dataset:  23%|██▎       | 1528/6690 [00:03<00:11, 463.67 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 454.39 examples/s]Tokenizing train dataset:  24%|██▍       | 1596/6690 [00:03<00:11, 453.75 examples/s]Tokenizing train dataset:  24%|██▍       | 1596/6690 [00:03<00:11, 454.81 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 454.67 examples/s]Tokenizing train dataset:  25%|██▍       | 1662/6690 [00:03<00:11, 447.54 examples/s]Tokenizing train dataset:  25%|██▍       | 1662/6690 [00:03<00:11, 448.71 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 448.75 examples/s]Tokenizing train dataset:  26%|██▌       | 1710/6690 [00:03<00:10, 453.30 examples/s]Tokenizing train dataset:  26%|██▌       | 1710/6690 [00:03<00:10, 454.53 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 449.98 examples/s]Tokenizing train dataset:  26%|██▋       | 1760/6690 [00:03<00:10, 461.77 examples/s]Tokenizing train dataset:  26%|██▋       | 1760/6690 [00:03<00:10, 462.89 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 462.60 examples/s]Tokenizing train dataset:  27%|██▋       | 1830/6690 [00:03<00:10, 458.75 examples/s]Tokenizing train dataset:  27%|██▋       | 1830/6690 [00:03<00:10, 459.78 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 452.03 examples/s]Tokenizing train dataset:  28%|██▊       | 1900/6690 [00:04<00:10, 453.67 examples/s]Tokenizing train dataset:  28%|██▊       | 1900/6690 [00:04<00:10, 454.57 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:04<00:10, 453.22 examples/s]Tokenizing train dataset:  29%|██▉       | 1947/6690 [00:04<00:10, 456.28 examples/s]Tokenizing train dataset:  29%|██▉       | 1947/6690 [00:04<00:10, 457.17 examples/s]Tokenizing train dataset:  29%|██▊       | 1912/6690 [00:04<00:10, 451.28 examples/s]Tokenizing train dataset:  29%|██▉       | 1960/6690 [00:04<00:10, 456.16 examples/s]Tokenizing train dataset:  30%|███       | 2008/6690 [00:04<00:10, 434.28 examples/s]Tokenizing train dataset:  30%|███       | 2008/6690 [00:04<00:10, 434.76 examples/s]Tokenizing train dataset:  31%|███       | 2054/6690 [00:04<00:10, 437.53 examples/s]Tokenizing train dataset:  31%|███       | 2054/6690 [00:04<00:10, 438.18 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:10, 432.89 examples/s]Tokenizing train dataset:  31%|███▏      | 2100/6690 [00:04<00:10, 437.14 examples/s]Tokenizing train dataset:  31%|███▏      | 2100/6690 [00:04<00:10, 437.63 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 433.62 examples/s]Tokenizing train dataset:  32%|███▏      | 2152/6690 [00:04<00:09, 455.82 examples/s]Tokenizing train dataset:  32%|███▏      | 2153/6690 [00:04<00:09, 457.56 examples/s]Tokenizing train dataset:  32%|███▏      | 2141/6690 [00:04<00:10, 447.92 examples/s]Tokenizing train dataset:  33%|███▎      | 2204/6690 [00:04<00:09, 471.16 examples/s]Tokenizing train dataset:  33%|███▎      | 2207/6690 [00:04<00:09, 473.29 examples/s]Tokenizing train dataset:  33%|███▎      | 2192/6690 [00:04<00:09, 462.54 examples/s]Tokenizing train dataset:  34%|███▎      | 2253/6690 [00:04<00:09, 470.68 examples/s]Tokenizing train dataset:  34%|███▍      | 2279/6690 [00:04<00:09, 471.75 examples/s]Tokenizing train dataset:  34%|███▎      | 2245/6690 [00:04<00:09, 476.84 examples/s]Tokenizing train dataset:  35%|███▍      | 2318/6690 [00:04<00:09, 453.51 examples/s]Tokenizing train dataset:  35%|███▌      | 2347/6690 [00:05<00:09, 457.76 examples/s]Tokenizing train dataset:  35%|███▍      | 2309/6690 [00:04<00:09, 451.02 examples/s]Tokenizing train dataset:  35%|███▌      | 2370/6690 [00:05<00:09, 463.16 examples/s]Tokenizing train dataset:  36%|███▌      | 2395/6690 [00:05<00:09, 462.92 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:05<00:09, 460.88 examples/s]Tokenizing train dataset:  36%|███▋      | 2440/6690 [00:05<00:09, 452.44 examples/s]Tokenizing train dataset:  37%|███▋      | 2469/6690 [00:05<00:09, 462.38 examples/s]Tokenizing train dataset:  36%|███▋      | 2430/6690 [00:05<00:09, 456.66 examples/s]Tokenizing train dataset:  37%|███▋      | 2492/6690 [00:05<00:09, 465.37 examples/s]Tokenizing train dataset:  38%|███▊      | 2516/6690 [00:05<00:09, 460.78 examples/s]Tokenizing train dataset:  37%|███▋      | 2478/6690 [00:05<00:09, 460.13 examples/s]Tokenizing train dataset:  38%|███▊      | 2545/6690 [00:05<00:08, 481.30 examples/s]Tokenizing train dataset:  38%|███▊      | 2570/6690 [00:05<00:08, 479.56 examples/s]Tokenizing train dataset:  38%|███▊      | 2526/6690 [00:05<00:09, 462.64 examples/s]Tokenizing train dataset:  39%|███▉      | 2596/6690 [00:05<00:08, 487.25 examples/s]Tokenizing train dataset:  39%|███▉      | 2620/6690 [00:05<00:08, 480.29 examples/s]Tokenizing train dataset:  39%|███▊      | 2581/6690 [00:05<00:08, 484.75 examples/s]Tokenizing train dataset:  40%|███▉      | 2660/6690 [00:05<00:08, 457.55 examples/s]Tokenizing train dataset:  40%|████      | 2684/6690 [00:05<00:08, 458.21 examples/s]Tokenizing train dataset:  40%|███▉      | 2649/6690 [00:05<00:08, 467.31 examples/s]Tokenizing train dataset:  41%|████      | 2728/6690 [00:05<00:08, 450.93 examples/s]Tokenizing train dataset:  41%|████      | 2750/6690 [00:05<00:08, 445.21 examples/s]Tokenizing train dataset:  41%|████      | 2714/6690 [00:05<00:08, 451.95 examples/s]Tokenizing train dataset:  42%|████▏     | 2788/6690 [00:06<00:09, 433.45 examples/s]Tokenizing train dataset:  42%|████▏     | 2811/6690 [00:06<00:09, 427.47 examples/s]Tokenizing train dataset:  41%|████▏     | 2776/6690 [00:06<00:08, 435.60 examples/s]Tokenizing train dataset:  42%|████▏     | 2833/6690 [00:06<00:08, 434.62 examples/s]Tokenizing train dataset:  43%|████▎     | 2859/6690 [00:06<00:08, 437.20 examples/s]Tokenizing train dataset:  43%|████▎     | 2878/6690 [00:06<00:08, 437.93 examples/s]Tokenizing train dataset:  42%|████▏     | 2841/6690 [00:06<00:08, 430.51 examples/s]Tokenizing train dataset:  43%|████▎     | 2909/6690 [00:06<00:08, 450.12 examples/s]Tokenizing train dataset:  44%|████▍     | 2930/6690 [00:06<00:08, 452.83 examples/s]Tokenizing train dataset:  44%|████▍     | 2960/6690 [00:06<00:08, 460.12 examples/s]Tokenizing train dataset:  44%|████▎     | 2911/6690 [00:06<00:08, 439.39 examples/s]Tokenizing train dataset:  45%|████▍     | 2981/6690 [00:06<00:07, 466.08 examples/s]Tokenizing train dataset:  45%|████▌     | 3012/6690 [00:06<00:07, 475.11 examples/s]Tokenizing train dataset:  44%|████▍     | 2960/6690 [00:06<00:08, 449.25 examples/s]Tokenizing train dataset:  45%|████▌     | 3033/6690 [00:06<00:07, 474.70 examples/s]Tokenizing train dataset:  46%|████▌     | 3065/6690 [00:06<00:07, 487.17 examples/s]Tokenizing train dataset:  45%|████▌     | 3012/6690 [00:06<00:07, 464.73 examples/s]Tokenizing train dataset:  46%|████▌     | 3090/6690 [00:06<00:07, 498.19 examples/s]Tokenizing train dataset:  47%|████▋     | 3116/6690 [00:06<00:07, 488.74 examples/s]Tokenizing train dataset:  46%|████▌     | 3064/6690 [00:06<00:07, 478.34 examples/s]Tokenizing train dataset:  47%|████▋     | 3157/6690 [00:06<00:07, 474.13 examples/s]Tokenizing train dataset:  47%|████▋     | 3116/6690 [00:06<00:07, 481.85 examples/s]Tokenizing train dataset:  48%|████▊     | 3182/6690 [00:06<00:07, 464.15 examples/s]Tokenizing train dataset:  48%|████▊     | 3229/6690 [00:06<00:07, 472.87 examples/s]Tokenizing train dataset:  48%|████▊     | 3234/6690 [00:06<00:07, 474.52 examples/s]Tokenizing train dataset:  48%|████▊     | 3182/6690 [00:06<00:07, 459.85 examples/s]Tokenizing train dataset:  49%|████▉     | 3278/6690 [00:07<00:07, 470.07 examples/s]Tokenizing train dataset:  48%|████▊     | 3234/6690 [00:06<00:07, 470.45 examples/s]Tokenizing train dataset:  49%|████▉     | 3300/6690 [00:07<00:07, 455.52 examples/s]Tokenizing train dataset:  50%|████▉     | 3335/6690 [00:07<00:07, 436.03 examples/s]Tokenizing train dataset:  49%|████▉     | 3300/6690 [00:07<00:07, 452.48 examples/s]Tokenizing train dataset:  50%|█████     | 3364/6690 [00:07<00:07, 441.19 examples/s]Tokenizing train dataset:  51%|█████     | 3380/6690 [00:07<00:07, 437.28 examples/s]Tokenizing train dataset:  50%|█████     | 3363/6690 [00:07<00:07, 437.08 examples/s]Tokenizing train dataset:  51%|█████     | 3428/6690 [00:07<00:07, 428.32 examples/s]Tokenizing train dataset:  51%|█████▏    | 3441/6690 [00:07<00:07, 422.65 examples/s]Tokenizing train dataset:  51%|█████     | 3408/6690 [00:07<00:07, 432.61 examples/s]Tokenizing train dataset:  52%|█████▏    | 3489/6690 [00:07<00:07, 432.66 examples/s]Tokenizing train dataset:  52%|█████▏    | 3494/6690 [00:07<00:07, 429.65 examples/s]Tokenizing train dataset:  52%|█████▏    | 3465/6690 [00:07<00:07, 411.71 examples/s]Tokenizing train dataset:  53%|█████▎    | 3547/6690 [00:07<00:07, 413.50 examples/s]Tokenizing train dataset:  53%|█████▎    | 3555/6690 [00:07<00:07, 418.29 examples/s]Tokenizing train dataset:  53%|█████▎    | 3514/6690 [00:07<00:07, 424.68 examples/s]Tokenizing train dataset:  54%|█████▍    | 3607/6690 [00:07<00:07, 405.38 examples/s]Tokenizing train dataset:  54%|█████▍    | 3612/6690 [00:07<00:07, 404.50 examples/s]Tokenizing train dataset:  53%|█████▎    | 3572/6690 [00:07<00:07, 407.08 examples/s]Tokenizing train dataset:  55%|█████▍    | 3653/6690 [00:07<00:07, 414.73 examples/s]Tokenizing train dataset:  55%|█████▍    | 3663/6690 [00:07<00:07, 421.86 examples/s]Tokenizing train dataset:  54%|█████▍    | 3640/6690 [00:07<00:07, 417.08 examples/s]Tokenizing train dataset:  55%|█████▌    | 3699/6690 [00:08<00:07, 423.91 examples/s]Tokenizing train dataset:  56%|█████▌    | 3719/6690 [00:08<00:07, 404.73 examples/s]Tokenizing train dataset:  55%|█████▌    | 3685/6690 [00:08<00:07, 420.99 examples/s]Tokenizing train dataset:  56%|█████▌    | 3754/6690 [00:08<00:07, 399.57 examples/s]Tokenizing train dataset:  56%|█████▌    | 3763/6690 [00:08<00:07, 409.84 examples/s]Tokenizing train dataset:  56%|█████▌    | 3743/6690 [00:08<00:07, 402.72 examples/s]Tokenizing train dataset:  57%|█████▋    | 3799/6690 [00:08<00:07, 411.52 examples/s]Tokenizing train dataset:  57%|█████▋    | 3809/6690 [00:08<00:06, 419.09 examples/s]Tokenizing train dataset:  57%|█████▋    | 3790/6690 [00:08<00:06, 415.62 examples/s]Tokenizing train dataset:  58%|█████▊    | 3847/6690 [00:08<00:06, 426.03 examples/s]Tokenizing train dataset:  58%|█████▊    | 3855/6690 [00:08<00:06, 427.46 examples/s]Tokenizing train dataset:  57%|█████▋    | 3835/6690 [00:08<00:06, 421.68 examples/s]Tokenizing train dataset:  58%|█████▊    | 3897/6690 [00:08<00:06, 441.08 examples/s]Tokenizing train dataset:  58%|█████▊    | 3907/6690 [00:08<00:06, 449.82 examples/s]Tokenizing train dataset:  58%|█████▊    | 3883/6690 [00:08<00:06, 434.07 examples/s]Tokenizing train dataset:  59%|█████▉    | 3949/6690 [00:08<00:05, 457.89 examples/s]Tokenizing train dataset:  59%|█████▉    | 3954/6690 [00:08<00:06, 454.78 examples/s]Tokenizing train dataset:  59%|█████▉    | 3931/6690 [00:08<00:06, 443.54 examples/s]Tokenizing train dataset:  60%|█████▉    | 3996/6690 [00:08<00:05, 459.45 examples/s]Tokenizing train dataset:  60%|█████▉    | 4006/6690 [00:08<00:05, 466.09 examples/s]Tokenizing train dataset:  59%|█████▉    | 3980/6690 [00:08<00:05, 454.28 examples/s]Tokenizing train dataset:  61%|██████    | 4064/6690 [00:08<00:05, 453.03 examples/s]Tokenizing train dataset:  61%|██████    | 4072/6690 [00:08<00:05, 451.41 examples/s]Tokenizing train dataset:  60%|██████    | 4030/6690 [00:08<00:05, 463.23 examples/s]Tokenizing train dataset:  62%|██████▏   | 4118/6690 [00:09<00:05, 453.00 examples/s]Tokenizing train dataset:  62%|██████▏   | 4133/6690 [00:09<00:05, 451.40 examples/s]Tokenizing train dataset:  61%|██████    | 4094/6690 [00:09<00:05, 446.82 examples/s]Tokenizing train dataset:  62%|██████▏   | 4168/6690 [00:09<00:05, 463.83 examples/s]Tokenizing train dataset:  63%|██████▎   | 4183/6690 [00:09<00:05, 460.92 examples/s]Tokenizing train dataset:  62%|██████▏   | 4142/6690 [00:09<00:05, 453.35 examples/s]Tokenizing train dataset:  63%|██████▎   | 4215/6690 [00:09<00:05, 461.26 examples/s]Tokenizing train dataset:  63%|██████▎   | 4193/6690 [00:09<00:05, 465.27 examples/s]Tokenizing train dataset:  64%|██████▎   | 4252/6690 [00:09<00:05, 457.85 examples/s]Tokenizing train dataset:  64%|██████▍   | 4282/6690 [00:09<00:05, 451.37 examples/s]Tokenizing train dataset:  64%|██████▎   | 4260/6690 [00:09<00:05, 451.65 examples/s]Tokenizing train dataset:  65%|██████▍   | 4322/6690 [00:09<00:05, 459.16 examples/s]Tokenizing train dataset:  65%|██████▍   | 4333/6690 [00:09<00:05, 463.15 examples/s]Tokenizing train dataset:  64%|██████▍   | 4307/6690 [00:09<00:05, 453.14 examples/s]Tokenizing train dataset:  66%|██████▌   | 4382/6690 [00:09<00:05, 434.94 examples/s]Tokenizing train dataset:  66%|██████▌   | 4394/6690 [00:09<00:05, 433.98 examples/s]Tokenizing train dataset:  65%|██████▌   | 4370/6690 [00:09<00:05, 437.39 examples/s]Tokenizing train dataset:  66%|██████▌   | 4429/6690 [00:09<00:05, 440.47 examples/s]Tokenizing train dataset:  66%|██████▋   | 4443/6690 [00:09<00:05, 444.18 examples/s]Tokenizing train dataset:  66%|██████▌   | 4415/6690 [00:09<00:05, 435.54 examples/s]Tokenizing train dataset:  67%|██████▋   | 4496/6690 [00:09<00:05, 437.90 examples/s]Tokenizing train dataset:  67%|██████▋   | 4508/6690 [00:09<00:05, 435.31 examples/s]Tokenizing train dataset:  67%|██████▋   | 4461/6690 [00:09<00:05, 439.36 examples/s]Tokenizing train dataset:  68%|██████▊   | 4545/6690 [00:09<00:04, 449.10 examples/s]Tokenizing train dataset:  68%|██████▊   | 4558/6690 [00:09<00:04, 448.55 examples/s]Tokenizing train dataset:  68%|██████▊   | 4530/6690 [00:09<00:04, 442.40 examples/s]Tokenizing train dataset:  69%|██████▉   | 4606/6690 [00:10<00:04, 451.42 examples/s]Tokenizing train dataset:  69%|██████▉   | 4619/6690 [00:10<00:04, 459.26 examples/s]Tokenizing train dataset:  68%|██████▊   | 4578/6690 [00:10<00:04, 450.27 examples/s]Tokenizing train dataset:  70%|██████▉   | 4658/6690 [00:10<00:04, 467.64 examples/s]Tokenizing train dataset:  70%|██████▉   | 4668/6690 [00:10<00:04, 465.11 examples/s]Tokenizing train dataset:  69%|██████▉   | 4629/6690 [00:10<00:04, 461.25 examples/s]Tokenizing train dataset:  71%|███████   | 4728/6690 [00:10<00:04, 462.21 examples/s]Tokenizing train dataset:  70%|██████▉   | 4677/6690 [00:10<00:04, 461.61 examples/s]Tokenizing train dataset:  71%|███████   | 4740/6690 [00:10<00:04, 465.65 examples/s]Tokenizing train dataset:  72%|███████▏  | 4788/6690 [00:10<00:04, 438.71 examples/s]Tokenizing train dataset:  71%|███████   | 4748/6690 [00:10<00:04, 462.77 examples/s]Tokenizing train dataset:  72%|███████▏  | 4800/6690 [00:10<00:04, 437.12 examples/s]Tokenizing train dataset:  72%|███████▏  | 4838/6690 [00:10<00:04, 451.25 examples/s]Tokenizing train dataset:  73%|███████▎  | 4851/6690 [00:10<00:04, 447.82 examples/s]Tokenizing train dataset:  72%|███████▏  | 4809/6690 [00:10<00:04, 441.67 examples/s]Tokenizing train dataset:  73%|███████▎  | 4884/6690 [00:10<00:04, 447.44 examples/s]Tokenizing train dataset:  73%|███████▎  | 4855/6690 [00:10<00:04, 444.45 examples/s]Tokenizing train dataset:  73%|███████▎  | 4914/6690 [00:10<00:04, 434.41 examples/s]Tokenizing train dataset:  74%|███████▍  | 4950/6690 [00:10<00:03, 443.08 examples/s]Tokenizing train dataset:  74%|███████▍  | 4962/6690 [00:10<00:03, 443.64 examples/s]Tokenizing train dataset:  74%|███████▎  | 4919/6690 [00:10<00:04, 434.31 examples/s]Tokenizing train dataset:  75%|███████▍  | 5017/6690 [00:11<00:03, 435.28 examples/s]Tokenizing train dataset:  74%|███████▍  | 4965/6690 [00:10<00:03, 439.64 examples/s]Tokenizing train dataset:  75%|███████▌  | 5027/6690 [00:11<00:03, 433.22 examples/s]Tokenizing train dataset:  76%|███████▌  | 5082/6690 [00:11<00:03, 427.63 examples/s]Tokenizing train dataset:  75%|███████▌  | 5029/6690 [00:11<00:03, 432.02 examples/s]Tokenizing train dataset:  76%|███████▌  | 5092/6690 [00:11<00:03, 429.34 examples/s]Tokenizing train dataset:  76%|███████▌  | 5073/6690 [00:11<00:03, 432.97 examples/s]Tokenizing train dataset:  77%|███████▋  | 5148/6690 [00:11<00:03, 427.21 examples/s]Tokenizing train dataset:  77%|███████▋  | 5160/6690 [00:11<00:03, 432.35 examples/s]Tokenizing train dataset:  77%|███████▋  | 5134/6690 [00:11<00:03, 420.38 examples/s]Tokenizing train dataset:  78%|███████▊  | 5200/6690 [00:11<00:03, 441.36 examples/s]Tokenizing train dataset:  78%|███████▊  | 5219/6690 [00:11<00:03, 416.85 examples/s]Tokenizing train dataset:  78%|███████▊  | 5185/6690 [00:11<00:03, 438.40 examples/s]Tokenizing train dataset:  79%|███████▊  | 5266/6690 [00:11<00:03, 438.95 examples/s]Tokenizing train dataset:  79%|███████▊  | 5264/6690 [00:11<00:03, 421.73 examples/s]Tokenizing train dataset:  78%|███████▊  | 5230/6690 [00:11<00:03, 435.49 examples/s]Tokenizing train dataset:  79%|███████▉  | 5307/6690 [00:11<00:03, 421.01 examples/s]Tokenizing train dataset:  79%|███████▉  | 5277/6690 [00:11<00:03, 442.99 examples/s]Tokenizing train dataset:  80%|███████▉  | 5330/6690 [00:11<00:03, 429.99 examples/s]Tokenizing train dataset:  80%|████████  | 5353/6690 [00:11<00:03, 426.10 examples/s]Tokenizing train dataset:  80%|████████  | 5385/6690 [00:11<00:02, 454.06 examples/s]Tokenizing train dataset:  80%|███████▉  | 5339/6690 [00:11<00:03, 427.95 examples/s]Tokenizing train dataset:  81%|████████  | 5406/6690 [00:11<00:02, 451.94 examples/s]Tokenizing train dataset:  81%|████████  | 5394/6690 [00:11<00:02, 458.67 examples/s]Tokenizing train dataset:  82%|████████▏ | 5453/6690 [00:12<00:02, 447.99 examples/s]Tokenizing train dataset:  82%|████████▏ | 5470/6690 [00:12<00:02, 434.12 examples/s]Tokenizing train dataset:  82%|████████▏ | 5461/6690 [00:12<00:02, 449.77 examples/s]Tokenizing train dataset:  83%|████████▎ | 5520/6690 [00:12<00:02, 444.04 examples/s]Tokenizing train dataset:  82%|████████▏ | 5519/6690 [00:12<00:02, 445.46 examples/s]Tokenizing train dataset:  82%|████████▏ | 5507/6690 [00:12<00:02, 449.04 examples/s]Tokenizing train dataset:  83%|████████▎ | 5585/6690 [00:12<00:02, 437.96 examples/s]Tokenizing train dataset:  83%|████████▎ | 5582/6690 [00:12<00:02, 434.62 examples/s]Tokenizing train dataset:  83%|████████▎ | 5571/6690 [00:12<00:02, 436.89 examples/s]Tokenizing train dataset:  84%|████████▍ | 5630/6690 [00:12<00:02, 435.76 examples/s]Tokenizing train dataset:  84%|████████▍ | 5649/6690 [00:12<00:02, 435.91 examples/s]Tokenizing train dataset:  85%|████████▍ | 5679/6690 [00:12<00:02, 441.50 examples/s]Tokenizing train dataset:  84%|████████▍ | 5640/6690 [00:12<00:02, 437.99 examples/s]Tokenizing train dataset:  85%|████████▌ | 5715/6690 [00:12<00:02, 430.91 examples/s]Tokenizing train dataset:  86%|████████▌ | 5750/6690 [00:12<00:02, 449.50 examples/s]Tokenizing train dataset:  85%|████████▌ | 5705/6690 [00:12<00:02, 434.89 examples/s]Tokenizing train dataset:  86%|████████▌ | 5764/6690 [00:12<00:02, 442.44 examples/s]Tokenizing train dataset:  86%|████████▌ | 5757/6690 [00:12<00:02, 448.57 examples/s]Tokenizing train dataset:  87%|████████▋ | 5818/6690 [00:12<00:01, 446.89 examples/s]Tokenizing train dataset:  87%|████████▋ | 5812/6690 [00:12<00:01, 449.24 examples/s]Tokenizing train dataset:  87%|████████▋ | 5821/6690 [00:12<00:01, 440.04 examples/s]Tokenizing train dataset:  88%|████████▊ | 5879/6690 [00:12<00:01, 429.79 examples/s]Tokenizing train dataset:  88%|████████▊ | 5871/6690 [00:13<00:01, 426.29 examples/s]Tokenizing train dataset:  89%|████████▊ | 5932/6690 [00:13<00:01, 445.24 examples/s]Tokenizing train dataset:  89%|████████▊ | 5923/6690 [00:13<00:01, 445.93 examples/s]Tokenizing train dataset:  88%|████████▊ | 5881/6690 [00:13<00:01, 425.11 examples/s]Tokenizing train dataset:  89%|████████▉ | 5970/6690 [00:13<00:01, 447.42 examples/s]Tokenizing train dataset:  89%|████████▊ | 5935/6690 [00:13<00:01, 447.02 examples/s]Tokenizing train dataset:  90%|████████▉ | 6003/6690 [00:13<00:01, 447.96 examples/s]Tokenizing train dataset:  90%|████████▉ | 6016/6690 [00:13<00:01, 449.56 examples/s]Tokenizing train dataset:  90%|████████▉ | 6003/6690 [00:13<00:01, 445.51 examples/s]Tokenizing train dataset:  91%|█████████ | 6075/6690 [00:13<00:01, 451.96 examples/s]Tokenizing train dataset:  91%|█████████ | 6086/6690 [00:13<00:01, 453.69 examples/s]Tokenizing train dataset:  92%|█████████▏| 6129/6690 [00:13<00:01, 471.05 examples/s]Tokenizing train dataset:  91%|█████████ | 6072/6690 [00:13<00:01, 448.71 examples/s]Tokenizing train dataset:  92%|█████████▏| 6140/6690 [00:13<00:01, 471.85 examples/s]Tokenizing train dataset:  92%|█████████▏| 6127/6690 [00:13<00:01, 467.35 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6690 [00:13<00:01, 458.24 examples/s]Tokenizing train dataset:  93%|█████████▎| 6205/6690 [00:13<00:01, 456.41 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6690 [00:13<00:01, 456.63 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:13<00:01, 421.62 examples/s]Tokenizing train dataset:  94%|█████████▎| 6260/6690 [00:13<00:01, 417.73 examples/s]Tokenizing train dataset:  94%|█████████▍| 6300/6690 [00:13<00:00, 436.72 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:13<00:01, 419.60 examples/s]Tokenizing train dataset:  94%|█████████▍| 6312/6690 [00:14<00:00, 434.36 examples/s]Tokenizing train dataset:  94%|█████████▍| 6299/6690 [00:13<00:00, 434.57 examples/s]Tokenizing train dataset:  95%|█████████▌| 6360/6690 [00:14<00:00, 418.64 examples/s]Tokenizing train dataset:  95%|█████████▌| 6371/6690 [00:14<00:00, 419.21 examples/s]Tokenizing train dataset:  96%|█████████▌| 6406/6690 [00:14<00:00, 426.81 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 419.46 examples/s]Tokenizing train dataset:  96%|█████████▌| 6420/6690 [00:14<00:00, 433.11 examples/s]Tokenizing train dataset:  96%|█████████▋| 6454/6690 [00:14<00:00, 435.79 examples/s]Tokenizing train dataset:  96%|█████████▌| 6404/6690 [00:14<00:00, 422.08 examples/s]Tokenizing train dataset:  97%|█████████▋| 6465/6690 [00:14<00:00, 434.39 examples/s]Tokenizing train dataset:  97%|█████████▋| 6500/6690 [00:14<00:00, 434.32 examples/s]Tokenizing train dataset:  96%|█████████▋| 6452/6690 [00:14<00:00, 433.70 examples/s]Tokenizing train dataset:  97%|█████████▋| 6510/6690 [00:14<00:00, 433.88 examples/s]Tokenizing train dataset:  98%|█████████▊| 6546/6690 [00:14<00:00, 440.20 examples/s]Tokenizing train dataset:  97%|█████████▋| 6497/6690 [00:14<00:00, 436.00 examples/s]Tokenizing train dataset:  98%|█████████▊| 6554/6690 [00:14<00:00, 430.45 examples/s]Tokenizing train dataset:  98%|█████████▊| 6542/6690 [00:14<00:00, 435.25 examples/s]Tokenizing train dataset:  99%|█████████▉| 6608/6690 [00:14<00:00, 427.83 examples/s]Tokenizing train dataset:  99%|█████████▊| 6599/6690 [00:14<00:00, 432.79 examples/s]Tokenizing train dataset:  98%|█████████▊| 6588/6690 [00:14<00:00, 439.97 examples/s]Tokenizing train dataset: 100%|█████████▉| 6670/6690 [00:14<00:00, 419.07 examples/s]Tokenizing train dataset: 100%|█████████▉| 6658/6690 [00:14<00:00, 415.64 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:14<00:00, 450.71 examples/s]
Tokenizing train dataset:  99%|█████████▉| 6648/6690 [00:14<00:00, 419.99 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:14<00:00, 449.14 examples/s]
Tokenizing train dataset: 100%|██████████| 6690/6690 [00:14<00:00, 448.39 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5664.08 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5664.19 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5597.87 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5643.19 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5642.98 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5585.53 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  33%|███▎      | 318/953 [00:00<00:00, 3152.83 examples/s]Applying chat template to eval dataset:  33%|███▎      | 315/953 [00:00<00:00, 3116.45 examples/s]Applying chat template to eval dataset:  33%|███▎      | 314/953 [00:00<00:00, 3104.35 examples/s]Applying chat template to eval dataset:  68%|██████▊   | 649/953 [00:00<00:00, 3237.41 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 642/953 [00:00<00:00, 3199.84 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 639/953 [00:00<00:00, 3187.60 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3226.07 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3193.67 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3178.29 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 322.80 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 323.82 examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 321.92 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 290.99 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 293.35 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 289.21 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:02, 278.90 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:03, 274.96 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 277.28 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 269.49 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 265.95 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 268.11 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 254.75 examples/s]Tokenizing eval dataset:  21%|██        | 196/953 [00:00<00:02, 256.14 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 253.58 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:02, 279.13 examples/s]Tokenizing eval dataset:  25%|██▍       | 235/953 [00:00<00:02, 286.40 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:02, 277.94 examples/s]Tokenizing eval dataset:  31%|███       | 297/953 [00:00<00:01, 378.29 examples/s]Tokenizing eval dataset:  32%|███▏      | 302/953 [00:00<00:01, 386.20 examples/s]Tokenizing eval dataset:  31%|███       | 297/953 [00:00<00:01, 376.81 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 442.40 examples/s]Tokenizing eval dataset:  38%|███▊      | 363/953 [00:01<00:01, 442.52 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 440.50 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 500.54 examples/s]Tokenizing eval dataset:  45%|████▌     | 432/953 [00:01<00:01, 506.93 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 499.00 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 544.04 examples/s]Tokenizing eval dataset:  52%|█████▏    | 500/953 [00:01<00:00, 551.49 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 542.05 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 577.95 examples/s]Tokenizing eval dataset:  59%|█████▉    | 566/953 [00:01<00:00, 574.24 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 576.07 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 599.79 examples/s]Tokenizing eval dataset:  67%|██████▋   | 634/953 [00:01<00:00, 604.10 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 597.24 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 602.61 examples/s]Tokenizing eval dataset:  73%|███████▎  | 696/953 [00:01<00:00, 600.76 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 601.22 examples/s]Tokenizing eval dataset:  82%|████████▏ | 777/953 [00:01<00:00, 578.89 examples/s]Tokenizing eval dataset:  82%|████████▏ | 780/953 [00:01<00:00, 575.88 examples/s]Tokenizing eval dataset:  81%|████████▏ | 776/953 [00:01<00:00, 580.23 examples/s]Tokenizing eval dataset:  89%|████████▉ | 852/953 [00:01<00:00, 541.40 examples/s]Tokenizing eval dataset:  90%|████████▉ | 855/953 [00:01<00:00, 542.31 examples/s]Tokenizing eval dataset:  89%|████████▉ | 850/953 [00:01<00:00, 540.95 examples/s]Tokenizing eval dataset:  97%|█████████▋| 929/953 [00:02<00:00, 526.51 examples/s]Tokenizing eval dataset:  98%|█████████▊| 935/953 [00:02<00:00, 529.83 examples/s]Tokenizing eval dataset:  97%|█████████▋| 924/953 [00:02<00:00, 520.00 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 458.82 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 459.08 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 457.18 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.425407886505127 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.345088243484497 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.357936382293701 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3426663875579834 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
[rank4]:[E612 20:59:56.441582599 ProcessGroupNCCL.cpp:552] [Rank 4] Collective WorkNCCL(SeqNum=3, OpType=ALLGATHER, NumelIn=1, NumelOut=12, Timeout(ms)=1800000) raised the following async exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn01.vega.pri<46094>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f0d6fd6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f0d1e4211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f0d1e42964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f0d1e42b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f0d1e42c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f0d704ee5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f0d725c0ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f0d72652a40 in /lib/x86_64-linux-gnu/libc.so.6)

[rank4]:[E612 20:59:56.445906117 ProcessGroupNCCL.cpp:2168] [PG ID 0 PG GUID 0(default_pg) Rank 4]  failure detected by watchdog at work sequence id: 3 PG status: last enqueued work: 3, last completed work: 2
[rank4]:[E612 20:59:56.445915277 ProcessGroupNCCL.cpp:667] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank4]:[E612 20:59:57.392944536 ProcessGroupNCCL.cpp:681] [Rank 4] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank4]:[E612 20:59:57.392957736 ProcessGroupNCCL.cpp:695] [Rank 4] To avoid data inconsistency, we are taking the entire process down.
[rank4]:[E612 20:59:57.392983896 ProcessGroupNCCL.cpp:1895] [PG ID 0 PG GUID 0(default_pg) Rank 4] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn01.vega.pri<46094>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f0d6fd6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f0d1e4211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f0d1e42964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f0d1e42b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f0d1e42c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f0d704ee5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f0d725c0ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f0d72652a40 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 4] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
socketProgress: Connection closed by remote peer vggn01.vega.pri<46094>
Exception raised from checkForNCCLErrorsInternal at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2363 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f0d6fd6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x7f0d1e4211c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7b (0x7f0d1e42964b in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x650 (0x7f0d1e42b590 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7f0d1e42c6ed in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x7f0d704ee5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x94ac3 (0x7f0d725c0ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #7: <unknown function> + 0x126a40 (0x7f0d72652a40 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1901 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7f0d6fd6c1b6 in /usr/local/lib/python3.10/dist-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe5c6fc (0x7f0d1e0876fc in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x7f0d704ee5c0 in /usr/local/lib/python3.10/dist-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x7f0d725c0ac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126a40 (0x7f0d72652a40 in /lib/x86_64-linux-gnu/libc.so.6)

W0612 20:59:58.016000 2366996 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2367189 closing signal SIGTERM
W0612 20:59:58.017000 2366996 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2367190 closing signal SIGTERM
W0612 20:59:58.018000 2366996 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2367191 closing signal SIGTERM
E0612 20:59:58.800000 2366996 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 0 (pid: 2367188) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 1182, in launch_command
    deepspeed_launcher(args)
  File "/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py", line 861, in deepspeed_launcher
    distrib_run.run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
train_curriculum.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-06-12_20:59:58
  host      : pm5-nod03.vega.pri
  rank      : 4 (local_rank: 0)
  exitcode  : -6 (pid: 2367188)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2367188
========================================================
