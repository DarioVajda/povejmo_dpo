cpu-bind=MASK - gn04, task  2  0 [2108845]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 2 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 2     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63111141     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.1 --curriculum_stage=1
-------------------------------------------
[2025-06-12 18:27:51,363] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 18:27:52.757000 2108897 torch/distributed/run.py:792] 
W0612 18:27:52.757000 2108897 torch/distributed/run.py:792] *****************************************
W0612 18:27:52.757000 2108897 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 18:27:52.757000 2108897 torch/distributed/run.py:792] *****************************************
[2025-06-12 18:27:57,625] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 18:27:57,670] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 18:27:57,687] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 18:27:57,690] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.1, curriculum_stage=1)
4e-07
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.1, curriculum_stage=1)
4e-07
World size: 12
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.1, curriculum_stage=1)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.1, curriculum_stage=1)
4e-07
[2025-06-12 18:28:05,698] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 6689
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-12 18:28:05,705] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 18:28:05,707] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 18:28:05,707] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curriculum-0
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:01, 20.61s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:03, 21.19s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:02, 20.92s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:02, 20.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:44<00:44, 22.20s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:44<00:44, 22.41s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:44<00:44, 22.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:44<00:45, 22.52s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:07<00:22, 22.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:08<00:23, 23.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:08<00:22, 22.97s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:07<00:23, 23.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.55s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.12s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.62s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.55s/it]
Loaded model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank9]:[W612 18:29:35.450504583 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank10]:[W612 18:29:35.474171594 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank11]:[W612 18:29:35.485621622 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6689 [00:00<00:01, 5643.95 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1151/6689 [00:00<00:00, 5720.35 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1730/6689 [00:00<00:00, 5714.81 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2560/6689 [00:00<00:00, 5605.24 examples/s]Extracting prompt in train dataset:  47%|████▋     | 3140/6689 [00:00<00:00, 5637.59 examples/s]Extracting prompt in train dataset:  60%|█████▉    | 3988/6689 [00:00<00:00, 5624.53 examples/s]Extracting prompt in train dataset:  68%|██████▊   | 4557/6689 [00:00<00:00, 5630.54 examples/s]Extracting prompt in train dataset:  77%|███████▋  | 5127/6689 [00:00<00:00, 5638.60 examples/s]Extracting prompt in train dataset:  85%|████████▌ | 5699/6689 [00:01<00:00, 5650.00 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 6521/6689 [00:01<00:00, 5548.34 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5544.12 examples/s]
Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 294/6689 [00:00<00:02, 2907.58 examples/s]Applying chat template to train dataset:   9%|▉         | 612/6689 [00:00<00:01, 3061.65 examples/s]Applying chat template to train dataset:  14%|█▍        | 930/6689 [00:00<00:01, 3107.89 examples/s]Applying chat template to train dataset:  19%|█▊        | 1250/6689 [00:00<00:01, 3130.30 examples/s]Applying chat template to train dataset:  23%|██▎       | 1565/6689 [00:00<00:01, 3133.21 examples/s]Applying chat template to train dataset:  28%|██▊       | 1880/6689 [00:00<00:01, 3131.87 examples/s]Applying chat template to train dataset:  33%|███▎      | 2195/6689 [00:00<00:01, 3135.87 examples/s]Applying chat template to train dataset:  38%|███▊      | 2509/6689 [00:00<00:01, 2603.00 examples/s]Applying chat template to train dataset:  42%|████▏     | 2822/6689 [00:00<00:01, 2745.34 examples/s]Applying chat template to train dataset:  47%|████▋     | 3136/6689 [00:01<00:01, 2853.99 examples/s]Applying chat template to train dataset:  52%|█████▏    | 3450/6689 [00:01<00:01, 2929.94 examples/s]Applying chat template to train dataset:  56%|█████▌    | 3758/6689 [00:01<00:00, 2972.91 examples/s]Applying chat template to train dataset:  61%|██████    | 4062/6689 [00:01<00:00, 2991.34 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4510/6689 [00:01<00:00, 2985.33 examples/s]Applying chat template to train dataset:  74%|███████▍  | 4960/6689 [00:01<00:00, 2980.96 examples/s]Applying chat template to train dataset:  79%|███████▊  | 5260/6689 [00:01<00:00, 2978.60 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5560/6689 [00:01<00:00, 2976.69 examples/s]Applying chat template to train dataset:  88%|████████▊ | 5860/6689 [00:01<00:00, 2976.03 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6160/6689 [00:02<00:00, 2976.97 examples/s]Applying chat template to train dataset:  99%|█████████▊| 6603/6689 [00:02<00:00, 2964.05 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 2961.01 examples/s]
Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   2%|▏         | 101/6689 [00:00<00:06, 997.98 examples/s]Tokenizing train dataset:   3%|▎         | 218/6689 [00:00<00:05, 1091.52 examples/s]Tokenizing train dataset:   5%|▌         | 335/6689 [00:00<00:05, 1103.09 examples/s]Tokenizing train dataset:   7%|▋         | 487/6689 [00:00<00:05, 1053.68 examples/s]Tokenizing train dataset:   9%|▉         | 594/6689 [00:00<00:05, 1050.64 examples/s]Tokenizing train dataset:  11%|█         | 732/6689 [00:00<00:06, 992.03 examples/s] Tokenizing train dataset:  13%|█▎        | 873/6689 [00:00<00:06, 959.96 examples/s]Tokenizing train dataset:  15%|█▍        | 1003/6689 [00:01<00:06, 921.01 examples/s]Tokenizing train dataset:  17%|█▋        | 1131/6689 [00:01<00:06, 885.06 examples/s]Tokenizing train dataset:  19%|█▊        | 1245/6689 [00:01<00:06, 844.22 examples/s]Tokenizing train dataset:  20%|██        | 1355/6689 [00:01<00:06, 807.50 examples/s]Tokenizing train dataset:  22%|██▏       | 1465/6689 [00:01<00:06, 774.53 examples/s]Tokenizing train dataset:  23%|██▎       | 1569/6689 [00:01<00:06, 744.23 examples/s]Tokenizing train dataset:  25%|██▌       | 1674/6689 [00:01<00:06, 727.89 examples/s]Tokenizing train dataset:  27%|██▋       | 1775/6689 [00:02<00:06, 706.58 examples/s]Tokenizing train dataset:  28%|██▊       | 1877/6689 [00:02<00:06, 693.80 examples/s]Tokenizing train dataset:  29%|██▉       | 1951/6689 [00:02<00:06, 698.04 examples/s]Tokenizing train dataset:  31%|███       | 2050/6689 [00:02<00:06, 682.62 examples/s]Tokenizing train dataset:  32%|███▏      | 2123/6689 [00:02<00:06, 689.87 examples/s]Tokenizing train dataset:  33%|███▎      | 2216/6689 [00:02<00:06, 663.88 examples/s]Tokenizing train dataset:  35%|███▍      | 2314/6689 [00:02<00:06, 648.78 examples/s]Tokenizing train dataset:  36%|███▌      | 2382/6689 [00:03<00:06, 649.55 examples/s]Tokenizing train dataset:  37%|███▋      | 2470/6689 [00:03<00:06, 623.02 examples/s]Tokenizing train dataset:  38%|███▊      | 2543/6689 [00:03<00:07, 575.07 examples/s]Tokenizing train dataset:  39%|███▉      | 2611/6689 [00:03<00:06, 595.21 examples/s]Tokenizing train dataset:  40%|████      | 2676/6689 [00:03<00:06, 608.33 examples/s]Tokenizing train dataset:  41%|████      | 2754/6689 [00:03<00:06, 573.68 examples/s]Tokenizing train dataset:  42%|████▏     | 2814/6689 [00:03<00:06, 578.63 examples/s]Tokenizing train dataset:  43%|████▎     | 2905/6689 [00:03<00:06, 582.56 examples/s]Tokenizing train dataset:  45%|████▍     | 2988/6689 [00:04<00:06, 567.89 examples/s]Tokenizing train dataset:  46%|████▌     | 3073/6689 [00:04<00:06, 560.82 examples/s]Tokenizing train dataset:  47%|████▋     | 3132/6689 [00:04<00:06, 560.64 examples/s]Tokenizing train dataset:  48%|████▊     | 3191/6689 [00:04<00:06, 561.92 examples/s]Tokenizing train dataset:  49%|████▉     | 3270/6689 [00:04<00:06, 545.02 examples/s]Tokenizing train dataset:  50%|█████     | 3352/6689 [00:04<00:06, 541.40 examples/s]Tokenizing train dataset:  51%|█████     | 3417/6689 [00:04<00:05, 563.22 examples/s]Tokenizing train dataset:  52%|█████▏    | 3510/6689 [00:05<00:05, 576.97 examples/s]Tokenizing train dataset:  54%|█████▎    | 3583/6689 [00:05<00:05, 541.67 examples/s]Tokenizing train dataset:  55%|█████▍    | 3664/6689 [00:05<00:05, 539.23 examples/s]Tokenizing train dataset:  56%|█████▌    | 3740/6689 [00:05<00:05, 527.24 examples/s]Tokenizing train dataset:  57%|█████▋    | 3821/6689 [00:05<00:05, 528.87 examples/s]Tokenizing train dataset:  58%|█████▊    | 3898/6689 [00:05<00:05, 520.92 examples/s]Tokenizing train dataset:  59%|█████▉    | 3970/6689 [00:05<00:05, 505.08 examples/s]Tokenizing train dataset:  60%|██████    | 4040/6689 [00:06<00:05, 490.39 examples/s]Tokenizing train dataset:  61%|██████▏   | 4100/6689 [00:06<00:05, 509.93 examples/s]Tokenizing train dataset:  62%|██████▏   | 4155/6689 [00:06<00:04, 515.97 examples/s]Tokenizing train dataset:  63%|██████▎   | 4213/6689 [00:06<00:04, 530.35 examples/s]Tokenizing train dataset:  64%|██████▍   | 4267/6689 [00:06<00:04, 531.92 examples/s]Tokenizing train dataset:  65%|██████▍   | 4339/6689 [00:06<00:04, 508.25 examples/s]Tokenizing train dataset:  66%|██████▌   | 4396/6689 [00:06<00:04, 520.25 examples/s]Tokenizing train dataset:  67%|██████▋   | 4453/6689 [00:06<00:04, 529.86 examples/s]Tokenizing train dataset:  68%|██████▊   | 4531/6689 [00:07<00:04, 523.48 examples/s]Tokenizing train dataset:  69%|██████▉   | 4601/6689 [00:07<00:04, 499.74 examples/s]Tokenizing train dataset:  70%|██████▉   | 4677/6689 [00:07<00:04, 498.32 examples/s]Tokenizing train dataset:  71%|███████   | 4734/6689 [00:07<00:03, 510.05 examples/s]Tokenizing train dataset:  72%|███████▏  | 4790/6689 [00:07<00:03, 519.67 examples/s]Tokenizing train dataset:  73%|███████▎  | 4868/6689 [00:07<00:03, 513.39 examples/s]Tokenizing train dataset:  74%|███████▍  | 4938/6689 [00:07<00:03, 493.18 examples/s]Tokenizing train dataset:  75%|███████▍  | 4995/6689 [00:07<00:03, 509.62 examples/s]Tokenizing train dataset:  76%|███████▌  | 5067/6689 [00:08<00:03, 496.92 examples/s]Tokenizing train dataset:  77%|███████▋  | 5145/6689 [00:08<00:03, 499.46 examples/s]Tokenizing train dataset:  78%|███████▊  | 5196/6689 [00:08<00:02, 498.97 examples/s]Tokenizing train dataset:  79%|███████▊  | 5252/6689 [00:08<00:03, 415.41 examples/s]Tokenizing train dataset:  79%|███████▉  | 5306/6689 [00:08<00:03, 440.38 examples/s]Tokenizing train dataset:  80%|████████  | 5378/6689 [00:08<00:02, 452.05 examples/s]Tokenizing train dataset:  81%|████████▏ | 5449/6689 [00:08<00:02, 452.85 examples/s]Tokenizing train dataset:  82%|████████▏ | 5500/6689 [00:09<00:02, 465.14 examples/s]Tokenizing train dataset:  83%|████████▎ | 5551/6689 [00:09<00:02, 472.25 examples/s]Tokenizing train dataset:  84%|████████▍ | 5603/6689 [00:09<00:02, 480.14 examples/s]Tokenizing train dataset:  85%|████████▍ | 5653/6689 [00:09<00:02, 482.94 examples/s]Tokenizing train dataset:  85%|████████▌ | 5705/6689 [00:09<00:02, 488.30 examples/s]Tokenizing train dataset:  86%|████████▌ | 5755/6689 [00:09<00:01, 489.64 examples/s]Tokenizing train dataset:  87%|████████▋ | 5806/6689 [00:09<00:01, 489.66 examples/s]Tokenizing train dataset:  88%|████████▊ | 5875/6689 [00:09<00:01, 473.52 examples/s]Tokenizing train dataset:  89%|████████▉ | 5949/6689 [00:09<00:01, 478.94 examples/s]Tokenizing train dataset:  90%|████████▉ | 6000/6689 [00:10<00:01, 478.77 examples/s]Tokenizing train dataset:  90%|█████████ | 6049/6689 [00:10<00:01, 480.11 examples/s]Tokenizing train dataset:  91%|█████████ | 6100/6689 [00:10<00:01, 481.45 examples/s]Tokenizing train dataset:  92%|█████████▏| 6163/6689 [00:10<00:01, 519.97 examples/s]Tokenizing train dataset:  93%|█████████▎| 6222/6689 [00:10<00:00, 534.44 examples/s]Tokenizing train dataset:  94%|█████████▍| 6278/6689 [00:10<00:00, 540.17 examples/s]Tokenizing train dataset:  95%|█████████▍| 6349/6689 [00:10<00:00, 510.97 examples/s]Tokenizing train dataset:  96%|█████████▌| 6410/6689 [00:10<00:00, 469.19 examples/s]Tokenizing train dataset:  97%|█████████▋| 6460/6689 [00:11<00:00, 468.39 examples/s]Tokenizing train dataset:  97%|█████████▋| 6508/6689 [00:11<00:00, 470.62 examples/s]Tokenizing train dataset:  98%|█████████▊| 6560/6689 [00:11<00:00, 482.87 examples/s]Tokenizing train dataset:  99%|█████████▉| 6612/6689 [00:11<00:00, 490.83 examples/s]Tokenizing train dataset: 100%|█████████▉| 6662/6689 [00:11<00:00, 490.03 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 582.29 examples/s]
[rank8]:[W612 18:29:51.447724180 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▊         | 578/6689 [00:00<00:01, 5736.87 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 567/953 [00:00<00:00, 5630.32 examples/s]Extracting prompt in train dataset:   9%|▊         | 578/6689 [00:00<00:01, 5739.83 examples/s]Extracting prompt in train dataset:   9%|▊         | 580/6689 [00:00<00:01, 5714.51 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5578.82 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1168/6689 [00:00<00:00, 5808.66 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1170/6689 [00:00<00:00, 5789.52 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1170/6689 [00:00<00:00, 5793.41 examples/s]Extracting prompt in train dataset:  26%|██▋       | 1756/6689 [00:00<00:00, 5820.62 examples/s]Extracting prompt in train dataset:  26%|██▋       | 1770/6689 [00:00<00:00, 5858.61 examples/s]Extracting prompt in train dataset:  26%|██▋       | 1770/6689 [00:00<00:00, 5848.88 examples/s]Extracting prompt in train dataset:  35%|███▍      | 2340/6689 [00:00<00:00, 5798.43 examples/s]Extracting prompt in train dataset:  35%|███▌      | 2360/6689 [00:00<00:00, 5851.25 examples/s]Extracting prompt in train dataset:  40%|███▉      | 2650/6689 [00:00<00:00, 5837.41 examples/s]Extracting prompt in train dataset:  44%|████▎     | 2923/6689 [00:00<00:00, 5807.90 examples/s]Extracting prompt in train dataset:  44%|████▍     | 2950/6689 [00:00<00:00, 5850.85 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  48%|████▊     | 3238/6689 [00:00<00:00, 5849.77 examples/s]Extracting prompt in train dataset:  52%|█████▏    | 3508/6689 [00:00<00:00, 5820.07 examples/s]Extracting prompt in train dataset:  53%|█████▎    | 3540/6689 [00:00<00:00, 5850.92 examples/s]Applying chat template to eval dataset:  32%|███▏      | 304/953 [00:00<00:00, 3013.31 examples/s]Extracting prompt in train dataset:  61%|██████▏   | 4099/6689 [00:00<00:00, 5789.08 examples/s]Applying chat template to eval dataset:  65%|██████▌   | 620/953 [00:00<00:00, 3092.25 examples/s]Extracting prompt in train dataset:  65%|██████▌   | 4360/6689 [00:00<00:00, 5741.62 examples/s]Extracting prompt in train dataset:  66%|██████▌   | 4405/6689 [00:00<00:00, 5796.49 examples/s]Applying chat template to eval dataset:  98%|█████████▊| 935/953 [00:00<00:00, 3114.78 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3082.24 examples/s]
Extracting prompt in train dataset:  74%|███████▍  | 4940/6689 [00:00<00:00, 5742.97 examples/s]Extracting prompt in train dataset:  74%|███████▍  | 4962/6689 [00:00<00:00, 5774.29 examples/s]Extracting prompt in train dataset:  79%|███████▉  | 5272/6689 [00:00<00:00, 5788.50 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 5520/6689 [00:00<00:00, 5740.88 examples/s]Extracting prompt in train dataset:  87%|████████▋ | 5830/6689 [00:01<00:00, 5773.76 examples/s]Extracting prompt in train dataset:  92%|█████████▏| 6142/6689 [00:01<00:00, 5789.74 examples/s]Extracting prompt in train dataset:  91%|█████████ | 6100/6689 [00:01<00:00, 5739.61 examples/s]Extracting prompt in train dataset: 100%|█████████▉| 6665/6689 [00:01<00:00, 5700.77 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5734.03 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5637.07 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5719.24 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5693.04 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 324.04 examples/s]Tokenizing eval dataset:   8%|▊         | 77/953 [00:00<00:03, 292.00 examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:02, 279.48 examples/s]Applying chat template to train dataset:   4%|▍         | 301/6689 [00:00<00:02, 2980.63 examples/s]Applying chat template to train dataset:   4%|▍         | 300/6689 [00:00<00:02, 2962.73 examples/s]Applying chat template to train dataset:   5%|▍         | 304/6689 [00:00<00:02, 2998.80 examples/s]Applying chat template to train dataset:   9%|▉         | 630/6689 [00:00<00:01, 3155.04 examples/s]Applying chat template to train dataset:   9%|▉         | 627/6689 [00:00<00:01, 3136.58 examples/s]Applying chat template to train dataset:   9%|▉         | 631/6689 [00:00<00:01, 3152.68 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 270.68 examples/s]Applying chat template to train dataset:  14%|█▍        | 959/6689 [00:00<00:01, 3214.25 examples/s]Applying chat template to train dataset:  14%|█▍        | 953/6689 [00:00<00:01, 3182.45 examples/s]Applying chat template to train dataset:  14%|█▍        | 959/6689 [00:00<00:01, 3206.51 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:02, 256.45 examples/s]Applying chat template to train dataset:  19%|█▉        | 1287/6689 [00:00<00:01, 3236.31 examples/s]Applying chat template to train dataset:  19%|█▉        | 1279/6689 [00:00<00:01, 3208.95 examples/s]Applying chat template to train dataset:  19%|█▉        | 1285/6689 [00:00<00:01, 3223.11 examples/s]Applying chat template to train dataset:  24%|██▍       | 1614/6689 [00:00<00:01, 3240.67 examples/s]Tokenizing eval dataset:  24%|██▍       | 231/953 [00:00<00:02, 280.93 examples/s]Applying chat template to train dataset:  24%|██▍       | 1602/6689 [00:00<00:01, 3209.18 examples/s]Applying chat template to train dataset:  24%|██▍       | 1610/6689 [00:00<00:01, 3224.19 examples/s]Applying chat template to train dataset:  29%|██▉       | 1940/6689 [00:00<00:01, 3243.69 examples/s]Tokenizing eval dataset:  31%|███       | 297/953 [00:00<00:01, 380.00 examples/s]Applying chat template to train dataset:  29%|██▉       | 1927/6689 [00:00<00:01, 3217.75 examples/s]Applying chat template to train dataset:  29%|██▉       | 1935/6689 [00:00<00:01, 3231.00 examples/s]Applying chat template to train dataset:  34%|███▍      | 2266/6689 [00:00<00:01, 3244.05 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 443.88 examples/s]Applying chat template to train dataset:  34%|███▎      | 2250/6689 [00:00<00:01, 3210.98 examples/s]Applying chat template to train dataset:  34%|███▍      | 2259/6689 [00:00<00:01, 3229.65 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 502.64 examples/s]Applying chat template to train dataset:  38%|███▊      | 2572/6689 [00:00<00:01, 3210.46 examples/s]Applying chat template to train dataset:  39%|███▊      | 2582/6689 [00:00<00:01, 3224.77 examples/s]Applying chat template to train dataset:  41%|████      | 2753/6689 [00:00<00:01, 3242.00 examples/s]Applying chat template to train dataset:  43%|████▎     | 2894/6689 [00:00<00:01, 3210.08 examples/s]Applying chat template to train dataset:  43%|████▎     | 2906/6689 [00:00<00:01, 3226.10 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:00, 545.40 examples/s]Applying chat template to train dataset:  46%|████▌     | 3078/6689 [00:00<00:01, 3242.15 examples/s]Applying chat template to train dataset:  48%|████▊     | 3229/6689 [00:01<00:01, 3222.48 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 578.39 examples/s]Applying chat template to train dataset:  50%|█████     | 3376/6689 [00:01<00:01, 3208.42 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3565/6689 [00:01<00:00, 3240.56 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3552/6689 [00:01<00:00, 3219.53 examples/s]Tokenizing eval dataset:  66%|██████▌   | 628/953 [00:01<00:00, 599.13 examples/s]Applying chat template to train dataset:  55%|█████▌    | 3697/6689 [00:01<00:00, 3207.16 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 600.94 examples/s]Applying chat template to train dataset:  60%|██████    | 4033/6689 [00:01<00:00, 3193.02 examples/s]Applying chat template to train dataset:  60%|██████    | 4018/6689 [00:01<00:00, 3165.20 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4150/6689 [00:01<00:00, 3130.73 examples/s]Tokenizing eval dataset:  81%|████████  | 774/953 [00:01<00:00, 572.27 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4504/6689 [00:01<00:00, 3173.01 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4470/6689 [00:01<00:00, 3147.17 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4491/6689 [00:01<00:00, 3156.92 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4828/6689 [00:01<00:00, 3187.49 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4791/6689 [00:01<00:00, 3162.03 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4813/6689 [00:01<00:00, 3170.90 examples/s]Tokenizing eval dataset:  89%|████████▉ | 848/953 [00:01<00:00, 539.73 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5153/6689 [00:01<00:00, 3197.34 examples/s]Applying chat template to train dataset:  76%|███████▋  | 5112/6689 [00:01<00:00, 3172.07 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5136/6689 [00:01<00:00, 3182.56 examples/s]Tokenizing eval dataset:  96%|█████████▋| 918/953 [00:02<00:00, 512.87 examples/s]Applying chat template to train dataset:  82%|████████▏ | 5476/6689 [00:01<00:00, 3205.05 examples/s]Applying chat template to train dataset:  81%|████████  | 5431/6689 [00:01<00:00, 3173.88 examples/s]Applying chat template to train dataset:  82%|████████▏ | 5459/6689 [00:01<00:00, 3192.13 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 457.89 examples/s]
Applying chat template to train dataset:  87%|████████▋ | 5788/6689 [00:01<00:00, 3217.26 examples/s]Applying chat template to train dataset:  89%|████████▉ | 5941/6689 [00:01<00:00, 3164.45 examples/s]Applying chat template to train dataset:  88%|████████▊ | 5901/6689 [00:01<00:00, 3155.06 examples/s]Applying chat template to train dataset:  91%|█████████▏| 6117/6689 [00:01<00:00, 3236.08 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6222/6689 [00:01<00:00, 3166.37 examples/s]Applying chat template to train dataset:  96%|█████████▌| 6396/6689 [00:02<00:00, 3117.70 examples/s]Applying chat template to train dataset:  98%|█████████▊| 6581/6689 [00:02<00:00, 3175.71 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3181.27 examples/s]
Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3188.27 examples/s]
Applying chat template to train dataset: 100%|█████████▉| 6680/6689 [00:02<00:00, 3123.87 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3160.38 examples/s]
Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   2%|▏         | 110/6689 [00:00<00:06, 1079.69 examples/s]Tokenizing train dataset:   2%|▏         | 110/6689 [00:00<00:06, 1079.20 examples/s]Tokenizing train dataset:   2%|▏         | 109/6689 [00:00<00:06, 1082.05 examples/s]Tokenizing train dataset:   3%|▎         | 234/6689 [00:00<00:05, 1159.85 examples/s]Tokenizing train dataset:   3%|▎         | 231/6689 [00:00<00:05, 1152.20 examples/s]Tokenizing train dataset:   3%|▎         | 230/6689 [00:00<00:05, 1145.41 examples/s]Tokenizing train dataset:   5%|▌         | 347/6689 [00:00<00:05, 1145.00 examples/s]Tokenizing train dataset:   5%|▌         | 346/6689 [00:00<00:05, 1142.56 examples/s]Tokenizing train dataset:   6%|▌         | 403/6689 [00:00<00:05, 1137.83 examples/s]Tokenizing train dataset:   8%|▊         | 509/6689 [00:00<00:05, 1108.01 examples/s]Tokenizing train dataset:   8%|▊         | 507/6689 [00:00<00:05, 1101.98 examples/s]Tokenizing train dataset:   8%|▊         | 565/6689 [00:00<00:05, 1107.62 examples/s]Tokenizing train dataset:  10%|▉         | 662/6689 [00:00<00:05, 1068.23 examples/s]Tokenizing train dataset:  10%|▉         | 661/6689 [00:00<00:05, 1066.73 examples/s]Tokenizing train dataset:  11%|█         | 715/6689 [00:00<00:05, 1056.39 examples/s]Tokenizing train dataset:  12%|█▏        | 807/6689 [00:00<00:05, 1025.28 examples/s]Tokenizing train dataset:  12%|█▏        | 803/6689 [00:00<00:05, 1016.48 examples/s]Tokenizing train dataset:  13%|█▎        | 864/6689 [00:00<00:05, 1024.07 examples/s]Tokenizing train dataset:  14%|█▍        | 939/6689 [00:00<00:05, 970.33 examples/s] Tokenizing train dataset:  14%|█▍        | 937/6689 [00:00<00:05, 964.75 examples/s] Tokenizing train dataset:  15%|█▍        | 999/6689 [00:00<00:05, 976.67 examples/s] Tokenizing train dataset:  16%|█▌        | 1071/6689 [00:01<00:06, 935.83 examples/s]Tokenizing train dataset:  16%|█▌        | 1070/6689 [00:01<00:06, 934.55 examples/s]Tokenizing train dataset:  17%|█▋        | 1127/6689 [00:01<00:05, 933.77 examples/s]Tokenizing train dataset:  18%|█▊        | 1192/6689 [00:01<00:06, 888.55 examples/s]Tokenizing train dataset:  18%|█▊        | 1190/6689 [00:01<00:06, 884.89 examples/s]Tokenizing train dataset:  19%|█▊        | 1246/6689 [00:01<00:06, 883.95 examples/s]Tokenizing train dataset:  20%|█▉        | 1312/6689 [00:01<00:06, 859.16 examples/s]Tokenizing train dataset:  20%|█▉        | 1310/6689 [00:01<00:06, 850.38 examples/s]Tokenizing train dataset:  20%|██        | 1364/6689 [00:01<00:06, 844.66 examples/s]Tokenizing train dataset:  21%|██        | 1406/6689 [00:01<00:06, 778.83 examples/s]Tokenizing train dataset:  22%|██▏       | 1472/6689 [00:01<00:06, 802.74 examples/s]Tokenizing train dataset:  21%|██▏       | 1428/6689 [00:01<00:07, 750.29 examples/s]Tokenizing train dataset:  23%|██▎       | 1515/6689 [00:01<00:06, 760.38 examples/s]Tokenizing train dataset:  24%|██▎       | 1579/6689 [00:01<00:06, 771.99 examples/s]Tokenizing train dataset:  23%|██▎       | 1540/6689 [00:01<00:06, 741.27 examples/s]Tokenizing train dataset:  24%|██▍       | 1620/6689 [00:01<00:06, 735.67 examples/s]Tokenizing train dataset:  25%|██▌       | 1690/6689 [00:01<00:06, 757.64 examples/s]Tokenizing train dataset:  25%|██▍       | 1640/6689 [00:01<00:07, 714.45 examples/s]Tokenizing train dataset:  25%|██▌       | 1696/6689 [00:01<00:06, 735.15 examples/s]Tokenizing train dataset:  26%|██▌       | 1716/6689 [00:01<00:06, 723.45 examples/s]Tokenizing train dataset:  27%|██▋       | 1791/6689 [00:02<00:06, 727.48 examples/s]Tokenizing train dataset:  27%|██▋       | 1795/6689 [00:02<00:06, 705.77 examples/s]Tokenizing train dataset:  27%|██▋       | 1815/6689 [00:02<00:06, 700.63 examples/s]Tokenizing train dataset:  28%|██▊       | 1897/6689 [00:02<00:06, 716.19 examples/s]Tokenizing train dataset:  28%|██▊       | 1887/6689 [00:02<00:06, 698.80 examples/s]Tokenizing train dataset:  28%|██▊       | 1900/6689 [00:02<00:06, 700.81 examples/s]Tokenizing train dataset:  30%|██▉       | 2006/6689 [00:02<00:06, 710.32 examples/s]Tokenizing train dataset:  29%|██▉       | 1959/6689 [00:02<00:06, 701.50 examples/s]Tokenizing train dataset:  30%|██▉       | 2005/6689 [00:02<00:06, 697.09 examples/s]Tokenizing train dataset:  32%|███▏      | 2118/6689 [00:02<00:06, 718.16 examples/s]Tokenizing train dataset:  31%|███       | 2062/6689 [00:02<00:06, 694.53 examples/s]Tokenizing train dataset:  32%|███▏      | 2114/6689 [00:02<00:06, 703.75 examples/s]Tokenizing train dataset:  32%|███▏      | 2137/6689 [00:02<00:06, 704.68 examples/s]Tokenizing train dataset:  33%|███▎      | 2213/6689 [00:02<00:06, 691.52 examples/s]Tokenizing train dataset:  33%|███▎      | 2210/6689 [00:02<00:06, 681.15 examples/s]Tokenizing train dataset:  33%|███▎      | 2230/6689 [00:02<00:06, 669.76 examples/s]Tokenizing train dataset:  35%|███▍      | 2310/6689 [00:02<00:06, 672.33 examples/s]Tokenizing train dataset:  34%|███▍      | 2302/6689 [00:02<00:06, 659.74 examples/s]Tokenizing train dataset:  36%|███▌      | 2380/6689 [00:02<00:06, 674.41 examples/s]Tokenizing train dataset:  35%|███▍      | 2332/6689 [00:02<00:06, 669.28 examples/s]Tokenizing train dataset:  35%|███▌      | 2372/6689 [00:02<00:06, 662.38 examples/s]Tokenizing train dataset:  37%|███▋      | 2468/6689 [00:03<00:06, 644.23 examples/s]Tokenizing train dataset:  36%|███▋      | 2429/6689 [00:03<00:06, 657.25 examples/s]Tokenizing train dataset:  37%|███▋      | 2461/6689 [00:03<00:06, 635.27 examples/s]Tokenizing train dataset:  38%|███▊      | 2565/6689 [00:03<00:06, 638.91 examples/s]Tokenizing train dataset:  38%|███▊      | 2514/6689 [00:03<00:06, 626.15 examples/s]Tokenizing train dataset:  38%|███▊      | 2553/6689 [00:03<00:06, 626.64 examples/s]Tokenizing train dataset:  39%|███▉      | 2638/6689 [00:03<00:06, 656.24 examples/s]Tokenizing train dataset:  39%|███▊      | 2583/6689 [00:03<00:06, 636.54 examples/s]Tokenizing train dataset:  39%|███▉      | 2629/6689 [00:03<00:06, 656.06 examples/s]Tokenizing train dataset:  40%|███▉      | 2653/6689 [00:03<00:06, 648.93 examples/s]Tokenizing train dataset:  41%|████      | 2727/6689 [00:03<00:06, 631.29 examples/s]Tokenizing train dataset:  41%|████      | 2716/6689 [00:03<00:06, 625.70 examples/s]Tokenizing train dataset:  41%|████      | 2738/6689 [00:03<00:06, 612.02 examples/s]Tokenizing train dataset:  42%|████▏     | 2813/6689 [00:03<00:06, 610.95 examples/s]Tokenizing train dataset:  42%|████▏     | 2800/6689 [00:03<00:06, 599.55 examples/s]Tokenizing train dataset:  42%|████▏     | 2830/6689 [00:03<00:06, 605.61 examples/s]Tokenizing train dataset:  43%|████▎     | 2905/6689 [00:03<00:06, 609.94 examples/s]Tokenizing train dataset:  43%|████▎     | 2861/6689 [00:03<00:06, 598.21 examples/s]Tokenizing train dataset:  44%|████▎     | 2920/6689 [00:03<00:06, 597.48 examples/s]Tokenizing train dataset:  45%|████▍     | 2990/6689 [00:03<00:06, 591.07 examples/s]Tokenizing train dataset:  44%|████▍     | 2951/6689 [00:03<00:06, 596.10 examples/s]Tokenizing train dataset:  45%|████▍     | 3006/6689 [00:04<00:06, 587.35 examples/s]Tokenizing train dataset:  46%|████▌     | 3078/6689 [00:04<00:06, 583.22 examples/s]Tokenizing train dataset:  45%|████▌     | 3030/6689 [00:04<00:06, 572.30 examples/s]Tokenizing train dataset:  46%|████▌     | 3087/6689 [00:04<00:06, 570.78 examples/s]Tokenizing train dataset:  47%|████▋     | 3164/6689 [00:04<00:06, 578.21 examples/s]Tokenizing train dataset:  47%|████▋     | 3120/6689 [00:04<00:06, 572.75 examples/s]Tokenizing train dataset:  47%|████▋     | 3150/6689 [00:04<00:06, 576.62 examples/s]Tokenizing train dataset:  49%|████▊     | 3249/6689 [00:04<00:06, 573.26 examples/s]Tokenizing train dataset:  48%|████▊     | 3209/6689 [00:04<00:06, 579.35 examples/s]Tokenizing train dataset:  48%|████▊     | 3209/6689 [00:04<00:06, 576.41 examples/s]Tokenizing train dataset:  50%|████▉     | 3327/6689 [00:04<00:06, 551.49 examples/s]Tokenizing train dataset:  49%|████▉     | 3289/6689 [00:04<00:06, 552.80 examples/s]Tokenizing train dataset:  49%|████▉     | 3288/6689 [00:04<00:06, 556.03 examples/s]Tokenizing train dataset:  51%|█████     | 3390/6689 [00:04<00:05, 566.68 examples/s]Tokenizing train dataset:  50%|█████     | 3373/6689 [00:04<00:06, 551.34 examples/s]Tokenizing train dataset:  50%|█████     | 3371/6689 [00:04<00:06, 549.51 examples/s]Tokenizing train dataset:  52%|█████▏    | 3454/6689 [00:04<00:05, 580.76 examples/s]Tokenizing train dataset:  51%|█████▏    | 3438/6689 [00:04<00:05, 573.26 examples/s]Tokenizing train dataset:  51%|█████▏    | 3436/6689 [00:04<00:05, 570.97 examples/s]Tokenizing train dataset:  53%|█████▎    | 3520/6689 [00:04<00:05, 598.04 examples/s]Tokenizing train dataset:  52%|█████▏    | 3498/6689 [00:04<00:05, 578.37 examples/s]Tokenizing train dataset:  52%|█████▏    | 3497/6689 [00:04<00:05, 575.31 examples/s]Tokenizing train dataset:  54%|█████▎    | 3591/6689 [00:05<00:05, 549.23 examples/s]Tokenizing train dataset:  53%|█████▎    | 3576/6689 [00:05<00:05, 555.65 examples/s]Tokenizing train dataset:  53%|█████▎    | 3576/6689 [00:05<00:05, 553.52 examples/s]Tokenizing train dataset:  55%|█████▍    | 3650/6689 [00:05<00:05, 553.01 examples/s]Tokenizing train dataset:  55%|█████▍    | 3659/6689 [00:05<00:05, 554.59 examples/s]Tokenizing train dataset:  55%|█████▍    | 3659/6689 [00:05<00:05, 551.88 examples/s]Tokenizing train dataset:  56%|█████▌    | 3729/6689 [00:05<00:05, 541.62 examples/s]Tokenizing train dataset:  56%|█████▌    | 3736/6689 [00:05<00:05, 536.23 examples/s]Tokenizing train dataset:  56%|█████▌    | 3736/6689 [00:05<00:05, 533.73 examples/s]Tokenizing train dataset:  57%|█████▋    | 3813/6689 [00:05<00:05, 545.42 examples/s]Tokenizing train dataset:  57%|█████▋    | 3790/6689 [00:05<00:05, 530.87 examples/s]Tokenizing train dataset:  57%|█████▋    | 3819/6689 [00:05<00:05, 537.31 examples/s]Tokenizing train dataset:  58%|█████▊    | 3893/6689 [00:05<00:05, 535.39 examples/s]Tokenizing train dataset:  57%|█████▋    | 3846/6689 [00:05<00:05, 534.90 examples/s]Tokenizing train dataset:  58%|█████▊    | 3898/6689 [00:05<00:05, 529.04 examples/s]Tokenizing train dataset:  59%|█████▉    | 3969/6689 [00:05<00:05, 517.09 examples/s]Tokenizing train dataset:  59%|█████▊    | 3925/6689 [00:05<00:05, 525.89 examples/s]Tokenizing train dataset:  59%|█████▉    | 3971/6689 [00:05<00:05, 508.57 examples/s]Tokenizing train dataset:  60%|██████    | 4039/6689 [00:05<00:05, 500.41 examples/s]Tokenizing train dataset:  60%|█████▉    | 3998/6689 [00:05<00:05, 505.51 examples/s]Tokenizing train dataset:  60%|██████    | 4023/6689 [00:05<00:05, 505.54 examples/s]Tokenizing train dataset:  61%|██████▏   | 4100/6689 [00:05<00:04, 520.20 examples/s]Tokenizing train dataset:  61%|██████    | 4076/6689 [00:06<00:05, 507.50 examples/s]Tokenizing train dataset:  61%|██████    | 4072/6689 [00:06<00:05, 499.78 examples/s]Tokenizing train dataset:  62%|██████▏   | 4155/6689 [00:06<00:04, 526.83 examples/s]Tokenizing train dataset:  62%|██████▏   | 4136/6689 [00:06<00:04, 525.99 examples/s]Tokenizing train dataset:  62%|██████▏   | 4132/6689 [00:06<00:04, 519.77 examples/s]Tokenizing train dataset:  63%|██████▎   | 4214/6689 [00:06<00:04, 541.22 examples/s]Tokenizing train dataset:  63%|██████▎   | 4193/6689 [00:06<00:04, 534.70 examples/s]Tokenizing train dataset:  63%|██████▎   | 4187/6689 [00:06<00:04, 523.88 examples/s]Tokenizing train dataset:  64%|██████▍   | 4270/6689 [00:06<00:04, 543.79 examples/s]Tokenizing train dataset:  64%|██████▎   | 4251/6689 [00:06<00:04, 544.29 examples/s]Tokenizing train dataset:  64%|██████▎   | 4248/6689 [00:06<00:04, 539.76 examples/s]Tokenizing train dataset:  65%|██████▍   | 4347/6689 [00:06<00:04, 525.98 examples/s]Tokenizing train dataset:  65%|██████▍   | 4322/6689 [00:06<00:04, 511.00 examples/s]Tokenizing train dataset:  65%|██████▍   | 4319/6689 [00:06<00:04, 513.50 examples/s]Tokenizing train dataset:  66%|██████▌   | 4405/6689 [00:06<00:05, 456.64 examples/s]Tokenizing train dataset:  66%|██████▌   | 4383/6689 [00:06<00:04, 531.65 examples/s]Tokenizing train dataset:  65%|██████▌   | 4377/6689 [00:06<00:04, 524.57 examples/s]Tokenizing train dataset:  67%|██████▋   | 4460/6689 [00:06<00:04, 474.16 examples/s]Tokenizing train dataset:  66%|██████▋   | 4440/6689 [00:06<00:04, 535.30 examples/s]Tokenizing train dataset:  66%|██████▋   | 4433/6689 [00:06<00:04, 528.75 examples/s]Tokenizing train dataset:  68%|██████▊   | 4517/6689 [00:06<00:04, 496.85 examples/s]Tokenizing train dataset:  68%|██████▊   | 4523/6689 [00:06<00:04, 536.89 examples/s]Tokenizing train dataset:  68%|██████▊   | 4516/6689 [00:06<00:04, 531.98 examples/s]Tokenizing train dataset:  69%|██████▊   | 4589/6689 [00:06<00:04, 487.42 examples/s]Tokenizing train dataset:  69%|██████▊   | 4595/6689 [00:07<00:04, 510.45 examples/s]Tokenizing train dataset:  69%|██████▊   | 4587/6689 [00:07<00:04, 507.74 examples/s]Tokenizing train dataset:  70%|██████▉   | 4664/6689 [00:07<00:04, 489.23 examples/s]Tokenizing train dataset:  70%|██████▉   | 4670/6689 [00:07<00:04, 500.81 examples/s]Tokenizing train dataset:  70%|██████▉   | 4662/6689 [00:07<00:04, 499.71 examples/s]Tokenizing train dataset:  71%|███████   | 4721/6689 [00:07<00:03, 505.18 examples/s]Tokenizing train dataset:  71%|███████   | 4728/6689 [00:07<00:03, 518.18 examples/s]Tokenizing train dataset:  71%|███████   | 4720/6689 [00:07<00:03, 515.05 examples/s]Tokenizing train dataset:  71%|███████▏  | 4778/6689 [00:07<00:03, 520.48 examples/s]Tokenizing train dataset:  72%|███████▏  | 4786/6689 [00:07<00:03, 530.98 examples/s]Tokenizing train dataset:  71%|███████▏  | 4776/6689 [00:07<00:03, 521.84 examples/s]Tokenizing train dataset:  73%|███████▎  | 4857/6689 [00:07<00:03, 514.55 examples/s]Tokenizing train dataset:  73%|███████▎  | 4862/6689 [00:07<00:03, 518.88 examples/s]Tokenizing train dataset:  73%|███████▎  | 4851/6689 [00:07<00:03, 513.31 examples/s]Tokenizing train dataset:  74%|███████▎  | 4931/6689 [00:07<00:03, 506.32 examples/s]Tokenizing train dataset:  73%|███████▎  | 4904/6689 [00:07<00:03, 512.15 examples/s]Tokenizing train dataset:  74%|███████▎  | 4933/6689 [00:07<00:03, 501.68 examples/s]Tokenizing train dataset:  75%|███████▍  | 4988/6689 [00:07<00:03, 517.68 examples/s]Tokenizing train dataset:  75%|███████▍  | 4990/6689 [00:07<00:03, 513.47 examples/s]Tokenizing train dataset:  74%|███████▍  | 4982/6689 [00:07<00:03, 510.41 examples/s]Tokenizing train dataset:  76%|███████▌  | 5062/6689 [00:07<00:03, 504.65 examples/s]Tokenizing train dataset:  75%|███████▌  | 5036/6689 [00:07<00:03, 515.15 examples/s]Tokenizing train dataset:  76%|███████▌  | 5065/6689 [00:07<00:03, 503.93 examples/s]Tokenizing train dataset:  77%|███████▋  | 5141/6689 [00:08<00:03, 507.67 examples/s]Tokenizing train dataset:  76%|███████▋  | 5106/6689 [00:08<00:03, 494.61 examples/s]Tokenizing train dataset:  77%|███████▋  | 5142/6689 [00:08<00:03, 505.19 examples/s]Tokenizing train dataset:  78%|███████▊  | 5193/6689 [00:08<00:02, 508.92 examples/s]Tokenizing train dataset:  77%|███████▋  | 5159/6689 [00:08<00:03, 499.83 examples/s]Tokenizing train dataset:  78%|███████▊  | 5194/6689 [00:08<00:02, 507.26 examples/s]Tokenizing train dataset:  79%|███████▊  | 5251/6689 [00:08<00:02, 522.89 examples/s]Tokenizing train dataset:  78%|███████▊  | 5212/6689 [00:08<00:02, 505.84 examples/s]Tokenizing train dataset:  79%|███████▊  | 5251/6689 [00:08<00:02, 518.67 examples/s]Tokenizing train dataset:  79%|███████▉  | 5306/6689 [00:08<00:02, 528.09 examples/s]Tokenizing train dataset:  79%|███████▉  | 5270/6689 [00:08<00:02, 520.98 examples/s]Tokenizing train dataset:  79%|███████▉  | 5306/6689 [00:08<00:02, 524.39 examples/s]Tokenizing train dataset:  80%|████████  | 5380/6689 [00:08<00:02, 510.16 examples/s]Tokenizing train dataset:  80%|███████▉  | 5345/6689 [00:08<00:02, 512.49 examples/s]Tokenizing train dataset:  80%|████████  | 5380/6689 [00:08<00:02, 506.46 examples/s]Tokenizing train dataset:  81%|████████▏ | 5450/6689 [00:08<00:02, 492.52 examples/s]Tokenizing train dataset:  81%|████████  | 5412/6689 [00:08<00:02, 484.67 examples/s]Tokenizing train dataset:  81%|████████▏ | 5450/6689 [00:08<00:02, 488.36 examples/s]Tokenizing train dataset:  82%|████████▏ | 5504/6689 [00:08<00:02, 500.31 examples/s]Tokenizing train dataset:  82%|████████▏ | 5464/6689 [00:08<00:02, 487.43 examples/s]Tokenizing train dataset:  82%|████████▏ | 5503/6689 [00:08<00:02, 496.65 examples/s]Tokenizing train dataset:  83%|████████▎ | 5555/6689 [00:08<00:02, 502.10 examples/s]Tokenizing train dataset:  83%|████████▎ | 5519/6689 [00:08<00:02, 498.49 examples/s]Tokenizing train dataset:  84%|████████▍ | 5609/6689 [00:08<00:02, 509.71 examples/s]Tokenizing train dataset:  83%|████████▎ | 5577/6689 [00:08<00:02, 494.62 examples/s]Tokenizing train dataset:  83%|████████▎ | 5570/6689 [00:08<00:02, 495.58 examples/s]Tokenizing train dataset:  84%|████████▍ | 5635/6689 [00:09<00:02, 511.19 examples/s]Tokenizing train dataset:  84%|████████▍ | 5625/6689 [00:09<00:02, 510.01 examples/s]Tokenizing train dataset:  85%|████████▌ | 5688/6689 [00:09<00:01, 511.43 examples/s]Tokenizing train dataset:  85%|████████▌ | 5710/6689 [00:09<00:01, 501.93 examples/s]Tokenizing train dataset:  85%|████████▌ | 5699/6689 [00:09<00:01, 501.76 examples/s]Tokenizing train dataset:  86%|████████▌ | 5764/6689 [00:09<00:01, 504.81 examples/s]Tokenizing train dataset:  87%|████████▋ | 5787/6689 [00:09<00:01, 504.99 examples/s]Tokenizing train dataset:  86%|████████▋ | 5777/6689 [00:09<00:01, 501.39 examples/s]Tokenizing train dataset:  87%|████████▋ | 5839/6689 [00:09<00:01, 496.68 examples/s]Tokenizing train dataset:  88%|████████▊ | 5859/6689 [00:09<00:01, 486.94 examples/s]Tokenizing train dataset:  87%|████████▋ | 5847/6689 [00:09<00:01, 487.86 examples/s]Tokenizing train dataset:  88%|████████▊ | 5910/6689 [00:09<00:01, 483.65 examples/s]Tokenizing train dataset:  89%|████████▉ | 5969/6689 [00:09<00:01, 501.59 examples/s]Tokenizing train dataset:  89%|████████▊ | 5934/6689 [00:09<00:01, 485.20 examples/s]Tokenizing train dataset:  88%|████████▊ | 5916/6689 [00:09<00:01, 476.08 examples/s]Tokenizing train dataset:  90%|████████▉ | 5988/6689 [00:09<00:01, 494.61 examples/s]Tokenizing train dataset:  89%|████████▉ | 5972/6689 [00:09<00:01, 492.40 examples/s]Tokenizing train dataset:  90%|█████████ | 6041/6689 [00:09<00:01, 487.90 examples/s]Tokenizing train dataset:  91%|█████████ | 6093/6689 [00:09<00:01, 494.83 examples/s]Tokenizing train dataset:  91%|█████████ | 6060/6689 [00:09<00:01, 483.23 examples/s]Tokenizing train dataset:  90%|█████████ | 6045/6689 [00:09<00:01, 481.39 examples/s]Tokenizing train dataset:  92%|█████████▏| 6158/6689 [00:10<00:01, 528.76 examples/s]Tokenizing train dataset:  91%|█████████▏| 6112/6689 [00:10<00:01, 490.02 examples/s]Tokenizing train dataset:  91%|█████████ | 6099/6689 [00:10<00:01, 490.47 examples/s]Tokenizing train dataset:  93%|█████████▎| 6214/6689 [00:10<00:00, 532.29 examples/s]Tokenizing train dataset:  92%|█████████▏| 6177/6689 [00:10<00:00, 528.87 examples/s]Tokenizing train dataset:  92%|█████████▏| 6160/6689 [00:10<00:01, 517.77 examples/s]Tokenizing train dataset:  94%|█████████▍| 6277/6689 [00:10<00:00, 553.91 examples/s]Tokenizing train dataset:  93%|█████████▎| 6235/6689 [00:10<00:00, 540.71 examples/s]Tokenizing train dataset:  93%|█████████▎| 6218/6689 [00:10<00:00, 530.74 examples/s]Tokenizing train dataset:  94%|█████████▍| 6291/6689 [00:10<00:00, 534.29 examples/s]Tokenizing train dataset:  94%|█████████▍| 6279/6689 [00:10<00:00, 541.50 examples/s]Tokenizing train dataset:  95%|█████████▍| 6349/6689 [00:10<00:00, 522.48 examples/s]Tokenizing train dataset:  95%|█████████▌| 6360/6689 [00:10<00:00, 505.50 examples/s]Tokenizing train dataset:  95%|█████████▍| 6350/6689 [00:10<00:00, 514.70 examples/s]Tokenizing train dataset:  96%|█████████▌| 6414/6689 [00:10<00:00, 480.69 examples/s]Tokenizing train dataset:  96%|█████████▌| 6427/6689 [00:10<00:00, 476.67 examples/s]Tokenizing train dataset:  96%|█████████▌| 6413/6689 [00:10<00:00, 474.60 examples/s]Tokenizing train dataset:  97%|█████████▋| 6490/6689 [00:10<00:00, 484.74 examples/s]Tokenizing train dataset:  97%|█████████▋| 6478/6689 [00:10<00:00, 481.54 examples/s]Tokenizing train dataset:  98%|█████████▊| 6542/6689 [00:10<00:00, 488.80 examples/s]Tokenizing train dataset:  97%|█████████▋| 6488/6689 [00:10<00:00, 479.38 examples/s]Tokenizing train dataset:  98%|█████████▊| 6554/6689 [00:10<00:00, 488.05 examples/s]Tokenizing train dataset:  99%|█████████▊| 6593/6689 [00:10<00:00, 488.72 examples/s]Tokenizing train dataset:  98%|█████████▊| 6540/6689 [00:10<00:00, 485.05 examples/s]Tokenizing train dataset:  99%|█████████▊| 6605/6689 [00:11<00:00, 487.84 examples/s]Tokenizing train dataset:  99%|█████████▉| 6649/6689 [00:11<00:00, 505.67 examples/s]Tokenizing train dataset:  99%|█████████▊| 6590/6689 [00:11<00:00, 481.10 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 601.45 examples/s]
Tokenizing train dataset: 100%|█████████▉| 6657/6689 [00:11<00:00, 494.93 examples/s]Tokenizing train dataset:  99%|█████████▉| 6646/6689 [00:11<00:00, 498.38 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 597.69 examples/s]
Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 595.96 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5535.82 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5537.30 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5476.50 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5536.00 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5531.77 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5385.20 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  29%|██▉       | 279/953 [00:00<00:00, 2759.61 examples/s]Applying chat template to eval dataset:  29%|██▊       | 273/953 [00:00<00:00, 2706.07 examples/s]Applying chat template to eval dataset:  29%|██▉       | 276/953 [00:00<00:00, 2733.51 examples/s]Applying chat template to eval dataset:  59%|█████▉    | 565/953 [00:00<00:00, 2814.29 examples/s]Applying chat template to eval dataset:  59%|█████▊    | 558/953 [00:00<00:00, 2784.97 examples/s]Applying chat template to eval dataset:  59%|█████▉    | 565/953 [00:00<00:00, 2815.10 examples/s]Applying chat template to eval dataset:  89%|████████▉ | 850/953 [00:00<00:00, 2828.72 examples/s]Applying chat template to eval dataset:  88%|████████▊ | 840/953 [00:00<00:00, 2790.70 examples/s]Applying chat template to eval dataset:  89%|████████▉ | 850/953 [00:00<00:00, 2825.48 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2804.86 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2766.78 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2797.15 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 31/953 [00:00<00:03, 296.56 examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 286.19 examples/s]Tokenizing eval dataset:   3%|▎         | 29/953 [00:00<00:03, 286.06 examples/s]Tokenizing eval dataset:   7%|▋         | 68/953 [00:00<00:03, 254.25 examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 250.91 examples/s]Tokenizing eval dataset:   7%|▋         | 66/953 [00:00<00:03, 252.79 examples/s]Tokenizing eval dataset:  10%|█         | 96/953 [00:00<00:03, 256.98 examples/s]Tokenizing eval dataset:  10%|▉         | 94/953 [00:00<00:03, 254.01 examples/s]Tokenizing eval dataset:  10%|▉         | 93/953 [00:00<00:03, 255.21 examples/s]Tokenizing eval dataset:  14%|█▍        | 132/953 [00:00<00:03, 245.34 examples/s]Tokenizing eval dataset:  14%|█▎        | 130/953 [00:00<00:03, 240.35 examples/s]Tokenizing eval dataset:  13%|█▎        | 128/953 [00:00<00:03, 240.71 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 240.61 examples/s]Tokenizing eval dataset:  17%|█▋        | 165/953 [00:00<00:03, 232.54 examples/s]Tokenizing eval dataset:  17%|█▋        | 163/953 [00:00<00:03, 232.83 examples/s]Tokenizing eval dataset:  20%|█▉        | 189/953 [00:00<00:03, 224.88 examples/s]Tokenizing eval dataset:  20%|██        | 195/953 [00:00<00:03, 222.77 examples/s]Tokenizing eval dataset:  21%|██        | 197/953 [00:00<00:03, 221.73 examples/s]Tokenizing eval dataset:  22%|██▏       | 214/953 [00:00<00:03, 230.43 examples/s]Tokenizing eval dataset:  23%|██▎       | 223/953 [00:00<00:03, 236.08 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:03, 237.48 examples/s]Tokenizing eval dataset:  28%|██▊       | 266/953 [00:00<00:02, 305.76 examples/s]Tokenizing eval dataset:  29%|██▉       | 280/953 [00:01<00:02, 322.06 examples/s]Tokenizing eval dataset:  30%|██▉       | 282/953 [00:01<00:02, 319.70 examples/s]Tokenizing eval dataset:  34%|███▍      | 326/953 [00:01<00:01, 384.49 examples/s]Tokenizing eval dataset:  36%|███▌      | 339/953 [00:01<00:01, 391.40 examples/s]Tokenizing eval dataset:  36%|███▌      | 339/953 [00:01<00:01, 382.50 examples/s]Tokenizing eval dataset:  40%|███▉      | 381/953 [00:01<00:01, 427.29 examples/s]Tokenizing eval dataset:  41%|████      | 392/953 [00:01<00:01, 427.63 examples/s]Tokenizing eval dataset:  41%|████      | 392/953 [00:01<00:01, 420.23 examples/s]Tokenizing eval dataset:  47%|████▋     | 444/953 [00:01<00:01, 481.50 examples/s]Tokenizing eval dataset:  48%|████▊     | 459/953 [00:01<00:00, 495.27 examples/s]Tokenizing eval dataset:  48%|████▊     | 458/953 [00:01<00:01, 486.68 examples/s]Tokenizing eval dataset:  53%|█████▎    | 504/953 [00:01<00:00, 508.21 examples/s]Tokenizing eval dataset:  59%|█████▉    | 561/953 [00:01<00:00, 523.72 examples/s]Tokenizing eval dataset:  56%|█████▌    | 536/953 [00:01<00:00, 499.09 examples/s]Tokenizing eval dataset:  56%|█████▌    | 534/953 [00:01<00:00, 491.22 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 529.07 examples/s]Tokenizing eval dataset:  63%|██████▎   | 600/953 [00:01<00:00, 527.72 examples/s]Tokenizing eval dataset:  62%|██████▏   | 595/953 [00:01<00:00, 519.69 examples/s]Tokenizing eval dataset:  70%|███████   | 670/953 [00:01<00:00, 530.77 examples/s]Tokenizing eval dataset:  71%|███████   | 672/953 [00:01<00:00, 515.42 examples/s]Tokenizing eval dataset:  71%|███████   | 678/953 [00:01<00:00, 521.86 examples/s]Tokenizing eval dataset:  76%|███████▌  | 724/953 [00:01<00:00, 521.95 examples/s]Tokenizing eval dataset:  78%|███████▊  | 744/953 [00:01<00:00, 489.33 examples/s]Tokenizing eval dataset:  78%|███████▊  | 742/953 [00:01<00:00, 489.80 examples/s]Tokenizing eval dataset:  83%|████████▎ | 787/953 [00:01<00:00, 475.58 examples/s]Tokenizing eval dataset:  85%|████████▍ | 809/953 [00:02<00:00, 466.85 examples/s]Tokenizing eval dataset:  85%|████████▍ | 806/953 [00:02<00:00, 461.32 examples/s]Tokenizing eval dataset:  89%|████████▉ | 851/953 [00:02<00:00, 451.90 examples/s]Tokenizing eval dataset:  91%|█████████ | 869/953 [00:02<00:00, 444.24 examples/s]Tokenizing eval dataset:  91%|█████████ | 869/953 [00:02<00:00, 441.14 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 430.88 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 397.73 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  98%|█████████▊| 933/953 [00:02<00:00, 432.28 examples/s]Tokenizing eval dataset:  98%|█████████▊| 933/953 [00:02<00:00, 432.24 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 392.33 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 388.82 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5066730976104736 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.344640016555786 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3267858028411865 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3263731002807617 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank8]:[W612 20:11:27.685832348 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 2 ---
