cpu-bind=MASK - gn58, task  3  0 [1233038]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 3 ---
Total Nodes: 4
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn49
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 4     --machine_rank 3     --main_process_ip gn49     --main_process_port 29500     --num_processes 16     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62793424     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.2 --curriculum_stage=1
-------------------------------------------
[2025-06-09 01:29:43,373] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0609 01:29:45.173000 1233088 torch/distributed/run.py:792] 
W0609 01:29:45.173000 1233088 torch/distributed/run.py:792] *****************************************
W0609 01:29:45.173000 1233088 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0609 01:29:45.173000 1233088 torch/distributed/run.py:792] *****************************************
[2025-06-09 01:29:50,199] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-09 01:29:50,241] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-09 01:29:50,250] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-09 01:29:50,251] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Training data of type 'bad_lang_examples':    Training data of type 'bad_lang_examples':    Training data of type 'bad_lang_examples':    Training data of type 'bad_lang_examples':    3489
3489
3489
3489
Training data of type 'short_examples':      Training data of type 'short_examples':       Training data of type 'short_examples':       Training data of type 'short_examples':        699699
699
699

Training data of type 'choose_examples':     Training data of type 'choose_examples':      Training data of type 'choose_examples':      Training data of type 'choose_examples':       1337913379
13379
13379

Training data of type 'bad_format_examples': Training data of type 'bad_format_examples':  Training data of type 'bad_format_examples':  Training data of type 'bad_format_examples':   31483148
3148
3148

****************************************************************************************************
**************************************************
**************************************************

Evaluation data size: Evaluation data size: Evaluation data size: 953
Evaluation data size: 953
953
Curriculum stage 0 training data size: 953
Curriculum stage 0 training data size: 7336
Curriculum stage 0 training data size: 7336Curriculum stage 0 training data size: Curriculum stage 1 training data size: 7336

7336
6689
Curriculum stage 1 training data size: Curriculum stage 1 training data size: Curriculum stage 1 training data size: 6689
Curriculum stage 2 training data size:6689
6689
 6690Curriculum stage 2 training data size: Curriculum stage 2 training data size: 
Curriculum stage 2 training data size: 66906690
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
6690

Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
4e-07Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=1)
4e-07

4e-07
4e-07
World size: 16
Setting gradient accumulation steps to: 1
[2025-06-09 01:29:55,576] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-09 01:29:55,581] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-09 01:29:55,630] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 6689
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-09 01:29:55,878] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curri-0
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:22<01:07, 22.64s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:09, 23.01s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:09, 23.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:09, 23.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:46<00:46, 23.32s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:09<00:23, 23.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 20.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.62s/it]
Loaded model
Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 20.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 20.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 20.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.70s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.70s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:26<00:00, 21.69s/it]
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank15]:[W609 01:31:27.000728073 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 15]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
[rank13]:[W609 01:31:27.034666511 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 13]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s][rank14]:[W609 01:31:27.113065427 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 14]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   9%|▉         | 590/6689 [00:00<00:01, 5783.27 examples/s]Extracting prompt in train dataset:  22%|██▏       | 1458/6689 [00:00<00:00, 5761.15 examples/s]Extracting prompt in train dataset:  31%|███       | 2050/6689 [00:00<00:00, 4712.75 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2534/6689 [00:00<00:00, 4745.35 examples/s]Extracting prompt in train dataset:  46%|████▋     | 3098/6689 [00:00<00:00, 5009.33 examples/s]Extracting prompt in train dataset:  55%|█████▍    | 3648/6689 [00:00<00:00, 5155.98 examples/s]Extracting prompt in train dataset:  66%|██████▌   | 4397/6689 [00:00<00:00, 5081.84 examples/s]Extracting prompt in train dataset:  74%|███████▎  | 4927/6689 [00:00<00:00, 5140.88 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 5720/6689 [00:01<00:00, 5178.75 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 6480/6689 [00:01<00:00, 5133.52 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5024.19 examples/s]
Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 296/6689 [00:00<00:02, 2932.22 examples/s]Applying chat template to train dataset:   9%|▉         | 593/6689 [00:00<00:02, 2947.32 examples/s]Applying chat template to train dataset:  15%|█▌        | 1030/6689 [00:00<00:01, 2922.49 examples/s]Applying chat template to train dataset:  20%|█▉        | 1323/6689 [00:00<00:01, 2922.30 examples/s]Applying chat template to train dataset:  24%|██▍       | 1624/6689 [00:00<00:01, 2948.44 examples/s]Applying chat template to train dataset:  29%|██▉       | 1945/6689 [00:00<00:01, 2580.64 examples/s]Applying chat template to train dataset:  34%|███▎      | 2241/6689 [00:00<00:01, 2685.42 examples/s]Applying chat template to train dataset:  40%|███▉      | 2660/6689 [00:00<00:01, 2719.18 examples/s]Applying chat template to train dataset:  44%|████▍     | 2950/6689 [00:01<00:01, 2765.24 examples/s]Applying chat template to train dataset:  48%|████▊     | 3233/6689 [00:01<00:01, 2780.47 examples/s]Applying chat template to train dataset:  55%|█████▍    | 3661/6689 [00:01<00:01, 2806.30 examples/s]Applying chat template to train dataset:  61%|██████▏   | 4099/6689 [00:01<00:00, 2838.30 examples/s]Applying chat template to train dataset:  66%|██████▌   | 4395/6689 [00:01<00:00, 2866.75 examples/s]Applying chat template to train dataset:  70%|███████   | 4690/6689 [00:01<00:00, 2881.00 examples/s]Applying chat template to train dataset:  76%|███████▋  | 5110/6689 [00:01<00:00, 2846.36 examples/s]Applying chat template to train dataset:  82%|████████▏ | 5490/6689 [00:01<00:00, 2731.26 examples/s]Applying chat template to train dataset:  87%|████████▋ | 5796/6689 [00:02<00:00, 2807.59 examples/s]Applying chat template to train dataset:  91%|█████████ | 6095/6689 [00:02<00:00, 2852.29 examples/s]Applying chat template to train dataset:  98%|█████████▊| 6535/6689 [00:02<00:00, 2876.54 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 2812.69 examples/s]
Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   2%|▏         | 107/6689 [00:00<00:06, 1060.36 examples/s]Tokenizing train dataset:   3%|▎         | 222/6689 [00:00<00:05, 1105.54 examples/s]Tokenizing train dataset:   5%|▌         | 364/6689 [00:00<00:06, 1013.83 examples/s]Tokenizing train dataset:   7%|▋         | 480/6689 [00:00<00:06, 903.10 examples/s] Tokenizing train dataset:   9%|▊         | 581/6689 [00:00<00:06, 933.77 examples/s]Tokenizing train dataset:  11%|█         | 721/6689 [00:00<00:06, 927.06 examples/s]Tokenizing train dataset:  13%|█▎        | 854/6689 [00:00<00:06, 910.03 examples/s]Tokenizing train dataset:  15%|█▍        | 983/6689 [00:01<00:06, 888.17 examples/s]Tokenizing train dataset:  16%|█▋        | 1100/6689 [00:01<00:06, 851.76 examples/s]Tokenizing train dataset:  18%|█▊        | 1211/6689 [00:01<00:06, 808.06 examples/s]Tokenizing train dataset:  20%|█▉        | 1319/6689 [00:01<00:06, 777.31 examples/s]Tokenizing train dataset:  21%|██▏       | 1428/6689 [00:01<00:06, 757.88 examples/s]Tokenizing train dataset:  23%|██▎       | 1522/6689 [00:01<00:07, 716.10 examples/s]Tokenizing train dataset:  24%|██▍       | 1617/6689 [00:01<00:07, 686.76 examples/s]Tokenizing train dataset:  25%|██▌       | 1687/6689 [00:02<00:07, 687.71 examples/s]Tokenizing train dataset:  27%|██▋       | 1783/6689 [00:02<00:07, 668.32 examples/s]Tokenizing train dataset:  28%|██▊       | 1879/6689 [00:02<00:07, 658.27 examples/s]Tokenizing train dataset:  29%|██▉       | 1966/6689 [00:02<00:07, 631.13 examples/s]Tokenizing train dataset:  30%|███       | 2030/6689 [00:02<00:07, 629.41 examples/s]Tokenizing train dataset:  32%|███▏      | 2122/6689 [00:02<00:07, 623.17 examples/s]Tokenizing train dataset:  33%|███▎      | 2205/6689 [00:02<00:07, 600.20 examples/s]Tokenizing train dataset:  34%|███▍      | 2285/6689 [00:03<00:07, 576.82 examples/s]Tokenizing train dataset:  35%|███▌      | 2350/6689 [00:03<00:07, 589.38 examples/s]Tokenizing train dataset:  36%|███▋      | 2440/6689 [00:03<00:07, 589.14 examples/s]Tokenizing train dataset:  38%|███▊      | 2524/6689 [00:03<00:07, 578.53 examples/s]Tokenizing train dataset:  39%|███▉      | 2619/6689 [00:03<00:06, 593.20 examples/s]Tokenizing train dataset:  40%|████      | 2699/6689 [00:03<00:06, 572.16 examples/s]Tokenizing train dataset:  42%|████▏     | 2780/6689 [00:03<00:07, 552.10 examples/s]Tokenizing train dataset:  42%|████▏     | 2840/6689 [00:04<00:06, 559.51 examples/s]Tokenizing train dataset:  44%|████▎     | 2925/6689 [00:04<00:06, 555.02 examples/s]Tokenizing train dataset:  45%|████▍     | 2981/6689 [00:04<00:06, 550.93 examples/s]Tokenizing train dataset:  46%|████▌     | 3063/6689 [00:04<00:06, 545.15 examples/s]Tokenizing train dataset:  47%|████▋     | 3118/6689 [00:04<00:06, 544.49 examples/s]Tokenizing train dataset:  48%|████▊     | 3202/6689 [00:04<00:06, 547.79 examples/s]Tokenizing train dataset:  49%|████▉     | 3280/6689 [00:04<00:06, 531.58 examples/s]Tokenizing train dataset:  50%|█████     | 3350/6689 [00:05<00:06, 508.38 examples/s]Tokenizing train dataset:  51%|█████     | 3414/6689 [00:05<00:06, 537.96 examples/s]Tokenizing train dataset:  52%|█████▏    | 3494/6689 [00:05<00:05, 533.04 examples/s]Tokenizing train dataset:  53%|█████▎    | 3556/6689 [00:05<00:06, 488.30 examples/s]Tokenizing train dataset:  54%|█████▍    | 3627/6689 [00:05<00:06, 478.64 examples/s]Tokenizing train dataset:  55%|█████▌    | 3680/6689 [00:05<00:06, 487.44 examples/s]Tokenizing train dataset:  56%|█████▌    | 3752/6689 [00:05<00:06, 478.83 examples/s]Tokenizing train dataset:  57%|█████▋    | 3807/6689 [00:05<00:05, 492.88 examples/s]Tokenizing train dataset:  58%|█████▊    | 3877/6689 [00:06<00:05, 478.47 examples/s]Tokenizing train dataset:  59%|█████▉    | 3930/6689 [00:06<00:05, 488.36 examples/s]Tokenizing train dataset:  60%|█████▉    | 3990/6689 [00:06<00:05, 454.62 examples/s]Tokenizing train dataset:  60%|██████    | 4037/6689 [00:06<00:05, 455.53 examples/s]Tokenizing train dataset:  61%|██████    | 4090/6689 [00:06<00:05, 470.71 examples/s]Tokenizing train dataset:  62%|██████▏   | 4139/6689 [00:06<00:05, 470.42 examples/s]Tokenizing train dataset:  63%|██████▎   | 4214/6689 [00:06<00:05, 478.97 examples/s]Tokenizing train dataset:  64%|██████▍   | 4267/6689 [00:06<00:04, 488.57 examples/s]Tokenizing train dataset:  65%|██████▍   | 4336/6689 [00:07<00:04, 474.03 examples/s]Tokenizing train dataset:  66%|██████▌   | 4388/6689 [00:07<00:04, 482.89 examples/s]Tokenizing train dataset:  66%|██████▋   | 4440/6689 [00:07<00:04, 489.85 examples/s]Tokenizing train dataset:  68%|██████▊   | 4518/6689 [00:07<00:04, 497.87 examples/s]Tokenizing train dataset:  69%|██████▊   | 4587/6689 [00:07<00:04, 480.61 examples/s]Tokenizing train dataset:  70%|██████▉   | 4649/6689 [00:07<00:04, 457.75 examples/s]Tokenizing train dataset:  71%|███████   | 4722/6689 [00:07<00:04, 461.32 examples/s]Tokenizing train dataset:  71%|███████▏  | 4769/6689 [00:08<00:04, 460.65 examples/s]Tokenizing train dataset:  72%|███████▏  | 4816/6689 [00:08<00:04, 459.60 examples/s]Tokenizing train dataset:  73%|███████▎  | 4882/6689 [00:08<00:04, 447.47 examples/s]Tokenizing train dataset:  74%|███████▎  | 4928/6689 [00:08<00:03, 449.35 examples/s]Tokenizing train dataset:  75%|███████▍  | 5006/6689 [00:08<00:03, 471.48 examples/s]Tokenizing train dataset:  76%|███████▌  | 5074/6689 [00:08<00:03, 457.90 examples/s]Tokenizing train dataset:  77%|███████▋  | 5151/6689 [00:08<00:03, 470.61 examples/s]Tokenizing train dataset:  78%|███████▊  | 5220/6689 [00:08<00:03, 462.59 examples/s]Tokenizing train dataset:  79%|███████▉  | 5275/6689 [00:09<00:02, 477.58 examples/s]Tokenizing train dataset:  80%|███████▉  | 5326/6689 [00:09<00:02, 484.73 examples/s]Tokenizing train dataset:  81%|████████  | 5399/6689 [00:09<00:02, 482.40 examples/s]Tokenizing train dataset:  81%|████████▏ | 5450/6689 [00:09<00:02, 432.27 examples/s]Tokenizing train dataset:  82%|████████▏ | 5517/6689 [00:09<00:02, 433.38 examples/s]Tokenizing train dataset:  83%|████████▎ | 5566/6689 [00:09<00:02, 444.11 examples/s]Tokenizing train dataset:  84%|████████▍ | 5617/6689 [00:09<00:02, 459.10 examples/s]Tokenizing train dataset:  85%|████████▍ | 5680/6689 [00:10<00:02, 435.32 examples/s]Tokenizing train dataset:  86%|████████▌ | 5740/6689 [00:10<00:02, 417.42 examples/s]Tokenizing train dataset:  87%|████████▋ | 5787/6689 [00:10<00:02, 427.44 examples/s]Tokenizing train dataset:  87%|████████▋ | 5850/6689 [00:10<00:02, 417.99 examples/s]Tokenizing train dataset:  88%|████████▊ | 5916/6689 [00:10<00:01, 419.91 examples/s]Tokenizing train dataset:  89%|████████▉ | 5970/6689 [00:10<00:01, 443.48 examples/s]Tokenizing train dataset:  90%|█████████ | 6038/6689 [00:10<00:01, 444.40 examples/s]Tokenizing train dataset:  91%|█████████ | 6086/6689 [00:10<00:01, 449.61 examples/s]Tokenizing train dataset:  92%|█████████▏| 6141/6689 [00:11<00:01, 468.65 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6689 [00:11<00:01, 481.34 examples/s]Tokenizing train dataset:  93%|█████████▎| 6254/6689 [00:11<00:00, 508.18 examples/s]Tokenizing train dataset:  95%|█████████▍| 6325/6689 [00:11<00:00, 490.92 examples/s]Tokenizing train dataset:  95%|█████████▌| 6385/6689 [00:11<00:00, 457.60 examples/s]Tokenizing train dataset:  96%|█████████▋| 6447/6689 [00:11<00:00, 440.90 examples/s]Tokenizing train dataset:  97%|█████████▋| 6510/6689 [00:11<00:00, 433.16 examples/s]Tokenizing train dataset:  98%|█████████▊| 6556/6689 [00:11<00:00, 437.52 examples/s]Tokenizing train dataset:  99%|█████████▊| 6605/6689 [00:12<00:00, 445.27 examples/s]Tokenizing train dataset:  99%|█████████▉| 6651/6689 [00:12<00:00, 447.86 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:12<00:00, 544.84 examples/s]
[rank12]:[W609 01:31:44.092704702 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 12]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▉         | 590/6689 [00:00<00:01, 5841.41 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5639.83 examples/s]Extracting prompt in train dataset:   9%|▉         | 591/6689 [00:00<00:01, 5865.40 examples/s]Extracting prompt in train dataset:   9%|▊         | 580/6689 [00:00<00:01, 5721.72 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5084.48 examples/s]
Extracting prompt in train dataset:  21%|██        | 1380/6689 [00:00<00:00, 5441.88 examples/s]Extracting prompt in train dataset:  20%|█▉        | 1305/6689 [00:00<00:01, 5111.85 examples/s]Extracting prompt in train dataset:  19%|█▊        | 1240/6689 [00:00<00:01, 4807.78 examples/s]Extracting prompt in train dataset:  29%|██▉       | 1950/6689 [00:00<00:00, 5524.68 examples/s]Extracting prompt in train dataset:  31%|███       | 2070/6689 [00:00<00:00, 5078.61 examples/s]Extracting prompt in train dataset:  30%|██▉       | 2000/6689 [00:00<00:00, 4923.82 examples/s]Extracting prompt in train dataset:  42%|████▏     | 2800/6689 [00:00<00:00, 5582.64 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2630/6689 [00:00<00:00, 5234.14 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2555/6689 [00:00<00:00, 5110.44 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  54%|█████▎    | 3580/6689 [00:00<00:00, 5413.80 examples/s]Extracting prompt in train dataset:  50%|█████     | 3350/6689 [00:00<00:00, 5029.53 examples/s]Extracting prompt in train dataset:  48%|████▊     | 3220/6689 [00:00<00:00, 4824.63 examples/s]Applying chat template to eval dataset:  33%|███▎      | 312/953 [00:00<00:00, 3083.37 examples/s]Extracting prompt in train dataset:  58%|█████▊    | 3901/6689 [00:00<00:00, 5161.73 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3760/6689 [00:00<00:00, 4974.06 examples/s]Extracting prompt in train dataset:  66%|██████▌   | 4410/6689 [00:00<00:00, 5438.09 examples/s]Applying chat template to eval dataset:  81%|████████▏ | 775/953 [00:00<00:00, 3083.11 examples/s]Extracting prompt in train dataset:  67%|██████▋   | 4470/6689 [00:00<00:00, 5290.02 examples/s]Extracting prompt in train dataset:  64%|██████▍   | 4301/6689 [00:00<00:00, 5095.55 examples/s]Extracting prompt in train dataset:  74%|███████▍  | 4980/6689 [00:00<00:00, 5486.21 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2845.30 examples/s]
Extracting prompt in train dataset:  78%|███████▊  | 5220/6689 [00:01<00:00, 5149.08 examples/s]Extracting prompt in train dataset:  75%|███████▍  | 5014/6689 [00:01<00:00, 4954.34 examples/s]Extracting prompt in train dataset:  87%|████████▋ | 5810/6689 [00:01<00:00, 5474.60 examples/s]Extracting prompt in train dataset:  89%|████████▊ | 5930/6689 [00:01<00:00, 4987.10 examples/s]Extracting prompt in train dataset:  84%|████████▍ | 5630/6689 [00:01<00:00, 4652.81 examples/s]Extracting prompt in train dataset:  99%|█████████▉| 6610/6689 [00:01<00:00, 5423.05 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5386.77 examples/s]
Extracting prompt in train dataset:  96%|█████████▋| 6450/6689 [00:01<00:00, 5026.19 examples/s]Extracting prompt in train dataset:  92%|█████████▏| 6141/6689 [00:01<00:00, 4756.05 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5062.03 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 4739.05 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 4804.88 examples/s]
Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 326.03 examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing eval dataset:   7%|▋         | 69/953 [00:00<00:03, 251.85 examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   5%|▍         | 306/6689 [00:00<00:02, 3020.08 examples/s]Tokenizing eval dataset:  10%|█         | 97/953 [00:00<00:03, 256.70 examples/s]Applying chat template to train dataset:   4%|▍         | 297/6689 [00:00<00:02, 2942.87 examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:  11%|█▏        | 755/6689 [00:00<00:01, 2996.38 examples/s]Tokenizing eval dataset:  14%|█▍        | 132/953 [00:00<00:03, 244.14 examples/s]Applying chat template to train dataset:   4%|▍         | 300/6689 [00:00<00:02, 2967.46 examples/s]Applying chat template to train dataset:  16%|█▌        | 1060/6689 [00:00<00:01, 3013.42 examples/s]Applying chat template to train dataset:  11%|█         | 742/6689 [00:00<00:02, 2947.92 examples/s]Tokenizing eval dataset:  16%|█▋        | 157/953 [00:00<00:03, 243.20 examples/s]Applying chat template to train dataset:   9%|▉         | 606/6689 [00:00<00:02, 3020.45 examples/s]Applying chat template to train dataset:  21%|██        | 1374/6689 [00:00<00:01, 3054.73 examples/s]Applying chat template to train dataset:  18%|█▊        | 1190/6689 [00:00<00:01, 2957.70 examples/s]Tokenizing eval dataset:  20%|█▉        | 189/953 [00:00<00:03, 230.20 examples/s]Applying chat template to train dataset:  16%|█▌        | 1062/6689 [00:00<00:01, 3028.99 examples/s]Applying chat template to train dataset:  27%|██▋       | 1830/6689 [00:00<00:01, 3039.19 examples/s]Tokenizing eval dataset:  23%|██▎       | 215/953 [00:00<00:03, 234.16 examples/s]Applying chat template to train dataset:  24%|██▍       | 1619/6689 [00:00<00:01, 2915.36 examples/s]Applying chat template to train dataset:  22%|██▏       | 1492/6689 [00:00<00:01, 2952.67 examples/s]Tokenizing eval dataset:  28%|██▊       | 271/953 [00:00<00:02, 320.44 examples/s]Applying chat template to train dataset:  34%|███▍      | 2262/6689 [00:00<00:01, 2976.28 examples/s]Applying chat template to train dataset:  31%|███       | 2062/6689 [00:00<00:01, 2923.02 examples/s]Tokenizing eval dataset:  32%|███▏      | 309/953 [00:01<00:01, 335.98 examples/s]Applying chat template to train dataset:  28%|██▊       | 1891/6689 [00:00<00:01, 2838.79 examples/s]Applying chat template to train dataset:  40%|███▉      | 2646/6689 [00:00<00:01, 2827.05 examples/s]Applying chat template to train dataset:  35%|███▌      | 2362/6689 [00:00<00:01, 2939.71 examples/s]Tokenizing eval dataset:  38%|███▊      | 363/953 [00:01<00:01, 391.23 examples/s]Applying chat template to train dataset:  33%|███▎      | 2197/6689 [00:00<00:01, 2895.82 examples/s]Applying chat template to train dataset:  44%|████▍     | 2955/6689 [00:01<00:01, 2891.60 examples/s]Tokenizing eval dataset:  44%|████▍     | 422/953 [00:01<00:01, 443.71 examples/s]Applying chat template to train dataset:  41%|████      | 2717/6689 [00:00<00:01, 2731.44 examples/s]Applying chat template to train dataset:  37%|███▋      | 2501/6689 [00:00<00:01, 2933.18 examples/s]Applying chat template to train dataset:  49%|████▉     | 3265/6689 [00:01<00:01, 2945.04 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 502.91 examples/s]Applying chat template to train dataset:  45%|████▌     | 3014/6689 [00:01<00:01, 2790.81 examples/s]Applying chat template to train dataset:  42%|████▏     | 2810/6689 [00:00<00:01, 2977.38 examples/s]Tokenizing eval dataset:  58%|█████▊    | 552/953 [00:01<00:00, 539.91 examples/s]Applying chat template to train dataset:  55%|█████▌    | 3688/6689 [00:01<00:01, 2896.06 examples/s]Applying chat template to train dataset:  50%|████▉     | 3318/6689 [00:01<00:01, 2855.42 examples/s]Applying chat template to train dataset:  49%|████▊     | 3247/6689 [00:01<00:01, 2948.38 examples/s]Tokenizing eval dataset:  65%|██████▍   | 615/953 [00:01<00:00, 561.38 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4126/6689 [00:01<00:00, 2901.31 examples/s]Applying chat template to train dataset:  55%|█████▌    | 3711/6689 [00:01<00:01, 2765.50 examples/s]Applying chat template to train dataset:  53%|█████▎    | 3558/6689 [00:01<00:01, 2985.52 examples/s]Tokenizing eval dataset:  71%|███████   | 674/953 [00:01<00:00, 563.84 examples/s]Applying chat template to train dataset:  66%|██████▋   | 4436/6689 [00:01<00:00, 2948.85 examples/s]Applying chat template to train dataset:  60%|█████▉    | 4007/6689 [00:01<00:00, 2813.70 examples/s]Applying chat template to train dataset:  60%|█████▉    | 4000/6689 [00:01<00:00, 2968.31 examples/s]Tokenizing eval dataset:  79%|███████▊  | 749/953 [00:01<00:00, 531.88 examples/s]Applying chat template to train dataset:  73%|███████▎  | 4861/6689 [00:01<00:00, 2906.40 examples/s]Applying chat template to train dataset:  66%|██████▋   | 4443/6689 [00:01<00:00, 2843.06 examples/s]Applying chat template to train dataset:  66%|██████▌   | 4423/6689 [00:01<00:00, 2912.17 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5165/6689 [00:01<00:00, 2936.79 examples/s]Tokenizing eval dataset:  86%|████████▌ | 820/953 [00:02<00:00, 506.56 examples/s]Applying chat template to train dataset:  73%|███████▎  | 4882/6689 [00:01<00:00, 2868.11 examples/s]Applying chat template to train dataset:  73%|███████▎  | 4859/6689 [00:01<00:00, 2908.19 examples/s]Applying chat template to train dataset:  84%|████████▍ | 5622/6689 [00:01<00:00, 2970.69 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5180/6689 [00:01<00:00, 2889.43 examples/s]Tokenizing eval dataset:  94%|█████████▎| 893/953 [00:02<00:00, 495.70 examples/s]Applying chat template to train dataset:  79%|███████▉  | 5288/6689 [00:01<00:00, 2887.93 examples/s]Applying chat template to train dataset:  90%|█████████ | 6030/6689 [00:02<00:00, 2884.96 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5565/6689 [00:01<00:00, 2777.00 examples/s]Tokenizing eval dataset: 100%|█████████▉| 949/953 [00:02<00:00, 453.36 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 406.67 examples/s]
Applying chat template to train dataset:  88%|████████▊ | 5860/6689 [00:02<00:00, 2817.04 examples/s]Applying chat template to train dataset:  86%|████████▌ | 5730/6689 [00:01<00:00, 2901.18 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6462/6689 [00:02<00:00, 2880.19 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 2920.02 examples/s]
Applying chat template to train dataset:  90%|█████████ | 6031/6689 [00:02<00:00, 2923.23 examples/s]Applying chat template to train dataset:  94%|█████████▍| 6279/6689 [00:02<00:00, 2804.63 examples/s]Applying chat template to train dataset:  95%|█████████▍| 6346/6689 [00:02<00:00, 2623.10 examples/s]Applying chat template to train dataset:  99%|█████████▉| 6647/6689 [00:02<00:00, 2685.02 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 2774.78 examples/s]
Applying chat template to train dataset:  99%|█████████▉| 6648/6689 [00:02<00:00, 2714.29 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 2813.30 examples/s]
Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   2%|▏         | 111/6689 [00:00<00:05, 1097.71 examples/s]Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   4%|▍         | 277/6689 [00:00<00:05, 1090.31 examples/s]Tokenizing train dataset:   2%|▏         | 110/6689 [00:00<00:06, 1087.50 examples/s]Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   3%|▎         | 228/6689 [00:00<00:05, 1139.35 examples/s]Tokenizing train dataset:   6%|▌         | 418/6689 [00:00<00:06, 1011.00 examples/s]Tokenizing train dataset:   2%|▏         | 109/6689 [00:00<00:06, 1078.92 examples/s]Tokenizing train dataset:   8%|▊         | 521/6689 [00:00<00:06, 1015.06 examples/s]Tokenizing train dataset:   6%|▌         | 391/6689 [00:00<00:05, 1105.10 examples/s]Tokenizing train dataset:   4%|▍         | 277/6689 [00:00<00:05, 1087.03 examples/s]Tokenizing train dataset:  10%|▉         | 658/6689 [00:00<00:06, 967.29 examples/s] Tokenizing train dataset:   8%|▊         | 547/6689 [00:00<00:05, 1073.24 examples/s]Tokenizing train dataset:   6%|▌         | 418/6689 [00:00<00:06, 1010.04 examples/s]Tokenizing train dataset:  12%|█▏        | 784/6689 [00:00<00:06, 913.60 examples/s]Tokenizing train dataset:  10%|█         | 689/6689 [00:00<00:05, 1022.36 examples/s]Tokenizing train dataset:   8%|▊         | 545/6689 [00:00<00:06, 941.75 examples/s] Tokenizing train dataset:  14%|█▎        | 907/6689 [00:00<00:06, 876.46 examples/s]Tokenizing train dataset:  10%|▉         | 643/6689 [00:00<00:06, 951.31 examples/s]Tokenizing train dataset:  12%|█▏        | 826/6689 [00:00<00:05, 978.38 examples/s] Tokenizing train dataset:  15%|█▌        | 1020/6689 [00:01<00:06, 832.06 examples/s]Tokenizing train dataset:  12%|█▏        | 787/6689 [00:00<00:07, 842.98 examples/s]Tokenizing train dataset:  14%|█▍        | 954/6689 [00:01<00:06, 840.52 examples/s]Tokenizing train dataset:  17%|█▋        | 1131/6689 [00:01<00:06, 796.11 examples/s]Tokenizing train dataset:  14%|█▎        | 906/6689 [00:01<00:07, 820.26 examples/s]Tokenizing train dataset:  16%|█▌        | 1078/6689 [00:01<00:06, 834.53 examples/s]Tokenizing train dataset:  18%|█▊        | 1232/6689 [00:01<00:07, 750.83 examples/s]Tokenizing train dataset:  15%|█▍        | 996/6689 [00:01<00:06, 834.60 examples/s]Tokenizing train dataset:  18%|█▊        | 1192/6689 [00:01<00:06, 809.17 examples/s]Tokenizing train dataset:  20%|█▉        | 1309/6689 [00:01<00:07, 748.44 examples/s]Tokenizing train dataset:  17%|█▋        | 1115/6689 [00:01<00:06, 818.85 examples/s]Tokenizing train dataset:  19%|█▉        | 1300/6689 [00:01<00:06, 781.27 examples/s]Tokenizing train dataset:  21%|██        | 1421/6689 [00:01<00:07, 742.51 examples/s]Tokenizing train dataset:  18%|█▊        | 1202/6689 [00:01<00:07, 737.67 examples/s]Tokenizing train dataset:  21%|██        | 1406/6689 [00:01<00:06, 756.05 examples/s]Tokenizing train dataset:  23%|██▎       | 1530/6689 [00:01<00:07, 733.58 examples/s]Tokenizing train dataset:  20%|█▉        | 1315/6689 [00:01<00:07, 738.64 examples/s]Tokenizing train dataset:  23%|██▎       | 1510/6689 [00:01<00:07, 736.68 examples/s]Tokenizing train dataset:  24%|██▍       | 1629/6689 [00:01<00:07, 705.31 examples/s]Tokenizing train dataset:  21%|██        | 1421/6689 [00:01<00:07, 720.06 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6689 [00:02<00:07, 700.14 examples/s]Tokenizing train dataset:  24%|██▍       | 1612/6689 [00:01<00:07, 715.91 examples/s]Tokenizing train dataset:  23%|██▎       | 1531/6689 [00:01<00:07, 719.95 examples/s]Tokenizing train dataset:  27%|██▋       | 1793/6689 [00:02<00:07, 669.77 examples/s]Tokenizing train dataset:  26%|██▌       | 1710/6689 [00:02<00:07, 685.69 examples/s]Tokenizing train dataset:  24%|██▍       | 1622/6689 [00:02<00:07, 681.25 examples/s]Tokenizing train dataset:  28%|██▊       | 1892/6689 [00:02<00:07, 663.29 examples/s]Tokenizing train dataset:  27%|██▋       | 1810/6689 [00:02<00:07, 671.99 examples/s]Tokenizing train dataset:  25%|██▌       | 1691/6689 [00:02<00:07, 681.40 examples/s]Tokenizing train dataset:  29%|██▉       | 1959/6689 [00:02<00:07, 659.50 examples/s]Tokenizing train dataset:  29%|██▊       | 1908/6689 [00:02<00:07, 659.57 examples/s]Tokenizing train dataset:  27%|██▋       | 1782/6689 [00:02<00:07, 656.14 examples/s]Tokenizing train dataset:  31%|███       | 2059/6689 [00:02<00:07, 660.90 examples/s]Tokenizing train dataset:  30%|██▉       | 2006/6689 [00:02<00:07, 654.58 examples/s]Tokenizing train dataset:  32%|███▏      | 2127/6689 [00:02<00:06, 662.32 examples/s]Tokenizing train dataset:  28%|██▊       | 1872/6689 [00:02<00:07, 633.76 examples/s]Tokenizing train dataset:  31%|███▏      | 2104/6689 [00:02<00:07, 650.00 examples/s]Tokenizing train dataset:  33%|███▎      | 2211/6689 [00:02<00:07, 625.74 examples/s]Tokenizing train dataset:  29%|██▉       | 1959/6689 [00:02<00:07, 613.51 examples/s]Tokenizing train dataset:  33%|███▎      | 2199/6689 [00:02<00:07, 640.16 examples/s]Tokenizing train dataset:  34%|███▍      | 2303/6689 [00:03<00:07, 613.79 examples/s]Tokenizing train dataset:  31%|███       | 2050/6689 [00:02<00:07, 607.38 examples/s]Tokenizing train dataset:  35%|███▌      | 2367/6689 [00:03<00:06, 619.30 examples/s]Tokenizing train dataset:  32%|███▏      | 2116/6689 [00:02<00:07, 617.49 examples/s]Tokenizing train dataset:  34%|███▍      | 2281/6689 [00:03<00:07, 607.94 examples/s]Tokenizing train dataset:  35%|███▌      | 2350/6689 [00:03<00:06, 622.58 examples/s]Tokenizing train dataset:  37%|███▋      | 2456/6689 [00:03<00:06, 606.35 examples/s]Tokenizing train dataset:  33%|███▎      | 2204/6689 [00:03<00:07, 601.00 examples/s]Tokenizing train dataset:  36%|███▋      | 2440/6689 [00:03<00:06, 614.15 examples/s]Tokenizing train dataset:  38%|███▊      | 2545/6689 [00:03<00:06, 599.00 examples/s]Tokenizing train dataset:  34%|███▍      | 2276/6689 [00:03<00:07, 560.50 examples/s]Tokenizing train dataset:  38%|███▊      | 2509/6689 [00:03<00:07, 564.18 examples/s]Tokenizing train dataset:  39%|███▉      | 2636/6689 [00:03<00:06, 597.72 examples/s]Tokenizing train dataset:  35%|███▌      | 2357/6689 [00:03<00:07, 551.11 examples/s]Tokenizing train dataset:  38%|███▊      | 2570/6689 [00:03<00:07, 573.27 examples/s]Tokenizing train dataset:  36%|███▌      | 2414/6689 [00:03<00:07, 551.92 examples/s]Tokenizing train dataset:  41%|████      | 2722/6689 [00:03<00:06, 586.17 examples/s]Tokenizing train dataset:  39%|███▉      | 2636/6689 [00:03<00:06, 592.12 examples/s]Tokenizing train dataset:  37%|███▋      | 2500/6689 [00:03<00:07, 554.88 examples/s]Tokenizing train dataset:  42%|████▏     | 2804/6689 [00:03<00:06, 569.40 examples/s]Tokenizing train dataset:  41%|████      | 2722/6689 [00:03<00:06, 580.10 examples/s]Tokenizing train dataset:  38%|███▊      | 2558/6689 [00:03<00:07, 560.32 examples/s]Tokenizing train dataset:  43%|████▎     | 2890/6689 [00:04<00:06, 565.67 examples/s]Tokenizing train dataset:  42%|████▏     | 2806/6689 [00:03<00:06, 565.95 examples/s]Tokenizing train dataset:  40%|███▉      | 2645/6689 [00:03<00:07, 565.61 examples/s]Tokenizing train dataset:  44%|████▍     | 2947/6689 [00:04<00:06, 566.52 examples/s]Tokenizing train dataset:  40%|████      | 2703/6689 [00:03<00:07, 566.25 examples/s]Tokenizing train dataset:  43%|████▎     | 2884/6689 [00:04<00:06, 548.02 examples/s]Tokenizing train dataset:  45%|████▌     | 3015/6689 [00:04<00:06, 526.55 examples/s]Tokenizing train dataset:  44%|████▍     | 2946/6689 [00:04<00:06, 557.09 examples/s]Tokenizing train dataset:  42%|████▏     | 2780/6689 [00:04<00:07, 544.98 examples/s]Tokenizing train dataset:  42%|████▏     | 2837/6689 [00:04<00:07, 548.58 examples/s]Tokenizing train dataset:  46%|████▌     | 3090/6689 [00:04<00:06, 516.29 examples/s]Tokenizing train dataset:  45%|████▌     | 3020/6689 [00:04<00:06, 528.87 examples/s]Tokenizing train dataset:  47%|████▋     | 3148/6689 [00:04<00:06, 527.03 examples/s]Tokenizing train dataset:  46%|████▌     | 3076/6689 [00:04<00:06, 533.02 examples/s]Tokenizing train dataset:  44%|████▎     | 2920/6689 [00:04<00:06, 546.04 examples/s]Tokenizing train dataset:  47%|████▋     | 3132/6689 [00:04<00:06, 536.93 examples/s]Tokenizing train dataset:  48%|████▊     | 3223/6689 [00:04<00:06, 513.85 examples/s]Tokenizing train dataset:  45%|████▍     | 3000/6689 [00:04<00:06, 537.01 examples/s]Tokenizing train dataset:  48%|████▊     | 3190/6689 [00:04<00:06, 541.99 examples/s]Tokenizing train dataset:  49%|████▉     | 3294/6689 [00:04<00:06, 498.35 examples/s]Tokenizing train dataset:  46%|████▌     | 3073/6689 [00:04<00:07, 514.24 examples/s]Tokenizing train dataset:  49%|████▉     | 3269/6689 [00:04<00:06, 530.73 examples/s]Tokenizing train dataset:  50%|█████     | 3372/6689 [00:05<00:06, 503.17 examples/s]Tokenizing train dataset:  47%|████▋     | 3128/6689 [00:04<00:06, 522.14 examples/s]Tokenizing train dataset:  50%|█████     | 3347/6689 [00:04<00:06, 523.39 examples/s]Tokenizing train dataset:  51%|█████▏    | 3431/6689 [00:05<00:06, 520.77 examples/s]Tokenizing train dataset:  48%|████▊     | 3206/6689 [00:04<00:06, 518.93 examples/s]Tokenizing train dataset:  51%|█████     | 3417/6689 [00:05<00:06, 483.46 examples/s]Tokenizing train dataset:  52%|█████▏    | 3494/6689 [00:05<00:06, 461.85 examples/s]Tokenizing train dataset:  49%|████▉     | 3270/6689 [00:05<00:07, 484.93 examples/s]Tokenizing train dataset:  52%|█████▏    | 3491/6689 [00:05<00:06, 482.95 examples/s]Tokenizing train dataset:  53%|█████▎    | 3566/6689 [00:05<00:06, 464.59 examples/s]Tokenizing train dataset:  50%|█████     | 3350/6689 [00:05<00:06, 494.19 examples/s]Tokenizing train dataset:  53%|█████▎    | 3547/6689 [00:05<00:06, 497.57 examples/s]Tokenizing train dataset:  51%|█████     | 3407/6689 [00:05<00:06, 508.65 examples/s]Tokenizing train dataset:  54%|█████▍    | 3640/6689 [00:05<00:06, 467.09 examples/s]Tokenizing train dataset:  52%|█████▏    | 3460/6689 [00:05<00:06, 510.22 examples/s]Tokenizing train dataset:  54%|█████▍    | 3616/6689 [00:05<00:06, 483.34 examples/s]Tokenizing train dataset:  55%|█████▌    | 3695/6689 [00:05<00:06, 435.07 examples/s]Tokenizing train dataset:  53%|█████▎    | 3528/6689 [00:05<00:07, 433.01 examples/s]Tokenizing train dataset:  55%|█████▍    | 3674/6689 [00:05<00:07, 397.17 examples/s]Tokenizing train dataset:  56%|█████▌    | 3751/6689 [00:05<00:07, 412.45 examples/s]Tokenizing train dataset:  54%|█████▎    | 3593/6689 [00:05<00:07, 428.76 examples/s]Tokenizing train dataset:  56%|█████▌    | 3744/6689 [00:05<00:07, 411.47 examples/s]Tokenizing train dataset:  57%|█████▋    | 3813/6689 [00:06<00:07, 403.28 examples/s]Tokenizing train dataset:  55%|█████▍    | 3649/6689 [00:05<00:07, 405.60 examples/s]Tokenizing train dataset:  57%|█████▋    | 3804/6689 [00:06<00:07, 378.18 examples/s]Tokenizing train dataset:  58%|█████▊    | 3868/6689 [00:06<00:07, 359.83 examples/s]Tokenizing train dataset:  58%|█████▊    | 3848/6689 [00:06<00:07, 389.04 examples/s]Tokenizing train dataset:  55%|█████▌    | 3701/6689 [00:06<00:07, 381.17 examples/s]Tokenizing train dataset:  59%|█████▊    | 3914/6689 [00:06<00:07, 378.23 examples/s]Tokenizing train dataset:  58%|█████▊    | 3896/6689 [00:06<00:06, 406.06 examples/s]Tokenizing train dataset:  56%|█████▌    | 3751/6689 [00:06<00:07, 403.84 examples/s]Tokenizing train dataset:  59%|█████▉    | 3963/6689 [00:06<00:07, 355.86 examples/s]Tokenizing train dataset:  59%|█████▉    | 3950/6689 [00:06<00:07, 367.93 examples/s]Tokenizing train dataset:  57%|█████▋    | 3811/6689 [00:06<00:08, 355.85 examples/s]Tokenizing train dataset:  60%|█████▉    | 4013/6689 [00:06<00:07, 339.76 examples/s]Tokenizing train dataset:  58%|█████▊    | 3861/6689 [00:06<00:07, 384.88 examples/s]Tokenizing train dataset:  61%|██████    | 4052/6689 [00:06<00:07, 349.42 examples/s]Tokenizing train dataset:  60%|█████▉    | 4001/6689 [00:06<00:07, 354.46 examples/s]Tokenizing train dataset:  59%|█████▊    | 3929/6689 [00:06<00:06, 402.53 examples/s]Tokenizing train dataset:  61%|██████▏   | 4113/6689 [00:06<00:07, 365.08 examples/s]Tokenizing train dataset:  61%|██████    | 4052/6689 [00:06<00:07, 336.10 examples/s]Tokenizing train dataset:  61%|██████▏   | 4101/6689 [00:06<00:07, 365.48 examples/s]Tokenizing train dataset:  60%|█████▉    | 3987/6689 [00:06<00:06, 395.63 examples/s]Tokenizing train dataset:  63%|██████▎   | 4181/6689 [00:07<00:06, 390.40 examples/s]Tokenizing train dataset:  62%|██████▏   | 4151/6689 [00:07<00:06, 396.51 examples/s]Tokenizing train dataset:  60%|██████    | 4031/6689 [00:06<00:06, 399.69 examples/s]Tokenizing train dataset:  63%|██████▎   | 4235/6689 [00:07<00:05, 422.56 examples/s]Tokenizing train dataset:  63%|██████▎   | 4207/6689 [00:07<00:05, 434.33 examples/s]Tokenizing train dataset:  61%|██████    | 4090/6689 [00:07<00:06, 396.82 examples/s]Tokenizing train dataset:  64%|██████▍   | 4292/6689 [00:07<00:05, 407.75 examples/s]Tokenizing train dataset:  64%|██████▍   | 4274/6689 [00:07<00:05, 434.64 examples/s]Tokenizing train dataset:  62%|██████▏   | 4132/6689 [00:07<00:06, 398.40 examples/s]Tokenizing train dataset:  65%|██████▌   | 4355/6689 [00:07<00:05, 408.78 examples/s]Tokenizing train dataset:  65%|██████▍   | 4320/6689 [00:07<00:05, 431.01 examples/s]Tokenizing train dataset:  63%|██████▎   | 4183/6689 [00:07<00:05, 425.12 examples/s]Tokenizing train dataset:  65%|██████▌   | 4374/6689 [00:07<00:05, 454.62 examples/s]Tokenizing train dataset:  63%|██████▎   | 4232/6689 [00:07<00:05, 437.06 examples/s]Tokenizing train dataset:  66%|██████▌   | 4423/6689 [00:07<00:05, 420.41 examples/s]Tokenizing train dataset:  66%|██████▋   | 4434/6689 [00:07<00:05, 434.64 examples/s]Tokenizing train dataset:  64%|██████▍   | 4294/6689 [00:07<00:05, 424.67 examples/s]Tokenizing train dataset:  67%|██████▋   | 4485/6689 [00:07<00:05, 411.13 examples/s]Tokenizing train dataset:  67%|██████▋   | 4485/6689 [00:07<00:04, 448.79 examples/s]Tokenizing train dataset:  68%|██████▊   | 4537/6689 [00:07<00:04, 432.17 examples/s]Tokenizing train dataset:  65%|██████▌   | 4370/6689 [00:07<00:05, 449.57 examples/s]Tokenizing train dataset:  68%|██████▊   | 4544/6689 [00:07<00:05, 422.37 examples/s]Tokenizing train dataset:  69%|██████▉   | 4600/6689 [00:08<00:04, 420.39 examples/s]Tokenizing train dataset:  66%|██████▌   | 4430/6689 [00:07<00:05, 428.01 examples/s]Tokenizing train dataset:  69%|██████▊   | 4590/6689 [00:07<00:04, 427.76 examples/s]Tokenizing train dataset:  70%|██████▉   | 4670/6689 [00:08<00:04, 425.86 examples/s]Tokenizing train dataset:  67%|██████▋   | 4494/6689 [00:07<00:05, 424.29 examples/s]Tokenizing train dataset:  70%|██████▉   | 4660/6689 [00:08<00:04, 434.36 examples/s]Tokenizing train dataset:  71%|███████   | 4720/6689 [00:08<00:04, 441.20 examples/s]Tokenizing train dataset:  68%|██████▊   | 4538/6689 [00:08<00:05, 425.36 examples/s]Tokenizing train dataset:  70%|███████   | 4711/6689 [00:08<00:04, 449.98 examples/s]Tokenizing train dataset:  71%|███████▏  | 4775/6689 [00:08<00:04, 462.64 examples/s]Tokenizing train dataset:  69%|██████▊   | 4586/6689 [00:08<00:04, 434.33 examples/s]Tokenizing train dataset:  71%|███████▏  | 4766/6689 [00:08<00:04, 474.32 examples/s]Tokenizing train dataset:  72%|███████▏  | 4823/6689 [00:08<00:04, 465.97 examples/s]Tokenizing train dataset:  70%|██████▉   | 4652/6689 [00:08<00:04, 429.16 examples/s]Tokenizing train dataset:  72%|███████▏  | 4833/6689 [00:08<00:04, 457.61 examples/s]Tokenizing train dataset:  73%|███████▎  | 4882/6689 [00:08<00:04, 437.52 examples/s]Tokenizing train dataset:  73%|███████▎  | 4880/6689 [00:08<00:03, 454.76 examples/s]Tokenizing train dataset:  71%|███████   | 4722/6689 [00:08<00:04, 438.77 examples/s]Tokenizing train dataset:  74%|███████▎  | 4928/6689 [00:08<00:03, 442.12 examples/s]Tokenizing train dataset:  71%|███████▏  | 4776/6689 [00:08<00:04, 458.13 examples/s]Tokenizing train dataset:  74%|███████▍  | 4980/6689 [00:08<00:03, 459.79 examples/s]Tokenizing train dataset:  74%|███████▍  | 4944/6689 [00:08<00:03, 443.37 examples/s]Tokenizing train dataset:  75%|███████▍  | 4990/6689 [00:08<00:03, 444.01 examples/s]Tokenizing train dataset:  72%|███████▏  | 4845/6689 [00:08<00:04, 451.71 examples/s]Tokenizing train dataset:  75%|███████▌  | 5043/6689 [00:09<00:03, 441.32 examples/s]Tokenizing train dataset:  75%|███████▌  | 5039/6689 [00:08<00:03, 450.03 examples/s]Tokenizing train dataset:  73%|███████▎  | 4895/6689 [00:08<00:03, 456.32 examples/s]Tokenizing train dataset:  76%|███████▌  | 5092/6689 [00:09<00:03, 451.11 examples/s]Tokenizing train dataset:  76%|███████▋  | 5104/6689 [00:09<00:03, 437.52 examples/s]Tokenizing train dataset:  77%|███████▋  | 5165/6689 [00:09<00:03, 459.59 examples/s]Tokenizing train dataset:  74%|███████▍  | 4970/6689 [00:09<00:03, 462.21 examples/s]Tokenizing train dataset:  77%|███████▋  | 5154/6689 [00:09<00:03, 447.41 examples/s]Tokenizing train dataset:  78%|███████▊  | 5235/6689 [00:09<00:03, 460.83 examples/s]Tokenizing train dataset:  75%|███████▌  | 5036/6689 [00:09<00:03, 451.00 examples/s]Tokenizing train dataset:  78%|███████▊  | 5202/6689 [00:09<00:03, 453.51 examples/s]Tokenizing train dataset:  79%|███████▉  | 5282/6689 [00:09<00:03, 457.83 examples/s]Tokenizing train dataset:  76%|███████▌  | 5100/6689 [00:09<00:03, 433.46 examples/s]Tokenizing train dataset:  79%|███████▉  | 5270/6689 [00:09<00:03, 448.89 examples/s]Tokenizing train dataset:  80%|███████▉  | 5346/6689 [00:09<00:03, 442.72 examples/s]Tokenizing train dataset:  77%|███████▋  | 5150/6689 [00:09<00:03, 448.06 examples/s]Tokenizing train dataset:  80%|███████▉  | 5321/6689 [00:09<00:02, 459.88 examples/s]Tokenizing train dataset:  81%|████████  | 5399/6689 [00:09<00:03, 392.29 examples/s]Tokenizing train dataset:  78%|███████▊  | 5202/6689 [00:09<00:04, 338.21 examples/s]Tokenizing train dataset:  80%|████████  | 5370/6689 [00:09<00:03, 346.80 examples/s]Tokenizing train dataset:  82%|████████▏ | 5464/6689 [00:10<00:03, 400.74 examples/s]Tokenizing train dataset:  79%|███████▊  | 5251/6689 [00:09<00:03, 365.93 examples/s]Tokenizing train dataset:  81%|████████  | 5410/6689 [00:09<00:03, 348.36 examples/s]Tokenizing train dataset:  82%|████████▏ | 5515/6689 [00:10<00:02, 423.46 examples/s]Tokenizing train dataset:  79%|███████▉  | 5304/6689 [00:09<00:03, 401.73 examples/s]Tokenizing train dataset:  83%|████████▎ | 5561/6689 [00:10<00:02, 429.03 examples/s]Tokenizing train dataset:  82%|████████▏ | 5462/6689 [00:10<00:03, 344.46 examples/s]Tokenizing train dataset:  80%|████████  | 5374/6689 [00:10<00:03, 417.97 examples/s]Tokenizing train dataset:  82%|████████▏ | 5501/6689 [00:10<00:03, 353.71 examples/s]Tokenizing train dataset:  84%|████████▍ | 5610/6689 [00:10<00:02, 439.41 examples/s]Tokenizing train dataset:  85%|████████▍ | 5655/6689 [00:10<00:02, 441.58 examples/s]Tokenizing train dataset:  83%|████████▎ | 5550/6689 [00:10<00:02, 383.42 examples/s]Tokenizing train dataset:  81%|████████▏ | 5439/6689 [00:10<00:02, 420.24 examples/s]Tokenizing train dataset:  82%|████████▏ | 5485/6689 [00:10<00:02, 423.40 examples/s]Tokenizing train dataset:  86%|████████▌ | 5720/6689 [00:10<00:02, 437.33 examples/s]Tokenizing train dataset:  84%|████████▍ | 5607/6689 [00:10<00:02, 377.68 examples/s]Tokenizing train dataset:  83%|████████▎ | 5533/6689 [00:10<00:02, 434.69 examples/s]Tokenizing train dataset:  86%|████████▌ | 5765/6689 [00:10<00:02, 438.52 examples/s]Tokenizing train dataset:  84%|████████▍ | 5650/6689 [00:10<00:02, 383.03 examples/s]Tokenizing train dataset:  83%|████████▎ | 5578/6689 [00:10<00:02, 436.95 examples/s]Tokenizing train dataset:  87%|████████▋ | 5812/6689 [00:10<00:01, 445.30 examples/s]Tokenizing train dataset:  85%|████████▌ | 5701/6689 [00:10<00:02, 414.42 examples/s]Tokenizing train dataset:  84%|████████▍ | 5624/6689 [00:10<00:02, 441.84 examples/s]Tokenizing train dataset:  88%|████████▊ | 5874/6689 [00:11<00:01, 431.13 examples/s]Tokenizing train dataset:  86%|████████▌ | 5768/6689 [00:10<00:02, 423.26 examples/s]Tokenizing train dataset:  85%|████████▌ | 5690/6689 [00:10<00:02, 439.26 examples/s]Tokenizing train dataset:  88%|████████▊ | 5918/6689 [00:11<00:01, 431.72 examples/s]Tokenizing train dataset:  87%|████████▋ | 5833/6689 [00:10<00:02, 422.34 examples/s]Tokenizing train dataset:  89%|████████▉ | 5967/6689 [00:11<00:01, 438.44 examples/s]Tokenizing train dataset:  86%|████████▌ | 5755/6689 [00:10<00:02, 434.39 examples/s]Tokenizing train dataset:  88%|████████▊ | 5885/6689 [00:11<00:02, 392.78 examples/s]Tokenizing train dataset:  87%|████████▋ | 5803/6689 [00:11<00:01, 444.93 examples/s]Tokenizing train dataset:  90%|█████████ | 6030/6689 [00:11<00:01, 425.75 examples/s]Tokenizing train dataset:  89%|████████▊ | 5934/6689 [00:11<00:01, 412.47 examples/s]Tokenizing train dataset:  91%|█████████ | 6079/6689 [00:11<00:01, 435.90 examples/s]Tokenizing train dataset:  88%|████████▊ | 5868/6689 [00:11<00:01, 436.23 examples/s]Tokenizing train dataset:  92%|█████████▏| 6131/6689 [00:11<00:01, 452.26 examples/s]Tokenizing train dataset:  90%|████████▉ | 5994/6689 [00:11<00:01, 403.63 examples/s]Tokenizing train dataset:  89%|████████▉ | 5937/6689 [00:11<00:01, 438.93 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6689 [00:11<00:01, 439.50 examples/s]Tokenizing train dataset:  90%|█████████ | 6041/6689 [00:11<00:01, 365.90 examples/s]Tokenizing train dataset:  90%|████████▉ | 5990/6689 [00:11<00:01, 454.43 examples/s]Tokenizing train dataset:  93%|█████████▎| 6240/6689 [00:11<00:01, 440.02 examples/s]Tokenizing train dataset:  91%|█████████ | 6084/6689 [00:11<00:01, 377.23 examples/s]Tokenizing train dataset:  91%|█████████ | 6060/6689 [00:11<00:01, 451.53 examples/s]Tokenizing train dataset:  94%|█████████▍| 6287/6689 [00:11<00:00, 444.88 examples/s]Tokenizing train dataset:  92%|█████████▏| 6141/6689 [00:11<00:01, 417.59 examples/s]Tokenizing train dataset:  93%|█████████▎| 6200/6689 [00:11<00:01, 458.52 examples/s]Tokenizing train dataset:  92%|█████████▏| 6135/6689 [00:11<00:01, 462.82 examples/s]Tokenizing train dataset:  95%|█████████▌| 6355/6689 [00:12<00:00, 443.56 examples/s]Tokenizing train dataset:  94%|█████████▎| 6266/6689 [00:12<00:00, 450.09 examples/s]Tokenizing train dataset:  93%|█████████▎| 6201/6689 [00:11<00:01, 451.78 examples/s]Tokenizing train dataset:  96%|█████████▌| 6406/6689 [00:12<00:00, 404.60 examples/s]Tokenizing train dataset:  94%|█████████▍| 6317/6689 [00:12<00:00, 407.07 examples/s]Tokenizing train dataset:  94%|█████████▎| 6270/6689 [00:12<00:00, 451.31 examples/s]Tokenizing train dataset:  97%|█████████▋| 6468/6689 [00:12<00:00, 403.07 examples/s]Tokenizing train dataset:  95%|█████████▌| 6360/6689 [00:12<00:00, 411.49 examples/s]Tokenizing train dataset:  95%|█████████▍| 6333/6689 [00:12<00:00, 439.18 examples/s]Tokenizing train dataset:  98%|█████████▊| 6535/6689 [00:12<00:00, 412.60 examples/s]Tokenizing train dataset:  96%|█████████▌| 6403/6689 [00:12<00:00, 411.22 examples/s]Tokenizing train dataset:  98%|█████████▊| 6579/6689 [00:12<00:00, 412.13 examples/s]Tokenizing train dataset:  96%|█████████▌| 6399/6689 [00:12<00:00, 436.99 examples/s]Tokenizing train dataset:  97%|█████████▋| 6471/6689 [00:12<00:00, 424.35 examples/s]Tokenizing train dataset:  99%|█████████▉| 6630/6689 [00:12<00:00, 431.07 examples/s]Tokenizing train dataset:  97%|█████████▋| 6516/6689 [00:12<00:00, 427.73 examples/s]Tokenizing train dataset:  97%|█████████▋| 6462/6689 [00:12<00:00, 430.75 examples/s]Tokenizing train dataset: 100%|█████████▉| 6675/6689 [00:12<00:00, 432.70 examples/s]Tokenizing train dataset:  98%|█████████▊| 6568/6689 [00:12<00:00, 448.31 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:12<00:00, 516.81 examples/s]
Tokenizing train dataset:  98%|█████████▊| 6530/6689 [00:12<00:00, 435.95 examples/s]Tokenizing train dataset:  99%|█████████▉| 6621/6689 [00:12<00:00, 464.63 examples/s]Tokenizing train dataset:  98%|█████████▊| 6579/6689 [00:12<00:00, 443.58 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:12<00:00, 458.82 examples/s]Tokenizing train dataset:  99%|█████████▉| 6629/6689 [00:12<00:00, 456.12 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:13<00:00, 513.19 examples/s]
Tokenizing train dataset: 100%|██████████| 6689/6689 [00:13<00:00, 435.22 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:13<00:00, 511.75 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5553.29 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5557.81 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5469.61 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5335.62 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5184.29 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 4978.01 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  27%|██▋       | 257/953 [00:00<00:00, 2533.72 examples/s]Applying chat template to eval dataset:  26%|██▋       | 251/953 [00:00<00:00, 2480.27 examples/s]Applying chat template to eval dataset:  33%|███▎      | 311/953 [00:00<00:00, 3079.05 examples/s]Applying chat template to eval dataset:  53%|█████▎    | 509/953 [00:00<00:00, 2536.06 examples/s]Applying chat template to eval dataset:  65%|██████▌   | 620/953 [00:00<00:00, 3072.82 examples/s]Applying chat template to eval dataset:  68%|██████▊   | 650/953 [00:00<00:00, 2573.76 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3017.17 examples/s]Applying chat template to eval dataset:  94%|█████████▍| 900/953 [00:00<00:00, 2564.41 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2469.80 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2641.94 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2172.25 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2099.25 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 32/953 [00:00<00:03, 306.81 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 258.02 examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 259.71 examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 244.75 examples/s]Tokenizing eval dataset:   6%|▌         | 53/953 [00:00<00:03, 242.06 examples/s]Tokenizing eval dataset:  10%|▉         | 95/953 [00:00<00:03, 254.66 examples/s]Tokenizing eval dataset:   6%|▋         | 60/953 [00:00<00:03, 228.40 examples/s]Tokenizing eval dataset:   9%|▉         | 86/953 [00:00<00:03, 235.94 examples/s]Tokenizing eval dataset:   9%|▉         | 87/953 [00:00<00:03, 226.50 examples/s]Tokenizing eval dataset:  14%|█▍        | 134/953 [00:00<00:03, 250.34 examples/s]Tokenizing eval dataset:  12%|█▏        | 116/953 [00:00<00:03, 216.99 examples/s]Tokenizing eval dataset:  12%|█▏        | 117/953 [00:00<00:03, 210.11 examples/s]Tokenizing eval dataset:  18%|█▊        | 169/953 [00:00<00:03, 240.65 examples/s]Tokenizing eval dataset:  15%|█▍        | 139/953 [00:00<00:04, 189.45 examples/s]Tokenizing eval dataset:  15%|█▍        | 142/953 [00:00<00:04, 172.11 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 200.63 examples/s]Tokenizing eval dataset:  17%|█▋        | 166/953 [00:00<00:04, 183.29 examples/s]Tokenizing eval dataset:  17%|█▋        | 162/953 [00:00<00:05, 154.68 examples/s]Tokenizing eval dataset:  24%|██▍       | 230/953 [00:01<00:03, 199.65 examples/s]Tokenizing eval dataset:  20%|█▉        | 186/953 [00:00<00:04, 165.40 examples/s]Tokenizing eval dataset:  19%|█▉        | 180/953 [00:01<00:04, 156.06 examples/s]Tokenizing eval dataset:  29%|██▉       | 280/953 [00:01<00:02, 264.77 examples/s]Tokenizing eval dataset:  22%|██▏       | 206/953 [00:01<00:04, 172.07 examples/s]Tokenizing eval dataset:  21%|██▏       | 203/953 [00:01<00:04, 168.32 examples/s]Tokenizing eval dataset:  35%|███▍      | 332/953 [00:01<00:01, 323.50 examples/s]Tokenizing eval dataset:  24%|██▍       | 229/953 [00:01<00:03, 185.45 examples/s]Tokenizing eval dataset:  24%|██▍       | 233/953 [00:01<00:03, 198.51 examples/s]Tokenizing eval dataset:  41%|████      | 391/953 [00:01<00:01, 384.86 examples/s]Tokenizing eval dataset:  28%|██▊       | 271/953 [00:01<00:02, 245.71 examples/s]Tokenizing eval dataset:  47%|████▋     | 452/953 [00:01<00:01, 442.53 examples/s]Tokenizing eval dataset:  29%|██▉       | 280/953 [00:01<00:02, 265.31 examples/s]Tokenizing eval dataset:  34%|███▍      | 322/953 [00:01<00:01, 315.79 examples/s]Tokenizing eval dataset:  34%|███▍      | 328/953 [00:01<00:01, 321.76 examples/s]Tokenizing eval dataset:  53%|█████▎    | 508/953 [00:01<00:00, 470.16 examples/s]Tokenizing eval dataset:  39%|███▉      | 370/953 [00:01<00:01, 360.27 examples/s]Tokenizing eval dataset:  40%|████      | 384/953 [00:01<00:01, 385.65 examples/s]Tokenizing eval dataset:  60%|██████    | 575/953 [00:01<00:00, 455.95 examples/s]Tokenizing eval dataset:  45%|████▍     | 426/953 [00:01<00:01, 415.71 examples/s]Tokenizing eval dataset:  47%|████▋     | 444/953 [00:01<00:01, 444.82 examples/s]Tokenizing eval dataset:  51%|█████     | 482/953 [00:01<00:01, 455.03 examples/s]Tokenizing eval dataset:  52%|█████▏    | 494/953 [00:01<00:01, 457.24 examples/s]Tokenizing eval dataset:  68%|██████▊   | 646/953 [00:01<00:00, 454.14 examples/s]Tokenizing eval dataset:  57%|█████▋    | 543/953 [00:01<00:00, 462.64 examples/s]Tokenizing eval dataset:  59%|█████▉    | 560/953 [00:01<00:00, 479.31 examples/s]Tokenizing eval dataset:  74%|███████▎  | 701/953 [00:02<00:00, 421.21 examples/s]Tokenizing eval dataset:  63%|██████▎   | 602/953 [00:01<00:00, 491.63 examples/s]Tokenizing eval dataset:  64%|██████▍   | 609/953 [00:01<00:00, 479.86 examples/s]Tokenizing eval dataset:  80%|████████  | 764/953 [00:02<00:00, 416.97 examples/s]Tokenizing eval dataset:  71%|███████   | 674/953 [00:02<00:00, 482.30 examples/s]Tokenizing eval dataset:  72%|███████▏  | 685/953 [00:02<00:00, 485.28 examples/s]Tokenizing eval dataset:  86%|████████▌ | 816/953 [00:02<00:00, 392.06 examples/s]Tokenizing eval dataset:  78%|███████▊  | 740/953 [00:02<00:00, 464.91 examples/s]Tokenizing eval dataset:  79%|███████▊  | 749/953 [00:02<00:00, 461.66 examples/s]Tokenizing eval dataset:  91%|█████████ | 867/953 [00:02<00:00, 372.09 examples/s]Tokenizing eval dataset:  84%|████████▎ | 796/953 [00:02<00:00, 461.85 examples/s]Tokenizing eval dataset:  85%|████████▍ | 808/953 [00:02<00:00, 459.48 examples/s]Tokenizing eval dataset:  95%|█████████▌| 907/953 [00:02<00:00, 374.22 examples/s]Tokenizing eval dataset:  90%|█████████ | 860/953 [00:02<00:00, 445.06 examples/s]Tokenizing eval dataset:  91%|█████████ | 869/953 [00:02<00:00, 440.35 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 368.23 examples/s]Tokenizing eval dataset:  95%|█████████▍| 905/953 [00:02<00:00, 444.39 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 345.35 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  98%|█████████▊| 932/953 [00:02<00:00, 429.33 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 438.23 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 347.49 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 343.23 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5539050102233887 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.6332991123199463 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combinationUsing /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...

Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5219309329986572 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5866878032684326 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank12]:[W609 02:50:30.442573512 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 3 ---
