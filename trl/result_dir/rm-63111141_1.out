cpu-bind=MASK - gn02, task  1  0 [2359566]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 1 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 1     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63111141     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.1 --curriculum_stage=1
-------------------------------------------
[2025-06-12 18:27:01,473] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 18:27:02.845000 2359615 torch/distributed/run.py:792] 
W0612 18:27:02.845000 2359615 torch/distributed/run.py:792] *****************************************
W0612 18:27:02.845000 2359615 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 18:27:02.845000 2359615 torch/distributed/run.py:792] *****************************************
[2025-06-12 18:27:57,602] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 18:27:57,650] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 18:27:57,661] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 18:27:57,669] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.1, curriculum_stage=1)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.1, curriculum_stage=1)
4e-07
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.1, curriculum_stage=1)
4e-07
World size: 12
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.1, curriculum_stage=1)
4e-07
[2025-06-12 18:28:02,187] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 18:28:02,206] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 6689
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-12 18:28:02,214] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 18:28:02,216] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curriculum-0
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:10, 23.40s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.71s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.71s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:23<01:11, 23.71s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:47, 23.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:47, 23.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:47, 23.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:47<00:47, 23.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:10<00:23, 23.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:11<00:23, 23.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:11<00:23, 23.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:11<00:23, 23.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 21.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.33s/it]
Loaded model
Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 21.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 21.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 21.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.39s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.39s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:29<00:00, 22.39s/it]
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank6]:[W612 18:29:35.459418383 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W612 18:29:35.482801683 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W612 18:29:35.489299305 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▊         | 576/6689 [00:00<00:01, 5719.33 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1160/6689 [00:00<00:00, 5761.17 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1741/6689 [00:00<00:00, 5782.27 examples/s]Extracting prompt in train dataset:  35%|███▍      | 2320/6689 [00:00<00:00, 5774.82 examples/s]Extracting prompt in train dataset:  43%|████▎     | 2900/6689 [00:00<00:00, 5776.79 examples/s]Extracting prompt in train dataset:  52%|█████▏    | 3480/6689 [00:00<00:00, 5779.34 examples/s]Extracting prompt in train dataset:  65%|██████▍   | 4330/6689 [00:00<00:00, 5714.62 examples/s]Extracting prompt in train dataset:  73%|███████▎  | 4910/6689 [00:00<00:00, 5716.46 examples/s]Extracting prompt in train dataset:  82%|████████▏ | 5490/6689 [00:00<00:00, 5716.95 examples/s]Extracting prompt in train dataset:  91%|█████████ | 6070/6689 [00:01<00:00, 5713.72 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5611.72 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5633.18 examples/s]
Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   5%|▍         | 306/6689 [00:00<00:02, 3024.20 examples/s]Applying chat template to train dataset:  10%|▉         | 636/6689 [00:00<00:01, 3180.86 examples/s]Applying chat template to train dataset:  14%|█▍        | 960/6689 [00:00<00:01, 3188.14 examples/s]Applying chat template to train dataset:  21%|██▏       | 1433/6689 [00:00<00:01, 3166.31 examples/s]Applying chat template to train dataset:  26%|██▋       | 1760/6689 [00:00<00:01, 3194.37 examples/s]Applying chat template to train dataset:  31%|███       | 2089/6689 [00:00<00:01, 3220.63 examples/s]Applying chat template to train dataset:  36%|███▌      | 2413/6689 [00:00<00:01, 3220.94 examples/s]Applying chat template to train dataset:  41%|████      | 2738/6689 [00:00<00:01, 3227.92 examples/s]Applying chat template to train dataset:  46%|████▌     | 3063/6689 [00:00<00:01, 3230.54 examples/s]Applying chat template to train dataset:  51%|█████     | 3390/6689 [00:01<00:01, 3235.28 examples/s]Applying chat template to train dataset:  58%|█████▊    | 3873/6689 [00:01<00:00, 3225.12 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4198/6689 [00:01<00:00, 3229.01 examples/s]Applying chat template to train dataset:  68%|██████▊   | 4523/6689 [00:01<00:00, 3232.21 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4848/6689 [00:01<00:00, 3235.96 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5173/6689 [00:01<00:00, 3234.53 examples/s]Applying chat template to train dataset:  82%|████████▏ | 5499/6689 [00:01<00:00, 3239.02 examples/s]Applying chat template to train dataset:  89%|████████▉ | 5985/6689 [00:01<00:00, 3235.05 examples/s]Applying chat template to train dataset:  96%|█████████▋| 6451/6689 [00:02<00:00, 3185.86 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3203.85 examples/s]
Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   2%|▏         | 103/6689 [00:00<00:06, 1016.49 examples/s]Tokenizing train dataset:   3%|▎         | 220/6689 [00:00<00:05, 1099.17 examples/s]Tokenizing train dataset:   5%|▌         | 335/6689 [00:00<00:05, 1110.66 examples/s]Tokenizing train dataset:   7%|▋         | 487/6689 [00:00<00:05, 1057.38 examples/s]Tokenizing train dataset:   9%|▉         | 594/6689 [00:00<00:05, 1053.39 examples/s]Tokenizing train dataset:  11%|█         | 732/6689 [00:00<00:05, 994.01 examples/s] Tokenizing train dataset:  13%|█▎        | 873/6689 [00:00<00:06, 961.77 examples/s]Tokenizing train dataset:  15%|█▍        | 1003/6689 [00:01<00:06, 923.51 examples/s]Tokenizing train dataset:  17%|█▋        | 1131/6689 [00:01<00:06, 887.70 examples/s]Tokenizing train dataset:  19%|█▊        | 1246/6689 [00:01<00:06, 846.96 examples/s]Tokenizing train dataset:  20%|██        | 1358/6689 [00:01<00:06, 812.09 examples/s]Tokenizing train dataset:  22%|██▏       | 1465/6689 [00:01<00:06, 775.54 examples/s]Tokenizing train dataset:  23%|██▎       | 1569/6689 [00:01<00:06, 745.60 examples/s]Tokenizing train dataset:  25%|██▌       | 1674/6689 [00:01<00:06, 729.74 examples/s]Tokenizing train dataset:  27%|██▋       | 1775/6689 [00:02<00:06, 708.70 examples/s]Tokenizing train dataset:  28%|██▊       | 1878/6689 [00:02<00:06, 693.44 examples/s]Tokenizing train dataset:  29%|██▉       | 1953/6689 [00:02<00:06, 699.98 examples/s]Tokenizing train dataset:  31%|███       | 2055/6689 [00:02<00:06, 685.64 examples/s]Tokenizing train dataset:  32%|███▏      | 2128/6689 [00:02<00:06, 693.12 examples/s]Tokenizing train dataset:  33%|███▎      | 2220/6689 [00:02<00:06, 664.41 examples/s]Tokenizing train dataset:  35%|███▍      | 2316/6689 [00:02<00:06, 652.70 examples/s]Tokenizing train dataset:  36%|███▌      | 2401/6689 [00:03<00:06, 620.95 examples/s]Tokenizing train dataset:  37%|███▋      | 2495/6689 [00:03<00:06, 616.40 examples/s]Tokenizing train dataset:  39%|███▉      | 2592/6689 [00:03<00:06, 623.11 examples/s]Tokenizing train dataset:  40%|███▉      | 2659/6689 [00:03<00:06, 631.12 examples/s]Tokenizing train dataset:  41%|████      | 2740/6689 [00:03<00:06, 596.53 examples/s]Tokenizing train dataset:  42%|████▏     | 2830/6689 [00:03<00:06, 593.18 examples/s]Tokenizing train dataset:  43%|████▎     | 2890/6689 [00:03<00:06, 589.89 examples/s]Tokenizing train dataset:  44%|████▍     | 2951/6689 [00:03<00:06, 587.48 examples/s]Tokenizing train dataset:  45%|████▌     | 3030/6689 [00:04<00:06, 563.76 examples/s]Tokenizing train dataset:  46%|████▌     | 3087/6689 [00:04<00:06, 559.92 examples/s]Tokenizing train dataset:  47%|████▋     | 3148/6689 [00:04<00:06, 570.69 examples/s]Tokenizing train dataset:  48%|████▊     | 3206/6689 [00:04<00:06, 567.57 examples/s]Tokenizing train dataset:  49%|████▉     | 3284/6689 [00:04<00:06, 546.37 examples/s]Tokenizing train dataset:  50%|█████     | 3367/6689 [00:04<00:06, 545.58 examples/s]Tokenizing train dataset:  51%|█████▏    | 3429/6689 [00:04<00:05, 560.48 examples/s]Tokenizing train dataset:  52%|█████▏    | 3490/6689 [00:04<00:05, 566.86 examples/s]Tokenizing train dataset:  53%|█████▎    | 3548/6689 [00:05<00:05, 569.22 examples/s]Tokenizing train dataset:  54%|█████▍    | 3620/6689 [00:05<00:05, 532.62 examples/s]Tokenizing train dataset:  55%|█████▍    | 3677/6689 [00:05<00:05, 540.22 examples/s]Tokenizing train dataset:  56%|█████▌    | 3751/6689 [00:05<00:05, 519.20 examples/s]Tokenizing train dataset:  57%|█████▋    | 3810/6689 [00:05<00:05, 531.69 examples/s]Tokenizing train dataset:  58%|█████▊    | 3885/6689 [00:05<00:05, 517.66 examples/s]Tokenizing train dataset:  59%|█████▉    | 3940/6689 [00:05<00:05, 520.68 examples/s]Tokenizing train dataset:  60%|█████▉    | 4010/6689 [00:05<00:05, 494.84 examples/s]Tokenizing train dataset:  61%|██████    | 4090/6689 [00:06<00:05, 502.90 examples/s]Tokenizing train dataset:  62%|██████▏   | 4145/6689 [00:06<00:04, 509.35 examples/s]Tokenizing train dataset:  63%|██████▎   | 4205/6689 [00:06<00:04, 527.99 examples/s]Tokenizing train dataset:  64%|██████▎   | 4260/6689 [00:06<00:04, 530.93 examples/s]Tokenizing train dataset:  65%|██████▍   | 4333/6689 [00:06<00:04, 507.18 examples/s]Tokenizing train dataset:  66%|██████▌   | 4390/6689 [00:06<00:04, 519.92 examples/s]Tokenizing train dataset:  67%|██████▋   | 4449/6689 [00:06<00:04, 534.13 examples/s]Tokenizing train dataset:  68%|██████▊   | 4526/6689 [00:06<00:04, 524.93 examples/s]Tokenizing train dataset:  69%|██████▊   | 4596/6689 [00:07<00:04, 499.83 examples/s]Tokenizing train dataset:  70%|██████▉   | 4670/6689 [00:07<00:04, 493.03 examples/s]Tokenizing train dataset:  71%|███████   | 4727/6689 [00:07<00:03, 509.41 examples/s]Tokenizing train dataset:  72%|███████▏  | 4783/6689 [00:07<00:03, 521.74 examples/s]Tokenizing train dataset:  73%|███████▎  | 4857/6689 [00:07<00:03, 511.16 examples/s]Tokenizing train dataset:  74%|███████▎  | 4930/6689 [00:07<00:03, 499.44 examples/s]Tokenizing train dataset:  75%|███████▍  | 4987/6689 [00:07<00:03, 512.33 examples/s]Tokenizing train dataset:  75%|███████▌  | 5039/6689 [00:07<00:03, 507.61 examples/s]Tokenizing train dataset:  76%|███████▋  | 5108/6689 [00:08<00:03, 487.46 examples/s]Tokenizing train dataset:  77%|███████▋  | 5163/6689 [00:08<00:03, 499.86 examples/s]Tokenizing train dataset:  78%|███████▊  | 5216/6689 [00:08<00:02, 501.68 examples/s]Tokenizing train dataset:  79%|███████▉  | 5274/6689 [00:08<00:02, 516.25 examples/s]Tokenizing train dataset:  80%|███████▉  | 5328/6689 [00:08<00:02, 520.36 examples/s]Tokenizing train dataset:  81%|████████  | 5404/6689 [00:08<00:02, 506.84 examples/s]Tokenizing train dataset:  82%|████████▏ | 5469/6689 [00:08<00:02, 479.90 examples/s]Tokenizing train dataset:  83%|████████▎ | 5524/6689 [00:08<00:02, 489.55 examples/s]Tokenizing train dataset:  83%|████████▎ | 5576/6689 [00:09<00:02, 493.83 examples/s]Tokenizing train dataset:  84%|████████▍ | 5630/6689 [00:09<00:02, 501.90 examples/s]Tokenizing train dataset:  85%|████████▌ | 5705/6689 [00:09<00:01, 496.58 examples/s]Tokenizing train dataset:  86%|████████▋ | 5781/6689 [00:09<00:01, 498.34 examples/s]Tokenizing train dataset:  87%|████████▋ | 5851/6689 [00:09<00:01, 481.87 examples/s]Tokenizing train dataset:  89%|████████▊ | 5921/6689 [00:09<00:01, 475.13 examples/s]Tokenizing train dataset:  89%|████████▉ | 5976/6689 [00:09<00:01, 488.47 examples/s]Tokenizing train dataset:  90%|█████████ | 6049/6689 [00:10<00:01, 483.91 examples/s]Tokenizing train dataset:  91%|█████████ | 6100/6689 [00:10<00:01, 484.34 examples/s]Tokenizing train dataset:  92%|█████████▏| 6163/6689 [00:10<00:01, 518.95 examples/s]Tokenizing train dataset:  93%|█████████▎| 6223/6689 [00:10<00:00, 530.78 examples/s]Tokenizing train dataset:  94%|█████████▍| 6280/6689 [00:10<00:00, 536.33 examples/s]Tokenizing train dataset:  95%|█████████▍| 6351/6689 [00:10<00:00, 509.88 examples/s]Tokenizing train dataset:  96%|█████████▌| 6414/6689 [00:10<00:00, 473.02 examples/s]Tokenizing train dataset:  97%|█████████▋| 6488/6689 [00:10<00:00, 477.98 examples/s]Tokenizing train dataset:  98%|█████████▊| 6540/6689 [00:11<00:00, 483.24 examples/s]Tokenizing train dataset:  99%|█████████▊| 6590/6689 [00:11<00:00, 479.34 examples/s]Tokenizing train dataset:  99%|█████████▉| 6646/6689 [00:11<00:00, 496.54 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 589.92 examples/s]
[rank4]:[W612 18:29:51.092442713 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▊         | 577/6689 [00:00<00:01, 5731.28 examples/s]Extracting prompt in train dataset:   9%|▊         | 580/6689 [00:00<00:01, 5739.26 examples/s]Extracting prompt in train dataset:   9%|▉         | 590/6689 [00:00<00:01, 5798.06 examples/s]Extracting prompt in eval dataset:  61%|██████    | 580/953 [00:00<00:00, 5681.31 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5667.83 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1170/6689 [00:00<00:00, 5824.77 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1160/6689 [00:00<00:00, 5765.57 examples/s]Extracting prompt in train dataset:  18%|█▊        | 1190/6689 [00:00<00:00, 5882.46 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1743/6689 [00:00<00:00, 5790.38 examples/s]Extracting prompt in train dataset:  26%|██▋       | 1770/6689 [00:00<00:00, 5863.33 examples/s]Extracting prompt in train dataset:  27%|██▋       | 1790/6689 [00:00<00:00, 5904.48 examples/s]Extracting prompt in train dataset:  35%|███▌      | 2360/6689 [00:00<00:00, 5855.10 examples/s]Extracting prompt in train dataset:  36%|███▌      | 2388/6689 [00:00<00:00, 5916.35 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2611/6689 [00:00<00:00, 5780.31 examples/s]Extracting prompt in train dataset:  45%|████▍     | 2980/6689 [00:00<00:00, 5902.19 examples/s]Extracting prompt in train dataset:  44%|████▍     | 2960/6689 [00:00<00:00, 5866.05 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  48%|████▊     | 3191/6689 [00:00<00:00, 5784.49 examples/s]Extracting prompt in train dataset:  53%|█████▎    | 3550/6689 [00:00<00:00, 5859.90 examples/s]Extracting prompt in train dataset:  54%|█████▎    | 3580/6689 [00:00<00:00, 5903.65 examples/s]Applying chat template to eval dataset:  33%|███▎      | 314/953 [00:00<00:00, 3108.17 examples/s]Extracting prompt in train dataset:  60%|██████    | 4040/6689 [00:00<00:00, 5728.43 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 640/953 [00:00<00:00, 3190.44 examples/s]Extracting prompt in train dataset:  67%|██████▋   | 4450/6689 [00:00<00:00, 5846.79 examples/s]Extracting prompt in train dataset:  66%|██████▌   | 4414/6689 [00:00<00:00, 5801.02 examples/s]Extracting prompt in train dataset:  69%|██████▉   | 4620/6689 [00:00<00:00, 5729.05 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3177.63 examples/s]
Extracting prompt in train dataset:  75%|███████▌  | 5040/6689 [00:00<00:00, 5845.38 examples/s]Extracting prompt in train dataset:  78%|███████▊  | 5200/6689 [00:00<00:00, 5726.61 examples/s]Extracting prompt in train dataset:  79%|███████▉  | 5280/6689 [00:00<00:00, 5788.07 examples/s]Extracting prompt in train dataset:  84%|████████▍ | 5630/6689 [00:00<00:00, 5850.05 examples/s]Extracting prompt in train dataset:  86%|████████▋ | 5780/6689 [00:01<00:00, 5727.16 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 5860/6689 [00:01<00:00, 5788.01 examples/s]Extracting prompt in train dataset:  93%|█████████▎| 6220/6689 [00:01<00:00, 5852.10 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5796.58 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5709.68 examples/s]Extracting prompt in train dataset:  99%|█████████▉| 6610/6689 [00:01<00:00, 5641.08 examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5742.20 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset: 100%|██████████| 6689/6689 [00:01<00:00, 5672.54 examples/s]
Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 326.23 examples/s]Tokenizing eval dataset:   8%|▊         | 79/953 [00:00<00:02, 295.62 examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:03, 277.23 examples/s]Applying chat template to train dataset:   4%|▍         | 295/6689 [00:00<00:02, 2922.69 examples/s]Applying chat template to train dataset:   5%|▍         | 303/6689 [00:00<00:02, 2992.24 examples/s]Applying chat template to train dataset:   4%|▍         | 299/6689 [00:00<00:02, 2959.06 examples/s]Applying chat template to train dataset:   9%|▉         | 616/6689 [00:00<00:01, 3089.11 examples/s]Applying chat template to train dataset:   9%|▉         | 630/6689 [00:00<00:01, 3146.82 examples/s]Applying chat template to train dataset:   9%|▉         | 624/6689 [00:00<00:01, 3124.39 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:02, 268.74 examples/s]Applying chat template to train dataset:  14%|█▍        | 937/6689 [00:00<00:01, 3140.70 examples/s]Applying chat template to train dataset:  14%|█▍        | 955/6689 [00:00<00:01, 3192.32 examples/s]Applying chat template to train dataset:  14%|█▍        | 949/6689 [00:00<00:01, 3178.69 examples/s]Applying chat template to train dataset:  19%|█▉        | 1257/6689 [00:00<00:01, 3161.51 examples/s]Tokenizing eval dataset:  21%|██        | 198/953 [00:00<00:02, 260.32 examples/s]Applying chat template to train dataset:  19%|█▉        | 1280/6689 [00:00<00:01, 3209.82 examples/s]Applying chat template to train dataset:  19%|█▉        | 1273/6689 [00:00<00:01, 3198.54 examples/s]Applying chat template to train dataset:  24%|██▎       | 1575/6689 [00:00<00:01, 3166.11 examples/s]Tokenizing eval dataset:  25%|██▍       | 238/953 [00:00<00:02, 292.42 examples/s]Applying chat template to train dataset:  24%|██▍       | 1603/6689 [00:00<00:01, 3216.59 examples/s]Applying chat template to train dataset:  24%|██▍       | 1596/6689 [00:00<00:01, 3207.19 examples/s]Applying chat template to train dataset:  28%|██▊       | 1893/6689 [00:00<00:01, 3168.11 examples/s]Tokenizing eval dataset:  32%|███▏      | 304/953 [00:00<00:01, 386.61 examples/s]Applying chat template to train dataset:  29%|██▉       | 1927/6689 [00:00<00:01, 3222.34 examples/s]Applying chat template to train dataset:  29%|██▊       | 1919/6689 [00:00<00:01, 3211.96 examples/s]Applying chat template to train dataset:  33%|███▎      | 2212/6689 [00:00<00:01, 3172.44 examples/s]Tokenizing eval dataset:  39%|███▊      | 368/953 [00:01<00:01, 453.51 examples/s]Applying chat template to train dataset:  34%|███▎      | 2250/6689 [00:00<00:01, 3215.39 examples/s]Applying chat template to train dataset:  34%|███▎      | 2242/6689 [00:00<00:01, 3212.38 examples/s]Applying chat template to train dataset:  38%|███▊      | 2530/6689 [00:00<00:01, 3165.07 examples/s]Tokenizing eval dataset:  46%|████▌     | 437/953 [00:01<00:01, 515.63 examples/s]Applying chat template to train dataset:  38%|███▊      | 2574/6689 [00:00<00:01, 3220.31 examples/s]Applying chat template to train dataset:  41%|████      | 2723/6689 [00:00<00:01, 3205.18 examples/s]Applying chat template to train dataset:  43%|████▎     | 2847/6689 [00:00<00:01, 3166.02 examples/s]Tokenizing eval dataset:  53%|█████▎    | 504/953 [00:01<00:00, 557.28 examples/s]Applying chat template to train dataset:  43%|████▎     | 2897/6689 [00:00<00:01, 3220.07 examples/s]Applying chat template to train dataset:  46%|████▌     | 3044/6689 [00:00<00:01, 3203.09 examples/s]Applying chat template to train dataset:  47%|████▋     | 3164/6689 [00:01<00:01, 3164.62 examples/s]Tokenizing eval dataset:  60%|█████▉    | 568/953 [00:01<00:00, 578.01 examples/s]Applying chat template to train dataset:  51%|█████     | 3379/6689 [00:01<00:01, 3215.52 examples/s]Applying chat template to train dataset:  50%|█████     | 3365/6689 [00:01<00:01, 3202.54 examples/s]Applying chat template to train dataset:  52%|█████▏    | 3481/6689 [00:01<00:01, 3164.01 examples/s]Tokenizing eval dataset:  67%|██████▋   | 637/953 [00:01<00:00, 606.90 examples/s]Applying chat template to train dataset:  55%|█████▌    | 3686/6689 [00:01<00:00, 3200.55 examples/s]Applying chat template to train dataset:  58%|█████▊    | 3856/6689 [00:01<00:00, 3197.14 examples/s]Applying chat template to train dataset:  59%|█████▉    | 3951/6689 [00:01<00:00, 3144.44 examples/s]Tokenizing eval dataset:  76%|███████▌  | 726/953 [00:01<00:00, 592.49 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4181/6689 [00:01<00:00, 3208.83 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4151/6689 [00:01<00:00, 3158.53 examples/s]Applying chat template to train dataset:  64%|██████▍   | 4270/6689 [00:01<00:00, 3152.85 examples/s]Applying chat template to train dataset:  67%|██████▋   | 4506/6689 [00:01<00:00, 3217.49 examples/s]Tokenizing eval dataset:  84%|████████▍ | 803/953 [00:01<00:00, 560.93 examples/s]Applying chat template to train dataset:  69%|██████▊   | 4587/6689 [00:01<00:00, 3154.54 examples/s]Applying chat template to train dataset:  69%|██████▉   | 4610/6689 [00:01<00:00, 3121.94 examples/s]Applying chat template to train dataset:  73%|███████▎  | 4903/6689 [00:01<00:00, 3153.97 examples/s]Applying chat template to train dataset:  75%|███████▍  | 4988/6689 [00:01<00:00, 3213.61 examples/s]Tokenizing eval dataset:  93%|█████████▎| 882/953 [00:01<00:00, 540.56 examples/s]Applying chat template to train dataset:  76%|███████▌  | 5070/6689 [00:01<00:00, 3098.90 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5220/6689 [00:01<00:00, 3154.18 examples/s]Applying chat template to train dataset:  79%|███████▉  | 5310/6689 [00:01<00:00, 3210.16 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 525.32 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 461.94 examples/s]
Applying chat template to train dataset:  83%|████████▎ | 5540/6689 [00:01<00:00, 3155.77 examples/s]Applying chat template to train dataset:  84%|████████▍ | 5632/6689 [00:01<00:00, 3209.37 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5530/6689 [00:01<00:00, 3086.13 examples/s]Applying chat template to train dataset:  88%|████████▊ | 5856/6689 [00:01<00:00, 3155.57 examples/s]Applying chat template to train dataset:  91%|█████████▏| 6113/6689 [00:01<00:00, 3204.66 examples/s]Applying chat template to train dataset:  90%|████████▉ | 5990/6689 [00:01<00:00, 3074.32 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6173/6689 [00:01<00:00, 3155.30 examples/s]Applying chat template to train dataset:  98%|█████████▊| 6584/6689 [00:02<00:00, 3172.52 examples/s]Applying chat template to train dataset:  96%|█████████▋| 6440/6689 [00:02<00:00, 3045.15 examples/s]Applying chat template to train dataset:  99%|█████████▉| 6629/6689 [00:02<00:00, 3105.00 examples/s]Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3192.28 examples/s]
Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3136.13 examples/s]
Applying chat template to train dataset: 100%|██████████| 6689/6689 [00:02<00:00, 3124.40 examples/s]
Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6689 [00:00<?, ? examples/s]Tokenizing train dataset:   2%|▏         | 108/6689 [00:00<00:06, 1066.14 examples/s]Tokenizing train dataset:   2%|▏         | 110/6689 [00:00<00:06, 1086.12 examples/s]Tokenizing train dataset:   2%|▏         | 110/6689 [00:00<00:06, 1081.74 examples/s]Tokenizing train dataset:   3%|▎         | 229/6689 [00:00<00:05, 1147.23 examples/s]Tokenizing train dataset:   3%|▎         | 234/6689 [00:00<00:05, 1163.42 examples/s]Tokenizing train dataset:   3%|▎         | 231/6689 [00:00<00:05, 1154.22 examples/s]Tokenizing train dataset:   5%|▌         | 345/6689 [00:00<00:05, 1147.14 examples/s]Tokenizing train dataset:   6%|▌         | 405/6689 [00:00<00:05, 1141.20 examples/s]Tokenizing train dataset:   5%|▌         | 347/6689 [00:00<00:05, 1146.54 examples/s]Tokenizing train dataset:   8%|▊         | 506/6689 [00:00<00:05, 1105.57 examples/s]Tokenizing train dataset:   8%|▊         | 567/6689 [00:00<00:05, 1110.57 examples/s]Tokenizing train dataset:   8%|▊         | 509/6689 [00:00<00:05, 1109.43 examples/s]Tokenizing train dataset:  10%|▉         | 661/6689 [00:00<00:05, 1070.28 examples/s]Tokenizing train dataset:  11%|█         | 715/6689 [00:00<00:05, 1056.90 examples/s]Tokenizing train dataset:  10%|▉         | 662/6689 [00:00<00:05, 1069.00 examples/s]Tokenizing train dataset:  12%|█▏        | 803/6689 [00:00<00:05, 1021.29 examples/s]Tokenizing train dataset:  13%|█▎        | 864/6689 [00:00<00:05, 1025.12 examples/s]Tokenizing train dataset:  12%|█▏        | 806/6689 [00:00<00:05, 1024.57 examples/s]Tokenizing train dataset:  14%|█▍        | 937/6689 [00:00<00:05, 969.46 examples/s] Tokenizing train dataset:  15%|█▍        | 999/6689 [00:00<00:05, 977.41 examples/s] Tokenizing train dataset:  14%|█▍        | 937/6689 [00:00<00:05, 969.39 examples/s] Tokenizing train dataset:  16%|█▌        | 1071/6689 [00:01<00:06, 935.78 examples/s]Tokenizing train dataset:  17%|█▋        | 1127/6689 [00:01<00:05, 934.23 examples/s]Tokenizing train dataset:  16%|█▌        | 1070/6689 [00:01<00:05, 938.84 examples/s]Tokenizing train dataset:  18%|█▊        | 1192/6689 [00:01<00:06, 888.95 examples/s]Tokenizing train dataset:  19%|█▊        | 1246/6689 [00:01<00:06, 885.01 examples/s]Tokenizing train dataset:  18%|█▊        | 1190/6689 [00:01<00:06, 889.48 examples/s]Tokenizing train dataset:  20%|█▉        | 1312/6689 [00:01<00:06, 859.11 examples/s]Tokenizing train dataset:  20%|██        | 1364/6689 [00:01<00:06, 846.93 examples/s]Tokenizing train dataset:  20%|█▉        | 1310/6689 [00:01<00:06, 855.35 examples/s]Tokenizing train dataset:  21%|██▏       | 1426/6689 [00:01<00:06, 825.44 examples/s]Tokenizing train dataset:  22%|██▏       | 1477/6689 [00:01<00:06, 811.43 examples/s]Tokenizing train dataset:  21%|██▏       | 1426/6689 [00:01<00:06, 825.78 examples/s]Tokenizing train dataset:  23%|██▎       | 1540/6689 [00:01<00:06, 792.55 examples/s]Tokenizing train dataset:  24%|██▎       | 1584/6689 [00:01<00:06, 776.57 examples/s]Tokenizing train dataset:  23%|██▎       | 1540/6689 [00:01<00:06, 793.23 examples/s]Tokenizing train dataset:  25%|██▍       | 1640/6689 [00:01<00:06, 748.28 examples/s]Tokenizing train dataset:  25%|██▌       | 1696/6689 [00:01<00:06, 763.49 examples/s]Tokenizing train dataset:  25%|██▍       | 1640/6689 [00:01<00:06, 749.36 examples/s]Tokenizing train dataset:  26%|██▌       | 1716/6689 [00:01<00:06, 749.91 examples/s]Tokenizing train dataset:  26%|██▌       | 1717/6689 [00:01<00:06, 752.22 examples/s]Tokenizing train dataset:  27%|██▋       | 1798/6689 [00:02<00:06, 729.31 examples/s]Tokenizing train dataset:  27%|██▋       | 1815/6689 [00:02<00:06, 717.62 examples/s]Tokenizing train dataset:  27%|██▋       | 1816/6689 [00:02<00:06, 719.82 examples/s]Tokenizing train dataset:  29%|██▊       | 1907/6689 [00:02<00:06, 724.13 examples/s]Tokenizing train dataset:  29%|██▉       | 1926/6689 [00:02<00:06, 717.06 examples/s]Tokenizing train dataset:  29%|██▉       | 1925/6689 [00:02<00:06, 717.59 examples/s]Tokenizing train dataset:  30%|███       | 2010/6689 [00:02<00:06, 709.68 examples/s]Tokenizing train dataset:  30%|███       | 2029/6689 [00:02<00:06, 698.70 examples/s]Tokenizing train dataset:  31%|███       | 2082/6689 [00:02<00:06, 707.31 examples/s]Tokenizing train dataset:  30%|███       | 2027/6689 [00:02<00:06, 700.87 examples/s]Tokenizing train dataset:  31%|███▏      | 2107/6689 [00:02<00:06, 712.13 examples/s]Tokenizing train dataset:  31%|███▏      | 2104/6689 [00:02<00:06, 712.35 examples/s]Tokenizing train dataset:  33%|███▎      | 2186/6689 [00:02<00:06, 699.26 examples/s]Tokenizing train dataset:  33%|███▎      | 2205/6689 [00:02<00:06, 687.24 examples/s]Tokenizing train dataset:  33%|███▎      | 2200/6689 [00:02<00:06, 683.80 examples/s]Tokenizing train dataset:  34%|███▍      | 2280/6689 [00:02<00:06, 670.62 examples/s]Tokenizing train dataset:  34%|███▍      | 2301/6689 [00:02<00:06, 668.78 examples/s]Tokenizing train dataset:  35%|███▌      | 2350/6689 [00:02<00:06, 676.27 examples/s]Tokenizing train dataset:  34%|███▍      | 2300/6689 [00:02<00:06, 673.38 examples/s]Tokenizing train dataset:  36%|███▌      | 2397/6689 [00:02<00:06, 658.10 examples/s]Tokenizing train dataset:  35%|███▌      | 2368/6689 [00:02<00:06, 670.66 examples/s]Tokenizing train dataset:  37%|███▋      | 2445/6689 [00:03<00:06, 659.40 examples/s]Tokenizing train dataset:  37%|███▋      | 2488/6689 [00:03<00:06, 639.72 examples/s]Tokenizing train dataset:  37%|███▋      | 2459/6689 [00:03<00:06, 646.31 examples/s]Tokenizing train dataset:  38%|███▊      | 2537/6689 [00:03<00:06, 638.28 examples/s]Tokenizing train dataset:  39%|███▊      | 2585/6689 [00:03<00:06, 640.40 examples/s]Tokenizing train dataset:  38%|███▊      | 2552/6689 [00:03<00:06, 635.70 examples/s]Tokenizing train dataset:  39%|███▉      | 2608/6689 [00:03<00:06, 650.66 examples/s]Tokenizing train dataset:  40%|███▉      | 2654/6689 [00:03<00:06, 649.71 examples/s]Tokenizing train dataset:  39%|███▉      | 2626/6689 [00:03<00:06, 658.24 examples/s]Tokenizing train dataset:  40%|███▉      | 2675/6689 [00:03<00:06, 652.10 examples/s]Tokenizing train dataset:  41%|████      | 2738/6689 [00:03<00:06, 614.24 examples/s]Tokenizing train dataset:  41%|████      | 2757/6689 [00:03<00:06, 612.07 examples/s]Tokenizing train dataset:  41%|████      | 2716/6689 [00:03<00:06, 629.19 examples/s]Tokenizing train dataset:  42%|████▏     | 2830/6689 [00:03<00:06, 607.32 examples/s]Tokenizing train dataset:  43%|████▎     | 2850/6689 [00:03<00:06, 607.88 examples/s]Tokenizing train dataset:  42%|████▏     | 2800/6689 [00:03<00:06, 603.05 examples/s]Tokenizing train dataset:  44%|████▎     | 2913/6689 [00:03<00:06, 608.00 examples/s]Tokenizing train dataset:  44%|████▎     | 2920/6689 [00:03<00:06, 599.78 examples/s]Tokenizing train dataset:  43%|████▎     | 2893/6689 [00:03<00:06, 603.89 examples/s]Tokenizing train dataset:  45%|████▍     | 2997/6689 [00:03<00:06, 588.81 examples/s]Tokenizing train dataset:  45%|████▍     | 3006/6689 [00:03<00:06, 589.21 examples/s]Tokenizing train dataset:  45%|████▍     | 2980/6689 [00:03<00:06, 593.04 examples/s]Tokenizing train dataset:  46%|████▌     | 3081/6689 [00:04<00:06, 574.89 examples/s]Tokenizing train dataset:  46%|████▌     | 3087/6689 [00:04<00:06, 572.34 examples/s]Tokenizing train dataset:  46%|████▌     | 3065/6689 [00:04<00:06, 575.55 examples/s]Tokenizing train dataset:  47%|████▋     | 3140/6689 [00:04<00:06, 576.42 examples/s]Tokenizing train dataset:  47%|████▋     | 3150/6689 [00:04<00:06, 577.87 examples/s]Tokenizing train dataset:  47%|████▋     | 3125/6689 [00:04<00:06, 579.34 examples/s]Tokenizing train dataset:  48%|████▊     | 3201/6689 [00:04<00:06, 576.82 examples/s]Tokenizing train dataset:  48%|████▊     | 3209/6689 [00:04<00:05, 580.15 examples/s]Tokenizing train dataset:  48%|████▊     | 3213/6689 [00:04<00:06, 576.17 examples/s]Tokenizing train dataset:  49%|████▉     | 3284/6689 [00:04<00:06, 560.03 examples/s]Tokenizing train dataset:  49%|████▉     | 3288/6689 [00:04<00:06, 556.61 examples/s]Tokenizing train dataset:  49%|████▉     | 3290/6689 [00:04<00:06, 552.12 examples/s]Tokenizing train dataset:  50%|█████     | 3369/6689 [00:04<00:05, 555.50 examples/s]Tokenizing train dataset:  50%|█████     | 3371/6689 [00:04<00:06, 550.04 examples/s]Tokenizing train dataset:  50%|█████     | 3346/6689 [00:04<00:06, 552.34 examples/s]Tokenizing train dataset:  51%|█████▏    | 3434/6689 [00:04<00:05, 575.55 examples/s]Tokenizing train dataset:  51%|█████▏    | 3437/6689 [00:04<00:05, 573.44 examples/s]Tokenizing train dataset:  51%|█████     | 3411/6689 [00:04<00:05, 573.84 examples/s]Tokenizing train dataset:  52%|█████▏    | 3497/6689 [00:04<00:05, 580.64 examples/s]Tokenizing train dataset:  52%|█████▏    | 3498/6689 [00:04<00:05, 578.49 examples/s]Tokenizing train dataset:  52%|█████▏    | 3504/6689 [00:04<00:05, 583.53 examples/s]Tokenizing train dataset:  53%|█████▎    | 3556/6689 [00:04<00:05, 579.04 examples/s]Tokenizing train dataset:  53%|█████▎    | 3576/6689 [00:05<00:05, 554.95 examples/s]Tokenizing train dataset:  54%|█████▎    | 3580/6689 [00:05<00:05, 554.78 examples/s]Tokenizing train dataset:  54%|█████▍    | 3632/6689 [00:05<00:05, 543.88 examples/s]Tokenizing train dataset:  55%|█████▍    | 3659/6689 [00:05<00:05, 553.91 examples/s]Tokenizing train dataset:  55%|█████▍    | 3662/6689 [00:05<00:05, 548.04 examples/s]Tokenizing train dataset:  55%|█████▌    | 3711/6689 [00:05<00:05, 532.46 examples/s]Tokenizing train dataset:  56%|█████▌    | 3736/6689 [00:05<00:05, 535.27 examples/s]Tokenizing train dataset:  56%|█████▌    | 3741/6689 [00:05<00:05, 534.66 examples/s]Tokenizing train dataset:  57%|█████▋    | 3794/6689 [00:05<00:05, 536.47 examples/s]Tokenizing train dataset:  57%|█████▋    | 3819/6689 [00:05<00:05, 537.01 examples/s]Tokenizing train dataset:  57%|█████▋    | 3796/6689 [00:05<00:05, 536.86 examples/s]Tokenizing train dataset:  58%|█████▊    | 3849/6689 [00:05<00:05, 535.42 examples/s]Tokenizing train dataset:  58%|█████▊    | 3903/6689 [00:05<00:05, 534.84 examples/s]Tokenizing train dataset:  58%|█████▊    | 3898/6689 [00:05<00:05, 528.41 examples/s]Tokenizing train dataset:  58%|█████▊    | 3875/6689 [00:05<00:05, 530.37 examples/s]Tokenizing train dataset:  59%|█████▊    | 3929/6689 [00:05<00:05, 530.07 examples/s]Tokenizing train dataset:  59%|█████▉    | 3974/6689 [00:05<00:05, 507.99 examples/s]Tokenizing train dataset:  59%|█████▉    | 3971/6689 [00:05<00:05, 507.94 examples/s]Tokenizing train dataset:  60%|██████    | 4026/6689 [00:05<00:05, 504.83 examples/s]Tokenizing train dataset:  60%|█████▉    | 4001/6689 [00:05<00:05, 505.08 examples/s]Tokenizing train dataset:  60%|██████    | 4046/6689 [00:05<00:05, 504.89 examples/s]Tokenizing train dataset:  61%|██████    | 4080/6689 [00:05<00:05, 509.14 examples/s]Tokenizing train dataset:  61%|██████    | 4052/6689 [00:05<00:05, 502.60 examples/s]Tokenizing train dataset:  61%|██████▏   | 4103/6689 [00:06<00:05, 515.63 examples/s]Tokenizing train dataset:  62%|██████▏   | 4139/6689 [00:06<00:04, 524.80 examples/s]Tokenizing train dataset:  61%|██████▏   | 4112/6689 [00:06<00:04, 519.00 examples/s]Tokenizing train dataset:  62%|██████▏   | 4159/6689 [00:06<00:04, 525.17 examples/s]Tokenizing train dataset:  63%|██████▎   | 4200/6689 [00:06<00:04, 543.20 examples/s]Tokenizing train dataset:  62%|██████▏   | 4170/6689 [00:06<00:04, 530.04 examples/s]Tokenizing train dataset:  63%|██████▎   | 4219/6689 [00:06<00:04, 537.14 examples/s]Tokenizing train dataset:  64%|██████▎   | 4255/6689 [00:06<00:04, 543.65 examples/s]Tokenizing train dataset:  63%|██████▎   | 4227/6689 [00:06<00:04, 540.21 examples/s]Tokenizing train dataset:  64%|██████▍   | 4298/6689 [00:06<00:04, 530.13 examples/s]Tokenizing train dataset:  65%|██████▍   | 4330/6689 [00:06<00:04, 520.68 examples/s]Tokenizing train dataset:  64%|██████▍   | 4304/6689 [00:06<00:04, 526.35 examples/s]Tokenizing train dataset:  65%|██████▌   | 4377/6689 [00:06<00:04, 524.54 examples/s]Tokenizing train dataset:  66%|██████▌   | 4389/6689 [00:06<00:04, 472.47 examples/s]Tokenizing train dataset:  65%|██████▌   | 4378/6689 [00:06<00:04, 511.85 examples/s]Tokenizing train dataset:  66%|██████▋   | 4433/6689 [00:06<00:04, 528.85 examples/s]Tokenizing train dataset:  66%|██████▋   | 4442/6689 [00:06<00:04, 484.68 examples/s]Tokenizing train dataset:  66%|██████▋   | 4433/6689 [00:06<00:04, 520.34 examples/s]Tokenizing train dataset:  67%|██████▋   | 4487/6689 [00:06<00:04, 526.59 examples/s]Tokenizing train dataset:  67%|██████▋   | 4496/6689 [00:06<00:04, 498.38 examples/s]Tokenizing train dataset:  67%|██████▋   | 4487/6689 [00:06<00:04, 520.54 examples/s]Tokenizing train dataset:  68%|██████▊   | 4549/6689 [00:06<00:04, 501.78 examples/s]Tokenizing train dataset:  68%|██████▊   | 4565/6689 [00:06<00:04, 520.03 examples/s]Tokenizing train dataset:  68%|██████▊   | 4565/6689 [00:06<00:04, 516.56 examples/s]Tokenizing train dataset:  69%|██████▉   | 4618/6689 [00:07<00:04, 484.43 examples/s]Tokenizing train dataset:  69%|██████▉   | 4634/6689 [00:07<00:04, 498.65 examples/s]Tokenizing train dataset:  69%|██████▉   | 4634/6689 [00:07<00:04, 496.28 examples/s]Tokenizing train dataset:  70%|██████▉   | 4670/6689 [00:07<00:04, 491.64 examples/s]Tokenizing train dataset:  70%|███████   | 4687/6689 [00:07<00:03, 502.11 examples/s]Tokenizing train dataset:  70%|███████   | 4687/6689 [00:07<00:03, 500.88 examples/s]Tokenizing train dataset:  71%|███████   | 4729/6689 [00:07<00:03, 515.42 examples/s]Tokenizing train dataset:  71%|███████   | 4745/6689 [00:07<00:03, 517.20 examples/s]Tokenizing train dataset:  71%|███████   | 4745/6689 [00:07<00:03, 516.27 examples/s]Tokenizing train dataset:  72%|███████▏  | 4787/6689 [00:07<00:03, 530.35 examples/s]Tokenizing train dataset:  72%|███████▏  | 4803/6689 [00:07<00:03, 531.98 examples/s]Tokenizing train dataset:  72%|███████▏  | 4804/6689 [00:07<00:03, 533.33 examples/s]Tokenizing train dataset:  73%|███████▎  | 4867/6689 [00:07<00:03, 523.98 examples/s]Tokenizing train dataset:  73%|███████▎  | 4880/6689 [00:07<00:03, 521.29 examples/s]Tokenizing train dataset:  73%|███████▎  | 4880/6689 [00:07<00:03, 521.65 examples/s]Tokenizing train dataset:  74%|███████▍  | 4938/6689 [00:07<00:03, 501.16 examples/s]Tokenizing train dataset:  74%|███████▍  | 4952/6689 [00:07<00:03, 503.43 examples/s]Tokenizing train dataset:  74%|███████▍  | 4953/6689 [00:07<00:03, 504.74 examples/s]Tokenizing train dataset:  75%|███████▍  | 4997/6689 [00:07<00:03, 521.73 examples/s]Tokenizing train dataset:  75%|███████▍  | 5012/6689 [00:07<00:03, 525.36 examples/s]Tokenizing train dataset:  75%|███████▍  | 5013/6689 [00:07<00:03, 523.68 examples/s]Tokenizing train dataset:  76%|███████▌  | 5070/6689 [00:07<00:03, 500.06 examples/s]Tokenizing train dataset:  76%|███████▌  | 5086/6689 [00:07<00:03, 505.50 examples/s]Tokenizing train dataset:  76%|███████▌  | 5086/6689 [00:07<00:03, 506.92 examples/s]Tokenizing train dataset:  77%|███████▋  | 5152/6689 [00:08<00:03, 508.53 examples/s]Tokenizing train dataset:  77%|███████▋  | 5162/6689 [00:08<00:03, 503.86 examples/s]Tokenizing train dataset:  77%|███████▋  | 5162/6689 [00:08<00:03, 505.09 examples/s]Tokenizing train dataset:  78%|███████▊  | 5205/6689 [00:08<00:02, 508.97 examples/s]Tokenizing train dataset:  78%|███████▊  | 5216/6689 [00:08<00:02, 505.84 examples/s]Tokenizing train dataset:  78%|███████▊  | 5216/6689 [00:08<00:02, 506.79 examples/s]Tokenizing train dataset:  79%|███████▊  | 5262/6689 [00:08<00:02, 523.71 examples/s]Tokenizing train dataset:  79%|███████▉  | 5274/6689 [00:08<00:02, 520.13 examples/s]Tokenizing train dataset:  79%|███████▉  | 5274/6689 [00:08<00:02, 520.93 examples/s]Tokenizing train dataset:  79%|███████▉  | 5315/6689 [00:08<00:02, 522.75 examples/s]Tokenizing train dataset:  80%|███████▉  | 5331/6689 [00:08<00:02, 525.83 examples/s]Tokenizing train dataset:  80%|███████▉  | 5331/6689 [00:08<00:02, 526.27 examples/s]Tokenizing train dataset:  81%|████████  | 5391/6689 [00:08<00:02, 512.96 examples/s]Tokenizing train dataset:  81%|████████  | 5406/6689 [00:08<00:02, 509.39 examples/s]Tokenizing train dataset:  81%|████████  | 5406/6689 [00:08<00:02, 509.09 examples/s]Tokenizing train dataset:  82%|████████▏ | 5460/6689 [00:08<00:02, 488.70 examples/s]Tokenizing train dataset:  82%|████████▏ | 5478/6689 [00:08<00:02, 491.50 examples/s]Tokenizing train dataset:  82%|████████▏ | 5466/6689 [00:08<00:02, 468.99 examples/s]Tokenizing train dataset:  82%|████████▏ | 5510/6689 [00:08<00:02, 488.09 examples/s]Tokenizing train dataset:  83%|████████▎ | 5552/6689 [00:08<00:02, 486.36 examples/s]Tokenizing train dataset:  83%|████████▎ | 5520/6689 [00:08<00:02, 484.16 examples/s]Tokenizing train dataset:  83%|████████▎ | 5563/6689 [00:08<00:02, 492.03 examples/s]Tokenizing train dataset:  84%|████████▍ | 5607/6689 [00:08<00:02, 495.68 examples/s]Tokenizing train dataset:  83%|████████▎ | 5574/6689 [00:08<00:02, 492.98 examples/s]Tokenizing train dataset:  84%|████████▍ | 5616/6689 [00:08<00:02, 499.83 examples/s]Tokenizing train dataset:  84%|████████▍ | 5627/6689 [00:09<00:02, 497.15 examples/s]Tokenizing train dataset:  85%|████████▍ | 5669/6689 [00:09<00:02, 502.62 examples/s]Tokenizing train dataset:  85%|████████▍ | 5682/6689 [00:09<00:02, 495.29 examples/s]Tokenizing train dataset:  85%|████████▍ | 5679/6689 [00:09<00:02, 501.72 examples/s]Tokenizing train dataset:  86%|████████▌ | 5720/6689 [00:09<00:01, 502.45 examples/s]Tokenizing train dataset:  86%|████████▌ | 5734/6689 [00:09<00:01, 499.80 examples/s]Tokenizing train dataset:  86%|████████▌ | 5730/6689 [00:09<00:01, 498.43 examples/s]Tokenizing train dataset:  86%|████████▋ | 5785/6689 [00:09<00:01, 500.34 examples/s]Tokenizing train dataset:  87%|████████▋ | 5795/6689 [00:09<00:01, 499.08 examples/s]Tokenizing train dataset:  87%|████████▋ | 5806/6689 [00:09<00:01, 494.19 examples/s]Tokenizing train dataset:  88%|████████▊ | 5854/6689 [00:09<00:01, 482.21 examples/s]Tokenizing train dataset:  88%|████████▊ | 5865/6689 [00:09<00:01, 484.76 examples/s]Tokenizing train dataset:  88%|████████▊ | 5875/6689 [00:09<00:01, 478.84 examples/s]Tokenizing train dataset:  89%|████████▊ | 5925/6689 [00:09<00:01, 475.37 examples/s]Tokenizing train dataset:  89%|████████▉ | 5938/6689 [00:09<00:01, 482.56 examples/s]Tokenizing train dataset:  89%|████████▉ | 5981/6689 [00:09<00:01, 492.91 examples/s]Tokenizing train dataset:  89%|████████▉ | 5950/6689 [00:09<00:01, 483.16 examples/s]Tokenizing train dataset:  90%|████████▉ | 5993/6689 [00:09<00:01, 492.92 examples/s]Tokenizing train dataset:  90%|████████▉ | 6000/6689 [00:09<00:01, 484.02 examples/s]Tokenizing train dataset:  90%|█████████ | 6053/6689 [00:09<00:01, 482.53 examples/s]Tokenizing train dataset:  91%|█████████ | 6065/6689 [00:09<00:01, 487.32 examples/s]Tokenizing train dataset:  90%|█████████ | 6049/6689 [00:09<00:01, 485.04 examples/s]Tokenizing train dataset:  91%|█████████▏| 6105/6689 [00:09<00:01, 487.60 examples/s]Tokenizing train dataset:  91%|█████████▏| 6117/6689 [00:09<00:01, 493.50 examples/s]Tokenizing train dataset:  91%|█████████ | 6100/6689 [00:09<00:01, 486.22 examples/s]Tokenizing train dataset:  92%|█████████▏| 6170/6689 [00:10<00:00, 525.97 examples/s]Tokenizing train dataset:  92%|█████████▏| 6184/6689 [00:10<00:00, 534.13 examples/s]Tokenizing train dataset:  92%|█████████▏| 6164/6689 [00:10<00:00, 526.71 examples/s]Tokenizing train dataset:  93%|█████████▎| 6228/6689 [00:10<00:00, 536.83 examples/s]Tokenizing train dataset:  93%|█████████▎| 6246/6689 [00:10<00:00, 554.65 examples/s]Tokenizing train dataset:  93%|█████████▎| 6223/6689 [00:10<00:00, 536.58 examples/s]Tokenizing train dataset:  94%|█████████▍| 6285/6689 [00:10<00:00, 545.29 examples/s]Tokenizing train dataset:  94%|█████████▍| 6280/6689 [00:10<00:00, 542.11 examples/s]Tokenizing train dataset:  94%|█████████▍| 6320/6689 [00:10<00:00, 524.36 examples/s]Tokenizing train dataset:  95%|█████████▍| 6353/6689 [00:10<00:00, 510.16 examples/s]Tokenizing train dataset:  95%|█████████▍| 6353/6689 [00:10<00:00, 510.26 examples/s]Tokenizing train dataset:  96%|█████████▌| 6390/6689 [00:10<00:00, 497.97 examples/s]Tokenizing train dataset:  96%|█████████▌| 6417/6689 [00:10<00:00, 478.19 examples/s]Tokenizing train dataset:  96%|█████████▌| 6417/6689 [00:10<00:00, 477.56 examples/s]Tokenizing train dataset:  97%|█████████▋| 6460/6689 [00:10<00:00, 479.27 examples/s]Tokenizing train dataset:  97%|█████████▋| 6468/6689 [00:10<00:00, 480.52 examples/s]Tokenizing train dataset:  97%|█████████▋| 6468/6689 [00:10<00:00, 479.49 examples/s]Tokenizing train dataset:  97%|█████████▋| 6510/6689 [00:10<00:00, 480.12 examples/s]Tokenizing train dataset:  98%|█████████▊| 6543/6689 [00:10<00:00, 482.82 examples/s]Tokenizing train dataset:  98%|█████████▊| 6564/6689 [00:10<00:00, 490.38 examples/s]Tokenizing train dataset:  98%|█████████▊| 6543/6689 [00:10<00:00, 482.35 examples/s]Tokenizing train dataset:  99%|█████████▊| 6594/6689 [00:10<00:00, 483.73 examples/s]Tokenizing train dataset:  99%|█████████▉| 6621/6689 [00:10<00:00, 505.48 examples/s]Tokenizing train dataset:  99%|█████████▊| 6594/6689 [00:10<00:00, 483.53 examples/s]Tokenizing train dataset:  99%|█████████▉| 6650/6689 [00:11<00:00, 499.41 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 497.76 examples/s]Tokenizing train dataset:  99%|█████████▉| 6650/6689 [00:11<00:00, 499.01 examples/s]Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 600.78 examples/s]
Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 599.65 examples/s]
Tokenizing train dataset: 100%|██████████| 6689/6689 [00:11<00:00, 599.23 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5556.93 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5538.42 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5507.91 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5549.57 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5524.21 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5497.27 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  32%|███▏      | 306/953 [00:00<00:00, 3026.34 examples/s]Applying chat template to eval dataset:  33%|███▎      | 310/953 [00:00<00:00, 3067.00 examples/s]Applying chat template to eval dataset:  33%|███▎      | 310/953 [00:00<00:00, 3064.03 examples/s]Applying chat template to eval dataset:  64%|██████▍   | 613/953 [00:00<00:00, 3044.12 examples/s]Applying chat template to eval dataset:  65%|██████▍   | 617/953 [00:00<00:00, 3060.86 examples/s]Applying chat template to eval dataset:  79%|███████▉  | 755/953 [00:00<00:00, 2993.40 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2889.59 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2912.75 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2910.60 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2924.57 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2933.12 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 283.71 examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 284.36 examples/s]Tokenizing eval dataset:   3%|▎         | 29/953 [00:00<00:03, 283.98 examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 253.10 examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 253.52 examples/s]Tokenizing eval dataset:   7%|▋         | 66/953 [00:00<00:03, 252.51 examples/s]Tokenizing eval dataset:  10%|▉         | 94/953 [00:00<00:03, 256.23 examples/s]Tokenizing eval dataset:  10%|▉         | 94/953 [00:00<00:03, 255.30 examples/s]Tokenizing eval dataset:  10%|▉         | 93/953 [00:00<00:03, 253.40 examples/s]Tokenizing eval dataset:  14%|█▎        | 130/953 [00:00<00:03, 239.28 examples/s]Tokenizing eval dataset:  14%|█▎        | 130/953 [00:00<00:03, 238.73 examples/s]Tokenizing eval dataset:  13%|█▎        | 127/953 [00:00<00:03, 234.99 examples/s]Tokenizing eval dataset:  17%|█▋        | 164/953 [00:00<00:03, 232.24 examples/s]Tokenizing eval dataset:  16%|█▌        | 151/953 [00:00<00:03, 229.04 examples/s]Tokenizing eval dataset:  17%|█▋        | 165/953 [00:00<00:03, 231.28 examples/s]Tokenizing eval dataset:  19%|█▉        | 182/953 [00:00<00:03, 217.57 examples/s]Tokenizing eval dataset:  21%|██        | 196/953 [00:00<00:03, 220.08 examples/s]Tokenizing eval dataset:  21%|██        | 196/953 [00:00<00:03, 219.51 examples/s]Tokenizing eval dataset:  22%|██▏       | 207/953 [00:00<00:03, 222.74 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:03, 236.04 examples/s]Tokenizing eval dataset:  24%|██▎       | 226/953 [00:00<00:03, 234.90 examples/s]Tokenizing eval dataset:  26%|██▌       | 248/953 [00:00<00:02, 269.86 examples/s]Tokenizing eval dataset:  30%|███       | 287/953 [00:01<00:02, 330.04 examples/s]Tokenizing eval dataset:  30%|███       | 287/953 [00:01<00:02, 328.97 examples/s]Tokenizing eval dataset:  32%|███▏      | 308/953 [00:01<00:01, 360.06 examples/s]Tokenizing eval dataset:  37%|███▋      | 348/953 [00:01<00:01, 401.66 examples/s]Tokenizing eval dataset:  37%|███▋      | 350/953 [00:01<00:01, 406.32 examples/s]Tokenizing eval dataset:  39%|███▊      | 369/953 [00:01<00:01, 426.95 examples/s]Tokenizing eval dataset:  43%|████▎     | 408/953 [00:01<00:01, 450.08 examples/s]Tokenizing eval dataset:  43%|████▎     | 408/953 [00:01<00:01, 452.90 examples/s]Tokenizing eval dataset:  46%|████▌     | 437/953 [00:01<00:01, 497.39 examples/s]Tokenizing eval dataset:  50%|█████     | 478/953 [00:01<00:00, 516.77 examples/s]Tokenizing eval dataset:  50%|█████     | 478/953 [00:01<00:00, 519.03 examples/s]Tokenizing eval dataset:  53%|█████▎    | 504/953 [00:01<00:00, 541.78 examples/s]Tokenizing eval dataset:  57%|█████▋    | 542/953 [00:01<00:00, 550.95 examples/s]Tokenizing eval dataset:  57%|█████▋    | 541/953 [00:01<00:00, 550.29 examples/s]Tokenizing eval dataset:  59%|█████▉    | 566/953 [00:01<00:00, 560.51 examples/s]Tokenizing eval dataset:  64%|██████▍   | 610/953 [00:01<00:00, 583.28 examples/s]Tokenizing eval dataset:  64%|██████▍   | 609/953 [00:01<00:00, 586.68 examples/s]Tokenizing eval dataset:  66%|██████▋   | 632/953 [00:01<00:00, 588.89 examples/s]Tokenizing eval dataset:  70%|███████   | 670/953 [00:01<00:00, 585.58 examples/s]Tokenizing eval dataset:  73%|███████▎  | 699/953 [00:01<00:00, 587.63 examples/s]Tokenizing eval dataset:  73%|███████▎  | 693/953 [00:01<00:00, 590.93 examples/s]Tokenizing eval dataset:  77%|███████▋  | 730/953 [00:01<00:00, 582.90 examples/s]Tokenizing eval dataset:  82%|████████▏ | 780/953 [00:01<00:00, 566.57 examples/s]Tokenizing eval dataset:  84%|████████▍ | 804/953 [00:01<00:00, 545.23 examples/s]Tokenizing eval dataset:  82%|████████▏ | 777/953 [00:01<00:00, 566.42 examples/s]Tokenizing eval dataset:  89%|████████▉ | 851/953 [00:02<00:00, 531.09 examples/s]Tokenizing eval dataset:  89%|████████▉ | 848/953 [00:02<00:00, 532.68 examples/s]Tokenizing eval dataset:  92%|█████████▏| 880/953 [00:02<00:00, 529.16 examples/s]Tokenizing eval dataset:  97%|█████████▋| 923/953 [00:02<00:00, 512.66 examples/s]Tokenizing eval dataset:  96%|█████████▌| 916/953 [00:02<00:00, 504.26 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 513.53 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 427.17 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 426.29 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 423.72 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5807957649230957 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.390639305114746 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3425657749176025 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3324053287506104 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank4]:[W612 20:11:45.297112545 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 1 ---
