cpu-bind=MASK - gn02, task  1  0 [2372362]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 1 ---
Total Nodes: 3
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn01
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 3     --machine_rank 1     --main_process_ip gn01     --main_process_port 29500     --num_processes 12     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_63118420     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=1e-6 --total_epochs=3 --beta=0.1 --curriculum_stage=2
-------------------------------------------
[2025-06-12 21:20:57,020] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0612 21:20:58.420000 2372413 torch/distributed/run.py:792] 
W0612 21:20:58.420000 2372413 torch/distributed/run.py:792] *****************************************
W0612 21:20:58.420000 2372413 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0612 21:20:58.420000 2372413 torch/distributed/run.py:792] *****************************************
[2025-06-12 21:21:54,692] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:21:54,713] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:21:54,754] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-12 21:21:54,756] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Training data of type 'bad_lang_examples':    5343
[load_data.py]: Training data of type 'short_examples':       699
[load_data.py]: Training data of type 'choose_examples':      13379
[load_data.py]: Training data of type 'bad_format_examples':  4806
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
[load_data.py]: Number of training examples: 24227
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
World size: 12
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=1e-06, total_epochs=3, beta=0.1, curriculum_stage=2)
1e-06
[2025-06-12 21:22:01,537] [INFO] [comm.py:658:init_distributed] cdb=None
Created datasets
Train dataset size: 6690
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-12 21:22:01,560] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 21:22:01,562] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-12 21:22:01,566] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curriculum-1
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.86s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.90s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:20,  6.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:11,  5.93s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:11,  5.97s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:11,  5.98s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:11,  5.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:05,  5.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:05,  5.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:05,  5.93s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.66s/it]
Loaded model
Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.69s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.69s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.70s/it]
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   8%|▊         | 551/6690 [00:00<00:01, 5467.24 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1120/6690 [00:00<00:01, 5565.53 examples/s]Extracting prompt in train dataset:  25%|██▌       | 1690/6690 [00:00<00:00, 5624.66 examples/s]Extracting prompt in train dataset:  34%|███▎      | 2255/6690 [00:00<00:00, 5616.74 examples/s][rank6]:[W612 21:22:28.869808203 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank7]:[W612 21:22:28.895620254 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank5]:[W612 21:22:28.905485815 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:  42%|████▏     | 2827/6690 [00:00<00:00, 5639.45 examples/s]Extracting prompt in train dataset:  51%|█████     | 3400/6690 [00:00<00:00, 5639.10 examples/s]Extracting prompt in train dataset:  59%|█████▉    | 3970/6690 [00:00<00:00, 5648.22 examples/s]Extracting prompt in train dataset:  72%|███████▏  | 4798/6690 [00:00<00:00, 5565.37 examples/s]Extracting prompt in train dataset:  80%|████████  | 5370/6690 [00:00<00:00, 5580.96 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 5950/6690 [00:01<00:00, 5610.58 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 6520/6690 [00:01<00:00, 5620.36 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5542.29 examples/s]
Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 291/6690 [00:00<00:02, 2887.43 examples/s]Applying chat template to train dataset:   9%|▉         | 610/6690 [00:00<00:01, 3057.89 examples/s]Applying chat template to train dataset:  14%|█▍        | 931/6690 [00:00<00:01, 3117.95 examples/s]Applying chat template to train dataset:  19%|█▊        | 1250/6690 [00:00<00:01, 3140.98 examples/s]Applying chat template to train dataset:  23%|██▎       | 1570/6690 [00:00<00:01, 3158.03 examples/s]Applying chat template to train dataset:  28%|██▊       | 1890/6690 [00:00<00:01, 3167.95 examples/s]Applying chat template to train dataset:  33%|███▎      | 2211/6690 [00:00<00:01, 3176.88 examples/s]Applying chat template to train dataset:  40%|████      | 2688/6690 [00:00<00:01, 3173.40 examples/s]Applying chat template to train dataset:  45%|████▍     | 3008/6690 [00:00<00:01, 3178.02 examples/s]Applying chat template to train dataset:  50%|████▉     | 3327/6690 [00:01<00:01, 3180.13 examples/s]Applying chat template to train dataset:  55%|█████▍    | 3647/6690 [00:01<00:00, 3183.80 examples/s]Applying chat template to train dataset:  59%|█████▉    | 3966/6690 [00:01<00:00, 3182.42 examples/s]Applying chat template to train dataset:  64%|██████▍   | 4287/6690 [00:01<00:00, 3186.07 examples/s]Applying chat template to train dataset:  71%|███████   | 4747/6690 [00:01<00:00, 3137.61 examples/s]Applying chat template to train dataset:  76%|███████▌  | 5067/6690 [00:01<00:00, 3150.63 examples/s]Applying chat template to train dataset:  81%|████████  | 5386/6690 [00:01<00:00, 3159.15 examples/s]Applying chat template to train dataset:  85%|████████▌ | 5705/6690 [00:01<00:00, 3164.73 examples/s]Applying chat template to train dataset:  90%|█████████ | 6023/6690 [00:01<00:00, 3166.67 examples/s]Applying chat template to train dataset:  95%|█████████▍| 6343/6690 [00:02<00:00, 3173.66 examples/s]Applying chat template to train dataset: 100%|█████████▉| 6661/6690 [00:02<00:00, 3171.57 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3152.89 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 40/6690 [00:00<00:17, 382.59 examples/s]Tokenizing train dataset:   1%|▏         | 89/6690 [00:00<00:15, 437.48 examples/s]Tokenizing train dataset:   2%|▏         | 156/6690 [00:00<00:14, 437.57 examples/s]Tokenizing train dataset:   3%|▎         | 204/6690 [00:00<00:14, 448.40 examples/s]Tokenizing train dataset:   4%|▍         | 254/6690 [00:00<00:14, 458.93 examples/s]Tokenizing train dataset:   5%|▍         | 319/6690 [00:00<00:14, 442.67 examples/s]Tokenizing train dataset:   6%|▌         | 370/6690 [00:00<00:13, 457.83 examples/s]Tokenizing train dataset:   7%|▋         | 442/6690 [00:00<00:13, 464.37 examples/s]Tokenizing train dataset:   7%|▋         | 491/6690 [00:01<00:13, 469.90 examples/s]Tokenizing train dataset:   8%|▊         | 540/6690 [00:01<00:13, 468.24 examples/s]Tokenizing train dataset:   9%|▉         | 589/6690 [00:01<00:12, 469.75 examples/s]Tokenizing train dataset:  10%|▉         | 640/6690 [00:01<00:12, 478.91 examples/s]Tokenizing train dataset:  10%|█         | 689/6690 [00:01<00:12, 475.32 examples/s]Tokenizing train dataset:  11%|█         | 740/6690 [00:01<00:12, 479.58 examples/s]Tokenizing train dataset:  12%|█▏        | 793/6690 [00:01<00:12, 483.92 examples/s]Tokenizing train dataset:  13%|█▎        | 865/6690 [00:01<00:12, 477.73 examples/s]Tokenizing train dataset:  14%|█▍        | 923/6690 [00:02<00:13, 441.52 examples/s]Tokenizing train dataset:  15%|█▍        | 971/6690 [00:02<00:12, 449.28 examples/s]Tokenizing train dataset:  16%|█▌        | 1039/6690 [00:02<00:12, 446.62 examples/s]Tokenizing train dataset:  16%|█▋        | 1088/6690 [00:02<00:12, 451.74 examples/s]Tokenizing train dataset:  17%|█▋        | 1155/6690 [00:02<00:12, 443.73 examples/s]Tokenizing train dataset:  18%|█▊        | 1203/6690 [00:02<00:12, 445.97 examples/s]Tokenizing train dataset:  19%|█▉        | 1268/6690 [00:02<00:12, 436.85 examples/s]Tokenizing train dataset:  20%|█▉        | 1317/6690 [00:02<00:12, 447.51 examples/s]Tokenizing train dataset:  20%|██        | 1363/6690 [00:02<00:11, 446.65 examples/s]Tokenizing train dataset:  21%|██▏       | 1422/6690 [00:03<00:12, 425.18 examples/s]Tokenizing train dataset:  22%|██▏       | 1485/6690 [00:03<00:12, 421.48 examples/s]Tokenizing train dataset:  23%|██▎       | 1540/6690 [00:03<00:11, 447.41 examples/s]Tokenizing train dataset:  24%|██▍       | 1603/6690 [00:03<00:11, 434.31 examples/s]Tokenizing train dataset:  25%|██▍       | 1668/6690 [00:03<00:11, 431.23 examples/s]Tokenizing train dataset:  26%|██▌       | 1715/6690 [00:03<00:11, 438.82 examples/s]Tokenizing train dataset:  26%|██▋       | 1762/6690 [00:03<00:11, 442.41 examples/s]Tokenizing train dataset:  27%|██▋       | 1807/6690 [00:04<00:11, 438.56 examples/s]Tokenizing train dataset:  28%|██▊       | 1876/6690 [00:04<00:10, 441.58 examples/s]Tokenizing train dataset:  29%|██▉       | 1946/6690 [00:04<00:10, 438.79 examples/s]Tokenizing train dataset:  30%|██▉       | 2003/6690 [00:04<00:11, 417.56 examples/s]Tokenizing train dataset:  31%|███       | 2047/6690 [00:04<00:11, 421.05 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 421.81 examples/s]Tokenizing train dataset:  32%|███▏      | 2139/6690 [00:04<00:10, 436.26 examples/s]Tokenizing train dataset:  33%|███▎      | 2190/6690 [00:04<00:10, 447.70 examples/s]Tokenizing train dataset:  33%|███▎      | 2241/6690 [00:05<00:09, 461.27 examples/s]Tokenizing train dataset:  34%|███▍      | 2303/6690 [00:05<00:09, 441.32 examples/s]Tokenizing train dataset:  35%|███▌      | 2348/6690 [00:05<00:09, 440.91 examples/s]Tokenizing train dataset:  36%|███▌      | 2396/6690 [00:05<00:09, 444.41 examples/s]Tokenizing train dataset:  37%|███▋      | 2465/6690 [00:05<00:09, 448.54 examples/s]Tokenizing train dataset:  38%|███▊      | 2536/6690 [00:05<00:09, 452.37 examples/s]Tokenizing train dataset:  39%|███▊      | 2587/6690 [00:05<00:08, 465.27 examples/s]Tokenizing train dataset:  40%|███▉      | 2651/6690 [00:05<00:09, 448.25 examples/s]Tokenizing train dataset:  41%|████      | 2715/6690 [00:06<00:09, 435.39 examples/s]Tokenizing train dataset:  41%|████▏     | 2774/6690 [00:06<00:09, 419.84 examples/s]Tokenizing train dataset:  42%|████▏     | 2838/6690 [00:06<00:09, 418.29 examples/s]Tokenizing train dataset:  43%|████▎     | 2882/6690 [00:06<00:09, 421.91 examples/s]Tokenizing train dataset:  44%|████▍     | 2932/6690 [00:06<00:08, 438.16 examples/s]Tokenizing train dataset:  45%|████▍     | 2985/6690 [00:06<00:08, 453.99 examples/s]Tokenizing train dataset:  45%|████▌     | 3032/6690 [00:06<00:07, 457.64 examples/s]Tokenizing train dataset:  46%|████▌     | 3088/6690 [00:06<00:07, 480.46 examples/s]Tokenizing train dataset:  47%|████▋     | 3152/6690 [00:07<00:07, 453.03 examples/s]Tokenizing train dataset:  48%|████▊     | 3199/6690 [00:07<00:07, 455.30 examples/s]Tokenizing train dataset:  49%|████▊     | 3248/6690 [00:07<00:07, 460.16 examples/s]Tokenizing train dataset:  49%|████▉     | 3306/6690 [00:07<00:07, 431.41 examples/s]Tokenizing train dataset:  50%|█████     | 3368/6690 [00:07<00:07, 422.03 examples/s]Tokenizing train dataset:  51%|█████▏    | 3429/6690 [00:07<00:07, 413.06 examples/s]Tokenizing train dataset:  52%|█████▏    | 3494/6690 [00:07<00:07, 415.17 examples/s]Tokenizing train dataset:  53%|█████▎    | 3553/6690 [00:08<00:07, 401.71 examples/s]Tokenizing train dataset:  54%|█████▍    | 3610/6690 [00:08<00:07, 389.21 examples/s]Tokenizing train dataset:  55%|█████▍    | 3658/6690 [00:08<00:07, 408.47 examples/s]Tokenizing train dataset:  55%|█████▌    | 3700/6690 [00:08<00:07, 396.14 examples/s]Tokenizing train dataset:  56%|█████▌    | 3759/6690 [00:08<00:07, 394.53 examples/s]Tokenizing train dataset:  57%|█████▋    | 3801/6690 [00:08<00:07, 400.37 examples/s]Tokenizing train dataset:  57%|█████▋    | 3846/6690 [00:08<00:06, 410.29 examples/s]Tokenizing train dataset:  58%|█████▊    | 3893/6690 [00:08<00:06, 424.82 examples/s]Tokenizing train dataset:  59%|█████▉    | 3942/6690 [00:08<00:06, 436.86 examples/s]Tokenizing train dataset:  60%|█████▉    | 3991/6690 [00:09<00:06, 446.15 examples/s]Tokenizing train dataset:  60%|██████    | 4036/6690 [00:09<00:06, 441.53 examples/s]Tokenizing train dataset:  61%|██████▏   | 4103/6690 [00:09<00:05, 435.66 examples/s]Tokenizing train dataset:  62%|██████▏   | 4148/6690 [00:09<00:05, 438.51 examples/s]Tokenizing train dataset:  63%|██████▎   | 4197/6690 [00:09<00:05, 448.52 examples/s]Tokenizing train dataset:  64%|██████▎   | 4261/6690 [00:09<00:05, 436.23 examples/s]Tokenizing train dataset:  64%|██████▍   | 4307/6690 [00:09<00:05, 439.80 examples/s]Tokenizing train dataset:  65%|██████▌   | 4369/6690 [00:09<00:05, 425.46 examples/s]Tokenizing train dataset:  66%|██████▋   | 4436/6690 [00:10<00:05, 429.49 examples/s]Tokenizing train dataset:  67%|██████▋   | 4498/6690 [00:10<00:05, 420.61 examples/s]Tokenizing train dataset:  68%|██████▊   | 4546/6690 [00:10<00:04, 433.06 examples/s]Tokenizing train dataset:  69%|██████▉   | 4615/6690 [00:10<00:04, 440.62 examples/s]Tokenizing train dataset:  70%|██████▉   | 4666/6690 [00:10<00:04, 448.71 examples/s]Tokenizing train dataset:  71%|███████   | 4735/6690 [00:10<00:04, 450.12 examples/s]Tokenizing train dataset:  72%|███████▏  | 4792/6690 [00:10<00:04, 422.96 examples/s]Tokenizing train dataset:  72%|███████▏  | 4842/6690 [00:11<00:04, 435.15 examples/s]Tokenizing train dataset:  73%|███████▎  | 4905/6690 [00:11<00:04, 426.63 examples/s]Tokenizing train dataset:  74%|███████▍  | 4950/6690 [00:11<00:04, 430.59 examples/s]Tokenizing train dataset:  75%|███████▍  | 5015/6690 [00:11<00:03, 422.44 examples/s]Tokenizing train dataset:  76%|███████▌  | 5058/6690 [00:11<00:03, 417.65 examples/s]Tokenizing train dataset:  76%|███████▌  | 5100/6690 [00:11<00:03, 415.47 examples/s]Tokenizing train dataset:  77%|███████▋  | 5166/6690 [00:11<00:03, 420.22 examples/s]Tokenizing train dataset:  78%|███████▊  | 5211/6690 [00:11<00:03, 425.69 examples/s]Tokenizing train dataset:  79%|███████▊  | 5256/6690 [00:12<00:03, 428.27 examples/s]Tokenizing train dataset:  79%|███████▉  | 5316/6690 [00:12<00:03, 416.09 examples/s]Tokenizing train dataset:  80%|████████  | 5365/6690 [00:12<00:03, 430.72 examples/s]Tokenizing train dataset:  81%|████████  | 5414/6690 [00:12<00:02, 442.45 examples/s]Tokenizing train dataset:  82%|████████▏ | 5478/6690 [00:12<00:02, 433.21 examples/s]Tokenizing train dataset:  83%|████████▎ | 5543/6690 [00:12<00:02, 429.55 examples/s]Tokenizing train dataset:  84%|████████▍ | 5606/6690 [00:12<00:02, 420.98 examples/s]Tokenizing train dataset:  84%|████████▍ | 5650/6690 [00:12<00:02, 420.28 examples/s]Tokenizing train dataset:  85%|████████▌ | 5694/6690 [00:13<00:02, 422.81 examples/s]Tokenizing train dataset:  86%|████████▌ | 5743/6690 [00:13<00:02, 433.30 examples/s]Tokenizing train dataset:  87%|████████▋ | 5788/6690 [00:13<00:02, 432.20 examples/s]Tokenizing train dataset:  87%|████████▋ | 5851/6690 [00:13<00:01, 420.95 examples/s]Tokenizing train dataset:  88%|████████▊ | 5899/6690 [00:13<00:01, 433.83 examples/s]Tokenizing train dataset:  89%|████████▉ | 5943/6690 [00:13<00:01, 431.33 examples/s]Tokenizing train dataset:  90%|████████▉ | 6009/6690 [00:13<00:01, 433.36 examples/s]Tokenizing train dataset:  90%|█████████ | 6053/6690 [00:13<00:01, 431.22 examples/s]Tokenizing train dataset:  91%|█████████ | 6104/6690 [00:13<00:01, 447.27 examples/s]Tokenizing train dataset:  92%|█████████▏| 6153/6690 [00:14<00:01, 453.72 examples/s]Tokenizing train dataset:  93%|█████████▎| 6216/6690 [00:14<00:01, 435.94 examples/s]Tokenizing train dataset:  94%|█████████▍| 6274/6690 [00:14<00:01, 411.08 examples/s]Tokenizing train dataset:  94%|█████████▍| 6320/6690 [00:14<00:00, 416.19 examples/s]Tokenizing train dataset:  95%|█████████▌| 6380/6690 [00:14<00:00, 400.47 examples/s]Tokenizing train dataset:  96%|█████████▌| 6430/6690 [00:14<00:00, 422.07 examples/s]Tokenizing train dataset:  97%|█████████▋| 6474/6690 [00:14<00:00, 421.13 examples/s]Tokenizing train dataset:  97%|█████████▋| 6518/6690 [00:14<00:00, 423.43 examples/s]Tokenizing train dataset:  98%|█████████▊| 6581/6690 [00:15<00:00, 417.27 examples/s]Tokenizing train dataset:  99%|█████████▉| 6642/6690 [00:15<00:00, 408.85 examples/s]Tokenizing train dataset: 100%|█████████▉| 6684/6690 [00:15<00:00, 408.81 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 434.16 examples/s]
[rank4]:[W612 21:22:47.160543427 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▊         | 572/6690 [00:00<00:01, 5681.94 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 571/953 [00:00<00:00, 5649.78 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5606.57 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5601.74 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5581.94 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1146/6690 [00:00<00:00, 5713.15 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1144/6690 [00:00<00:00, 5680.91 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1150/6690 [00:00<00:00, 5690.36 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1735/6690 [00:00<00:00, 5790.92 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1728/6690 [00:00<00:00, 5747.91 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1740/6690 [00:00<00:00, 5765.65 examples/s]Extracting prompt in train dataset:  35%|███▍      | 2320/6690 [00:00<00:00, 5747.92 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2610/6690 [00:00<00:00, 5797.08 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2572/6690 [00:00<00:00, 5670.05 examples/s]Extracting prompt in train dataset:  44%|████▎     | 2915/6690 [00:00<00:00, 5801.82 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  48%|████▊     | 3200/6690 [00:00<00:00, 5824.51 examples/s]Extracting prompt in train dataset:  51%|█████     | 3410/6690 [00:00<00:00, 5632.22 examples/s]Extracting prompt in train dataset:  52%|█████▏    | 3507/6690 [00:00<00:00, 5825.46 examples/s]Applying chat template to eval dataset:  33%|███▎      | 314/953 [00:00<00:00, 3111.69 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 3800/6690 [00:00<00:00, 5844.86 examples/s]Extracting prompt in train dataset:  61%|██████▏   | 4099/6690 [00:00<00:00, 5840.79 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 640/953 [00:00<00:00, 3192.49 examples/s]Extracting prompt in train dataset:  64%|██████▎   | 4253/6690 [00:00<00:00, 5621.53 examples/s]Extracting prompt in train dataset:  70%|██████▉   | 4680/6690 [00:00<00:00, 5769.37 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3186.38 examples/s]
Extracting prompt in train dataset:  74%|███████▍  | 4970/6690 [00:00<00:00, 5736.58 examples/s]Extracting prompt in train dataset:  76%|███████▌  | 5090/6690 [00:00<00:00, 5592.68 examples/s]Extracting prompt in train dataset:  79%|███████▉  | 5270/6690 [00:00<00:00, 5799.57 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 5560/6690 [00:00<00:00, 5769.03 examples/s]Extracting prompt in train dataset:  85%|████████▍ | 5672/6690 [00:01<00:00, 5648.45 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 5870/6690 [00:01<00:00, 5823.10 examples/s]Extracting prompt in train dataset:  92%|█████████▏| 6150/6690 [00:01<00:00, 5795.15 examples/s]Extracting prompt in train dataset:  93%|█████████▎| 6255/6690 [00:01<00:00, 5694.38 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 6468/6690 [00:01<00:00, 5853.91 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5773.17 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5743.47 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5630.34 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.81 examples/s]Tokenizing eval dataset:   8%|▊         | 78/953 [00:00<00:03, 290.50 examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 273.79 examples/s]Applying chat template to train dataset:   4%|▍         | 300/6690 [00:00<00:02, 2964.88 examples/s]Applying chat template to train dataset:   4%|▍         | 297/6690 [00:00<00:02, 2942.52 examples/s]Applying chat template to train dataset:   4%|▍         | 290/6690 [00:00<00:02, 2862.41 examples/s]Applying chat template to train dataset:   9%|▉         | 630/6690 [00:00<00:01, 3147.51 examples/s]Applying chat template to train dataset:   9%|▉         | 620/6690 [00:00<00:01, 3109.91 examples/s]Applying chat template to train dataset:   9%|▉         | 610/6690 [00:00<00:02, 3037.80 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 266.19 examples/s]Applying chat template to train dataset:  14%|█▍        | 959/6690 [00:00<00:01, 3209.21 examples/s]Applying chat template to train dataset:  14%|█▍        | 946/6690 [00:00<00:01, 3177.13 examples/s]Applying chat template to train dataset:  14%|█▍        | 930/6690 [00:00<00:01, 3099.15 examples/s]Applying chat template to train dataset:  19%|█▉        | 1284/6690 [00:00<00:01, 3222.37 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 252.20 examples/s]Applying chat template to train dataset:  19%|█▉        | 1271/6690 [00:00<00:01, 3196.59 examples/s]Applying chat template to train dataset:  19%|█▊        | 1248/6690 [00:00<00:01, 3123.21 examples/s]Applying chat template to train dataset:  24%|██▍       | 1611/6690 [00:00<00:01, 3238.38 examples/s]Tokenizing eval dataset:  24%|██▍       | 230/953 [00:00<00:02, 274.96 examples/s]Applying chat template to train dataset:  24%|██▍       | 1597/6690 [00:00<00:01, 3216.61 examples/s]Applying chat template to train dataset:  23%|██▎       | 1565/6690 [00:00<00:01, 3135.57 examples/s]Applying chat template to train dataset:  29%|██▉       | 1940/6690 [00:00<00:01, 3249.39 examples/s]Applying chat template to train dataset:  29%|██▊       | 1921/6690 [00:00<00:01, 3222.08 examples/s]Tokenizing eval dataset:  31%|███       | 295/953 [00:00<00:01, 370.56 examples/s]Applying chat template to train dataset:  28%|██▊       | 1882/6690 [00:00<00:01, 3145.02 examples/s]Applying chat template to train dataset:  34%|███▎      | 2248/6690 [00:00<00:01, 3234.30 examples/s]Tokenizing eval dataset:  38%|███▊      | 359/953 [00:01<00:01, 442.13 examples/s]Applying chat template to train dataset:  33%|███▎      | 2200/6690 [00:00<00:01, 3152.96 examples/s]Applying chat template to train dataset:  36%|███▋      | 2428/6690 [00:00<00:01, 3248.76 examples/s]Tokenizing eval dataset:  44%|████▍     | 420/953 [00:01<00:01, 486.49 examples/s]Applying chat template to train dataset:  41%|████      | 2756/6690 [00:00<00:01, 3255.27 examples/s]Applying chat template to train dataset:  41%|████      | 2731/6690 [00:00<00:01, 3223.39 examples/s]Applying chat template to train dataset:  40%|███▉      | 2663/6690 [00:00<00:01, 3123.39 examples/s]Tokenizing eval dataset:  51%|█████     | 488/953 [00:01<00:00, 539.10 examples/s]Applying chat template to train dataset:  46%|████▌     | 3085/6690 [00:00<00:01, 3260.95 examples/s]Applying chat template to train dataset:  46%|████▌     | 3057/6690 [00:00<00:01, 3231.87 examples/s]Applying chat template to train dataset:  45%|████▍     | 2980/6690 [00:00<00:01, 3127.42 examples/s]Tokenizing eval dataset:  58%|█████▊    | 555/953 [00:01<00:00, 573.84 examples/s]Applying chat template to train dataset:  51%|█████     | 3412/6690 [00:01<00:01, 3261.54 examples/s]Applying chat template to train dataset:  51%|█████     | 3383/6690 [00:01<00:01, 3234.12 examples/s]Applying chat template to train dataset:  49%|████▉     | 3300/6690 [00:01<00:01, 3140.55 examples/s]Tokenizing eval dataset:  65%|██████▍   | 616/953 [00:01<00:00, 583.18 examples/s]Applying chat template to train dataset:  56%|█████▌    | 3740/6690 [00:01<00:00, 3261.00 examples/s]Applying chat template to train dataset:  55%|█████▌    | 3708/6690 [00:01<00:00, 3237.49 examples/s]Applying chat template to train dataset:  54%|█████▍    | 3617/6690 [00:01<00:00, 3145.79 examples/s]Tokenizing eval dataset:  71%|███████▏  | 680/953 [00:01<00:00, 595.15 examples/s]Applying chat template to train dataset:  61%|██████    | 4069/6690 [00:01<00:00, 3267.17 examples/s]Applying chat template to train dataset:  60%|██████    | 4034/6690 [00:01<00:00, 3238.39 examples/s]Applying chat template to train dataset:  59%|█████▉    | 3934/6690 [00:01<00:00, 3151.29 examples/s]Applying chat template to train dataset:  65%|██████▌   | 4360/6690 [00:01<00:00, 3238.71 examples/s]Tokenizing eval dataset:  80%|███████▉  | 762/953 [00:01<00:00, 571.68 examples/s]Applying chat template to train dataset:  64%|██████▎   | 4251/6690 [00:01<00:00, 3155.45 examples/s]Applying chat template to train dataset:  68%|██████▊   | 4553/6690 [00:01<00:00, 3212.80 examples/s]Applying chat template to train dataset:  73%|███████▎  | 4880/6690 [00:01<00:00, 3223.37 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4834/6690 [00:01<00:00, 3201.87 examples/s]Tokenizing eval dataset:  88%|████████▊ | 839/953 [00:01<00:00, 541.14 examples/s]Applying chat template to train dataset:  70%|███████   | 4710/6690 [00:01<00:00, 3106.92 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5207/6690 [00:01<00:00, 3232.70 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5158/6690 [00:01<00:00, 3209.40 examples/s]Applying chat template to train dataset:  75%|███████▌  | 5026/6690 [00:01<00:00, 3119.81 examples/s]Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 516.45 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5534/6690 [00:01<00:00, 3240.47 examples/s]Applying chat template to train dataset:  82%|████████▏ | 5483/6690 [00:01<00:00, 3215.87 examples/s]Applying chat template to train dataset:  80%|███████▉  | 5342/6690 [00:01<00:00, 3128.23 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 452.56 examples/s]
Applying chat template to train dataset:  88%|████████▊ | 5860/6690 [00:01<00:00, 3242.16 examples/s]Applying chat template to train dataset:  87%|████████▋ | 5808/6690 [00:01<00:00, 3221.87 examples/s]Applying chat template to train dataset:  85%|████████▍ | 5656/6690 [00:01<00:00, 3129.15 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6188/6690 [00:01<00:00, 3250.83 examples/s]Applying chat template to train dataset:  92%|█████████▏| 6132/6690 [00:01<00:00, 3221.55 examples/s]Applying chat template to train dataset:  91%|█████████▏| 6120/6690 [00:01<00:00, 3108.64 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6514/6690 [00:02<00:00, 3247.25 examples/s]Applying chat template to train dataset:  97%|█████████▋| 6456/6690 [00:02<00:00, 3224.66 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3229.74 examples/s]
Applying chat template to train dataset:  96%|█████████▋| 6440/6690 [00:02<00:00, 3129.23 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3203.20 examples/s]
Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3121.49 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 43/6690 [00:00<00:15, 419.14 examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 43/6690 [00:00<00:15, 423.28 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:15, 437.24 examples/s]Tokenizing train dataset:   1%|          | 43/6690 [00:00<00:15, 422.51 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:14, 442.79 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 439.45 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:14, 440.83 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 443.61 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 471.92 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 439.92 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 475.75 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 471.50 examples/s]Tokenizing train dataset:   3%|▎         | 189/6690 [00:00<00:13, 472.46 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 475.15 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 469.07 examples/s]Tokenizing train dataset:   4%|▎         | 237/6690 [00:00<00:13, 472.28 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 473.22 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 470.33 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 459.40 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 463.15 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 473.78 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 460.46 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 477.26 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 481.80 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 474.65 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 485.34 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 484.01 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 482.69 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 487.15 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 485.27 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 485.47 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 488.22 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 485.72 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 486.17 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 488.55 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 486.45 examples/s]Tokenizing train dataset:   9%|▉         | 609/6690 [00:01<00:12, 486.41 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 488.69 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 493.40 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 485.45 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 496.15 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 504.58 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 493.15 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 507.73 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 509.79 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 505.23 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 512.37 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 510.26 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:12, 482.76 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:11, 485.15 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:12, 483.12 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 463.25 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:01<00:12, 465.78 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 464.29 examples/s]Tokenizing train dataset:  15%|█▌        | 1025/6690 [00:02<00:12, 459.85 examples/s]Tokenizing train dataset:  15%|█▌        | 1026/6690 [00:02<00:12, 463.30 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 469.82 examples/s]Tokenizing train dataset:  15%|█▌        | 1025/6690 [00:02<00:12, 460.80 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 471.36 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 470.25 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:12, 461.09 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:11, 462.82 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 460.62 examples/s]Tokenizing train dataset:  17%|█▋        | 1145/6690 [00:02<00:12, 461.18 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 462.00 examples/s]Tokenizing train dataset:  18%|█▊        | 1192/6690 [00:02<00:11, 461.31 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 453.12 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 454.93 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 452.99 examples/s]Tokenizing train dataset:  19%|█▉        | 1260/6690 [00:02<00:12, 452.31 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 455.33 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 467.34 examples/s]Tokenizing train dataset:  20%|█▉        | 1307/6690 [00:02<00:11, 452.76 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 469.43 examples/s]Tokenizing train dataset:  20%|██        | 1360/6690 [00:02<00:11, 469.65 examples/s]Tokenizing train dataset:  21%|██        | 1420/6690 [00:03<00:12, 436.73 examples/s]Tokenizing train dataset:  21%|██        | 1421/6690 [00:03<00:11, 440.12 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 437.55 examples/s]Tokenizing train dataset:  21%|██        | 1420/6690 [00:03<00:12, 436.43 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 439.48 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 452.22 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 437.69 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 454.06 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 452.37 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 452.26 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 454.65 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 452.06 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 446.68 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 448.86 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 446.84 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 447.23 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 449.71 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 459.97 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 448.12 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 462.58 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 460.89 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 449.82 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 452.60 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:04<00:10, 450.84 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 450.48 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:04<00:10, 453.79 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:04<00:10, 451.64 examples/s]Tokenizing train dataset:  29%|██▊       | 1912/6690 [00:04<00:10, 451.35 examples/s]Tokenizing train dataset:  29%|██▉       | 1933/6690 [00:04<00:10, 447.03 examples/s]Tokenizing train dataset:  29%|██▊       | 1912/6690 [00:04<00:10, 449.38 examples/s]Tokenizing train dataset:  29%|██▉       | 1960/6690 [00:04<00:10, 456.45 examples/s]Tokenizing train dataset:  30%|██▉       | 1997/6690 [00:04<00:10, 437.23 examples/s]Tokenizing train dataset:  29%|██▉       | 1960/6690 [00:04<00:10, 454.38 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:10, 433.33 examples/s]Tokenizing train dataset:  31%|███       | 2061/6690 [00:04<00:10, 430.53 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:10, 430.32 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 433.77 examples/s]Tokenizing train dataset:  32%|███▏      | 2111/6690 [00:04<00:10, 444.77 examples/s]Tokenizing train dataset:  32%|███▏      | 2141/6690 [00:04<00:10, 448.11 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 431.36 examples/s]Tokenizing train dataset:  32%|███▏      | 2160/6690 [00:04<00:10, 451.55 examples/s]Tokenizing train dataset:  33%|███▎      | 2193/6690 [00:04<00:09, 462.75 examples/s]Tokenizing train dataset:  32%|███▏      | 2141/6690 [00:04<00:10, 445.81 examples/s]Tokenizing train dataset:  33%|███▎      | 2211/6690 [00:04<00:09, 465.92 examples/s]Tokenizing train dataset:  33%|███▎      | 2192/6690 [00:04<00:09, 460.76 examples/s]Tokenizing train dataset:  34%|███▎      | 2246/6690 [00:04<00:09, 477.53 examples/s]Tokenizing train dataset:  34%|███▍      | 2280/6690 [00:04<00:09, 462.37 examples/s]Tokenizing train dataset:  34%|███▎      | 2245/6690 [00:04<00:09, 474.37 examples/s]Tokenizing train dataset:  35%|███▍      | 2309/6690 [00:04<00:09, 451.11 examples/s]Tokenizing train dataset:  35%|███▌      | 2347/6690 [00:05<00:09, 454.01 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:05<00:09, 460.80 examples/s]Tokenizing train dataset:  35%|███▍      | 2309/6690 [00:05<00:09, 448.44 examples/s]Tokenizing train dataset:  36%|███▌      | 2395/6690 [00:05<00:09, 458.75 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:05<00:09, 458.48 examples/s]Tokenizing train dataset:  36%|███▋      | 2430/6690 [00:05<00:09, 456.68 examples/s]Tokenizing train dataset:  37%|███▋      | 2469/6690 [00:05<00:09, 458.27 examples/s]Tokenizing train dataset:  37%|███▋      | 2478/6690 [00:05<00:09, 460.07 examples/s]Tokenizing train dataset:  36%|███▋      | 2430/6690 [00:05<00:09, 454.04 examples/s]Tokenizing train dataset:  38%|███▊      | 2526/6690 [00:05<00:08, 462.70 examples/s]Tokenizing train dataset:  37%|███▋      | 2478/6690 [00:05<00:09, 456.99 examples/s]Tokenizing train dataset:  38%|███▊      | 2544/6690 [00:05<00:08, 469.40 examples/s]Tokenizing train dataset:  39%|███▊      | 2581/6690 [00:05<00:08, 484.43 examples/s]Tokenizing train dataset:  38%|███▊      | 2526/6690 [00:05<00:09, 459.80 examples/s]Tokenizing train dataset:  39%|███▉      | 2595/6690 [00:05<00:08, 476.59 examples/s]Tokenizing train dataset:  39%|███▊      | 2580/6690 [00:05<00:08, 480.15 examples/s]Tokenizing train dataset:  40%|███▉      | 2649/6690 [00:05<00:08, 466.85 examples/s]Tokenizing train dataset:  40%|███▉      | 2658/6690 [00:05<00:08, 455.76 examples/s]Tokenizing train dataset:  39%|███▉      | 2629/6690 [00:05<00:08, 477.12 examples/s]Tokenizing train dataset:  41%|████      | 2714/6690 [00:05<00:08, 451.49 examples/s]Tokenizing train dataset:  41%|████      | 2724/6690 [00:05<00:08, 445.39 examples/s]Tokenizing train dataset:  40%|████      | 2693/6690 [00:05<00:08, 453.92 examples/s]Tokenizing train dataset:  41%|████▏     | 2776/6690 [00:06<00:08, 435.38 examples/s]Tokenizing train dataset:  42%|████▏     | 2787/6690 [00:06<00:09, 428.31 examples/s]Tokenizing train dataset:  41%|████      | 2755/6690 [00:05<00:09, 434.48 examples/s]Tokenizing train dataset:  42%|████▏     | 2833/6690 [00:06<00:08, 430.97 examples/s]Tokenizing train dataset:  42%|████▏     | 2841/6690 [00:06<00:08, 430.04 examples/s]Tokenizing train dataset:  42%|████▏     | 2816/6690 [00:06<00:09, 419.83 examples/s]Tokenizing train dataset:  43%|████▎     | 2878/6690 [00:06<00:08, 434.23 examples/s]Tokenizing train dataset:  43%|████▎     | 2889/6690 [00:06<00:08, 439.07 examples/s]Tokenizing train dataset:  43%|████▎     | 2867/6690 [00:06<00:08, 436.83 examples/s]Tokenizing train dataset:  44%|████▍     | 2930/6690 [00:06<00:09, 385.58 examples/s]Tokenizing train dataset:  44%|████▍     | 2946/6690 [00:06<00:09, 414.98 examples/s]Tokenizing train dataset:  44%|████▎     | 2917/6690 [00:06<00:08, 450.36 examples/s]Tokenizing train dataset:  45%|████▍     | 2981/6690 [00:06<00:09, 411.98 examples/s]Tokenizing train dataset:  45%|████▍     | 2996/6690 [00:06<00:08, 432.80 examples/s]Tokenizing train dataset:  44%|████▍     | 2967/6690 [00:06<00:08, 460.73 examples/s]Tokenizing train dataset:  46%|████▌     | 3046/6690 [00:06<00:08, 448.13 examples/s]Tokenizing train dataset:  45%|████▌     | 3033/6690 [00:06<00:08, 432.91 examples/s]Tokenizing train dataset:  45%|████▌     | 3018/6690 [00:06<00:07, 468.30 examples/s]Tokenizing train dataset:  46%|████▋     | 3102/6690 [00:06<00:07, 475.74 examples/s]Tokenizing train dataset:  46%|████▌     | 3090/6690 [00:06<00:07, 464.67 examples/s]Tokenizing train dataset:  46%|████▌     | 3071/6690 [00:06<00:07, 481.46 examples/s]Tokenizing train dataset:  47%|████▋     | 3120/6690 [00:06<00:07, 478.97 examples/s]Tokenizing train dataset:  47%|████▋     | 3167/6690 [00:06<00:07, 455.95 examples/s]Tokenizing train dataset:  47%|████▋     | 3156/6690 [00:06<00:07, 451.93 examples/s]Tokenizing train dataset:  48%|████▊     | 3186/6690 [00:06<00:07, 459.20 examples/s]Tokenizing train dataset:  48%|████▊     | 3241/6690 [00:07<00:07, 464.91 examples/s]Tokenizing train dataset:  48%|████▊     | 3228/6690 [00:07<00:07, 457.03 examples/s]Tokenizing train dataset:  48%|████▊     | 3240/6690 [00:07<00:07, 476.30 examples/s]Tokenizing train dataset:  49%|████▉     | 3276/6690 [00:07<00:07, 458.26 examples/s]Tokenizing train dataset:  49%|████▉     | 3302/6690 [00:07<00:07, 442.55 examples/s]Tokenizing train dataset:  49%|████▉     | 3301/6690 [00:07<00:07, 448.95 examples/s]Tokenizing train dataset:  50%|████▉     | 3333/6690 [00:07<00:07, 428.00 examples/s]Tokenizing train dataset:  50%|█████     | 3367/6690 [00:07<00:07, 435.33 examples/s]Tokenizing train dataset:  50%|█████     | 3378/6690 [00:07<00:07, 430.91 examples/s]Tokenizing train dataset:  50%|█████     | 3364/6690 [00:07<00:07, 434.82 examples/s]Tokenizing train dataset:  51%|█████▏    | 3429/6690 [00:07<00:07, 423.14 examples/s]Tokenizing train dataset:  51%|█████▏    | 3437/6690 [00:07<00:07, 415.95 examples/s]Tokenizing train dataset:  51%|█████     | 3427/6690 [00:07<00:07, 422.61 examples/s]Tokenizing train dataset:  52%|█████▏    | 3496/6690 [00:07<00:07, 425.85 examples/s]Tokenizing train dataset:  52%|█████▏    | 3483/6690 [00:07<00:07, 423.70 examples/s]Tokenizing train dataset:  52%|█████▏    | 3494/6690 [00:07<00:07, 423.64 examples/s]Tokenizing train dataset:  53%|█████▎    | 3526/6690 [00:07<00:07, 423.02 examples/s]Tokenizing train dataset:  53%|█████▎    | 3557/6690 [00:07<00:07, 414.55 examples/s]Tokenizing train dataset:  53%|█████▎    | 3554/6690 [00:07<00:07, 411.60 examples/s]Tokenizing train dataset:  54%|█████▎    | 3581/6690 [00:07<00:07, 394.49 examples/s]Tokenizing train dataset:  54%|█████▍    | 3615/6690 [00:07<00:07, 402.28 examples/s]Tokenizing train dataset:  54%|█████▍    | 3624/6690 [00:08<00:07, 401.54 examples/s]Tokenizing train dataset:  54%|█████▍    | 3612/6690 [00:07<00:07, 399.46 examples/s]Tokenizing train dataset:  55%|█████▍    | 3664/6690 [00:08<00:07, 417.14 examples/s]Tokenizing train dataset:  55%|█████▍    | 3670/6690 [00:08<00:07, 409.24 examples/s]Tokenizing train dataset:  55%|█████▍    | 3663/6690 [00:08<00:07, 417.03 examples/s]Tokenizing train dataset:  56%|█████▌    | 3722/6690 [00:08<00:07, 400.91 examples/s]Tokenizing train dataset:  56%|█████▌    | 3730/6690 [00:08<00:07, 397.30 examples/s]Tokenizing train dataset:  56%|█████▌    | 3719/6690 [00:08<00:07, 400.06 examples/s]Tokenizing train dataset:  56%|█████▋    | 3770/6690 [00:08<00:07, 411.59 examples/s]Tokenizing train dataset:  56%|█████▋    | 3775/6690 [00:08<00:07, 406.98 examples/s]Tokenizing train dataset:  56%|█████▌    | 3763/6690 [00:08<00:07, 405.22 examples/s]Tokenizing train dataset:  57%|█████▋    | 3819/6690 [00:08<00:06, 423.77 examples/s]Tokenizing train dataset:  57%|█████▋    | 3823/6690 [00:08<00:06, 422.73 examples/s]Tokenizing train dataset:  57%|█████▋    | 3808/6690 [00:08<00:06, 414.74 examples/s]Tokenizing train dataset:  58%|█████▊    | 3866/6690 [00:08<00:06, 420.65 examples/s]Tokenizing train dataset:  58%|█████▊    | 3891/6690 [00:08<00:06, 435.14 examples/s]Tokenizing train dataset:  58%|█████▊    | 3854/6690 [00:08<00:06, 423.52 examples/s]Tokenizing train dataset:  59%|█████▊    | 3919/6690 [00:08<00:06, 446.10 examples/s]Tokenizing train dataset:  59%|█████▉    | 3941/6690 [00:08<00:06, 448.34 examples/s]Tokenizing train dataset:  58%|█████▊    | 3904/6690 [00:08<00:06, 442.90 examples/s]Tokenizing train dataset:  59%|█████▉    | 3966/6690 [00:08<00:06, 450.69 examples/s]Tokenizing train dataset:  60%|█████▉    | 3990/6690 [00:08<00:05, 456.47 examples/s]Tokenizing train dataset:  59%|█████▉    | 3951/6690 [00:08<00:06, 450.07 examples/s]Tokenizing train dataset:  60%|██████    | 4018/6690 [00:08<00:05, 465.31 examples/s]Tokenizing train dataset:  60%|█████▉    | 4003/6690 [00:08<00:05, 461.90 examples/s]Tokenizing train dataset:  61%|██████    | 4058/6690 [00:08<00:05, 452.48 examples/s]Tokenizing train dataset:  61%|██████    | 4079/6690 [00:09<00:05, 440.78 examples/s]Tokenizing train dataset:  61%|██████    | 4068/6690 [00:08<00:05, 449.98 examples/s]Tokenizing train dataset:  62%|██████▏   | 4126/6690 [00:09<00:05, 445.93 examples/s]Tokenizing train dataset:  62%|██████▏   | 4127/6690 [00:09<00:05, 448.21 examples/s]Tokenizing train dataset:  62%|██████▏   | 4176/6690 [00:09<00:05, 458.48 examples/s]Tokenizing train dataset:  62%|██████▏   | 4136/6690 [00:09<00:05, 448.54 examples/s]Tokenizing train dataset:  62%|██████▏   | 4179/6690 [00:09<00:05, 463.08 examples/s]Tokenizing train dataset:  63%|██████▎   | 4187/6690 [00:09<00:05, 459.00 examples/s]Tokenizing train dataset:  63%|██████▎   | 4247/6690 [00:09<00:05, 458.83 examples/s]Tokenizing train dataset:  63%|██████▎   | 4248/6690 [00:09<00:05, 456.69 examples/s]Tokenizing train dataset:  64%|██████▎   | 4253/6690 [00:09<00:05, 450.72 examples/s]Tokenizing train dataset:  64%|██████▍   | 4314/6690 [00:09<00:05, 453.15 examples/s]Tokenizing train dataset:  65%|██████▍   | 4317/6690 [00:09<00:05, 451.14 examples/s]Tokenizing train dataset:  65%|██████▍   | 4323/6690 [00:09<00:05, 454.02 examples/s]Tokenizing train dataset:  65%|██████▌   | 4376/6690 [00:09<00:05, 435.87 examples/s]Tokenizing train dataset:  65%|██████▌   | 4379/6690 [00:09<00:05, 434.33 examples/s]Tokenizing train dataset:  66%|██████▌   | 4382/6690 [00:09<00:05, 429.44 examples/s]Tokenizing train dataset:  66%|██████▌   | 4423/6690 [00:09<00:05, 439.83 examples/s]Tokenizing train dataset:  66%|██████▌   | 4425/6690 [00:09<00:05, 436.00 examples/s]Tokenizing train dataset:  66%|██████▌   | 4429/6690 [00:09<00:05, 435.41 examples/s]Tokenizing train dataset:  67%|██████▋   | 4489/6690 [00:09<00:05, 438.49 examples/s]Tokenizing train dataset:  67%|██████▋   | 4469/6690 [00:09<00:05, 431.33 examples/s]Tokenizing train dataset:  68%|██████▊   | 4535/6690 [00:10<00:04, 440.34 examples/s]Tokenizing train dataset:  67%|██████▋   | 4495/6690 [00:09<00:05, 432.32 examples/s]Tokenizing train dataset:  68%|██████▊   | 4516/6690 [00:10<00:04, 438.42 examples/s]Tokenizing train dataset:  68%|██████▊   | 4544/6690 [00:10<00:04, 443.26 examples/s]Tokenizing train dataset:  68%|██████▊   | 4562/6690 [00:10<00:04, 439.99 examples/s]Tokenizing train dataset:  69%|██████▊   | 4585/6690 [00:10<00:04, 444.43 examples/s]Tokenizing train dataset:  69%|██████▉   | 4610/6690 [00:10<00:04, 448.03 examples/s]Tokenizing train dataset:  69%|██████▉   | 4636/6690 [00:10<00:04, 459.33 examples/s]Tokenizing train dataset:  69%|██████▉   | 4617/6690 [00:10<00:04, 455.49 examples/s]Tokenizing train dataset:  70%|██████▉   | 4662/6690 [00:10<00:04, 465.33 examples/s]Tokenizing train dataset:  70%|███████   | 4685/6690 [00:10<00:04, 462.22 examples/s]Tokenizing train dataset:  70%|██████▉   | 4666/6690 [00:10<00:04, 459.61 examples/s]Tokenizing train dataset:  70%|███████   | 4710/6690 [00:10<00:04, 464.38 examples/s]Tokenizing train dataset:  71%|███████   | 4736/6690 [00:10<00:04, 469.71 examples/s]Tokenizing train dataset:  71%|███████   | 4737/6690 [00:10<00:04, 462.33 examples/s]Tokenizing train dataset:  71%|███████   | 4757/6690 [00:10<00:04, 460.81 examples/s]Tokenizing train dataset:  72%|███████▏  | 4795/6690 [00:10<00:04, 434.74 examples/s]Tokenizing train dataset:  72%|███████▏  | 4795/6690 [00:10<00:04, 431.61 examples/s]Tokenizing train dataset:  72%|███████▏  | 4815/6690 [00:10<00:04, 427.85 examples/s]Tokenizing train dataset:  72%|███████▏  | 4846/6690 [00:10<00:04, 449.84 examples/s]Tokenizing train dataset:  72%|███████▏  | 4846/6690 [00:10<00:04, 446.00 examples/s]Tokenizing train dataset:  73%|███████▎  | 4863/6690 [00:10<00:04, 439.21 examples/s]Tokenizing train dataset:  73%|███████▎  | 4910/6690 [00:10<00:04, 433.27 examples/s]Tokenizing train dataset:  73%|███████▎  | 4910/6690 [00:10<00:04, 431.20 examples/s]Tokenizing train dataset:  74%|███████▎  | 4928/6690 [00:11<00:04, 430.55 examples/s]Tokenizing train dataset:  74%|███████▍  | 4957/6690 [00:10<00:03, 440.43 examples/s]Tokenizing train dataset:  74%|███████▍  | 4956/6690 [00:10<00:03, 436.57 examples/s]Tokenizing train dataset:  74%|███████▍  | 4972/6690 [00:11<00:04, 428.78 examples/s]Tokenizing train dataset:  75%|███████▌  | 5022/6690 [00:11<00:03, 434.09 examples/s]Tokenizing train dataset:  75%|███████▌  | 5019/6690 [00:11<00:03, 433.16 examples/s]Tokenizing train dataset:  75%|███████▌  | 5022/6690 [00:11<00:03, 432.53 examples/s]Tokenizing train dataset:  76%|███████▌  | 5085/6690 [00:11<00:03, 427.50 examples/s]Tokenizing train dataset:  76%|███████▌  | 5081/6690 [00:11<00:03, 424.85 examples/s]Tokenizing train dataset:  76%|███████▌  | 5084/6690 [00:11<00:03, 425.84 examples/s]Tokenizing train dataset:  77%|███████▋  | 5129/6690 [00:11<00:03, 427.75 examples/s]Tokenizing train dataset:  77%|███████▋  | 5143/6690 [00:11<00:03, 418.74 examples/s]Tokenizing train dataset:  77%|███████▋  | 5178/6690 [00:11<00:03, 438.63 examples/s]Tokenizing train dataset:  77%|███████▋  | 5144/6690 [00:11<00:03, 414.24 examples/s]Tokenizing train dataset:  78%|███████▊  | 5195/6690 [00:11<00:03, 441.11 examples/s]Tokenizing train dataset:  78%|███████▊  | 5223/6690 [00:11<00:03, 438.64 examples/s]Tokenizing train dataset:  78%|███████▊  | 5195/6690 [00:11<00:03, 434.90 examples/s]Tokenizing train dataset:  79%|███████▉  | 5269/6690 [00:11<00:03, 442.67 examples/s]Tokenizing train dataset:  79%|███████▊  | 5260/6690 [00:11<00:03, 435.55 examples/s]Tokenizing train dataset:  79%|███████▊  | 5260/6690 [00:11<00:03, 432.15 examples/s]Tokenizing train dataset:  80%|███████▉  | 5333/6690 [00:11<00:03, 426.26 examples/s]Tokenizing train dataset:  80%|███████▉  | 5323/6690 [00:11<00:03, 427.65 examples/s]Tokenizing train dataset:  80%|███████▉  | 5323/6690 [00:11<00:03, 425.55 examples/s]Tokenizing train dataset:  81%|████████  | 5388/6690 [00:11<00:02, 456.14 examples/s]Tokenizing train dataset:  80%|████████  | 5375/6690 [00:12<00:02, 447.25 examples/s]Tokenizing train dataset:  80%|████████  | 5375/6690 [00:11<00:02, 445.40 examples/s]Tokenizing train dataset:  82%|████████▏ | 5454/6690 [00:12<00:02, 444.54 examples/s]Tokenizing train dataset:  81%|████████▏ | 5440/6690 [00:12<00:02, 436.53 examples/s]Tokenizing train dataset:  81%|████████▏ | 5440/6690 [00:12<00:02, 435.85 examples/s]Tokenizing train dataset:  82%|████████▏ | 5500/6690 [00:12<00:02, 444.24 examples/s]Tokenizing train dataset:  82%|████████▏ | 5490/6690 [00:12<00:02, 446.57 examples/s]Tokenizing train dataset:  82%|████████▏ | 5490/6690 [00:12<00:02, 446.29 examples/s]Tokenizing train dataset:  83%|████████▎ | 5566/6690 [00:12<00:02, 438.36 examples/s]Tokenizing train dataset:  83%|████████▎ | 5554/6690 [00:12<00:02, 437.48 examples/s]Tokenizing train dataset:  83%|████████▎ | 5554/6690 [00:12<00:02, 437.55 examples/s]Tokenizing train dataset:  84%|████████▍ | 5632/6690 [00:12<00:02, 435.60 examples/s]Tokenizing train dataset:  84%|████████▍ | 5617/6690 [00:12<00:02, 428.34 examples/s]Tokenizing train dataset:  84%|████████▍ | 5617/6690 [00:12<00:02, 428.45 examples/s]Tokenizing train dataset:  85%|████████▍ | 5679/6690 [00:12<00:02, 437.74 examples/s]Tokenizing train dataset:  85%|████████▍ | 5661/6690 [00:12<00:02, 429.12 examples/s]Tokenizing train dataset:  85%|████████▍ | 5661/6690 [00:12<00:02, 429.43 examples/s]Tokenizing train dataset:  85%|████████▌ | 5707/6690 [00:12<00:02, 434.98 examples/s]Tokenizing train dataset:  86%|████████▌ | 5750/6690 [00:12<00:02, 445.75 examples/s]Tokenizing train dataset:  85%|████████▌ | 5707/6690 [00:12<00:02, 435.54 examples/s]Tokenizing train dataset:  86%|████████▌ | 5757/6690 [00:12<00:02, 449.83 examples/s]Tokenizing train dataset:  86%|████████▌ | 5757/6690 [00:12<00:02, 450.49 examples/s]Tokenizing train dataset:  87%|████████▋ | 5817/6690 [00:12<00:01, 443.75 examples/s]Tokenizing train dataset:  87%|████████▋ | 5821/6690 [00:13<00:01, 439.08 examples/s]Tokenizing train dataset:  87%|████████▋ | 5821/6690 [00:12<00:01, 439.07 examples/s]Tokenizing train dataset:  88%|████████▊ | 5876/6690 [00:13<00:01, 425.52 examples/s]Tokenizing train dataset:  88%|████████▊ | 5881/6690 [00:13<00:01, 422.28 examples/s]Tokenizing train dataset:  89%|████████▊ | 5926/6690 [00:13<00:01, 442.11 examples/s]Tokenizing train dataset:  88%|████████▊ | 5881/6690 [00:13<00:01, 422.68 examples/s]Tokenizing train dataset:  89%|████████▊ | 5935/6690 [00:13<00:01, 445.02 examples/s]Tokenizing train dataset:  89%|████████▉ | 5973/6690 [00:13<00:01, 444.17 examples/s]Tokenizing train dataset:  89%|████████▊ | 5935/6690 [00:13<00:01, 445.37 examples/s]Tokenizing train dataset:  90%|████████▉ | 6019/6690 [00:13<00:01, 446.82 examples/s]Tokenizing train dataset:  90%|████████▉ | 6003/6690 [00:13<00:01, 442.66 examples/s]Tokenizing train dataset:  90%|████████▉ | 6003/6690 [00:13<00:01, 443.53 examples/s]Tokenizing train dataset:  91%|█████████ | 6090/6690 [00:13<00:01, 453.88 examples/s]Tokenizing train dataset:  91%|█████████ | 6072/6690 [00:13<00:01, 445.38 examples/s]Tokenizing train dataset:  91%|█████████ | 6072/6690 [00:13<00:01, 446.33 examples/s]Tokenizing train dataset:  92%|█████████▏| 6142/6690 [00:13<00:01, 469.90 examples/s]Tokenizing train dataset:  92%|█████████▏| 6127/6690 [00:13<00:01, 463.85 examples/s]Tokenizing train dataset:  92%|█████████▏| 6127/6690 [00:13<00:01, 465.11 examples/s]Tokenizing train dataset:  93%|█████████▎| 6207/6690 [00:13<00:01, 451.50 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6690 [00:13<00:01, 453.01 examples/s]Tokenizing train dataset:  93%|█████████▎| 6195/6690 [00:13<00:01, 454.47 examples/s]Tokenizing train dataset:  94%|█████████▎| 6260/6690 [00:13<00:01, 416.64 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:14<00:01, 416.19 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:13<00:01, 417.58 examples/s]Tokenizing train dataset:  94%|█████████▍| 6312/6690 [00:14<00:00, 433.31 examples/s]Tokenizing train dataset:  94%|█████████▍| 6298/6690 [00:14<00:00, 430.42 examples/s]Tokenizing train dataset:  94%|█████████▍| 6299/6690 [00:14<00:00, 432.47 examples/s]Tokenizing train dataset:  95%|█████████▌| 6371/6690 [00:14<00:00, 417.88 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 416.07 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 417.40 examples/s]Tokenizing train dataset:  96%|█████████▌| 6420/6690 [00:14<00:00, 431.75 examples/s]Tokenizing train dataset:  96%|█████████▌| 6403/6690 [00:14<00:00, 417.05 examples/s]Tokenizing train dataset:  96%|█████████▌| 6404/6690 [00:14<00:00, 420.23 examples/s]Tokenizing train dataset:  97%|█████████▋| 6465/6690 [00:14<00:00, 432.99 examples/s]Tokenizing train dataset:  96%|█████████▋| 6451/6690 [00:14<00:00, 430.45 examples/s]Tokenizing train dataset:  96%|█████████▋| 6451/6690 [00:14<00:00, 431.05 examples/s]Tokenizing train dataset:  97%|█████████▋| 6510/6690 [00:14<00:00, 432.28 examples/s]Tokenizing train dataset:  97%|█████████▋| 6496/6690 [00:14<00:00, 435.28 examples/s]Tokenizing train dataset:  97%|█████████▋| 6497/6690 [00:14<00:00, 433.89 examples/s]Tokenizing train dataset:  98%|█████████▊| 6554/6690 [00:14<00:00, 428.90 examples/s]Tokenizing train dataset:  98%|█████████▊| 6541/6690 [00:14<00:00, 432.13 examples/s]Tokenizing train dataset:  98%|█████████▊| 6560/6690 [00:14<00:00, 427.34 examples/s]Tokenizing train dataset:  99%|█████████▊| 6599/6690 [00:14<00:00, 430.99 examples/s]Tokenizing train dataset:  98%|█████████▊| 6588/6690 [00:14<00:00, 437.88 examples/s]Tokenizing train dataset:  99%|█████████▉| 6623/6690 [00:14<00:00, 421.79 examples/s]Tokenizing train dataset: 100%|█████████▉| 6658/6690 [00:14<00:00, 413.34 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:14<00:00, 447.01 examples/s]
Tokenizing train dataset:  99%|█████████▉| 6648/6690 [00:14<00:00, 418.01 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 423.24 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 443.52 examples/s]
Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 445.80 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  59%|█████▉    | 567/953 [00:00<00:00, 5633.71 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5532.24 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5583.05 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5574.89 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5581.25 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5545.45 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  33%|███▎      | 319/953 [00:00<00:00, 3164.25 examples/s]Applying chat template to eval dataset:  32%|███▏      | 306/953 [00:00<00:00, 3019.61 examples/s]Applying chat template to eval dataset:  33%|███▎      | 313/953 [00:00<00:00, 3105.20 examples/s]Applying chat template to eval dataset:  81%|████████  | 769/953 [00:00<00:00, 3039.91 examples/s]Applying chat template to eval dataset:  78%|███████▊  | 739/953 [00:00<00:00, 2920.55 examples/s]Applying chat template to eval dataset:  78%|███████▊  | 746/953 [00:00<00:00, 2950.40 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2995.92 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2873.49 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2885.15 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 266.53 examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 266.19 examples/s]Tokenizing eval dataset:   3%|▎         | 29/953 [00:00<00:03, 279.39 examples/s]Tokenizing eval dataset:   7%|▋         | 63/953 [00:00<00:03, 242.19 examples/s]Tokenizing eval dataset:   7%|▋         | 63/953 [00:00<00:03, 242.35 examples/s]Tokenizing eval dataset:   7%|▋         | 65/953 [00:00<00:03, 244.99 examples/s]Tokenizing eval dataset:  10%|▉         | 91/953 [00:00<00:03, 248.77 examples/s]Tokenizing eval dataset:  10%|▉         | 91/953 [00:00<00:03, 247.73 examples/s]Tokenizing eval dataset:  10%|▉         | 91/953 [00:00<00:03, 245.49 examples/s]Tokenizing eval dataset:  12%|█▏        | 116/953 [00:00<00:03, 242.66 examples/s]Tokenizing eval dataset:  13%|█▎        | 126/953 [00:00<00:03, 235.00 examples/s]Tokenizing eval dataset:  13%|█▎        | 126/953 [00:00<00:03, 234.50 examples/s]Tokenizing eval dataset:  16%|█▌        | 150/953 [00:00<00:03, 229.99 examples/s]Tokenizing eval dataset:  16%|█▌        | 151/953 [00:00<00:03, 227.48 examples/s]Tokenizing eval dataset:  17%|█▋        | 160/953 [00:00<00:03, 227.40 examples/s]Tokenizing eval dataset:  19%|█▉        | 182/953 [00:00<00:03, 216.98 examples/s]Tokenizing eval dataset:  19%|█▉        | 182/953 [00:00<00:03, 216.49 examples/s]Tokenizing eval dataset:  20%|██        | 191/953 [00:00<00:03, 215.19 examples/s]Tokenizing eval dataset:  22%|██▏       | 207/953 [00:00<00:03, 222.30 examples/s]Tokenizing eval dataset:  22%|██▏       | 207/953 [00:00<00:03, 222.20 examples/s]Tokenizing eval dataset:  23%|██▎       | 218/953 [00:00<00:03, 226.82 examples/s]Tokenizing eval dataset:  26%|██▌       | 248/953 [00:01<00:02, 269.85 examples/s]Tokenizing eval dataset:  26%|██▌       | 247/953 [00:01<00:02, 268.29 examples/s]Tokenizing eval dataset:  29%|██▊       | 273/953 [00:01<00:02, 308.78 examples/s]Tokenizing eval dataset:  33%|███▎      | 310/953 [00:01<00:01, 363.73 examples/s]Tokenizing eval dataset:  33%|███▎      | 311/953 [00:01<00:01, 367.47 examples/s]Tokenizing eval dataset:  36%|███▌      | 339/953 [00:01<00:01, 399.14 examples/s]Tokenizing eval dataset:  39%|███▊      | 369/953 [00:01<00:01, 426.58 examples/s]Tokenizing eval dataset:  39%|███▉      | 370/953 [00:01<00:01, 428.36 examples/s]Tokenizing eval dataset:  42%|████▏     | 397/953 [00:01<00:01, 446.71 examples/s]Tokenizing eval dataset:  46%|████▌     | 437/953 [00:01<00:01, 496.69 examples/s]Tokenizing eval dataset:  46%|████▌     | 439/953 [00:01<00:01, 498.69 examples/s]Tokenizing eval dataset:  49%|████▉     | 469/953 [00:01<00:00, 517.53 examples/s]Tokenizing eval dataset:  53%|█████▎    | 503/953 [00:01<00:00, 542.33 examples/s]Tokenizing eval dataset:  53%|█████▎    | 505/953 [00:01<00:00, 540.38 examples/s]Tokenizing eval dataset:  56%|█████▌    | 529/953 [00:01<00:00, 537.12 examples/s]Tokenizing eval dataset:  59%|█████▉    | 565/953 [00:01<00:00, 563.22 examples/s]Tokenizing eval dataset:  59%|█████▉    | 567/953 [00:01<00:00, 562.05 examples/s]Tokenizing eval dataset:  63%|██████▎   | 599/953 [00:01<00:00, 580.03 examples/s]Tokenizing eval dataset:  66%|██████▌   | 630/953 [00:01<00:00, 583.85 examples/s]Tokenizing eval dataset:  66%|██████▋   | 633/953 [00:01<00:00, 588.11 examples/s]Tokenizing eval dataset:  69%|██████▉   | 660/953 [00:01<00:00, 581.00 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 586.66 examples/s]Tokenizing eval dataset:  73%|███████▎  | 693/953 [00:01<00:00, 588.01 examples/s]Tokenizing eval dataset:  76%|███████▌  | 720/953 [00:01<00:00, 579.61 examples/s]Tokenizing eval dataset:  81%|████████  | 772/953 [00:01<00:00, 564.46 examples/s]Tokenizing eval dataset:  81%|████████▏ | 776/953 [00:01<00:00, 564.39 examples/s]Tokenizing eval dataset:  83%|████████▎ | 793/953 [00:01<00:00, 541.61 examples/s]Tokenizing eval dataset:  88%|████████▊ | 842/953 [00:02<00:00, 526.48 examples/s]Tokenizing eval dataset:  89%|████████▉ | 848/953 [00:02<00:00, 529.04 examples/s]Tokenizing eval dataset:  91%|█████████ | 864/953 [00:02<00:00, 512.51 examples/s]Tokenizing eval dataset:  96%|█████████▌| 913/953 [00:02<00:00, 500.80 examples/s]Tokenizing eval dataset:  96%|█████████▌| 915/953 [00:02<00:00, 499.46 examples/s]Tokenizing eval dataset:  99%|█████████▊| 941/953 [00:02<00:00, 504.82 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 421.47 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 421.07 examples/s]
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 422.50 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.5929019451141357 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3704121112823486 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.388195753097534 seconds
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.4771242141723633 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank4]:[W612 23:16:12.833669914 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 1 ---
