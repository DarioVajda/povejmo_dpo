cpu-bind=MASK - gn52, task  2  0 [803819]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 2 ---
Total Nodes: 4
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn33
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 4     --machine_rank 2     --main_process_ip gn33     --main_process_port 29500     --num_processes 16     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62950704     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.2 --curriculum_stage=2
-------------------------------------------
[2025-06-10 23:53:46,123] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0610 23:53:47.990000 803872 torch/distributed/run.py:792] 
W0610 23:53:47.990000 803872 torch/distributed/run.py:792] *****************************************
W0610 23:53:47.990000 803872 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0610 23:53:47.990000 803872 torch/distributed/run.py:792] *****************************************
[2025-06-10 23:53:53,934] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 23:53:53,950] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 23:53:53,972] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 23:53:53,984] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=2)
4e-07
World size: 16
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=2)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=2)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=2)
4e-07
Created datasets
Train dataset size: 6690
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-10 23:54:00,494] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-10 23:54:00,499] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-10 23:54:00,512] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-10 23:54:00,516] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curri-1
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.50s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.78s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.78s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:50<00:50, 25.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:50<00:50, 25.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:50<00:50, 25.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:50<00:50, 25.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:15<00:25, 25.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:15<00:25, 25.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:15<00:25, 25.26s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:15<00:25, 25.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 21.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.84s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 21.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.89s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 21.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 21.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.89s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.89s/it]
Loaded model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank10]:[W610 23:55:36.066317120 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 10]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank11]:[W610 23:55:36.253902302 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 11]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s][rank9]:[W610 23:55:36.329914712 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 9]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   8%|▊         | 550/6690 [00:00<00:01, 5444.32 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1120/6690 [00:00<00:01, 5559.36 examples/s]Extracting prompt in train dataset:  25%|██▌       | 1690/6690 [00:00<00:00, 5620.71 examples/s]Extracting prompt in train dataset:  34%|███▎      | 2253/6690 [00:00<00:00, 5605.89 examples/s]Extracting prompt in train dataset:  42%|████▏     | 2829/6690 [00:00<00:00, 5641.44 examples/s]Extracting prompt in train dataset:  51%|█████     | 3418/6690 [00:00<00:00, 5710.80 examples/s]Extracting prompt in train dataset:  60%|█████▉    | 3990/6690 [00:00<00:00, 5695.19 examples/s]Extracting prompt in train dataset:  72%|███████▏  | 4837/6690 [00:00<00:00, 5659.39 examples/s]Extracting prompt in train dataset:  81%|████████  | 5410/6690 [00:00<00:00, 5658.23 examples/s]Extracting prompt in train dataset:  90%|████████▉ | 5990/6690 [00:01<00:00, 5684.64 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 6580/6690 [00:01<00:00, 5735.86 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5606.21 examples/s]
Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 286/6690 [00:00<00:02, 2832.06 examples/s]Applying chat template to train dataset:   9%|▉         | 605/6690 [00:00<00:02, 3035.43 examples/s]Applying chat template to train dataset:  14%|█▍        | 921/6690 [00:00<00:01, 3089.62 examples/s]Applying chat template to train dataset:  19%|█▊        | 1239/6690 [00:00<00:01, 3115.38 examples/s]Applying chat template to train dataset:  23%|██▎       | 1560/6690 [00:00<00:01, 3146.39 examples/s]Applying chat template to train dataset:  28%|██▊       | 1884/6690 [00:00<00:01, 3176.54 examples/s]Applying chat template to train dataset:  35%|███▌      | 2355/6690 [00:00<00:01, 3155.55 examples/s]Applying chat template to train dataset:  42%|████▏     | 2829/6690 [00:00<00:01, 3152.71 examples/s]Applying chat template to train dataset:  49%|████▉     | 3302/6690 [00:01<00:01, 3149.28 examples/s]Applying chat template to train dataset:  54%|█████▍    | 3618/6690 [00:01<00:00, 3146.46 examples/s]Applying chat template to train dataset:  59%|█████▉    | 3933/6690 [00:01<00:00, 3144.69 examples/s]Applying chat template to train dataset:  64%|██████▎   | 4250/6690 [00:01<00:00, 3145.56 examples/s]Applying chat template to train dataset:  70%|███████   | 4705/6690 [00:01<00:00, 3097.23 examples/s]Applying chat template to train dataset:  75%|███████▌  | 5019/6690 [00:01<00:00, 3106.52 examples/s]Applying chat template to train dataset:  80%|███████▉  | 5332/6690 [00:01<00:00, 3112.24 examples/s]Applying chat template to train dataset:  84%|████████▍ | 5648/6690 [00:01<00:00, 3120.48 examples/s]Applying chat template to train dataset:  89%|████████▉ | 5962/6690 [00:01<00:00, 3123.88 examples/s]Applying chat template to train dataset:  94%|█████████▍| 6278/6690 [00:02<00:00, 3129.99 examples/s]Applying chat template to train dataset:  99%|█████████▊| 6592/6690 [00:02<00:00, 3129.83 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3118.39 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 38/6690 [00:00<00:17, 372.93 examples/s]Tokenizing train dataset:   1%|▏         | 88/6690 [00:00<00:15, 436.63 examples/s]Tokenizing train dataset:   2%|▏         | 155/6690 [00:00<00:15, 432.64 examples/s]Tokenizing train dataset:   3%|▎         | 204/6690 [00:00<00:14, 447.85 examples/s]Tokenizing train dataset:   4%|▍         | 254/6690 [00:00<00:14, 458.39 examples/s]Tokenizing train dataset:   5%|▍         | 319/6690 [00:00<00:14, 442.32 examples/s]Tokenizing train dataset:   6%|▌         | 370/6690 [00:00<00:13, 456.95 examples/s]Tokenizing train dataset:   7%|▋         | 442/6690 [00:00<00:13, 463.11 examples/s]Tokenizing train dataset:   7%|▋         | 491/6690 [00:01<00:13, 468.51 examples/s]Tokenizing train dataset:   8%|▊         | 540/6690 [00:01<00:13, 467.63 examples/s]Tokenizing train dataset:   9%|▉         | 589/6690 [00:01<00:12, 469.48 examples/s]Tokenizing train dataset:  10%|▉         | 641/6690 [00:01<00:12, 476.54 examples/s]Tokenizing train dataset:  10%|█         | 690/6690 [00:01<00:12, 474.46 examples/s]Tokenizing train dataset:  11%|█         | 742/6690 [00:01<00:12, 484.68 examples/s]Tokenizing train dataset:  12%|█▏        | 793/6690 [00:01<00:12, 484.81 examples/s]Tokenizing train dataset:  13%|█▎        | 865/6690 [00:01<00:12, 478.68 examples/s]Tokenizing train dataset:  14%|█▍        | 923/6690 [00:02<00:13, 441.89 examples/s]Tokenizing train dataset:  15%|█▍        | 972/6690 [00:02<00:12, 451.56 examples/s]Tokenizing train dataset:  16%|█▌        | 1039/6690 [00:02<00:12, 445.94 examples/s]Tokenizing train dataset:  16%|█▋        | 1088/6690 [00:02<00:12, 451.31 examples/s]Tokenizing train dataset:  17%|█▋        | 1155/6690 [00:02<00:12, 444.20 examples/s]Tokenizing train dataset:  18%|█▊        | 1203/6690 [00:02<00:12, 446.56 examples/s]Tokenizing train dataset:  19%|█▉        | 1268/6690 [00:02<00:12, 437.38 examples/s]Tokenizing train dataset:  20%|█▉        | 1317/6690 [00:02<00:12, 447.33 examples/s]Tokenizing train dataset:  20%|██        | 1363/6690 [00:02<00:11, 446.50 examples/s]Tokenizing train dataset:  21%|██▏       | 1422/6690 [00:03<00:12, 424.99 examples/s]Tokenizing train dataset:  22%|██▏       | 1485/6690 [00:03<00:12, 421.55 examples/s]Tokenizing train dataset:  23%|██▎       | 1540/6690 [00:03<00:11, 447.73 examples/s]Tokenizing train dataset:  24%|██▍       | 1603/6690 [00:03<00:11, 434.48 examples/s]Tokenizing train dataset:  25%|██▍       | 1668/6690 [00:03<00:11, 431.27 examples/s]Tokenizing train dataset:  26%|██▌       | 1715/6690 [00:03<00:11, 438.86 examples/s]Tokenizing train dataset:  26%|██▋       | 1762/6690 [00:03<00:11, 442.16 examples/s]Tokenizing train dataset:  27%|██▋       | 1807/6690 [00:04<00:11, 438.73 examples/s]Tokenizing train dataset:  28%|██▊       | 1876/6690 [00:04<00:10, 441.74 examples/s]Tokenizing train dataset:  29%|██▉       | 1946/6690 [00:04<00:10, 438.94 examples/s]Tokenizing train dataset:  30%|██▉       | 2003/6690 [00:04<00:11, 417.88 examples/s]Tokenizing train dataset:  31%|███       | 2046/6690 [00:04<00:11, 419.49 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 422.45 examples/s]Tokenizing train dataset:  32%|███▏      | 2139/6690 [00:04<00:10, 436.98 examples/s]Tokenizing train dataset:  33%|███▎      | 2188/6690 [00:04<00:09, 450.50 examples/s]Tokenizing train dataset:  33%|███▎      | 2238/6690 [00:05<00:09, 462.22 examples/s]Tokenizing train dataset:  34%|███▍      | 2300/6690 [00:05<00:09, 439.37 examples/s]Tokenizing train dataset:  35%|███▌      | 2347/6690 [00:05<00:09, 443.01 examples/s]Tokenizing train dataset:  36%|███▌      | 2395/6690 [00:05<00:09, 448.05 examples/s]Tokenizing train dataset:  37%|███▋      | 2465/6690 [00:05<00:09, 447.70 examples/s]Tokenizing train dataset:  38%|███▊      | 2536/6690 [00:05<00:09, 451.21 examples/s]Tokenizing train dataset:  39%|███▊      | 2587/6690 [00:05<00:08, 464.29 examples/s]Tokenizing train dataset:  40%|███▉      | 2651/6690 [00:05<00:09, 448.03 examples/s]Tokenizing train dataset:  41%|████      | 2715/6690 [00:06<00:09, 435.26 examples/s]Tokenizing train dataset:  41%|████▏     | 2774/6690 [00:06<00:09, 419.94 examples/s]Tokenizing train dataset:  42%|████▏     | 2838/6690 [00:06<00:09, 418.91 examples/s]Tokenizing train dataset:  43%|████▎     | 2882/6690 [00:06<00:09, 422.67 examples/s]Tokenizing train dataset:  44%|████▍     | 2933/6690 [00:06<00:08, 438.96 examples/s]Tokenizing train dataset:  45%|████▍     | 2986/6690 [00:06<00:08, 455.28 examples/s]Tokenizing train dataset:  45%|████▌     | 3033/6690 [00:06<00:07, 458.53 examples/s]Tokenizing train dataset:  46%|████▌     | 3088/6690 [00:06<00:07, 481.70 examples/s]Tokenizing train dataset:  47%|████▋     | 3152/6690 [00:07<00:07, 453.97 examples/s]Tokenizing train dataset:  48%|████▊     | 3199/6690 [00:07<00:07, 456.46 examples/s]Tokenizing train dataset:  49%|████▊     | 3248/6690 [00:07<00:07, 460.76 examples/s]Tokenizing train dataset:  49%|████▉     | 3306/6690 [00:07<00:07, 431.70 examples/s]Tokenizing train dataset:  50%|█████     | 3368/6690 [00:07<00:07, 422.07 examples/s]Tokenizing train dataset:  51%|█████▏    | 3429/6690 [00:07<00:07, 412.25 examples/s]Tokenizing train dataset:  52%|█████▏    | 3494/6690 [00:07<00:07, 414.76 examples/s]Tokenizing train dataset:  53%|█████▎    | 3553/6690 [00:08<00:07, 401.72 examples/s]Tokenizing train dataset:  54%|█████▍    | 3610/6690 [00:08<00:07, 389.02 examples/s]Tokenizing train dataset:  55%|█████▍    | 3658/6690 [00:08<00:07, 408.65 examples/s]Tokenizing train dataset:  55%|█████▌    | 3700/6690 [00:08<00:07, 396.79 examples/s]Tokenizing train dataset:  56%|█████▌    | 3759/6690 [00:08<00:07, 394.96 examples/s]Tokenizing train dataset:  57%|█████▋    | 3801/6690 [00:08<00:07, 400.40 examples/s]Tokenizing train dataset:  58%|█████▊    | 3848/6690 [00:08<00:06, 414.15 examples/s]Tokenizing train dataset:  58%|█████▊    | 3896/6690 [00:08<00:06, 429.12 examples/s]Tokenizing train dataset:  59%|█████▉    | 3947/6690 [00:08<00:06, 446.25 examples/s]Tokenizing train dataset:  60%|██████    | 4016/6690 [00:09<00:05, 449.31 examples/s]Tokenizing train dataset:  61%|██████    | 4077/6690 [00:09<00:06, 428.43 examples/s]Tokenizing train dataset:  62%|██████▏   | 4125/6690 [00:09<00:05, 438.02 examples/s]Tokenizing train dataset:  62%|██████▏   | 4172/6690 [00:09<00:05, 445.37 examples/s]Tokenizing train dataset:  63%|██████▎   | 4242/6690 [00:09<00:05, 446.79 examples/s]Tokenizing train dataset:  64%|██████▍   | 4307/6690 [00:09<00:05, 439.19 examples/s]Tokenizing train dataset:  65%|██████▌   | 4369/6690 [00:09<00:05, 425.89 examples/s]Tokenizing train dataset:  66%|██████▋   | 4435/6690 [00:10<00:05, 426.76 examples/s]Tokenizing train dataset:  67%|██████▋   | 4497/6690 [00:10<00:05, 419.71 examples/s]Tokenizing train dataset:  68%|██████▊   | 4543/6690 [00:10<00:05, 428.49 examples/s]Tokenizing train dataset:  69%|██████▉   | 4610/6690 [00:10<00:04, 430.98 examples/s]Tokenizing train dataset:  70%|██████▉   | 4661/6690 [00:10<00:04, 444.75 examples/s]Tokenizing train dataset:  71%|███████   | 4728/6690 [00:10<00:04, 441.02 examples/s]Tokenizing train dataset:  72%|███████▏  | 4786/6690 [00:10<00:04, 418.00 examples/s]Tokenizing train dataset:  72%|███████▏  | 4831/6690 [00:11<00:04, 424.07 examples/s]Tokenizing train dataset:  73%|███████▎  | 4878/6690 [00:11<00:04, 432.73 examples/s]Tokenizing train dataset:  74%|███████▍  | 4938/6690 [00:11<00:04, 417.64 examples/s]Tokenizing train dataset:  75%|███████▍  | 5005/6690 [00:11<00:04, 419.56 examples/s]Tokenizing train dataset:  76%|███████▌  | 5066/6690 [00:11<00:03, 414.26 examples/s]Tokenizing train dataset:  77%|███████▋  | 5125/6690 [00:11<00:03, 404.00 examples/s]Tokenizing train dataset:  77%|███████▋  | 5172/6690 [00:11<00:03, 416.32 examples/s]Tokenizing train dataset:  78%|███████▊  | 5218/6690 [00:11<00:03, 423.85 examples/s]Tokenizing train dataset:  79%|███████▉  | 5282/6690 [00:12<00:03, 419.79 examples/s]Tokenizing train dataset:  80%|███████▉  | 5340/6690 [00:12<00:03, 405.87 examples/s]Tokenizing train dataset:  81%|████████  | 5394/6690 [00:12<00:02, 434.71 examples/s]Tokenizing train dataset:  82%|████████▏ | 5456/6690 [00:12<00:02, 425.58 examples/s]Tokenizing train dataset:  82%|████████▏ | 5502/6690 [00:12<00:02, 427.82 examples/s]Tokenizing train dataset:  83%|████████▎ | 5564/6690 [00:12<00:02, 418.32 examples/s]Tokenizing train dataset:  84%|████████▍ | 5626/6690 [00:12<00:02, 413.20 examples/s]Tokenizing train dataset:  85%|████████▍ | 5670/6690 [00:13<00:02, 416.44 examples/s]Tokenizing train dataset:  85%|████████▌ | 5713/6690 [00:13<00:02, 413.81 examples/s]Tokenizing train dataset:  86%|████████▌ | 5763/6690 [00:13<00:02, 428.06 examples/s]Tokenizing train dataset:  87%|████████▋ | 5810/6690 [00:13<00:02, 436.69 examples/s]Tokenizing train dataset:  88%|████████▊ | 5868/6690 [00:13<00:01, 414.79 examples/s]Tokenizing train dataset:  88%|████████▊ | 5918/6690 [00:13<00:01, 433.68 examples/s]Tokenizing train dataset:  89%|████████▉ | 5981/6690 [00:13<00:01, 423.17 examples/s]Tokenizing train dataset:  90%|█████████ | 6028/6690 [00:13<00:01, 431.01 examples/s]Tokenizing train dataset:  91%|█████████ | 6075/6690 [00:13<00:01, 439.50 examples/s]Tokenizing train dataset:  92%|█████████▏| 6127/6690 [00:14<00:01, 458.72 examples/s]Tokenizing train dataset:  93%|█████████▎| 6193/6690 [00:14<00:01, 445.93 examples/s]Tokenizing train dataset:  93%|█████████▎| 6248/6690 [00:14<00:01, 413.92 examples/s]Tokenizing train dataset:  94%|█████████▍| 6292/6690 [00:14<00:00, 417.68 examples/s]Tokenizing train dataset:  95%|█████████▍| 6352/6690 [00:14<00:00, 410.24 examples/s]Tokenizing train dataset:  96%|█████████▌| 6421/6690 [00:14<00:00, 421.34 examples/s]Tokenizing train dataset:  97%|█████████▋| 6485/6690 [00:14<00:00, 420.57 examples/s]Tokenizing train dataset:  98%|█████████▊| 6528/6690 [00:15<00:00, 421.80 examples/s]Tokenizing train dataset:  99%|█████████▊| 6593/6690 [00:15<00:00, 422.13 examples/s]Tokenizing train dataset:  99%|█████████▉| 6651/6690 [00:15<00:00, 404.72 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 432.95 examples/s]
[rank8]:[W610 23:55:56.127756671 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in eval dataset:  61%|██████    | 580/953 [00:00<00:00, 5753.59 examples/s]Extracting prompt in train dataset:   8%|▊         | 566/6690 [00:00<00:01, 5569.69 examples/s]Extracting prompt in train dataset:   8%|▊         | 560/6690 [00:00<00:01, 5503.38 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5560.77 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5744.21 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1133/6690 [00:00<00:00, 5631.92 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1148/6690 [00:00<00:00, 5685.75 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1150/6690 [00:00<00:00, 5669.98 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1715/6690 [00:00<00:00, 5715.56 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1730/6690 [00:00<00:00, 5736.22 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1752/6690 [00:00<00:00, 5806.24 examples/s]Extracting prompt in train dataset:  35%|███▍      | 2320/6690 [00:00<00:00, 5778.18 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2570/6690 [00:00<00:00, 5697.71 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2624/6690 [00:00<00:00, 5806.10 examples/s]Extracting prompt in train dataset:  43%|████▎     | 2910/6690 [00:00<00:00, 5805.92 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  47%|████▋     | 3154/6690 [00:00<00:00, 5732.94 examples/s]Extracting prompt in train dataset:  48%|████▊     | 3220/6690 [00:00<00:00, 5843.07 examples/s]Extracting prompt in train dataset:  52%|█████▏    | 3510/6690 [00:00<00:00, 5863.98 examples/s]Applying chat template to eval dataset:  33%|███▎      | 311/953 [00:00<00:00, 3080.88 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3730/6690 [00:00<00:00, 5740.67 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 3820/6690 [00:00<00:00, 5871.93 examples/s]Extracting prompt in train dataset:  61%|██████▏   | 4113/6690 [00:00<00:00, 5910.92 examples/s]Applying chat template to eval dataset:  67%|██████▋   | 641/953 [00:00<00:00, 3204.06 examples/s]Extracting prompt in train dataset:  64%|██████▍   | 4312/6690 [00:00<00:00, 5763.43 examples/s]Extracting prompt in train dataset:  70%|██████▉   | 4670/6690 [00:00<00:00, 5777.62 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3199.96 examples/s]
Extracting prompt in train dataset:  74%|███████▍  | 4970/6690 [00:00<00:00, 5789.71 examples/s]Extracting prompt in train dataset:  77%|███████▋  | 5150/6690 [00:00<00:00, 5678.21 examples/s]Extracting prompt in train dataset:  79%|███████▊  | 5260/6690 [00:00<00:00, 5802.84 examples/s]Extracting prompt in train dataset:  83%|████████▎ | 5560/6690 [00:00<00:00, 5818.89 examples/s]Extracting prompt in train dataset:  86%|████████▌ | 5730/6690 [00:01<00:00, 5701.35 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 5860/6690 [00:01<00:00, 5837.45 examples/s]Extracting prompt in train dataset:  92%|█████████▏| 6161/6690 [00:01<00:00, 5870.69 examples/s]Extracting prompt in train dataset:  94%|█████████▍| 6310/6690 [00:01<00:00, 5725.63 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 6460/6690 [00:01<00:00, 5873.12 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5791.02 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5782.48 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5673.85 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 32/953 [00:00<00:02, 311.98 examples/s]Tokenizing eval dataset:   8%|▊         | 72/953 [00:00<00:03, 274.79 examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing eval dataset:  10%|█         | 100/953 [00:00<00:03, 273.04 examples/s]Applying chat template to train dataset:   4%|▍         | 292/6690 [00:00<00:02, 2887.02 examples/s]Applying chat template to train dataset:   4%|▍         | 290/6690 [00:00<00:02, 2847.68 examples/s]Applying chat template to train dataset:   4%|▍         | 288/6690 [00:00<00:02, 2844.08 examples/s]Applying chat template to train dataset:   9%|▉         | 603/6690 [00:00<00:02, 3009.95 examples/s]Tokenizing eval dataset:  15%|█▍        | 140/953 [00:00<00:03, 266.36 examples/s]Applying chat template to train dataset:   9%|▉         | 613/6690 [00:00<00:01, 3062.16 examples/s]Applying chat template to train dataset:   9%|▉         | 608/6690 [00:00<00:01, 3046.37 examples/s]Applying chat template to train dataset:  14%|█▍        | 925/6690 [00:00<00:01, 3102.68 examples/s]Applying chat template to train dataset:  14%|█▍        | 938/6690 [00:00<00:01, 3145.21 examples/s]Applying chat template to train dataset:  14%|█▍        | 930/6690 [00:00<00:01, 3118.25 examples/s]Tokenizing eval dataset:  19%|█▊        | 177/953 [00:00<00:03, 254.07 examples/s]Applying chat template to train dataset:  19%|█▊        | 1245/6690 [00:00<00:01, 3135.34 examples/s]Applying chat template to train dataset:  19%|█▉        | 1261/6690 [00:00<00:01, 3174.64 examples/s]Applying chat template to train dataset:  19%|█▊        | 1250/6690 [00:00<00:01, 3146.44 examples/s]Tokenizing eval dataset:  21%|██▏       | 203/953 [00:00<00:02, 253.70 examples/s]Applying chat template to train dataset:  23%|██▎       | 1564/6690 [00:00<00:01, 3153.23 examples/s]Applying chat template to train dataset:  24%|██▎       | 1585/6690 [00:00<00:01, 3196.89 examples/s]Applying chat template to train dataset:  23%|██▎       | 1572/6690 [00:00<00:01, 3168.62 examples/s]Tokenizing eval dataset:  26%|██▌       | 244/953 [00:00<00:02, 296.29 examples/s]Applying chat template to train dataset:  28%|██▊       | 1885/6690 [00:00<00:01, 3171.63 examples/s]Applying chat template to train dataset:  29%|██▊       | 1910/6690 [00:00<00:01, 3212.37 examples/s]Applying chat template to train dataset:  28%|██▊       | 1894/6690 [00:00<00:01, 3185.17 examples/s]Tokenizing eval dataset:  32%|███▏      | 308/953 [00:00<00:01, 391.84 examples/s]Applying chat template to train dataset:  33%|███▎      | 2206/6690 [00:00<00:01, 3179.70 examples/s]Applying chat template to train dataset:  33%|███▎      | 2237/6690 [00:00<00:01, 3227.21 examples/s]Applying chat template to train dataset:  33%|███▎      | 2216/6690 [00:00<00:01, 3194.10 examples/s]Tokenizing eval dataset:  39%|███▊      | 369/953 [00:01<00:01, 450.92 examples/s]Applying chat template to train dataset:  40%|████      | 2681/6690 [00:00<00:01, 3170.52 examples/s]Applying chat template to train dataset:  41%|████      | 2713/6690 [00:00<00:01, 3200.14 examples/s]Applying chat template to train dataset:  40%|████      | 2682/6690 [00:00<00:01, 3154.28 examples/s]Tokenizing eval dataset:  46%|████▌     | 437/953 [00:01<00:01, 514.56 examples/s]Applying chat template to train dataset:  45%|████▍     | 3002/6690 [00:00<00:01, 3178.29 examples/s]Applying chat template to train dataset:  45%|████▌     | 3037/6690 [00:00<00:01, 3209.25 examples/s]Applying chat template to train dataset:  45%|████▍     | 3002/6690 [00:00<00:01, 3165.35 examples/s]Tokenizing eval dataset:  53%|█████▎    | 503/953 [00:01<00:00, 555.93 examples/s]Applying chat template to train dataset:  50%|████▉     | 3323/6690 [00:01<00:01, 3184.09 examples/s]Applying chat template to train dataset:  50%|█████     | 3363/6690 [00:01<00:01, 3218.05 examples/s]Applying chat template to train dataset:  50%|████▉     | 3322/6690 [00:01<00:01, 3175.09 examples/s]Tokenizing eval dataset:  59%|█████▉    | 566/953 [00:01<00:00, 569.94 examples/s]Applying chat template to train dataset:  54%|█████▍    | 3644/6690 [00:01<00:00, 3188.16 examples/s]Applying chat template to train dataset:  55%|█████▌    | 3688/6690 [00:01<00:00, 3223.88 examples/s]Applying chat template to train dataset:  54%|█████▍    | 3642/6690 [00:01<00:00, 3180.91 examples/s]Tokenizing eval dataset:  66%|██████▋   | 633/953 [00:01<00:00, 596.94 examples/s]Applying chat template to train dataset:  59%|█████▉    | 3965/6690 [00:01<00:00, 3192.67 examples/s]Applying chat template to train dataset:  60%|█████▉    | 4013/6690 [00:01<00:00, 3228.19 examples/s]Applying chat template to train dataset:  59%|█████▉    | 3963/6690 [00:01<00:00, 3187.82 examples/s]Tokenizing eval dataset:  73%|███████▎  | 694/953 [00:01<00:00, 598.55 examples/s]Applying chat template to train dataset:  64%|██████▍   | 4288/6690 [00:01<00:00, 3199.46 examples/s]Applying chat template to train dataset:  65%|██████▍   | 4338/6690 [00:01<00:00, 3233.65 examples/s]Applying chat template to train dataset:  64%|██████▍   | 4285/6690 [00:01<00:00, 3193.53 examples/s]Tokenizing eval dataset:  82%|████████▏ | 777/953 [00:01<00:00, 569.35 examples/s]Applying chat template to train dataset:  71%|███████   | 4751/6690 [00:01<00:00, 3146.77 examples/s]Applying chat template to train dataset:  72%|███████▏  | 4797/6690 [00:01<00:00, 3162.54 examples/s]Applying chat template to train dataset:  71%|███████   | 4740/6690 [00:01<00:00, 3128.08 examples/s]Applying chat template to train dataset:  76%|███████▌  | 5070/6690 [00:01<00:00, 3155.90 examples/s]Tokenizing eval dataset:  89%|████████▉ | 850/953 [00:01<00:00, 533.26 examples/s]Applying chat template to train dataset:  77%|███████▋  | 5120/6690 [00:01<00:00, 3173.75 examples/s]Applying chat template to train dataset:  76%|███████▌  | 5060/6690 [00:01<00:00, 3140.48 examples/s]Applying chat template to train dataset:  81%|████████  | 5391/6690 [00:01<00:00, 3168.15 examples/s]Applying chat template to train dataset:  81%|████████▏ | 5444/6690 [00:01<00:00, 3190.12 examples/s]Applying chat template to train dataset:  80%|████████  | 5380/6690 [00:01<00:00, 3152.78 examples/s]Tokenizing eval dataset:  97%|█████████▋| 920/953 [00:02<00:00, 507.55 examples/s]Applying chat template to train dataset:  85%|████████▌ | 5711/6690 [00:01<00:00, 3176.11 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 449.61 examples/s]
Applying chat template to train dataset:  86%|████████▌ | 5769/6690 [00:01<00:00, 3203.51 examples/s]Applying chat template to train dataset:  85%|████████▌ | 5701/6690 [00:01<00:00, 3163.93 examples/s]Applying chat template to train dataset:  90%|█████████ | 6031/6690 [00:01<00:00, 3180.56 examples/s]Applying chat template to train dataset:  91%|█████████ | 6093/6690 [00:01<00:00, 3211.89 examples/s]Applying chat template to train dataset:  90%|█████████ | 6022/6690 [00:01<00:00, 3173.31 examples/s]Applying chat template to train dataset:  95%|█████████▍| 6351/6690 [00:02<00:00, 3186.11 examples/s]Applying chat template to train dataset:  96%|█████████▌| 6418/6690 [00:02<00:00, 3220.80 examples/s]Applying chat template to train dataset:  95%|█████████▍| 6343/6690 [00:02<00:00, 3181.42 examples/s]Applying chat template to train dataset: 100%|█████████▉| 6671/6690 [00:02<00:00, 3187.84 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3158.76 examples/s]
Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3189.06 examples/s]
Applying chat template to train dataset: 100%|█████████▉| 6662/6690 [00:02<00:00, 3181.92 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3153.23 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 44/6690 [00:00<00:16, 411.55 examples/s]Tokenizing train dataset:   1%|          | 43/6690 [00:00<00:16, 415.10 examples/s]Tokenizing train dataset:   1%|          | 43/6690 [00:00<00:15, 417.23 examples/s]Tokenizing train dataset:   1%|▏         | 91/6690 [00:00<00:15, 437.33 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:15, 435.58 examples/s]Tokenizing train dataset:   1%|▏         | 90/6690 [00:00<00:15, 437.45 examples/s]Tokenizing train dataset:   2%|▏         | 137/6690 [00:00<00:14, 444.98 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 437.65 examples/s]Tokenizing train dataset:   2%|▏         | 135/6690 [00:00<00:14, 438.87 examples/s]Tokenizing train dataset:   3%|▎         | 190/6690 [00:00<00:13, 472.33 examples/s]Tokenizing train dataset:   3%|▎         | 188/6690 [00:00<00:13, 472.49 examples/s]Tokenizing train dataset:   3%|▎         | 188/6690 [00:00<00:13, 473.85 examples/s]Tokenizing train dataset:   4%|▎         | 238/6690 [00:00<00:13, 473.87 examples/s]Tokenizing train dataset:   4%|▎         | 236/6690 [00:00<00:13, 467.32 examples/s]Tokenizing train dataset:   4%|▎         | 236/6690 [00:00<00:13, 469.10 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 467.83 examples/s]Tokenizing train dataset:   5%|▍         | 307/6690 [00:00<00:13, 460.78 examples/s]Tokenizing train dataset:   4%|▍         | 285/6690 [00:00<00:13, 469.17 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 463.69 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:13, 458.06 examples/s]Tokenizing train dataset:   5%|▌         | 353/6690 [00:00<00:13, 460.89 examples/s]Tokenizing train dataset:   6%|▌         | 408/6690 [00:00<00:13, 477.60 examples/s]Tokenizing train dataset:   6%|▌         | 403/6690 [00:00<00:13, 463.99 examples/s]Tokenizing train dataset:   6%|▌         | 403/6690 [00:00<00:13, 470.42 examples/s]Tokenizing train dataset:   7%|▋         | 459/6690 [00:00<00:12, 485.13 examples/s]Tokenizing train dataset:   7%|▋         | 456/6690 [00:00<00:13, 477.34 examples/s]Tokenizing train dataset:   7%|▋         | 456/6690 [00:00<00:12, 482.36 examples/s]Tokenizing train dataset:   8%|▊         | 510/6690 [00:01<00:12, 486.80 examples/s]Tokenizing train dataset:   8%|▊         | 508/6690 [00:01<00:12, 486.09 examples/s]Tokenizing train dataset:   8%|▊         | 508/6690 [00:01<00:12, 489.43 examples/s]Tokenizing train dataset:   8%|▊         | 560/6690 [00:01<00:12, 487.49 examples/s]Tokenizing train dataset:   9%|▊         | 583/6690 [00:01<00:12, 487.44 examples/s]Tokenizing train dataset:   9%|▉         | 610/6690 [00:01<00:12, 487.87 examples/s]Tokenizing train dataset:   9%|▊         | 584/6690 [00:01<00:12, 489.89 examples/s]Tokenizing train dataset:   9%|▉         | 632/6690 [00:01<00:12, 486.86 examples/s]Tokenizing train dataset:  10%|▉         | 660/6690 [00:01<00:12, 487.76 examples/s]Tokenizing train dataset:   9%|▉         | 635/6690 [00:01<00:12, 488.06 examples/s]Tokenizing train dataset:  11%|█         | 712/6690 [00:01<00:12, 494.03 examples/s]Tokenizing train dataset:  11%|█         | 704/6690 [00:01<00:12, 479.81 examples/s]Tokenizing train dataset:  10%|█         | 687/6690 [00:01<00:14, 411.00 examples/s]Tokenizing train dataset:  11%|█▏        | 766/6690 [00:01<00:11, 504.66 examples/s]Tokenizing train dataset:  11%|█▏        | 758/6690 [00:01<00:12, 493.62 examples/s]Tokenizing train dataset:  11%|█         | 740/6690 [00:01<00:13, 435.12 examples/s]Tokenizing train dataset:  12%|█▏        | 819/6690 [00:01<00:11, 509.16 examples/s]Tokenizing train dataset:  12%|█▏        | 812/6690 [00:01<00:11, 500.87 examples/s]Tokenizing train dataset:  12%|█▏        | 793/6690 [00:01<00:12, 454.94 examples/s]Tokenizing train dataset:  13%|█▎        | 888/6690 [00:01<00:12, 482.28 examples/s]Tokenizing train dataset:  13%|█▎        | 880/6690 [00:01<00:12, 480.40 examples/s]Tokenizing train dataset:  13%|█▎        | 841/6690 [00:01<00:12, 458.87 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 462.89 examples/s]Tokenizing train dataset:  14%|█▍        | 944/6690 [00:02<00:12, 461.60 examples/s]Tokenizing train dataset:  14%|█▎        | 906/6690 [00:01<00:13, 442.17 examples/s]Tokenizing train dataset:  14%|█▍        | 955/6690 [00:02<00:12, 446.63 examples/s]Tokenizing train dataset:  15%|█▌        | 1025/6690 [00:02<00:12, 459.29 examples/s]Tokenizing train dataset:  15%|█▌        | 1012/6690 [00:02<00:12, 456.34 examples/s]Tokenizing train dataset:  15%|█▍        | 1001/6690 [00:02<00:12, 445.46 examples/s]Tokenizing train dataset:  16%|█▌        | 1077/6690 [00:02<00:11, 469.48 examples/s]Tokenizing train dataset:  16%|█▌        | 1063/6690 [00:02<00:12, 466.84 examples/s]Tokenizing train dataset:  16%|█▌        | 1050/6690 [00:02<00:12, 453.35 examples/s]Tokenizing train dataset:  17%|█▋        | 1146/6690 [00:02<00:12, 461.14 examples/s]Tokenizing train dataset:  17%|█▋        | 1129/6690 [00:02<00:12, 455.17 examples/s]Tokenizing train dataset:  16%|█▋        | 1100/6690 [00:02<00:12, 460.49 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:11, 460.18 examples/s]Tokenizing train dataset:  18%|█▊        | 1178/6690 [00:02<00:11, 459.79 examples/s]Tokenizing train dataset:  17%|█▋        | 1168/6690 [00:02<00:12, 454.64 examples/s]Tokenizing train dataset:  19%|█▉        | 1262/6690 [00:02<00:11, 452.89 examples/s]Tokenizing train dataset:  19%|█▊        | 1244/6690 [00:02<00:12, 446.85 examples/s]Tokenizing train dataset:  20%|█▉        | 1308/6690 [00:02<00:11, 453.06 examples/s]Tokenizing train dataset:  18%|█▊        | 1234/6690 [00:02<00:12, 442.78 examples/s]Tokenizing train dataset:  20%|█▉        | 1318/6690 [00:02<00:11, 456.00 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:02<00:11, 467.66 examples/s]Tokenizing train dataset:  19%|█▉        | 1280/6690 [00:02<00:12, 441.31 examples/s]Tokenizing train dataset:  20%|██        | 1368/6690 [00:02<00:11, 458.41 examples/s]Tokenizing train dataset:  20%|█▉        | 1334/6690 [00:02<00:11, 462.39 examples/s]Tokenizing train dataset:  21%|██        | 1421/6690 [00:03<00:12, 438.72 examples/s]Tokenizing train dataset:  21%|██▏       | 1430/6690 [00:03<00:11, 439.93 examples/s]Tokenizing train dataset:  22%|██▏       | 1466/6690 [00:03<00:11, 438.15 examples/s]Tokenizing train dataset:  21%|██        | 1400/6690 [00:03<00:11, 448.06 examples/s]Tokenizing train dataset:  23%|██▎       | 1516/6690 [00:03<00:11, 452.67 examples/s]Tokenizing train dataset:  22%|██▏       | 1495/6690 [00:03<00:12, 430.10 examples/s]Tokenizing train dataset:  22%|██▏       | 1465/6690 [00:03<00:11, 438.07 examples/s]Tokenizing train dataset:  23%|██▎       | 1563/6690 [00:03<00:11, 453.12 examples/s]Tokenizing train dataset:  23%|██▎       | 1550/6690 [00:03<00:11, 453.39 examples/s]Tokenizing train dataset:  23%|██▎       | 1512/6690 [00:03<00:11, 444.58 examples/s]Tokenizing train dataset:  24%|██▍       | 1630/6690 [00:03<00:11, 447.38 examples/s]Tokenizing train dataset:  23%|██▎       | 1560/6690 [00:03<00:11, 450.36 examples/s]Tokenizing train dataset:  24%|██▍       | 1616/6690 [00:03<00:11, 442.96 examples/s]Tokenizing train dataset:  25%|██▌       | 1700/6690 [00:03<00:11, 448.35 examples/s]Tokenizing train dataset:  24%|██▍       | 1628/6690 [00:03<00:11, 446.29 examples/s]Tokenizing train dataset:  25%|██▌       | 1683/6690 [00:03<00:11, 441.30 examples/s]Tokenizing train dataset:  26%|██▌       | 1751/6690 [00:03<00:10, 461.22 examples/s]Tokenizing train dataset:  26%|██▌       | 1731/6690 [00:03<00:11, 448.54 examples/s]Tokenizing train dataset:  25%|██▌       | 1698/6690 [00:03<00:11, 448.24 examples/s]Tokenizing train dataset:  27%|██▋       | 1779/6690 [00:03<00:10, 451.18 examples/s]Tokenizing train dataset:  26%|██▌       | 1746/6690 [00:03<00:10, 455.21 examples/s]Tokenizing train dataset:  27%|██▋       | 1820/6690 [00:03<00:10, 451.22 examples/s]Tokenizing train dataset:  27%|██▋       | 1826/6690 [00:03<00:10, 453.22 examples/s]Tokenizing train dataset:  28%|██▊       | 1866/6690 [00:04<00:10, 451.97 examples/s]Tokenizing train dataset:  27%|██▋       | 1812/6690 [00:03<00:10, 447.21 examples/s]Tokenizing train dataset:  29%|██▊       | 1912/6690 [00:04<00:10, 450.31 examples/s]Tokenizing train dataset:  28%|██▊       | 1897/6690 [00:04<00:10, 455.61 examples/s]Tokenizing train dataset:  28%|██▊       | 1859/6690 [00:04<00:10, 449.17 examples/s]Tokenizing train dataset:  29%|██▉       | 1960/6690 [00:04<00:10, 454.60 examples/s]Tokenizing train dataset:  28%|██▊       | 1905/6690 [00:04<00:10, 447.25 examples/s]Tokenizing train dataset:  29%|██▉       | 1963/6690 [00:04<00:10, 446.37 examples/s]Tokenizing train dataset:  29%|██▉       | 1953/6690 [00:04<00:10, 450.29 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:10, 431.52 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:10, 427.13 examples/s]Tokenizing train dataset:  30%|███       | 2011/6690 [00:04<00:11, 424.96 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 432.39 examples/s]Tokenizing train dataset:  31%|███       | 2090/6690 [00:04<00:10, 427.64 examples/s]Tokenizing train dataset:  31%|███       | 2057/6690 [00:04<00:10, 430.08 examples/s]Tokenizing train dataset:  32%|███▏      | 2141/6690 [00:04<00:10, 446.51 examples/s]Tokenizing train dataset:  32%|███▏      | 2140/6690 [00:04<00:10, 440.63 examples/s]Tokenizing train dataset:  31%|███▏      | 2105/6690 [00:04<00:10, 439.72 examples/s]Tokenizing train dataset:  33%|███▎      | 2192/6690 [00:04<00:09, 461.54 examples/s]Tokenizing train dataset:  33%|███▎      | 2190/6690 [00:04<00:09, 453.61 examples/s]Tokenizing train dataset:  32%|███▏      | 2155/6690 [00:04<00:10, 452.02 examples/s]Tokenizing train dataset:  34%|███▎      | 2245/6690 [00:04<00:09, 475.02 examples/s]Tokenizing train dataset:  34%|███▎      | 2245/6690 [00:04<00:09, 471.06 examples/s]Tokenizing train dataset:  33%|███▎      | 2207/6690 [00:04<00:09, 468.44 examples/s]Tokenizing train dataset:  35%|███▍      | 2309/6690 [00:05<00:09, 449.33 examples/s]Tokenizing train dataset:  34%|███▍      | 2277/6690 [00:05<00:09, 466.87 examples/s]Tokenizing train dataset:  35%|███▍      | 2309/6690 [00:05<00:09, 445.66 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:05<00:09, 459.03 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:05<00:09, 455.34 examples/s]Tokenizing train dataset:  35%|███▌      | 2344/6690 [00:05<00:09, 455.38 examples/s]Tokenizing train dataset:  36%|███▋      | 2430/6690 [00:05<00:09, 454.59 examples/s]Tokenizing train dataset:  36%|███▌      | 2391/6690 [00:05<00:09, 456.75 examples/s]Tokenizing train dataset:  36%|███▋      | 2426/6690 [00:05<00:09, 447.15 examples/s]Tokenizing train dataset:  37%|███▋      | 2478/6690 [00:05<00:09, 458.24 examples/s]Tokenizing train dataset:  37%|███▋      | 2476/6690 [00:05<00:09, 454.46 examples/s]Tokenizing train dataset:  38%|███▊      | 2526/6690 [00:05<00:09, 460.97 examples/s]Tokenizing train dataset:  37%|███▋      | 2463/6690 [00:05<00:09, 456.98 examples/s]Tokenizing train dataset:  39%|███▊      | 2581/6690 [00:05<00:08, 482.99 examples/s]Tokenizing train dataset:  38%|███▊      | 2509/6690 [00:05<00:09, 453.59 examples/s]Tokenizing train dataset:  38%|███▊      | 2552/6690 [00:05<00:08, 467.70 examples/s]Tokenizing train dataset:  38%|███▊      | 2561/6690 [00:05<00:08, 468.20 examples/s]Tokenizing train dataset:  39%|███▉      | 2601/6690 [00:05<00:08, 471.24 examples/s]Tokenizing train dataset:  40%|███▉      | 2649/6690 [00:05<00:08, 465.20 examples/s]Tokenizing train dataset:  39%|███▉      | 2611/6690 [00:05<00:08, 476.01 examples/s]Tokenizing train dataset:  40%|███▉      | 2663/6690 [00:05<00:08, 450.07 examples/s]Tokenizing train dataset:  41%|████      | 2713/6690 [00:05<00:08, 449.75 examples/s]Tokenizing train dataset:  40%|███▉      | 2675/6690 [00:05<00:08, 450.37 examples/s]Tokenizing train dataset:  41%|████      | 2728/6690 [00:05<00:08, 441.27 examples/s]Tokenizing train dataset:  41%|████▏     | 2774/6690 [00:06<00:09, 431.39 examples/s]Tokenizing train dataset:  41%|████      | 2741/6690 [00:06<00:09, 436.04 examples/s]Tokenizing train dataset:  42%|████▏     | 2788/6690 [00:06<00:09, 424.76 examples/s]Tokenizing train dataset:  42%|████▏     | 2841/6690 [00:06<00:08, 429.45 examples/s]Tokenizing train dataset:  42%|████▏     | 2833/6690 [00:06<00:09, 426.32 examples/s]Tokenizing train dataset:  42%|████▏     | 2806/6690 [00:06<00:09, 426.03 examples/s]Tokenizing train dataset:  43%|████▎     | 2889/6690 [00:06<00:08, 438.49 examples/s]Tokenizing train dataset:  43%|████▎     | 2878/6690 [00:06<00:08, 429.64 examples/s]Tokenizing train dataset:  43%|████▎     | 2854/6690 [00:06<00:08, 435.86 examples/s]Tokenizing train dataset:  44%|████▍     | 2941/6690 [00:06<00:08, 456.97 examples/s]Tokenizing train dataset:  44%|████▍     | 2930/6690 [00:06<00:08, 444.67 examples/s]Tokenizing train dataset:  43%|████▎     | 2899/6690 [00:06<00:08, 437.62 examples/s]Tokenizing train dataset:  45%|████▍     | 2991/6690 [00:06<00:07, 465.79 examples/s]Tokenizing train dataset:  45%|████▍     | 2981/6690 [00:06<00:08, 457.88 examples/s]Tokenizing train dataset:  44%|████▍     | 2953/6690 [00:06<00:08, 459.22 examples/s]Tokenizing train dataset:  45%|████▌     | 3040/6690 [00:06<00:07, 468.12 examples/s]Tokenizing train dataset:  45%|████▌     | 3033/6690 [00:06<00:07, 466.36 examples/s]Tokenizing train dataset:  45%|████▍     | 3004/6690 [00:06<00:07, 469.91 examples/s]Tokenizing train dataset:  46%|████▋     | 3099/6690 [00:06<00:07, 500.22 examples/s]Tokenizing train dataset:  46%|████▌     | 3090/6690 [00:06<00:07, 489.61 examples/s]Tokenizing train dataset:  46%|████▌     | 3055/6690 [00:06<00:07, 475.10 examples/s]Tokenizing train dataset:  47%|████▋     | 3163/6690 [00:06<00:07, 468.42 examples/s]Tokenizing train dataset:  46%|████▋     | 3107/6690 [00:06<00:07, 486.68 examples/s]Tokenizing train dataset:  47%|████▋     | 3153/6690 [00:06<00:07, 461.86 examples/s]Tokenizing train dataset:  48%|████▊     | 3211/6690 [00:06<00:07, 467.86 examples/s]Tokenizing train dataset:  48%|████▊     | 3200/6690 [00:07<00:07, 461.70 examples/s]Tokenizing train dataset:  47%|████▋     | 3170/6690 [00:06<00:07, 459.17 examples/s]Tokenizing train dataset:  49%|████▊     | 3259/6690 [00:07<00:07, 468.22 examples/s]Tokenizing train dataset:  49%|████▊     | 3250/6690 [00:07<00:07, 466.69 examples/s]Tokenizing train dataset:  48%|████▊     | 3218/6690 [00:07<00:07, 459.19 examples/s]Tokenizing train dataset:  50%|████▉     | 3318/6690 [00:07<00:07, 436.21 examples/s]Tokenizing train dataset:  49%|████▉     | 3267/6690 [00:07<00:07, 466.89 examples/s]Tokenizing train dataset:  49%|████▉     | 3310/6690 [00:07<00:07, 437.20 examples/s]Tokenizing train dataset:  50%|█████     | 3363/6690 [00:07<00:07, 435.67 examples/s]Tokenizing train dataset:  50%|█████     | 3355/6690 [00:07<00:07, 436.64 examples/s]Tokenizing train dataset:  50%|████▉     | 3324/6690 [00:07<00:07, 431.20 examples/s]Tokenizing train dataset:  51%|█████     | 3408/6690 [00:07<00:07, 431.02 examples/s]Tokenizing train dataset:  50%|█████     | 3370/6690 [00:07<00:07, 432.43 examples/s]Tokenizing train dataset:  51%|█████     | 3415/6690 [00:07<00:07, 420.69 examples/s]Tokenizing train dataset:  52%|█████▏    | 3464/6690 [00:07<00:07, 406.32 examples/s]Tokenizing train dataset:  51%|█████▏    | 3432/6690 [00:07<00:07, 419.73 examples/s]Tokenizing train dataset:  53%|█████▎    | 3514/6690 [00:07<00:07, 423.13 examples/s]Tokenizing train dataset:  52%|█████▏    | 3478/6690 [00:07<00:07, 417.00 examples/s]Tokenizing train dataset:  53%|█████▎    | 3522/6690 [00:07<00:07, 418.83 examples/s]Tokenizing train dataset:  52%|█████▏    | 3498/6690 [00:07<00:07, 422.13 examples/s]Tokenizing train dataset:  53%|█████▎    | 3572/6690 [00:07<00:07, 405.56 examples/s]Tokenizing train dataset:  53%|█████▎    | 3579/6690 [00:07<00:07, 398.83 examples/s]Tokenizing train dataset:  53%|█████▎    | 3559/6690 [00:07<00:07, 413.53 examples/s]Tokenizing train dataset:  54%|█████▍    | 3640/6690 [00:08<00:07, 415.92 examples/s]Tokenizing train dataset:  55%|█████▌    | 3685/6690 [00:08<00:07, 420.07 examples/s]Tokenizing train dataset:  54%|█████▍    | 3644/6690 [00:08<00:07, 407.18 examples/s]Tokenizing train dataset:  54%|█████▍    | 3617/6690 [00:08<00:07, 399.90 examples/s]Tokenizing train dataset:  55%|█████▌    | 3687/6690 [00:08<00:07, 411.09 examples/s]Tokenizing train dataset:  55%|█████▍    | 3665/6690 [00:08<00:07, 413.24 examples/s]Tokenizing train dataset:  56%|█████▌    | 3743/6690 [00:08<00:07, 401.94 examples/s]Tokenizing train dataset:  57%|█████▋    | 3790/6690 [00:08<00:06, 414.98 examples/s]Tokenizing train dataset:  56%|█████▌    | 3745/6690 [00:08<00:07, 395.95 examples/s]Tokenizing train dataset:  56%|█████▌    | 3722/6690 [00:08<00:07, 398.35 examples/s]Tokenizing train dataset:  57%|█████▋    | 3835/6690 [00:08<00:06, 420.96 examples/s]Tokenizing train dataset:  57%|█████▋    | 3790/6690 [00:08<00:07, 408.65 examples/s]Tokenizing train dataset:  56%|█████▋    | 3769/6690 [00:08<00:07, 412.67 examples/s]Tokenizing train dataset:  58%|█████▊    | 3883/6690 [00:08<00:06, 432.95 examples/s]Tokenizing train dataset:  57%|█████▋    | 3835/6690 [00:08<00:06, 415.03 examples/s]Tokenizing train dataset:  57%|█████▋    | 3835/6690 [00:08<00:06, 420.14 examples/s]Tokenizing train dataset:  59%|█████▉    | 3931/6690 [00:08<00:06, 442.34 examples/s]Tokenizing train dataset:  58%|█████▊    | 3882/6690 [00:08<00:06, 428.38 examples/s]Tokenizing train dataset:  58%|█████▊    | 3883/6690 [00:08<00:06, 430.96 examples/s]Tokenizing train dataset:  59%|█████▉    | 3980/6690 [00:08<00:05, 453.24 examples/s]Tokenizing train dataset:  59%|█████▊    | 3929/6690 [00:08<00:06, 437.98 examples/s]Tokenizing train dataset:  59%|█████▉    | 3931/6690 [00:08<00:06, 439.56 examples/s]Tokenizing train dataset:  60%|██████    | 4030/6690 [00:08<00:05, 461.87 examples/s]Tokenizing train dataset:  59%|█████▉    | 3978/6690 [00:08<00:06, 450.00 examples/s]Tokenizing train dataset:  59%|█████▉    | 3980/6690 [00:08<00:06, 449.66 examples/s]Tokenizing train dataset:  60%|██████    | 4026/6690 [00:08<00:05, 454.94 examples/s]Tokenizing train dataset:  61%|██████    | 4094/6690 [00:09<00:05, 445.48 examples/s]Tokenizing train dataset:  60%|██████    | 4030/6690 [00:09<00:05, 458.50 examples/s]Tokenizing train dataset:  62%|██████▏   | 4141/6690 [00:09<00:05, 451.13 examples/s]Tokenizing train dataset:  61%|██████    | 4090/6690 [00:09<00:05, 437.86 examples/s]Tokenizing train dataset:  61%|██████    | 4094/6690 [00:09<00:05, 443.20 examples/s]Tokenizing train dataset:  63%|██████▎   | 4192/6690 [00:09<00:05, 465.78 examples/s]Tokenizing train dataset:  62%|██████▏   | 4138/6690 [00:09<00:05, 445.08 examples/s]Tokenizing train dataset:  62%|██████▏   | 4141/6690 [00:09<00:05, 449.06 examples/s]Tokenizing train dataset:  63%|██████▎   | 4191/6690 [00:09<00:05, 461.59 examples/s]Tokenizing train dataset:  64%|██████▎   | 4257/6690 [00:09<00:05, 450.07 examples/s]Tokenizing train dataset:  63%|██████▎   | 4192/6690 [00:09<00:05, 463.22 examples/s]Tokenizing train dataset:  64%|██████▍   | 4305/6690 [00:09<00:05, 451.78 examples/s]Tokenizing train dataset:  64%|██████▎   | 4256/6690 [00:09<00:05, 444.92 examples/s]Tokenizing train dataset:  64%|██████▎   | 4257/6690 [00:09<00:05, 447.38 examples/s]Tokenizing train dataset:  64%|██████▍   | 4302/6690 [00:09<00:05, 447.41 examples/s]Tokenizing train dataset:  65%|██████▌   | 4370/6690 [00:09<00:05, 436.79 examples/s]Tokenizing train dataset:  64%|██████▍   | 4305/6690 [00:09<00:05, 448.82 examples/s]Tokenizing train dataset:  66%|██████▌   | 4415/6690 [00:09<00:05, 434.79 examples/s]Tokenizing train dataset:  65%|██████▌   | 4366/6690 [00:09<00:05, 433.48 examples/s]Tokenizing train dataset:  65%|██████▌   | 4370/6690 [00:09<00:05, 434.24 examples/s]Tokenizing train dataset:  67%|██████▋   | 4461/6690 [00:09<00:05, 439.04 examples/s]Tokenizing train dataset:  66%|██████▌   | 4410/6690 [00:09<00:05, 425.91 examples/s]Tokenizing train dataset:  66%|██████▌   | 4415/6690 [00:09<00:05, 432.31 examples/s]Tokenizing train dataset:  67%|██████▋   | 4458/6690 [00:09<00:05, 437.75 examples/s]Tokenizing train dataset:  68%|██████▊   | 4530/6690 [00:10<00:04, 441.60 examples/s]Tokenizing train dataset:  67%|██████▋   | 4461/6690 [00:09<00:05, 436.27 examples/s]Tokenizing train dataset:  68%|██████▊   | 4578/6690 [00:10<00:04, 448.87 examples/s]Tokenizing train dataset:  68%|██████▊   | 4526/6690 [00:10<00:04, 438.28 examples/s]Tokenizing train dataset:  68%|██████▊   | 4530/6690 [00:10<00:04, 439.24 examples/s]Tokenizing train dataset:  69%|██████▉   | 4629/6690 [00:10<00:04, 459.92 examples/s]Tokenizing train dataset:  69%|██████▊   | 4593/6690 [00:10<00:04, 436.51 examples/s]Tokenizing train dataset:  68%|██████▊   | 4577/6690 [00:10<00:04, 445.19 examples/s]Tokenizing train dataset:  70%|██████▉   | 4677/6690 [00:10<00:04, 460.60 examples/s]Tokenizing train dataset:  69%|██████▉   | 4647/6690 [00:10<00:04, 458.71 examples/s]Tokenizing train dataset:  69%|██████▉   | 4629/6690 [00:10<00:04, 457.84 examples/s]Tokenizing train dataset:  71%|███████   | 4747/6690 [00:10<00:04, 461.46 examples/s]Tokenizing train dataset:  70%|███████   | 4694/6690 [00:10<00:04, 454.28 examples/s]Tokenizing train dataset:  70%|██████▉   | 4677/6690 [00:10<00:04, 458.15 examples/s]Tokenizing train dataset:  71%|███████   | 4744/6690 [00:10<00:04, 459.40 examples/s]Tokenizing train dataset:  72%|███████▏  | 4808/6690 [00:10<00:04, 438.87 examples/s]Tokenizing train dataset:  71%|███████   | 4747/6690 [00:10<00:04, 459.66 examples/s]Tokenizing train dataset:  73%|███████▎  | 4855/6690 [00:10<00:04, 443.69 examples/s]Tokenizing train dataset:  72%|███████▏  | 4804/6690 [00:10<00:04, 434.27 examples/s]Tokenizing train dataset:  72%|███████▏  | 4808/6690 [00:10<00:04, 436.69 examples/s]Tokenizing train dataset:  73%|███████▎  | 4851/6690 [00:10<00:04, 439.86 examples/s]Tokenizing train dataset:  74%|███████▎  | 4919/6690 [00:10<00:04, 433.21 examples/s]Tokenizing train dataset:  73%|███████▎  | 4855/6690 [00:10<00:04, 441.37 examples/s]Tokenizing train dataset:  74%|███████▍  | 4966/6690 [00:10<00:03, 440.65 examples/s]Tokenizing train dataset:  73%|███████▎  | 4912/6690 [00:10<00:04, 423.79 examples/s]Tokenizing train dataset:  74%|███████▎  | 4918/6690 [00:11<00:04, 430.12 examples/s]Tokenizing train dataset:  74%|███████▍  | 4962/6690 [00:11<00:03, 436.70 examples/s]Tokenizing train dataset:  75%|███████▌  | 5030/6690 [00:11<00:03, 430.06 examples/s]Tokenizing train dataset:  74%|███████▍  | 4964/6690 [00:11<00:03, 434.15 examples/s]Tokenizing train dataset:  76%|███████▌  | 5074/6690 [00:11<00:03, 431.82 examples/s]Tokenizing train dataset:  75%|███████▍  | 5008/6690 [00:11<00:03, 432.68 examples/s]Tokenizing train dataset:  75%|███████▌  | 5026/6690 [00:11<00:03, 426.99 examples/s]Tokenizing train dataset:  77%|███████▋  | 5137/6690 [00:11<00:03, 423.04 examples/s]Tokenizing train dataset:  76%|███████▌  | 5070/6690 [00:11<00:03, 424.10 examples/s]Tokenizing train dataset:  76%|███████▌  | 5091/6690 [00:11<00:03, 424.26 examples/s]Tokenizing train dataset:  78%|███████▊  | 5187/6690 [00:11<00:03, 438.64 examples/s]Tokenizing train dataset:  76%|███████▋  | 5115/6690 [00:11<00:03, 424.59 examples/s]Tokenizing train dataset:  77%|███████▋  | 5156/6690 [00:11<00:03, 423.63 examples/s]Tokenizing train dataset:  79%|███████▊  | 5255/6690 [00:11<00:03, 440.91 examples/s]Tokenizing train dataset:  77%|███████▋  | 5160/6690 [00:11<00:03, 425.96 examples/s]Tokenizing train dataset:  78%|███████▊  | 5207/6690 [00:11<00:03, 438.86 examples/s]Tokenizing train dataset:  78%|███████▊  | 5207/6690 [00:11<00:03, 435.71 examples/s]Tokenizing train dataset:  79%|███████▉  | 5312/6690 [00:11<00:03, 418.73 examples/s]Tokenizing train dataset:  79%|███████▉  | 5272/6690 [00:11<00:03, 431.97 examples/s]Tokenizing train dataset:  80%|████████  | 5363/6690 [00:11<00:03, 436.04 examples/s]Tokenizing train dataset:  79%|███████▉  | 5272/6690 [00:11<00:03, 429.77 examples/s]Tokenizing train dataset:  81%|████████  | 5414/6690 [00:12<00:02, 448.41 examples/s]Tokenizing train dataset:  80%|███████▉  | 5333/6690 [00:11<00:03, 421.67 examples/s]Tokenizing train dataset:  80%|███████▉  | 5335/6690 [00:11<00:03, 422.85 examples/s]Tokenizing train dataset:  81%|████████  | 5388/6690 [00:12<00:02, 448.24 examples/s]Tokenizing train dataset:  82%|████████▏ | 5480/6690 [00:12<00:02, 441.44 examples/s]Tokenizing train dataset:  81%|████████  | 5390/6690 [00:12<00:02, 447.54 examples/s]Tokenizing train dataset:  82%|████████▏ | 5454/6690 [00:12<00:02, 438.07 examples/s]Tokenizing train dataset:  83%|████████▎ | 5545/6690 [00:12<00:02, 437.75 examples/s]Tokenizing train dataset:  82%|████████▏ | 5455/6690 [00:12<00:02, 439.30 examples/s]Tokenizing train dataset:  82%|████████▏ | 5500/6690 [00:12<00:02, 438.19 examples/s]Tokenizing train dataset:  82%|████████▏ | 5500/6690 [00:12<00:02, 440.61 examples/s]Tokenizing train dataset:  84%|████████▍ | 5609/6690 [00:12<00:02, 430.38 examples/s]Tokenizing train dataset:  83%|████████▎ | 5566/6690 [00:12<00:02, 431.36 examples/s]Tokenizing train dataset:  83%|████████▎ | 5566/6690 [00:12<00:02, 434.22 examples/s]Tokenizing train dataset:  85%|████████▍ | 5679/6690 [00:12<00:02, 436.06 examples/s]Tokenizing train dataset:  84%|████████▍ | 5631/6690 [00:12<00:02, 427.84 examples/s]Tokenizing train dataset:  86%|████████▌ | 5723/6690 [00:12<00:02, 434.24 examples/s]Tokenizing train dataset:  84%|████████▍ | 5632/6690 [00:12<00:02, 431.65 examples/s]Tokenizing train dataset:  85%|████████▍ | 5677/6690 [00:12<00:02, 433.91 examples/s]Tokenizing train dataset:  86%|████████▌ | 5768/6690 [00:12<00:02, 435.25 examples/s]Tokenizing train dataset:  85%|████████▍ | 5678/6690 [00:12<00:02, 435.65 examples/s]Tokenizing train dataset:  87%|████████▋ | 5817/6690 [00:12<00:01, 447.82 examples/s]Tokenizing train dataset:  86%|████████▌ | 5723/6690 [00:12<00:02, 431.56 examples/s]Tokenizing train dataset:  86%|████████▌ | 5744/6690 [00:12<00:02, 436.58 examples/s]Tokenizing train dataset:  87%|████████▋ | 5788/6690 [00:13<00:02, 434.54 examples/s]Tokenizing train dataset:  86%|████████▌ | 5768/6690 [00:12<00:02, 431.90 examples/s]Tokenizing train dataset:  88%|████████▊ | 5876/6690 [00:13<00:01, 425.58 examples/s]Tokenizing train dataset:  87%|████████▋ | 5817/6690 [00:13<00:01, 445.31 examples/s]Tokenizing train dataset:  89%|████████▊ | 5926/6690 [00:13<00:01, 443.39 examples/s]Tokenizing train dataset:  87%|████████▋ | 5851/6690 [00:13<00:01, 424.59 examples/s]Tokenizing train dataset:  89%|████████▉ | 5973/6690 [00:13<00:01, 445.22 examples/s]Tokenizing train dataset:  88%|████████▊ | 5876/6690 [00:13<00:01, 422.27 examples/s]Tokenizing train dataset:  88%|████████▊ | 5900/6690 [00:13<00:01, 438.08 examples/s]Tokenizing train dataset:  90%|████████▉ | 6019/6690 [00:13<00:01, 447.42 examples/s]Tokenizing train dataset:  89%|████████▊ | 5926/6690 [00:13<00:01, 440.61 examples/s]Tokenizing train dataset:  89%|████████▉ | 5966/6690 [00:13<00:01, 437.44 examples/s]Tokenizing train dataset:  89%|████████▉ | 5973/6690 [00:13<00:01, 442.44 examples/s]Tokenizing train dataset:  91%|█████████ | 6090/6690 [00:13<00:01, 454.21 examples/s]Tokenizing train dataset:  90%|█████████ | 6030/6690 [00:13<00:01, 431.50 examples/s]Tokenizing train dataset:  90%|████████▉ | 6018/6690 [00:13<00:01, 444.13 examples/s]Tokenizing train dataset:  92%|█████████▏| 6142/6690 [00:13<00:01, 470.36 examples/s]Tokenizing train dataset:  91%|█████████ | 6080/6690 [00:13<00:01, 445.59 examples/s]Tokenizing train dataset:  91%|█████████ | 6090/6690 [00:13<00:01, 452.12 examples/s]Tokenizing train dataset:  93%|█████████▎| 6207/6690 [00:13<00:01, 451.22 examples/s]Tokenizing train dataset:  92%|█████████▏| 6133/6690 [00:13<00:01, 465.66 examples/s]Tokenizing train dataset:  92%|█████████▏| 6142/6690 [00:13<00:01, 468.05 examples/s]Tokenizing train dataset:  94%|█████████▎| 6260/6690 [00:13<00:01, 415.74 examples/s]Tokenizing train dataset:  93%|█████████▎| 6199/6690 [00:13<00:01, 451.79 examples/s]Tokenizing train dataset:  93%|█████████▎| 6206/6690 [00:13<00:01, 451.00 examples/s]Tokenizing train dataset:  94%|█████████▍| 6310/6690 [00:14<00:00, 432.36 examples/s]Tokenizing train dataset:  93%|█████████▎| 6250/6690 [00:14<00:01, 412.44 examples/s]Tokenizing train dataset:  94%|█████████▎| 6260/6690 [00:14<00:01, 412.30 examples/s]Tokenizing train dataset:  95%|█████████▌| 6369/6690 [00:14<00:00, 417.61 examples/s]Tokenizing train dataset:  94%|█████████▍| 6298/6690 [00:14<00:00, 427.36 examples/s]Tokenizing train dataset:  94%|█████████▍| 6309/6690 [00:14<00:00, 430.16 examples/s]Tokenizing train dataset:  96%|█████████▌| 6416/6690 [00:14<00:00, 427.28 examples/s]Tokenizing train dataset:  95%|█████████▌| 6359/6690 [00:14<00:00, 412.84 examples/s]Tokenizing train dataset:  97%|█████████▋| 6461/6690 [00:14<00:00, 427.58 examples/s]Tokenizing train dataset:  95%|█████████▌| 6369/6690 [00:14<00:00, 414.35 examples/s]Tokenizing train dataset:  96%|█████████▌| 6403/6690 [00:14<00:00, 413.64 examples/s]Tokenizing train dataset:  97%|█████████▋| 6508/6690 [00:14<00:00, 433.89 examples/s]Tokenizing train dataset:  96%|█████████▌| 6414/6690 [00:14<00:00, 422.16 examples/s]Tokenizing train dataset:  96%|█████████▋| 6451/6690 [00:14<00:00, 427.19 examples/s]Tokenizing train dataset:  98%|█████████▊| 6552/6690 [00:14<00:00, 430.24 examples/s]Tokenizing train dataset:  97%|█████████▋| 6459/6690 [00:14<00:00, 426.82 examples/s]Tokenizing train dataset:  97%|█████████▋| 6496/6690 [00:14<00:00, 431.47 examples/s]Tokenizing train dataset:  99%|█████████▊| 6596/6690 [00:14<00:00, 432.19 examples/s]Tokenizing train dataset:  97%|█████████▋| 6504/6690 [00:14<00:00, 429.87 examples/s]Tokenizing train dataset:  98%|█████████▊| 6540/6690 [00:14<00:00, 427.51 examples/s]Tokenizing train dataset:  98%|█████████▊| 6549/6690 [00:14<00:00, 431.40 examples/s]Tokenizing train dataset:  99%|█████████▉| 6656/6690 [00:14<00:00, 414.82 examples/s]Tokenizing train dataset:  98%|█████████▊| 6584/6690 [00:14<00:00, 429.31 examples/s]Tokenizing train dataset:  99%|█████████▊| 6593/6690 [00:14<00:00, 432.00 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:14<00:00, 446.86 examples/s]
Tokenizing train dataset:  99%|█████████▉| 6643/6690 [00:15<00:00, 412.09 examples/s]Tokenizing train dataset:  99%|█████████▉| 6651/6690 [00:15<00:00, 411.10 examples/s]Tokenizing train dataset: 100%|█████████▉| 6687/6690 [00:15<00:00, 418.39 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 442.28 examples/s]
Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 442.28 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5554.73 examples/s]Extracting prompt in eval dataset:  60%|██████    | 575/953 [00:00<00:00, 5669.43 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5516.82 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5653.23 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5588.00 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5527.91 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  33%|███▎      | 310/953 [00:00<00:00, 3056.17 examples/s]Applying chat template to eval dataset:  32%|███▏      | 301/953 [00:00<00:00, 2975.24 examples/s]Applying chat template to eval dataset:  31%|███▏      | 300/953 [00:00<00:00, 2958.24 examples/s]Applying chat template to eval dataset:  66%|██████▋   | 633/953 [00:00<00:00, 3147.45 examples/s]Applying chat template to eval dataset:  77%|███████▋  | 738/953 [00:00<00:00, 2927.07 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3141.42 examples/s]
Applying chat template to eval dataset:  73%|███████▎  | 700/953 [00:00<00:00, 2742.31 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2877.63 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 2760.97 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   4%|▎         | 34/953 [00:00<00:02, 319.25 examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   8%|▊         | 76/953 [00:00<00:03, 287.85 examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 253.89 examples/s]Tokenizing eval dataset:   3%|▎         | 27/953 [00:00<00:03, 260.78 examples/s]Tokenizing eval dataset:   6%|▌         | 53/953 [00:00<00:03, 246.72 examples/s]Tokenizing eval dataset:  12%|█▏        | 118/953 [00:00<00:03, 276.87 examples/s]Tokenizing eval dataset:   6%|▋         | 61/953 [00:00<00:03, 232.97 examples/s]Tokenizing eval dataset:   9%|▉         | 88/953 [00:00<00:03, 235.90 examples/s]Tokenizing eval dataset:  17%|█▋        | 158/953 [00:00<00:02, 267.56 examples/s]Tokenizing eval dataset:   9%|▉         | 88/953 [00:00<00:03, 241.36 examples/s]Tokenizing eval dataset:  13%|█▎        | 120/953 [00:00<00:03, 220.82 examples/s]Tokenizing eval dataset:  20%|██        | 194/953 [00:00<00:03, 252.85 examples/s]Tokenizing eval dataset:  13%|█▎        | 121/953 [00:00<00:03, 223.81 examples/s]Tokenizing eval dataset:  15%|█▌        | 143/953 [00:00<00:03, 222.19 examples/s]Tokenizing eval dataset:  24%|██▍       | 230/953 [00:00<00:02, 275.79 examples/s]Tokenizing eval dataset:  15%|█▌        | 145/953 [00:00<00:03, 224.97 examples/s]Tokenizing eval dataset:  31%|███       | 295/953 [00:00<00:01, 371.58 examples/s]Tokenizing eval dataset:  18%|█▊        | 173/953 [00:00<00:03, 210.08 examples/s]Tokenizing eval dataset:  38%|███▊      | 360/953 [00:01<00:01, 439.71 examples/s]Tokenizing eval dataset:  19%|█▊        | 177/953 [00:00<00:03, 212.78 examples/s]Tokenizing eval dataset:  22%|██▏       | 206/953 [00:00<00:03, 208.99 examples/s]Tokenizing eval dataset:  44%|████▍     | 424/953 [00:01<00:01, 494.65 examples/s]Tokenizing eval dataset:  22%|██▏       | 209/953 [00:00<00:03, 210.17 examples/s]Tokenizing eval dataset:  52%|█████▏    | 491/953 [00:01<00:00, 542.99 examples/s]Tokenizing eval dataset:  25%|██▌       | 240/953 [00:01<00:03, 237.05 examples/s]Tokenizing eval dataset:  26%|██▌       | 249/953 [00:01<00:02, 255.39 examples/s]Tokenizing eval dataset:  59%|█████▊    | 558/953 [00:01<00:00, 574.46 examples/s]Tokenizing eval dataset:  31%|███       | 295/953 [00:01<00:02, 314.96 examples/s]Tokenizing eval dataset:  32%|███▏      | 303/953 [00:01<00:01, 325.06 examples/s]Tokenizing eval dataset:  65%|██████▌   | 623/953 [00:01<00:00, 592.45 examples/s]Tokenizing eval dataset:  37%|███▋      | 348/953 [00:01<00:01, 369.36 examples/s]Tokenizing eval dataset:  37%|███▋      | 355/953 [00:01<00:01, 372.35 examples/s]Tokenizing eval dataset:  72%|███████▏  | 687/953 [00:01<00:00, 604.39 examples/s]Tokenizing eval dataset:  41%|████      | 393/953 [00:01<00:01, 390.34 examples/s]Tokenizing eval dataset:  42%|████▏     | 404/953 [00:01<00:01, 402.46 examples/s]Tokenizing eval dataset:  47%|████▋     | 452/953 [00:01<00:01, 443.48 examples/s]Tokenizing eval dataset:  81%|████████  | 769/953 [00:01<00:00, 577.26 examples/s]Tokenizing eval dataset:  49%|████▉     | 465/953 [00:01<00:01, 458.65 examples/s]Tokenizing eval dataset:  53%|█████▎    | 502/953 [00:01<00:00, 455.28 examples/s]Tokenizing eval dataset:  54%|█████▍    | 516/953 [00:01<00:00, 472.14 examples/s]Tokenizing eval dataset:  88%|████████▊ | 841/953 [00:01<00:00, 541.16 examples/s]Tokenizing eval dataset:  59%|█████▉    | 565/953 [00:01<00:00, 502.03 examples/s]Tokenizing eval dataset:  61%|██████    | 581/953 [00:01<00:00, 520.39 examples/s]Tokenizing eval dataset:  66%|██████▌   | 630/953 [00:01<00:00, 539.27 examples/s]Tokenizing eval dataset:  96%|█████████▌| 913/953 [00:02<00:00, 514.96 examples/s]Tokenizing eval dataset:  68%|██████▊   | 646/953 [00:01<00:00, 552.52 examples/s]Tokenizing eval dataset:  73%|███████▎  | 691/953 [00:01<00:00, 555.82 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 455.22 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset:  74%|███████▍  | 703/953 [00:01<00:00, 556.66 examples/s]Tokenizing eval dataset:  81%|████████  | 773/953 [00:02<00:00, 547.86 examples/s]Tokenizing eval dataset:  82%|████████▏ | 785/953 [00:02<00:00, 545.35 examples/s]Tokenizing eval dataset:  88%|████████▊ | 842/953 [00:02<00:00, 514.31 examples/s]Tokenizing eval dataset:  90%|████████▉ | 856/953 [00:02<00:00, 515.89 examples/s]Tokenizing eval dataset:  96%|█████████▌| 912/953 [00:02<00:00, 495.01 examples/s]Tokenizing eval dataset:  98%|█████████▊| 934/953 [00:02<00:00, 511.42 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 394.51 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 399.53 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.474186658859253 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3690967559814453 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3704984188079834 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.368703842163086 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank8]:[W611 01:23:12.589334286 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 2 ---
