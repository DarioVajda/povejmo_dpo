cpu-bind=MASK - gn56, task  3  0 [2058568]: mask 0xffff0000ffff0000ffffffffffffffffffff0000ffff0000ffffffffffffffff set
*******STARTING********
--- Running on Node Rank: 3 ---
Total Nodes: 4
--- CUDA_VISIBLE_DEVICES for this srun task: 0,1,2,3 ---
GPUs per Node: 4
Master Address: gn33
Master Port: 29500
-------------------------------------------
accelerate launch     --config_file /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/accelerate_config.yaml     --num_machines 4     --machine_rank 3     --main_process_ip gn33     --main_process_port 29500     --num_processes 16     --deepspeed_hostfile /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/result_dir/hostfile_62950704     --deepspeed_multinode_launcher standard     --rdzv_backend static     --same_network     --mixed_precision bf16     "train_curriculum.py"     --rank=64 --learning_rate=4e-7 --total_epochs=3 --beta=0.2 --curriculum_stage=2
-------------------------------------------
[2025-06-10 23:53:46,237] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W0610 23:53:48.174000 2058623 torch/distributed/run.py:792] 
W0610 23:53:48.174000 2058623 torch/distributed/run.py:792] *****************************************
W0610 23:53:48.174000 2058623 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0610 23:53:48.174000 2058623 torch/distributed/run.py:792] *****************************************
[2025-06-10 23:53:53,850] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 23:53:53,852] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 23:53:53,861] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-10 23:53:53,863] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[load_data_curriculum.py]: Training data of type 'bad_lang_examples':    3489
[load_data_curriculum.py]: Training data of type 'short_examples':       699
[load_data_curriculum.py]: Training data of type 'choose_examples':      13379
[load_data_curriculum.py]: Training data of type 'bad_format_examples':  3148
[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *[load_data_curriculum.py]: *
[load_data_curriculum.py]: Evaluation data size: 953
[load_data_curriculum.py]: Curriculum stage 0 training data size: 4890
[load_data_curriculum.py]: Curriculum stage 1 training data size: 6689
[load_data_curriculum.py]: Curriculum stage 2 training data size: 6690
[load_data.py]: Number of validation examples: 953
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=2)
4e-07
World size: 16
Setting gradient accumulation steps to: 1
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=2)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=2)
4e-07
Namespace(rank=64, learning_rate=4e-07, total_epochs=3, beta=0.2, curriculum_stage=2)
4e-07
Created datasets
Train dataset size: 6690
Validation dataset size: 953
Steps per epoch: 418
Evaluate each 209 steps
[2025-06-10 23:54:00,535] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-10 23:54:00,659] [INFO] [comm.py:658:init_distributed] cdb=None
Set up DPO configuration
Loading model from: /ceph/hpc/data/s24o01-42-users/translation_optimization/trl/trained_models/Curriculum_DPO_models/GaMS-9B-DPO-Curri-1
[2025-06-10 23:54:00,679] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-06-10 23:54:00,715] [INFO] [comm.py:658:init_distributed] cdb=None
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:13, 24.61s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.95s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.95s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:24<01:14, 24.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:49<00:49, 24.99s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:50<00:50, 25.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:50<00:50, 25.28s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:50<00:50, 25.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:24, 24.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:24, 24.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:24, 24.74s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:14<00:24, 24.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:30<00:00, 21.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:30<00:00, 22.74s/it]
Loaded model
Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 21.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 21.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 21.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.83s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.83s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:31<00:00, 22.83s/it]
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Using LoRA and set up the model
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
Total Parameters: 9457.78M
Trainable Parameters (LoRA): 216.07M
Percentage of Trainable Params: 2.2846%
[rank14]:[W610 23:55:36.325086755 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 14]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank15]:[W610 23:55:36.326155047 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 15]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Loaded tokenizer
[rank13]:[W610 23:55:36.364265172 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 13]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   8%|▊         | 556/6690 [00:00<00:01, 5517.63 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1120/6690 [00:00<00:00, 5588.35 examples/s]Extracting prompt in train dataset:  25%|██▌       | 1700/6690 [00:00<00:00, 5662.91 examples/s]Extracting prompt in train dataset:  38%|███▊      | 2550/6690 [00:00<00:00, 5663.16 examples/s]Extracting prompt in train dataset:  47%|████▋     | 3133/6690 [00:00<00:00, 5706.49 examples/s]Extracting prompt in train dataset:  56%|█████▌    | 3713/6690 [00:00<00:00, 5734.80 examples/s]Extracting prompt in train dataset:  64%|██████▍   | 4293/6690 [00:00<00:00, 5753.34 examples/s]Extracting prompt in train dataset:  76%|███████▋  | 5102/6690 [00:00<00:00, 5607.39 examples/s]Extracting prompt in train dataset:  85%|████████▍ | 5682/6690 [00:01<00:00, 5659.24 examples/s]Extracting prompt in train dataset:  94%|█████████▎| 6264/6690 [00:01<00:00, 5702.82 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5612.17 examples/s]
Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 285/6690 [00:00<00:02, 2813.64 examples/s]Applying chat template to train dataset:   9%|▉         | 596/6690 [00:00<00:02, 2975.11 examples/s]Applying chat template to train dataset:  14%|█▎        | 907/6690 [00:00<00:01, 3035.17 examples/s]Applying chat template to train dataset:  18%|█▊        | 1216/6690 [00:00<00:01, 3053.77 examples/s]Applying chat template to train dataset:  23%|██▎       | 1526/6690 [00:00<00:01, 3070.21 examples/s]Applying chat template to train dataset:  28%|██▊       | 1843/6690 [00:00<00:01, 3099.58 examples/s]Applying chat template to train dataset:  32%|███▏      | 2162/6690 [00:00<00:01, 3127.56 examples/s]Applying chat template to train dataset:  39%|███▉      | 2633/6690 [00:00<00:01, 3128.65 examples/s]Applying chat template to train dataset:  44%|████▍     | 2951/6690 [00:00<00:01, 3140.40 examples/s]Applying chat template to train dataset:  49%|████▉     | 3271/6690 [00:01<00:01, 3154.87 examples/s]Applying chat template to train dataset:  54%|█████▎    | 3590/6690 [00:01<00:00, 3159.63 examples/s]Applying chat template to train dataset:  58%|█████▊    | 3910/6690 [00:01<00:00, 3166.62 examples/s]Applying chat template to train dataset:  63%|██████▎   | 4230/6690 [00:01<00:00, 3172.39 examples/s]Applying chat template to train dataset:  70%|██████▉   | 4671/6690 [00:01<00:00, 3078.57 examples/s]Applying chat template to train dataset:  74%|███████▍  | 4981/6690 [00:01<00:00, 3082.23 examples/s]Applying chat template to train dataset:  79%|███████▉  | 5292/6690 [00:01<00:00, 3086.93 examples/s]Applying chat template to train dataset:  84%|████████▍ | 5610/6690 [00:01<00:00, 3108.45 examples/s]Applying chat template to train dataset:  89%|████████▊ | 5930/6690 [00:01<00:00, 3127.57 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6250/6690 [00:02<00:00, 3140.24 examples/s]Applying chat template to train dataset:  98%|█████████▊| 6570/6690 [00:02<00:00, 3148.44 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3106.48 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 37/6690 [00:00<00:18, 350.36 examples/s]Tokenizing train dataset:   1%|▏         | 87/6690 [00:00<00:15, 424.17 examples/s]Tokenizing train dataset:   2%|▏         | 150/6690 [00:00<00:15, 413.60 examples/s]Tokenizing train dataset:   3%|▎         | 201/6690 [00:00<00:14, 442.39 examples/s]Tokenizing train dataset:   4%|▎         | 246/6690 [00:00<00:14, 441.90 examples/s]Tokenizing train dataset:   5%|▍         | 310/6690 [00:00<00:14, 429.42 examples/s]Tokenizing train dataset:   5%|▌         | 355/6690 [00:00<00:14, 428.87 examples/s]Tokenizing train dataset:   6%|▌         | 404/6690 [00:00<00:14, 437.53 examples/s]Tokenizing train dataset:   7%|▋         | 456/6690 [00:01<00:13, 456.24 examples/s]Tokenizing train dataset:   8%|▊         | 506/6690 [00:01<00:13, 464.33 examples/s]Tokenizing train dataset:   9%|▊         | 579/6690 [00:01<00:13, 467.59 examples/s]Tokenizing train dataset:   9%|▉         | 626/6690 [00:01<00:13, 462.82 examples/s]Tokenizing train dataset:  10%|█         | 694/6690 [00:01<00:13, 457.69 examples/s]Tokenizing train dataset:  11%|█         | 747/6690 [00:01<00:12, 472.99 examples/s]Tokenizing train dataset:  12%|█▏        | 798/6690 [00:01<00:12, 482.22 examples/s]Tokenizing train dataset:  13%|█▎        | 866/6690 [00:01<00:12, 462.80 examples/s]Tokenizing train dataset:  14%|█▍        | 923/6690 [00:02<00:13, 431.42 examples/s]Tokenizing train dataset:  14%|█▍        | 969/6690 [00:02<00:13, 437.16 examples/s]Tokenizing train dataset:  15%|█▌        | 1014/6690 [00:02<00:13, 435.04 examples/s]Tokenizing train dataset:  16%|█▌        | 1060/6690 [00:02<00:12, 439.57 examples/s]Tokenizing train dataset:  17%|█▋        | 1126/6690 [00:02<00:12, 435.13 examples/s]Tokenizing train dataset:  18%|█▊        | 1192/6690 [00:02<00:12, 433.39 examples/s]Tokenizing train dataset:  19%|█▉        | 1255/6690 [00:02<00:12, 422.43 examples/s]Tokenizing train dataset:  19%|█▉        | 1298/6690 [00:02<00:12, 422.45 examples/s]Tokenizing train dataset:  20%|██        | 1349/6690 [00:03<00:12, 443.45 examples/s]Tokenizing train dataset:  21%|██        | 1409/6690 [00:03<00:12, 422.08 examples/s]Tokenizing train dataset:  22%|██▏       | 1471/6690 [00:03<00:12, 415.14 examples/s]Tokenizing train dataset:  23%|██▎       | 1520/6690 [00:03<00:12, 430.68 examples/s]Tokenizing train dataset:  24%|██▎       | 1584/6690 [00:03<00:11, 425.53 examples/s]Tokenizing train dataset:  24%|██▍       | 1628/6690 [00:03<00:11, 422.50 examples/s]Tokenizing train dataset:  25%|██▌       | 1695/6690 [00:03<00:11, 422.75 examples/s]Tokenizing train dataset:  26%|██▌       | 1741/6690 [00:03<00:11, 428.29 examples/s]Tokenizing train dataset:  27%|██▋       | 1785/6690 [00:04<00:11, 426.99 examples/s]Tokenizing train dataset:  27%|██▋       | 1831/6690 [00:04<00:11, 433.19 examples/s]Tokenizing train dataset:  28%|██▊       | 1898/6690 [00:04<00:11, 433.65 examples/s]Tokenizing train dataset:  29%|██▉       | 1958/6690 [00:04<00:11, 419.57 examples/s]Tokenizing train dataset:  30%|███       | 2012/6690 [00:04<00:11, 396.59 examples/s]Tokenizing train dataset:  31%|███       | 2056/6690 [00:04<00:11, 404.51 examples/s]Tokenizing train dataset:  31%|███▏      | 2100/6690 [00:04<00:11, 407.96 examples/s]Tokenizing train dataset:  32%|███▏      | 2150/6690 [00:04<00:10, 426.43 examples/s]Tokenizing train dataset:  33%|███▎      | 2199/6690 [00:05<00:10, 438.29 examples/s]Tokenizing train dataset:  34%|███▎      | 2247/6690 [00:05<00:09, 446.41 examples/s]Tokenizing train dataset:  35%|███▍      | 2310/6690 [00:05<00:10, 423.37 examples/s]Tokenizing train dataset:  35%|███▌      | 2359/6690 [00:05<00:09, 435.40 examples/s]Tokenizing train dataset:  36%|███▌      | 2423/6690 [00:05<00:10, 426.58 examples/s]Tokenizing train dataset:  37%|███▋      | 2471/6690 [00:05<00:09, 435.66 examples/s]Tokenizing train dataset:  38%|███▊      | 2517/6690 [00:05<00:09, 439.25 examples/s]Tokenizing train dataset:  38%|███▊      | 2566/6690 [00:05<00:09, 451.98 examples/s]Tokenizing train dataset:  39%|███▉      | 2613/6690 [00:06<00:09, 451.08 examples/s]Tokenizing train dataset:  40%|███▉      | 2673/6690 [00:06<00:09, 428.94 examples/s]Tokenizing train dataset:  41%|████      | 2734/6690 [00:06<00:09, 418.77 examples/s]Tokenizing train dataset:  42%|████▏     | 2796/6690 [00:06<00:09, 412.38 examples/s]Tokenizing train dataset:  42%|████▏     | 2838/6690 [00:06<00:09, 409.35 examples/s]Tokenizing train dataset:  43%|████▎     | 2880/6690 [00:06<00:09, 411.42 examples/s]Tokenizing train dataset:  44%|████▍     | 2930/6690 [00:06<00:08, 430.18 examples/s]Tokenizing train dataset:  45%|████▍     | 2980/6690 [00:06<00:08, 443.40 examples/s]Tokenizing train dataset:  45%|████▌     | 3028/6690 [00:06<00:08, 450.70 examples/s]Tokenizing train dataset:  46%|████▌     | 3082/6690 [00:07<00:07, 472.21 examples/s]Tokenizing train dataset:  47%|████▋     | 3146/6690 [00:07<00:07, 451.93 examples/s]Tokenizing train dataset:  48%|████▊     | 3212/6690 [00:07<00:07, 442.38 examples/s]Tokenizing train dataset:  49%|████▊     | 3258/6690 [00:07<00:07, 445.69 examples/s]Tokenizing train dataset:  50%|████▉     | 3315/6690 [00:07<00:08, 417.52 examples/s]Tokenizing train dataset:  50%|█████     | 3360/6690 [00:07<00:07, 418.41 examples/s]Tokenizing train dataset:  51%|█████     | 3416/6690 [00:07<00:08, 400.99 examples/s]Tokenizing train dataset:  52%|█████▏    | 3476/6690 [00:08<00:08, 397.31 examples/s]Tokenizing train dataset:  53%|█████▎    | 3518/6690 [00:08<00:07, 400.84 examples/s]Tokenizing train dataset:  53%|█████▎    | 3571/6690 [00:08<00:08, 383.35 examples/s]Tokenizing train dataset:  54%|█████▍    | 3610/6690 [00:08<00:08, 376.23 examples/s]Tokenizing train dataset:  55%|█████▍    | 3656/6690 [00:08<00:07, 395.82 examples/s]Tokenizing train dataset:  55%|█████▌    | 3697/6690 [00:08<00:07, 398.66 examples/s]Tokenizing train dataset:  56%|█████▌    | 3748/6690 [00:08<00:07, 372.00 examples/s]Tokenizing train dataset:  57%|█████▋    | 3793/6690 [00:08<00:07, 387.66 examples/s]Tokenizing train dataset:  57%|█████▋    | 3835/6690 [00:08<00:07, 394.12 examples/s]Tokenizing train dataset:  58%|█████▊    | 3880/6690 [00:09<00:06, 405.12 examples/s]Tokenizing train dataset:  59%|█████▉    | 3940/6690 [00:09<00:06, 401.30 examples/s]Tokenizing train dataset:  60%|█████▉    | 3987/6690 [00:09<00:06, 415.60 examples/s]Tokenizing train dataset:  60%|██████    | 4031/6690 [00:09<00:06, 418.70 examples/s]Tokenizing train dataset:  61%|██████    | 4093/6690 [00:09<00:06, 412.53 examples/s]Tokenizing train dataset:  62%|██████▏   | 4138/6690 [00:09<00:06, 418.79 examples/s]Tokenizing train dataset:  63%|██████▎   | 4185/6690 [00:09<00:05, 429.71 examples/s]Tokenizing train dataset:  64%|██████▎   | 4251/6690 [00:09<00:05, 429.67 examples/s]Tokenizing train dataset:  65%|██████▍   | 4317/6690 [00:10<00:05, 428.54 examples/s]Tokenizing train dataset:  65%|██████▌   | 4376/6690 [00:10<00:05, 412.57 examples/s]Tokenizing train dataset:  66%|██████▌   | 4419/6690 [00:10<00:05, 415.61 examples/s]Tokenizing train dataset:  67%|██████▋   | 4461/6690 [00:10<00:05, 414.21 examples/s]Tokenizing train dataset:  68%|██████▊   | 4527/6690 [00:10<00:05, 417.10 examples/s]Tokenizing train dataset:  68%|██████▊   | 4570/6690 [00:10<00:05, 418.23 examples/s]Tokenizing train dataset:  69%|██████▉   | 4619/6690 [00:10<00:04, 435.50 examples/s]Tokenizing train dataset:  70%|██████▉   | 4666/6690 [00:10<00:04, 438.97 examples/s]Tokenizing train dataset:  71%|███████   | 4731/6690 [00:11<00:04, 434.65 examples/s]Tokenizing train dataset:  72%|███████▏  | 4789/6690 [00:11<00:04, 413.21 examples/s]Tokenizing train dataset:  72%|███████▏  | 4836/6690 [00:11<00:04, 422.75 examples/s]Tokenizing train dataset:  73%|███████▎  | 4880/6690 [00:11<00:04, 420.34 examples/s]Tokenizing train dataset:  74%|███████▍  | 4940/6690 [00:11<00:04, 410.97 examples/s]Tokenizing train dataset:  74%|███████▍  | 4983/6690 [00:11<00:04, 410.99 examples/s]Tokenizing train dataset:  75%|███████▌  | 5026/6690 [00:11<00:04, 412.18 examples/s]Tokenizing train dataset:  76%|███████▌  | 5068/6690 [00:11<00:03, 411.58 examples/s]Tokenizing train dataset:  77%|███████▋  | 5125/6690 [00:12<00:03, 397.27 examples/s]Tokenizing train dataset:  77%|███████▋  | 5170/6690 [00:12<00:03, 410.20 examples/s]Tokenizing train dataset:  78%|███████▊  | 5217/6690 [00:12<00:03, 421.10 examples/s]Tokenizing train dataset:  79%|███████▉  | 5280/6690 [00:12<00:03, 417.60 examples/s]Tokenizing train dataset:  80%|███████▉  | 5340/6690 [00:12<00:03, 405.79 examples/s]Tokenizing train dataset:  81%|████████  | 5394/6690 [00:12<00:02, 435.63 examples/s]Tokenizing train dataset:  82%|████████▏ | 5455/6690 [00:12<00:02, 423.20 examples/s]Tokenizing train dataset:  82%|████████▏ | 5499/6690 [00:12<00:02, 424.35 examples/s]Tokenizing train dataset:  83%|████████▎ | 5561/6690 [00:13<00:02, 412.74 examples/s]Tokenizing train dataset:  84%|████████▍ | 5622/6690 [00:13<00:02, 406.24 examples/s]Tokenizing train dataset:  85%|████████▍ | 5665/6690 [00:13<00:02, 410.89 examples/s]Tokenizing train dataset:  85%|████████▌ | 5709/6690 [00:13<00:02, 414.64 examples/s]Tokenizing train dataset:  86%|████████▌ | 5757/6690 [00:13<00:02, 429.05 examples/s]Tokenizing train dataset:  87%|████████▋ | 5819/6690 [00:13<00:02, 419.94 examples/s]Tokenizing train dataset:  88%|████████▊ | 5877/6690 [00:13<00:02, 401.92 examples/s]Tokenizing train dataset:  89%|████████▊ | 5926/6690 [00:13<00:01, 418.97 examples/s]Tokenizing train dataset:  89%|████████▉ | 5970/6690 [00:14<00:01, 420.31 examples/s]Tokenizing train dataset:  90%|████████▉ | 6014/6690 [00:14<00:01, 420.91 examples/s]Tokenizing train dataset:  91%|█████████ | 6059/6690 [00:14<00:01, 423.17 examples/s]Tokenizing train dataset:  91%|█████████▏| 6112/6690 [00:14<00:01, 449.00 examples/s]Tokenizing train dataset:  92%|█████████▏| 6176/6690 [00:14<00:01, 437.07 examples/s]Tokenizing train dataset:  93%|█████████▎| 6233/6690 [00:14<00:01, 413.04 examples/s]Tokenizing train dataset:  94%|█████████▍| 6293/6690 [00:14<00:00, 405.71 examples/s]Tokenizing train dataset:  95%|█████████▍| 6352/6690 [00:15<00:00, 400.74 examples/s]Tokenizing train dataset:  96%|█████████▌| 6417/6690 [00:15<00:00, 409.34 examples/s]Tokenizing train dataset:  97%|█████████▋| 6461/6690 [00:15<00:00, 408.52 examples/s]Tokenizing train dataset:  97%|█████████▋| 6505/6690 [00:15<00:00, 415.50 examples/s]Tokenizing train dataset:  98%|█████████▊| 6548/6690 [00:15<00:00, 415.46 examples/s]Tokenizing train dataset:  99%|█████████▊| 6590/6690 [00:15<00:00, 412.20 examples/s]Tokenizing train dataset:  99%|█████████▉| 6647/6690 [00:15<00:00, 397.22 examples/s]Tokenizing train dataset: 100%|█████████▉| 6688/6690 [00:15<00:00, 399.98 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 422.13 examples/s]
[rank12]:[W610 23:55:57.852187377 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 12]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:   9%|▊         | 571/6690 [00:00<00:01, 5653.05 examples/s]Extracting prompt in eval dataset:  60%|██████    | 574/953 [00:00<00:00, 5689.14 examples/s]Extracting prompt in train dataset:   9%|▊         | 580/6690 [00:00<00:01, 5732.91 examples/s]Extracting prompt in train dataset:   9%|▊         | 570/6690 [00:00<00:01, 5602.88 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5655.44 examples/s]
Extracting prompt in train dataset:  17%|█▋        | 1158/6690 [00:00<00:00, 5776.71 examples/s]Extracting prompt in train dataset:  17%|█▋        | 1150/6690 [00:00<00:00, 5694.27 examples/s]Extracting prompt in train dataset:  18%|█▊        | 1180/6690 [00:00<00:00, 5830.76 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1750/6690 [00:00<00:00, 5820.05 examples/s]Extracting prompt in train dataset:  26%|██▌       | 1737/6690 [00:00<00:00, 5770.11 examples/s]Extracting prompt in train dataset:  27%|██▋       | 1790/6690 [00:00<00:00, 5931.72 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2630/6690 [00:00<00:00, 5809.16 examples/s]Extracting prompt in train dataset:  39%|███▉      | 2600/6690 [00:00<00:00, 5737.86 examples/s]Extracting prompt in train dataset:  40%|████      | 2684/6690 [00:00<00:00, 5925.11 examples/s]Extracting prompt in train dataset:  48%|████▊     | 3222/6690 [00:00<00:00, 5843.49 examples/s]Extracting prompt in train dataset:  48%|████▊     | 3189/6690 [00:00<00:00, 5783.54 examples/s]Extracting prompt in train dataset:  49%|████▉     | 3290/6690 [00:00<00:00, 5949.74 examples/s]Extracting prompt in train dataset:  57%|█████▋    | 3818/6690 [00:00<00:00, 5863.40 examples/s]Extracting prompt in train dataset:  56%|█████▋    | 3770/6690 [00:00<00:00, 5773.03 examples/s]Extracting prompt in train dataset:  58%|█████▊    | 3890/6690 [00:00<00:00, 5958.29 examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in train dataset:  65%|██████▌   | 4360/6690 [00:00<00:00, 5796.52 examples/s]Applying chat template to eval dataset:  32%|███▏      | 302/953 [00:00<00:00, 2989.94 examples/s]Extracting prompt in train dataset:  71%|███████   | 4740/6690 [00:00<00:00, 5825.50 examples/s]Extracting prompt in train dataset:  70%|██████▉   | 4680/6690 [00:00<00:00, 5724.78 examples/s]Applying chat template to eval dataset:  65%|██████▍   | 618/953 [00:00<00:00, 3081.61 examples/s]Extracting prompt in train dataset:  78%|███████▊  | 5189/6690 [00:00<00:00, 5678.59 examples/s]Extracting prompt in train dataset:  80%|███████▉  | 5342/6690 [00:00<00:00, 5877.70 examples/s]Extracting prompt in train dataset:  79%|███████▉  | 5280/6690 [00:00<00:00, 5774.97 examples/s]Applying chat template to eval dataset:  98%|█████████▊| 933/953 [00:00<00:00, 3109.48 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3074.20 examples/s]
Extracting prompt in train dataset:  86%|████████▌ | 5767/6690 [00:01<00:00, 5705.23 examples/s]Extracting prompt in train dataset:  89%|████████▉ | 5941/6690 [00:01<00:00, 5908.83 examples/s]Extracting prompt in train dataset:  88%|████████▊ | 5879/6690 [00:01<00:00, 5820.10 examples/s]Extracting prompt in train dataset:  98%|█████████▊| 6546/6690 [00:01<00:00, 5948.47 examples/s]Extracting prompt in train dataset:  95%|█████████▍| 6352/6690 [00:01<00:00, 5730.56 examples/s]Extracting prompt in train dataset:  97%|█████████▋| 6470/6690 [00:01<00:00, 5829.91 examples/s]Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5862.13 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5756.82 examples/s]
Extracting prompt in train dataset: 100%|██████████| 6690/6690 [00:01<00:00, 5686.90 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 32/953 [00:00<00:03, 293.20 examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Applying chat template to train dataset:   4%|▍         | 300/6690 [00:00<00:02, 2959.80 examples/s]Tokenizing eval dataset:   7%|▋         | 70/953 [00:00<00:03, 263.26 examples/s]Applying chat template to train dataset:   4%|▍         | 291/6690 [00:00<00:02, 2872.47 examples/s]Applying chat template to train dataset:   4%|▍         | 290/6690 [00:00<00:02, 2858.95 examples/s]Applying chat template to train dataset:   9%|▉         | 631/6690 [00:00<00:01, 3159.87 examples/s]Tokenizing eval dataset:  10%|█         | 97/953 [00:00<00:03, 262.11 examples/s]Applying chat template to train dataset:   9%|▉         | 611/6690 [00:00<00:01, 3055.94 examples/s]Applying chat template to train dataset:   9%|▉         | 610/6690 [00:00<00:01, 3045.92 examples/s]Applying chat template to train dataset:  14%|█▍        | 965/6690 [00:00<00:01, 3237.41 examples/s]Applying chat template to train dataset:  14%|█▍        | 932/6690 [00:00<00:01, 3122.36 examples/s]Applying chat template to train dataset:  14%|█▍        | 930/6690 [00:00<00:01, 3112.13 examples/s]Tokenizing eval dataset:  14%|█▍        | 136/953 [00:00<00:03, 258.94 examples/s]Applying chat template to train dataset:  19%|█▉        | 1296/6690 [00:00<00:01, 3263.03 examples/s]Applying chat template to train dataset:  19%|█▊        | 1251/6690 [00:00<00:01, 3143.89 examples/s]Applying chat template to train dataset:  19%|█▊        | 1249/6690 [00:00<00:01, 3139.03 examples/s]Applying chat template to train dataset:  24%|██▍       | 1629/6690 [00:00<00:01, 3285.05 examples/s]Tokenizing eval dataset:  18%|█▊        | 170/953 [00:00<00:03, 242.43 examples/s]Applying chat template to train dataset:  23%|██▎       | 1572/6690 [00:00<00:01, 3163.50 examples/s]Applying chat template to train dataset:  23%|██▎       | 1568/6690 [00:00<00:01, 3157.04 examples/s]Applying chat template to train dataset:  29%|██▉       | 1961/6690 [00:00<00:01, 3296.41 examples/s]Applying chat template to train dataset:  28%|██▊       | 1893/6690 [00:00<00:01, 3177.04 examples/s]Applying chat template to train dataset:  28%|██▊       | 1888/6690 [00:00<00:01, 3169.95 examples/s]Tokenizing eval dataset:  22%|██▏       | 207/953 [00:00<00:03, 241.08 examples/s]Applying chat template to train dataset:  33%|███▎      | 2215/6690 [00:00<00:01, 3188.29 examples/s]Applying chat template to train dataset:  33%|███▎      | 2208/6690 [00:00<00:01, 3177.53 examples/s]Applying chat template to train dataset:  37%|███▋      | 2455/6690 [00:00<00:01, 3290.32 examples/s]Tokenizing eval dataset:  27%|██▋       | 253/953 [00:00<00:02, 292.61 examples/s]Applying chat template to train dataset:  42%|████▏     | 2788/6690 [00:00<00:01, 3297.64 examples/s]Applying chat template to train dataset:  40%|████      | 2691/6690 [00:00<00:01, 3174.92 examples/s]Applying chat template to train dataset:  40%|████      | 2680/6690 [00:00<00:01, 3159.47 examples/s]Tokenizing eval dataset:  33%|███▎      | 317/953 [00:01<00:01, 382.74 examples/s]Applying chat template to train dataset:  47%|████▋     | 3120/6690 [00:00<00:01, 3301.59 examples/s]Applying chat template to train dataset:  45%|████▌     | 3012/6690 [00:00<00:01, 3182.17 examples/s]Applying chat template to train dataset:  45%|████▍     | 3000/6690 [00:00<00:01, 3166.18 examples/s]Tokenizing eval dataset:  39%|███▉      | 372/953 [00:01<00:01, 426.53 examples/s]Applying chat template to train dataset:  52%|█████▏    | 3452/6690 [00:01<00:00, 3302.77 examples/s]Applying chat template to train dataset:  50%|████▉     | 3333/6690 [00:01<00:01, 3187.37 examples/s]Applying chat template to train dataset:  50%|████▉     | 3320/6690 [00:01<00:01, 3171.11 examples/s]Tokenizing eval dataset:  46%|████▌     | 438/953 [00:01<00:01, 490.26 examples/s]Applying chat template to train dataset:  57%|█████▋    | 3784/6690 [00:01<00:00, 3304.64 examples/s]Applying chat template to train dataset:  55%|█████▍    | 3653/6690 [00:01<00:00, 3188.32 examples/s]Applying chat template to train dataset:  54%|█████▍    | 3640/6690 [00:01<00:00, 3171.15 examples/s]Tokenizing eval dataset:  53%|█████▎    | 502/953 [00:01<00:00, 530.39 examples/s]Applying chat template to train dataset:  62%|██████▏   | 4118/6690 [00:01<00:00, 3312.22 examples/s]Applying chat template to train dataset:  59%|█████▉    | 3974/6690 [00:01<00:00, 3192.96 examples/s]Applying chat template to train dataset:  59%|█████▉    | 3960/6690 [00:01<00:00, 3173.89 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 549.81 examples/s]Applying chat template to train dataset:  64%|██████▍   | 4295/6690 [00:01<00:00, 3197.20 examples/s]Applying chat template to train dataset:  64%|██████▍   | 4280/6690 [00:01<00:00, 3176.47 examples/s]Tokenizing eval dataset:  66%|██████▌   | 625/953 [00:01<00:00, 571.06 examples/s]Applying chat template to train dataset:  69%|██████▊   | 4585/6690 [00:01<00:00, 3231.87 examples/s]Applying chat template to train dataset:  73%|███████▎  | 4916/6690 [00:01<00:00, 3250.60 examples/s]Tokenizing eval dataset:  72%|███████▏  | 685/953 [00:01<00:00, 576.14 examples/s]Applying chat template to train dataset:  71%|███████   | 4743/6690 [00:01<00:00, 3114.77 examples/s]Applying chat template to train dataset:  71%|███████   | 4727/6690 [00:01<00:00, 3097.59 examples/s]Applying chat template to train dataset:  78%|███████▊  | 5246/6690 [00:01<00:00, 3262.55 examples/s]Applying chat template to train dataset:  76%|███████▌  | 5061/6690 [00:01<00:00, 3130.00 examples/s]Applying chat template to train dataset:  75%|███████▌  | 5043/6690 [00:01<00:00, 3113.83 examples/s]Tokenizing eval dataset:  80%|████████  | 765/953 [00:01<00:00, 551.62 examples/s]Applying chat template to train dataset:  83%|████████▎ | 5579/6690 [00:01<00:00, 3278.04 examples/s]Applying chat template to train dataset:  80%|████████  | 5380/6690 [00:01<00:00, 3145.11 examples/s]Applying chat template to train dataset:  80%|████████  | 5361/6690 [00:01<00:00, 3130.00 examples/s]Applying chat template to train dataset:  88%|████████▊ | 5910/6690 [00:01<00:00, 3283.13 examples/s]Tokenizing eval dataset:  88%|████████▊ | 838/953 [00:01<00:00, 519.88 examples/s]Applying chat template to train dataset:  85%|████████▌ | 5700/6690 [00:01<00:00, 3157.46 examples/s]Applying chat template to train dataset:  85%|████████▍ | 5680/6690 [00:01<00:00, 3143.08 examples/s]Applying chat template to train dataset:  93%|█████████▎| 6242/6690 [00:01<00:00, 3291.77 examples/s]Applying chat template to train dataset:  90%|████████▉ | 6020/6690 [00:01<00:00, 3166.33 examples/s]Applying chat template to train dataset:  90%|████████▉ | 6000/6690 [00:01<00:00, 3152.70 examples/s]Tokenizing eval dataset:  95%|█████████▌| 908/953 [00:02<00:00, 494.35 examples/s]Applying chat template to train dataset:  98%|█████████▊| 6573/6690 [00:02<00:00, 3293.99 examples/s]Applying chat template to train dataset:  95%|█████████▍| 6340/6690 [00:02<00:00, 3169.22 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3266.27 examples/s]
Applying chat template to train dataset:  94%|█████████▍| 6320/6690 [00:02<00:00, 3156.66 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 431.15 examples/s]
Applying chat template to train dataset: 100%|█████████▉| 6660/6690 [00:02<00:00, 3170.03 examples/s]Applying chat template to train dataset:  99%|█████████▉| 6638/6690 [00:02<00:00, 3161.36 examples/s]Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3149.74 examples/s]
Applying chat template to train dataset: 100%|██████████| 6690/6690 [00:02<00:00, 3136.32 examples/s]
Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|          | 37/6690 [00:00<00:18, 353.73 examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   0%|          | 0/6690 [00:00<?, ? examples/s]Tokenizing train dataset:   1%|▏         | 84/6690 [00:00<00:15, 416.95 examples/s]Tokenizing train dataset:   1%|          | 37/6690 [00:00<00:18, 356.43 examples/s]Tokenizing train dataset:   1%|          | 37/6690 [00:00<00:18, 356.81 examples/s]Tokenizing train dataset:   1%|▏         | 86/6690 [00:00<00:15, 421.31 examples/s]Tokenizing train dataset:   1%|▏         | 87/6690 [00:00<00:15, 425.29 examples/s]Tokenizing train dataset:   2%|▏         | 149/6690 [00:00<00:15, 416.49 examples/s]Tokenizing train dataset:   3%|▎         | 200/6690 [00:00<00:14, 441.84 examples/s]Tokenizing train dataset:   2%|▏         | 149/6690 [00:00<00:15, 413.52 examples/s]Tokenizing train dataset:   2%|▏         | 150/6690 [00:00<00:15, 411.19 examples/s]Tokenizing train dataset:   4%|▎         | 247/6690 [00:00<00:14, 445.21 examples/s]Tokenizing train dataset:   3%|▎         | 199/6690 [00:00<00:14, 441.71 examples/s]Tokenizing train dataset:   3%|▎         | 201/6690 [00:00<00:14, 441.44 examples/s]Tokenizing train dataset:   4%|▎         | 247/6690 [00:00<00:14, 443.91 examples/s]Tokenizing train dataset:   5%|▍         | 312/6690 [00:00<00:14, 433.58 examples/s]Tokenizing train dataset:   4%|▍         | 267/6690 [00:00<00:14, 443.85 examples/s]Tokenizing train dataset:   5%|▌         | 356/6690 [00:00<00:14, 433.52 examples/s]Tokenizing train dataset:   5%|▍         | 312/6690 [00:00<00:14, 433.44 examples/s]Tokenizing train dataset:   5%|▍         | 330/6690 [00:00<00:14, 427.07 examples/s]Tokenizing train dataset:   6%|▌         | 405/6690 [00:00<00:14, 444.73 examples/s]Tokenizing train dataset:   5%|▌         | 357/6690 [00:00<00:14, 435.95 examples/s]Tokenizing train dataset:   6%|▌         | 383/6690 [00:00<00:14, 446.59 examples/s]Tokenizing train dataset:   7%|▋         | 456/6690 [00:01<00:13, 461.39 examples/s]Tokenizing train dataset:   6%|▌         | 406/6690 [00:00<00:14, 445.84 examples/s]Tokenizing train dataset:   6%|▋         | 430/6690 [00:00<00:14, 446.44 examples/s]Tokenizing train dataset:   8%|▊         | 506/6690 [00:01<00:13, 467.95 examples/s]Tokenizing train dataset:   7%|▋         | 457/6690 [00:01<00:13, 459.82 examples/s]Tokenizing train dataset:   7%|▋         | 480/6690 [00:01<00:13, 456.35 examples/s]Tokenizing train dataset:   8%|▊         | 508/6690 [00:01<00:13, 468.52 examples/s]Tokenizing train dataset:   9%|▊         | 580/6690 [00:01<00:12, 472.07 examples/s]Tokenizing train dataset:   8%|▊         | 530/6690 [00:01<00:13, 464.98 examples/s]Tokenizing train dataset:   9%|▊         | 579/6690 [00:01<00:12, 470.49 examples/s]Tokenizing train dataset:   9%|▊         | 581/6690 [00:01<00:13, 469.74 examples/s]Tokenizing train dataset:  10%|▉         | 649/6690 [00:01<00:12, 466.57 examples/s]Tokenizing train dataset:   9%|▉         | 630/6690 [00:01<00:12, 468.54 examples/s]Tokenizing train dataset:  10%|▉         | 649/6690 [00:01<00:12, 464.86 examples/s]Tokenizing train dataset:  11%|█         | 721/6690 [00:01<00:12, 468.07 examples/s]Tokenizing train dataset:  12%|█▏        | 771/6690 [00:01<00:12, 473.89 examples/s]Tokenizing train dataset:  10%|█         | 700/6690 [00:01<00:12, 463.41 examples/s]Tokenizing train dataset:  11%|█         | 721/6690 [00:01<00:12, 465.63 examples/s]Tokenizing train dataset:  11%|█         | 752/6690 [00:01<00:12, 476.70 examples/s]Tokenizing train dataset:  12%|█▏        | 822/6690 [00:01<00:12, 477.72 examples/s]Tokenizing train dataset:  12%|█▏        | 771/6690 [00:01<00:12, 471.84 examples/s]Tokenizing train dataset:  12%|█▏        | 805/6690 [00:01<00:12, 487.63 examples/s]Tokenizing train dataset:  12%|█▏        | 822/6690 [00:01<00:12, 475.69 examples/s]Tokenizing train dataset:  13%|█▎        | 887/6690 [00:01<00:12, 458.22 examples/s]Tokenizing train dataset:  13%|█▎        | 870/6690 [00:01<00:12, 465.41 examples/s]Tokenizing train dataset:  13%|█▎        | 886/6690 [00:01<00:12, 454.52 examples/s]Tokenizing train dataset:  14%|█▍        | 949/6690 [00:02<00:13, 440.35 examples/s]Tokenizing train dataset:  14%|█▍        | 932/6690 [00:02<00:13, 439.85 examples/s]Tokenizing train dataset:  14%|█▍        | 950/6690 [00:02<00:13, 439.65 examples/s]Tokenizing train dataset:  15%|█▌        | 1015/6690 [00:02<00:13, 435.75 examples/s]Tokenizing train dataset:  16%|█▌        | 1064/6690 [00:02<00:12, 446.34 examples/s]Tokenizing train dataset:  15%|█▍        | 998/6690 [00:02<00:13, 436.09 examples/s]Tokenizing train dataset:  15%|█▌        | 1014/6690 [00:02<00:13, 434.87 examples/s]Tokenizing train dataset:  16%|█▌        | 1044/6690 [00:02<00:12, 438.88 examples/s]Tokenizing train dataset:  16%|█▌        | 1060/6690 [00:02<00:12, 439.76 examples/s]Tokenizing train dataset:  17%|█▋        | 1128/6690 [00:02<00:12, 434.32 examples/s]Tokenizing train dataset:  16%|█▋        | 1091/6690 [00:02<00:12, 445.54 examples/s]Tokenizing train dataset:  18%|█▊        | 1172/6690 [00:02<00:12, 433.56 examples/s]Tokenizing train dataset:  17%|█▋        | 1126/6690 [00:02<00:12, 436.98 examples/s]Tokenizing train dataset:  17%|█▋        | 1156/6690 [00:02<00:12, 437.53 examples/s]Tokenizing train dataset:  18%|█▊        | 1234/6690 [00:02<00:12, 423.62 examples/s]Tokenizing train dataset:  18%|█▊        | 1193/6690 [00:02<00:12, 435.46 examples/s]Tokenizing train dataset:  18%|█▊        | 1202/6690 [00:02<00:12, 439.19 examples/s]Tokenizing train dataset:  19%|█▉        | 1278/6690 [00:02<00:12, 423.57 examples/s]Tokenizing train dataset:  19%|█▉        | 1255/6690 [00:02<00:12, 425.53 examples/s]Tokenizing train dataset:  20%|█▉        | 1330/6690 [00:02<00:12, 445.87 examples/s]Tokenizing train dataset:  19%|█▉        | 1265/6690 [00:02<00:12, 426.57 examples/s]Tokenizing train dataset:  20%|█▉        | 1311/6690 [00:02<00:12, 433.51 examples/s]Tokenizing train dataset:  20%|█▉        | 1328/6690 [00:02<00:12, 441.62 examples/s]Tokenizing train dataset:  21%|██        | 1390/6690 [00:03<00:12, 429.28 examples/s]Tokenizing train dataset:  20%|██        | 1361/6690 [00:03<00:11, 445.61 examples/s]Tokenizing train dataset:  21%|██        | 1390/6690 [00:03<00:12, 426.63 examples/s]Tokenizing train dataset:  22%|██▏       | 1451/6690 [00:03<00:12, 417.06 examples/s]Tokenizing train dataset:  21%|██        | 1416/6690 [00:03<00:12, 416.14 examples/s]Tokenizing train dataset:  22%|██▏       | 1450/6690 [00:03<00:12, 416.82 examples/s]Tokenizing train dataset:  23%|██▎       | 1524/6690 [00:03<00:11, 434.70 examples/s]Tokenizing train dataset:  22%|██▏       | 1478/6690 [00:03<00:12, 410.03 examples/s]Tokenizing train dataset:  22%|██▏       | 1492/6690 [00:03<00:12, 414.20 examples/s]Tokenizing train dataset:  24%|██▎       | 1588/6690 [00:03<00:11, 429.84 examples/s]Tokenizing train dataset:  23%|██▎       | 1532/6690 [00:03<00:11, 440.03 examples/s]Tokenizing train dataset:  23%|██▎       | 1544/6690 [00:03<00:11, 439.34 examples/s]Tokenizing train dataset:  24%|██▍       | 1595/6690 [00:03<00:11, 429.25 examples/s]Tokenizing train dataset:  25%|██▍       | 1650/6690 [00:03<00:12, 419.30 examples/s]Tokenizing train dataset:  24%|██▍       | 1605/6690 [00:03<00:12, 422.18 examples/s]Tokenizing train dataset:  25%|██▌       | 1695/6690 [00:03<00:11, 424.53 examples/s]Tokenizing train dataset:  25%|██▍       | 1649/6690 [00:03<00:11, 423.03 examples/s]Tokenizing train dataset:  25%|██▍       | 1655/6690 [00:03<00:12, 416.23 examples/s]Tokenizing train dataset:  26%|██▌       | 1741/6690 [00:03<00:11, 431.52 examples/s]Tokenizing train dataset:  25%|██▌       | 1695/6690 [00:03<00:11, 424.98 examples/s]Tokenizing train dataset:  25%|██▌       | 1704/6690 [00:03<00:11, 429.01 examples/s]Tokenizing train dataset:  27%|██▋       | 1785/6690 [00:04<00:11, 430.23 examples/s]Tokenizing train dataset:  26%|██▌       | 1741/6690 [00:03<00:11, 431.88 examples/s]Tokenizing train dataset:  26%|██▌       | 1752/6690 [00:03<00:11, 438.40 examples/s]Tokenizing train dataset:  27%|██▋       | 1832/6690 [00:04<00:11, 437.10 examples/s]Tokenizing train dataset:  27%|██▋       | 1785/6690 [00:04<00:11, 430.14 examples/s]Tokenizing train dataset:  27%|██▋       | 1814/6690 [00:04<00:11, 422.84 examples/s]Tokenizing train dataset:  28%|██▊       | 1899/6690 [00:04<00:10, 436.78 examples/s]Tokenizing train dataset:  27%|██▋       | 1832/6690 [00:04<00:11, 436.10 examples/s]Tokenizing train dataset:  28%|██▊       | 1859/6690 [00:04<00:11, 428.54 examples/s]Tokenizing train dataset:  28%|██▊       | 1898/6690 [00:04<00:10, 436.48 examples/s]Tokenizing train dataset:  29%|██▉       | 1960/6690 [00:04<00:11, 423.55 examples/s]Tokenizing train dataset:  29%|██▊       | 1920/6690 [00:04<00:11, 419.03 examples/s]Tokenizing train dataset:  29%|██▉       | 1958/6690 [00:04<00:11, 420.51 examples/s]Tokenizing train dataset:  29%|██▉       | 1967/6690 [00:04<00:11, 429.25 examples/s]Tokenizing train dataset:  30%|███       | 2018/6690 [00:04<00:11, 403.77 examples/s]Tokenizing train dataset:  31%|███       | 2059/6690 [00:04<00:11, 402.78 examples/s]Tokenizing train dataset:  30%|███       | 2012/6690 [00:04<00:11, 396.20 examples/s]Tokenizing train dataset:  30%|███       | 2023/6690 [00:04<00:11, 408.67 examples/s]Tokenizing train dataset:  31%|███▏      | 2105/6690 [00:04<00:11, 414.68 examples/s]Tokenizing train dataset:  31%|███       | 2056/6690 [00:04<00:11, 404.63 examples/s]Tokenizing train dataset:  31%|███       | 2065/6690 [00:04<00:11, 405.70 examples/s]Tokenizing train dataset:  32%|███▏      | 2154/6690 [00:04<00:10, 430.62 examples/s]Tokenizing train dataset:  31%|███▏      | 2100/6690 [00:04<00:11, 408.08 examples/s]Tokenizing train dataset:  32%|███▏      | 2114/6690 [00:04<00:10, 424.33 examples/s]Tokenizing train dataset:  33%|███▎      | 2202/6690 [00:05<00:10, 441.09 examples/s]Tokenizing train dataset:  32%|███▏      | 2159/6690 [00:04<00:10, 429.77 examples/s]Tokenizing train dataset:  32%|███▏      | 2150/6690 [00:04<00:10, 427.43 examples/s]Tokenizing train dataset:  34%|███▎      | 2250/6690 [00:05<00:10, 442.18 examples/s]Tokenizing train dataset:  33%|███▎      | 2210/6690 [00:05<00:10, 446.72 examples/s]Tokenizing train dataset:  33%|███▎      | 2199/6690 [00:05<00:10, 440.27 examples/s]Tokenizing train dataset:  34%|███▎      | 2247/6690 [00:05<00:09, 449.22 examples/s]Tokenizing train dataset:  35%|███▍      | 2312/6690 [00:05<00:10, 427.36 examples/s]Tokenizing train dataset:  34%|███▍      | 2278/6690 [00:05<00:09, 444.31 examples/s]Tokenizing train dataset:  35%|███▌      | 2360/6690 [00:05<00:09, 438.31 examples/s]Tokenizing train dataset:  35%|███▍      | 2310/6690 [00:05<00:10, 424.74 examples/s]Tokenizing train dataset:  35%|███▍      | 2340/6690 [00:05<00:10, 428.40 examples/s]Tokenizing train dataset:  36%|███▌      | 2423/6690 [00:05<00:09, 428.74 examples/s]Tokenizing train dataset:  35%|███▌      | 2359/6690 [00:05<00:09, 436.80 examples/s]Tokenizing train dataset:  36%|███▌      | 2386/6690 [00:05<00:09, 432.68 examples/s]Tokenizing train dataset:  37%|███▋      | 2471/6690 [00:05<00:09, 437.09 examples/s]Tokenizing train dataset:  36%|███▌      | 2423/6690 [00:05<00:10, 424.66 examples/s]Tokenizing train dataset:  37%|███▋      | 2450/6690 [00:05<00:09, 428.87 examples/s]Tokenizing train dataset:  38%|███▊      | 2516/6690 [00:05<00:09, 438.93 examples/s]Tokenizing train dataset:  37%|███▋      | 2470/6690 [00:05<00:09, 435.24 examples/s]Tokenizing train dataset:  37%|███▋      | 2496/6690 [00:05<00:09, 434.85 examples/s]Tokenizing train dataset:  38%|███▊      | 2566/6690 [00:05<00:09, 454.41 examples/s]Tokenizing train dataset:  38%|███▊      | 2549/6690 [00:05<00:09, 454.19 examples/s]Tokenizing train dataset:  39%|███▉      | 2613/6690 [00:05<00:08, 453.79 examples/s]Tokenizing train dataset:  38%|███▊      | 2541/6690 [00:05<00:09, 445.75 examples/s]Tokenizing train dataset:  39%|███▉      | 2597/6690 [00:05<00:08, 457.92 examples/s]Tokenizing train dataset:  39%|███▊      | 2590/6690 [00:05<00:09, 454.77 examples/s]Tokenizing train dataset:  40%|███▉      | 2673/6690 [00:06<00:09, 430.06 examples/s]Tokenizing train dataset:  40%|███▉      | 2655/6690 [00:06<00:09, 429.37 examples/s]Tokenizing train dataset:  40%|███▉      | 2651/6690 [00:06<00:09, 433.29 examples/s]Tokenizing train dataset:  41%|████      | 2734/6690 [00:06<00:09, 417.42 examples/s]Tokenizing train dataset:  40%|████      | 2700/6690 [00:06<00:09, 428.72 examples/s]Tokenizing train dataset:  41%|████      | 2713/6690 [00:06<00:09, 421.88 examples/s]Tokenizing train dataset:  42%|████▏     | 2796/6690 [00:06<00:09, 409.09 examples/s]Tokenizing train dataset:  41%|████▏     | 2760/6690 [00:06<00:09, 407.67 examples/s]Tokenizing train dataset:  42%|████▏     | 2838/6690 [00:06<00:09, 406.11 examples/s]Tokenizing train dataset:  41%|████▏     | 2772/6690 [00:06<00:09, 407.21 examples/s]Tokenizing train dataset:  42%|████▏     | 2802/6690 [00:06<00:09, 405.91 examples/s]Tokenizing train dataset:  43%|████▎     | 2881/6690 [00:06<00:09, 408.94 examples/s]Tokenizing train dataset:  43%|████▎     | 2844/6690 [00:06<00:09, 404.10 examples/s]Tokenizing train dataset:  42%|████▏     | 2835/6690 [00:06<00:09, 404.33 examples/s]Tokenizing train dataset:  44%|████▍     | 2932/6690 [00:06<00:08, 429.28 examples/s]Tokenizing train dataset:  43%|████▎     | 2888/6690 [00:06<00:09, 410.65 examples/s]Tokenizing train dataset:  43%|████▎     | 2879/6690 [00:06<00:09, 409.72 examples/s]Tokenizing train dataset:  45%|████▍     | 2981/6690 [00:06<00:08, 443.39 examples/s]Tokenizing train dataset:  44%|████▍     | 2936/6690 [00:06<00:08, 428.32 examples/s]Tokenizing train dataset:  44%|████▍     | 2928/6690 [00:06<00:08, 426.04 examples/s]Tokenizing train dataset:  45%|████▌     | 3029/6690 [00:06<00:08, 451.87 examples/s]Tokenizing train dataset:  45%|████▍     | 2985/6690 [00:06<00:08, 442.99 examples/s]Tokenizing train dataset:  45%|████▍     | 2978/6690 [00:06<00:08, 439.95 examples/s]Tokenizing train dataset:  46%|████▌     | 3081/6690 [00:07<00:07, 470.25 examples/s]Tokenizing train dataset:  45%|████▌     | 3031/6690 [00:06<00:08, 445.91 examples/s]Tokenizing train dataset:  45%|████▌     | 3025/6690 [00:06<00:08, 446.06 examples/s]Tokenizing train dataset:  47%|████▋     | 3146/6690 [00:07<00:07, 451.38 examples/s]Tokenizing train dataset:  46%|████▌     | 3088/6690 [00:07<00:07, 471.15 examples/s]Tokenizing train dataset:  46%|████▌     | 3078/6690 [00:07<00:07, 463.44 examples/s]Tokenizing train dataset:  48%|████▊     | 3212/6690 [00:07<00:07, 442.39 examples/s]Tokenizing train dataset:  47%|████▋     | 3141/6690 [00:07<00:07, 445.12 examples/s]Tokenizing train dataset:  47%|████▋     | 3151/6690 [00:07<00:08, 439.68 examples/s]Tokenizing train dataset:  49%|████▊     | 3258/6690 [00:07<00:07, 446.42 examples/s]Tokenizing train dataset:  48%|████▊     | 3196/6690 [00:07<00:07, 440.74 examples/s]Tokenizing train dataset:  48%|████▊     | 3210/6690 [00:07<00:07, 443.09 examples/s]Tokenizing train dataset:  48%|████▊     | 3244/6690 [00:07<00:07, 447.28 examples/s]Tokenizing train dataset:  50%|████▉     | 3315/6690 [00:07<00:08, 417.54 examples/s]Tokenizing train dataset:  49%|████▊     | 3256/6690 [00:07<00:07, 442.68 examples/s]Tokenizing train dataset:  50%|█████     | 3360/6690 [00:07<00:07, 418.31 examples/s]Tokenizing train dataset:  49%|████▉     | 3302/6690 [00:07<00:08, 422.07 examples/s]Tokenizing train dataset:  50%|████▉     | 3313/6690 [00:07<00:08, 413.46 examples/s]Tokenizing train dataset:  51%|█████     | 3416/6690 [00:07<00:08, 400.39 examples/s]Tokenizing train dataset:  50%|█████     | 3363/6690 [00:07<00:08, 409.83 examples/s]Tokenizing train dataset:  50%|█████     | 3356/6690 [00:07<00:08, 414.38 examples/s]Tokenizing train dataset:  52%|█████▏    | 3476/6690 [00:08<00:08, 396.77 examples/s]Tokenizing train dataset:  51%|█████     | 3420/6690 [00:07<00:08, 396.50 examples/s]Tokenizing train dataset:  51%|█████     | 3413/6690 [00:07<00:08, 399.57 examples/s]Tokenizing train dataset:  53%|█████▎    | 3518/6690 [00:08<00:07, 400.75 examples/s]Tokenizing train dataset:  52%|█████▏    | 3484/6690 [00:08<00:07, 402.90 examples/s]Tokenizing train dataset:  52%|█████▏    | 3470/6690 [00:08<00:08, 387.99 examples/s]Tokenizing train dataset:  53%|█████▎    | 3571/6690 [00:08<00:08, 383.00 examples/s]Tokenizing train dataset:  53%|█████▎    | 3515/6690 [00:08<00:07, 398.09 examples/s]Tokenizing train dataset:  53%|█████▎    | 3541/6690 [00:08<00:08, 392.30 examples/s]Tokenizing train dataset:  54%|█████▍    | 3610/6690 [00:08<00:08, 375.28 examples/s]Tokenizing train dataset:  53%|█████▎    | 3570/6690 [00:08<00:08, 383.80 examples/s]Tokenizing train dataset:  55%|█████▍    | 3656/6690 [00:08<00:07, 395.35 examples/s]Tokenizing train dataset:  54%|█████▎    | 3593/6690 [00:08<00:08, 375.24 examples/s]Tokenizing train dataset:  55%|█████▌    | 3697/6690 [00:08<00:07, 397.87 examples/s]Tokenizing train dataset:  54%|█████▍    | 3640/6690 [00:08<00:07, 392.69 examples/s]Tokenizing train dataset:  54%|█████▍    | 3630/6690 [00:08<00:07, 384.66 examples/s]Tokenizing train dataset:  55%|█████▍    | 3671/6690 [00:08<00:07, 387.49 examples/s]Tokenizing train dataset:  56%|█████▌    | 3747/6690 [00:08<00:07, 370.70 examples/s]Tokenizing train dataset:  55%|█████▌    | 3700/6690 [00:08<00:07, 379.68 examples/s]Tokenizing train dataset:  57%|█████▋    | 3793/6690 [00:08<00:07, 386.64 examples/s]Tokenizing train dataset:  56%|█████▌    | 3722/6690 [00:08<00:08, 370.10 examples/s]Tokenizing train dataset:  56%|█████▌    | 3756/6690 [00:08<00:07, 376.44 examples/s]Tokenizing train dataset:  57%|█████▋    | 3835/6690 [00:08<00:07, 393.30 examples/s]Tokenizing train dataset:  56%|█████▋    | 3764/6690 [00:08<00:07, 380.99 examples/s]Tokenizing train dataset:  57%|█████▋    | 3798/6690 [00:08<00:07, 385.96 examples/s]Tokenizing train dataset:  58%|█████▊    | 3880/6690 [00:09<00:06, 404.37 examples/s]Tokenizing train dataset:  57%|█████▋    | 3805/6690 [00:08<00:07, 387.39 examples/s]Tokenizing train dataset:  57%|█████▋    | 3843/6690 [00:08<00:07, 396.88 examples/s]Tokenizing train dataset:  59%|█████▊    | 3926/6690 [00:09<00:06, 418.83 examples/s]Tokenizing train dataset:  58%|█████▊    | 3850/6690 [00:09<00:07, 400.24 examples/s]Tokenizing train dataset:  58%|█████▊    | 3890/6690 [00:09<00:06, 414.16 examples/s]Tokenizing train dataset:  59%|█████▉    | 3972/6690 [00:09<00:06, 426.49 examples/s]Tokenizing train dataset:  58%|█████▊    | 3898/6690 [00:09<00:06, 418.66 examples/s]Tokenizing train dataset:  59%|█████▉    | 3936/6690 [00:09<00:06, 425.35 examples/s]Tokenizing train dataset:  60%|██████    | 4020/6690 [00:09<00:06, 438.13 examples/s]Tokenizing train dataset:  59%|█████▉    | 3947/6690 [00:09<00:06, 435.69 examples/s]Tokenizing train dataset:  60%|█████▉    | 3981/6690 [00:09<00:06, 429.62 examples/s]Tokenizing train dataset:  61%|██████    | 4078/6690 [00:09<00:06, 415.44 examples/s]Tokenizing train dataset:  60%|██████    | 4016/6690 [00:09<00:06, 438.77 examples/s]Tokenizing train dataset:  60%|██████    | 4029/6690 [00:09<00:06, 438.97 examples/s]Tokenizing train dataset:  62%|██████▏   | 4125/6690 [00:09<00:06, 427.18 examples/s]Tokenizing train dataset:  61%|██████    | 4076/6690 [00:09<00:06, 419.08 examples/s]Tokenizing train dataset:  61%|██████    | 4088/6690 [00:09<00:06, 419.10 examples/s]Tokenizing train dataset:  62%|██████▏   | 4172/6690 [00:09<00:05, 435.70 examples/s]Tokenizing train dataset:  62%|██████▏   | 4121/6690 [00:09<00:06, 423.43 examples/s]Tokenizing train dataset:  62%|██████▏   | 4134/6690 [00:09<00:06, 425.70 examples/s]Tokenizing train dataset:  63%|██████▎   | 4239/6690 [00:09<00:05, 435.52 examples/s]Tokenizing train dataset:  62%|██████▏   | 4181/6690 [00:09<00:05, 433.76 examples/s]Tokenizing train dataset:  62%|██████▏   | 4170/6690 [00:09<00:05, 435.26 examples/s]Tokenizing train dataset:  63%|██████▎   | 4226/6690 [00:09<00:05, 433.02 examples/s]Tokenizing train dataset:  63%|██████▎   | 4215/6690 [00:09<00:05, 434.66 examples/s]Tokenizing train dataset:  64%|██████▍   | 4302/6690 [00:10<00:05, 428.34 examples/s]Tokenizing train dataset:  65%|██████▍   | 4346/6690 [00:10<00:05, 427.60 examples/s]Tokenizing train dataset:  64%|██████▍   | 4289/6690 [00:10<00:05, 424.16 examples/s]Tokenizing train dataset:  64%|██████▍   | 4278/6690 [00:10<00:05, 425.30 examples/s]Tokenizing train dataset:  65%|██████▍   | 4325/6690 [00:10<00:05, 434.37 examples/s]Tokenizing train dataset:  65%|██████▍   | 4337/6690 [00:10<00:05, 436.05 examples/s]Tokenizing train dataset:  66%|██████▌   | 4403/6690 [00:10<00:05, 406.19 examples/s]Tokenizing train dataset:  67%|██████▋   | 4450/6690 [00:10<00:05, 418.73 examples/s]Tokenizing train dataset:  65%|██████▌   | 4380/6690 [00:10<00:05, 407.86 examples/s]Tokenizing train dataset:  66%|██████▌   | 4393/6690 [00:10<00:05, 408.19 examples/s]Tokenizing train dataset:  66%|██████▌   | 4425/6690 [00:10<00:05, 412.49 examples/s]Tokenizing train dataset:  66%|██████▋   | 4439/6690 [00:10<00:05, 417.14 examples/s]Tokenizing train dataset:  67%|██████▋   | 4510/6690 [00:10<00:05, 407.15 examples/s]Tokenizing train dataset:  68%|██████▊   | 4557/6690 [00:10<00:05, 419.93 examples/s]Tokenizing train dataset:  67%|██████▋   | 4498/6690 [00:10<00:05, 405.87 examples/s]Tokenizing train dataset:  67%|██████▋   | 4489/6690 [00:10<00:05, 411.80 examples/s]Tokenizing train dataset:  69%|██████▉   | 4600/6690 [00:10<00:04, 418.12 examples/s]Tokenizing train dataset:  68%|██████▊   | 4545/6690 [00:10<00:05, 420.62 examples/s]Tokenizing train dataset:  68%|██████▊   | 4533/6690 [00:10<00:05, 413.73 examples/s]Tokenizing train dataset:  70%|██████▉   | 4650/6690 [00:10<00:04, 436.73 examples/s]Tokenizing train dataset:  68%|██████▊   | 4580/6690 [00:10<00:05, 421.84 examples/s]Tokenizing train dataset:  69%|██████▉   | 4610/6690 [00:10<00:04, 422.05 examples/s]Tokenizing train dataset:  70%|███████   | 4695/6690 [00:10<00:04, 435.55 examples/s]Tokenizing train dataset:  69%|██████▉   | 4628/6690 [00:10<00:04, 433.39 examples/s]Tokenizing train dataset:  70%|██████▉   | 4661/6690 [00:10<00:04, 439.03 examples/s]Tokenizing train dataset:  71%|███████   | 4743/6690 [00:11<00:04, 445.50 examples/s]Tokenizing train dataset:  70%|██████▉   | 4674/6690 [00:10<00:04, 436.28 examples/s]Tokenizing train dataset:  71%|███████   | 4728/6690 [00:11<00:04, 436.36 examples/s]Tokenizing train dataset:  72%|███████▏  | 4797/6690 [00:11<00:04, 407.50 examples/s]Tokenizing train dataset:  71%|███████   | 4743/6690 [00:11<00:04, 442.73 examples/s]Tokenizing train dataset:  72%|███████▏  | 4845/6690 [00:11<00:04, 419.23 examples/s]Tokenizing train dataset:  72%|███████▏  | 4786/6690 [00:11<00:04, 412.16 examples/s]Tokenizing train dataset:  72%|███████▏  | 4797/6690 [00:11<00:04, 409.16 examples/s]Tokenizing train dataset:  72%|███████▏  | 4831/6690 [00:11<00:04, 419.11 examples/s]Tokenizing train dataset:  73%|███████▎  | 4905/6690 [00:11<00:04, 411.65 examples/s]Tokenizing train dataset:  72%|███████▏  | 4845/6690 [00:11<00:04, 420.24 examples/s]Tokenizing train dataset:  73%|███████▎  | 4877/6690 [00:11<00:04, 425.16 examples/s]Tokenizing train dataset:  74%|███████▍  | 4949/6690 [00:11<00:04, 415.95 examples/s]Tokenizing train dataset:  73%|███████▎  | 4888/6690 [00:11<00:04, 417.48 examples/s]Tokenizing train dataset:  74%|███████▍  | 4937/6690 [00:11<00:04, 410.88 examples/s]Tokenizing train dataset:  75%|███████▍  | 5009/6690 [00:11<00:04, 405.86 examples/s]Tokenizing train dataset:  74%|███████▍  | 4952/6690 [00:11<00:04, 412.44 examples/s]Tokenizing train dataset:  74%|███████▍  | 4979/6690 [00:11<00:04, 409.82 examples/s]Tokenizing train dataset:  75%|███████▍  | 4995/6690 [00:11<00:04, 410.99 examples/s]Tokenizing train dataset:  76%|███████▌  | 5071/6690 [00:11<00:04, 402.94 examples/s]Tokenizing train dataset:  75%|███████▌  | 5022/6690 [00:11<00:04, 410.10 examples/s]Tokenizing train dataset:  76%|███████▌  | 5056/6690 [00:11<00:04, 406.42 examples/s]Tokenizing train dataset:  77%|███████▋  | 5130/6690 [00:12<00:03, 395.37 examples/s]Tokenizing train dataset:  76%|███████▌  | 5082/6690 [00:11<00:03, 403.60 examples/s]Tokenizing train dataset:  77%|███████▋  | 5179/6690 [00:12<00:03, 415.09 examples/s]Tokenizing train dataset:  76%|███████▋  | 5116/6690 [00:12<00:03, 400.17 examples/s]Tokenizing train dataset:  77%|███████▋  | 5144/6690 [00:12<00:03, 401.25 examples/s]Tokenizing train dataset:  77%|███████▋  | 5159/6690 [00:12<00:03, 405.66 examples/s]Tokenizing train dataset:  78%|███████▊  | 5241/6690 [00:12<00:03, 410.36 examples/s]Tokenizing train dataset:  78%|███████▊  | 5194/6690 [00:12<00:03, 420.27 examples/s]Tokenizing train dataset:  78%|███████▊  | 5207/6690 [00:12<00:03, 421.23 examples/s]Tokenizing train dataset:  79%|███████▉  | 5285/6690 [00:12<00:03, 415.79 examples/s]Tokenizing train dataset:  79%|███████▊  | 5257/6690 [00:12<00:03, 416.06 examples/s]Tokenizing train dataset:  79%|███████▉  | 5270/6690 [00:12<00:03, 414.48 examples/s]Tokenizing train dataset:  80%|███████▉  | 5343/6690 [00:12<00:03, 402.62 examples/s]Tokenizing train dataset:  79%|███████▉  | 5315/6690 [00:12<00:03, 404.87 examples/s]Tokenizing train dataset:  81%|████████  | 5397/6690 [00:12<00:03, 428.56 examples/s]Tokenizing train dataset:  80%|███████▉  | 5330/6690 [00:12<00:03, 403.32 examples/s]Tokenizing train dataset:  80%|████████  | 5363/6690 [00:12<00:03, 419.11 examples/s]Tokenizing train dataset:  80%|████████  | 5382/6690 [00:12<00:03, 428.34 examples/s]Tokenizing train dataset:  82%|████████▏ | 5460/6690 [00:12<00:02, 421.72 examples/s]Tokenizing train dataset:  81%|████████  | 5413/6690 [00:12<00:02, 431.35 examples/s]Tokenizing train dataset:  81%|████████▏ | 5443/6690 [00:12<00:02, 418.74 examples/s]Tokenizing train dataset:  83%|████████▎ | 5522/6690 [00:12<00:02, 416.21 examples/s]Tokenizing train dataset:  82%|████████▏ | 5471/6690 [00:12<00:02, 413.77 examples/s]Tokenizing train dataset:  82%|████████▏ | 5490/6690 [00:12<00:02, 425.34 examples/s]Tokenizing train dataset:  82%|████████▏ | 5518/6690 [00:12<00:02, 422.77 examples/s]Tokenizing train dataset:  83%|████████▎ | 5582/6690 [00:13<00:02, 409.97 examples/s]Tokenizing train dataset:  83%|████████▎ | 5550/6690 [00:13<00:02, 413.81 examples/s]Tokenizing train dataset:  83%|████████▎ | 5579/6690 [00:13<00:02, 413.57 examples/s]Tokenizing train dataset:  84%|████████▍ | 5646/6690 [00:13<00:02, 410.92 examples/s]Tokenizing train dataset:  84%|████████▍ | 5613/6690 [00:13<00:02, 408.87 examples/s]Tokenizing train dataset:  84%|████████▍ | 5642/6690 [00:13<00:02, 409.38 examples/s]Tokenizing train dataset:  85%|████████▌ | 5708/6690 [00:13<00:02, 409.65 examples/s]Tokenizing train dataset:  85%|████████▍ | 5684/6690 [00:13<00:02, 405.79 examples/s]Tokenizing train dataset:  85%|████████▍ | 5679/6690 [00:13<00:02, 413.06 examples/s]Tokenizing train dataset:  86%|████████▌ | 5757/6690 [00:13<00:02, 423.04 examples/s]Tokenizing train dataset:  86%|████████▌ | 5729/6690 [00:13<00:02, 416.15 examples/s]Tokenizing train dataset:  86%|████████▌ | 5746/6690 [00:13<00:02, 419.22 examples/s]Tokenizing train dataset:  87%|████████▋ | 5819/6690 [00:13<00:02, 416.25 examples/s]Tokenizing train dataset:  87%|████████▋ | 5796/6690 [00:13<00:02, 418.99 examples/s]Tokenizing train dataset:  87%|████████▋ | 5812/6690 [00:13<00:02, 419.87 examples/s]Tokenizing train dataset:  88%|████████▊ | 5877/6690 [00:13<00:02, 400.44 examples/s]Tokenizing train dataset:  88%|████████▊ | 5854/6690 [00:13<00:02, 404.11 examples/s]Tokenizing train dataset:  89%|████████▊ | 5926/6690 [00:13<00:01, 418.28 examples/s]Tokenizing train dataset:  88%|████████▊ | 5869/6690 [00:13<00:02, 402.97 examples/s]Tokenizing train dataset:  88%|████████▊ | 5902/6690 [00:13<00:01, 419.70 examples/s]Tokenizing train dataset:  89%|████████▉ | 5970/6690 [00:14<00:01, 420.36 examples/s]Tokenizing train dataset:  88%|████████▊ | 5918/6690 [00:13<00:01, 421.54 examples/s]Tokenizing train dataset:  89%|████████▉ | 5946/6690 [00:13<00:01, 421.30 examples/s]Tokenizing train dataset:  90%|████████▉ | 6014/6690 [00:14<00:01, 419.92 examples/s]Tokenizing train dataset:  89%|████████▉ | 5980/6690 [00:14<00:01, 411.96 examples/s]Tokenizing train dataset:  91%|█████████ | 6059/6690 [00:14<00:01, 420.72 examples/s]Tokenizing train dataset:  90%|████████▉ | 6011/6690 [00:14<00:01, 420.80 examples/s]Tokenizing train dataset:  90%|█████████ | 6024/6690 [00:14<00:01, 415.53 examples/s]Tokenizing train dataset:  91%|█████████▏| 6112/6690 [00:14<00:01, 446.12 examples/s]Tokenizing train dataset:  91%|█████████ | 6055/6690 [00:14<00:01, 418.69 examples/s]Tokenizing train dataset:  91%|█████████ | 6071/6690 [00:14<00:01, 425.53 examples/s]Tokenizing train dataset:  91%|█████████ | 6104/6690 [00:14<00:01, 435.58 examples/s]Tokenizing train dataset:  92%|█████████▏| 6176/6690 [00:14<00:01, 434.98 examples/s]Tokenizing train dataset:  92%|█████████▏| 6123/6690 [00:14<00:01, 448.43 examples/s]Tokenizing train dataset:  92%|█████████▏| 6153/6690 [00:14<00:01, 442.52 examples/s]Tokenizing train dataset:  93%|█████████▎| 6233/6690 [00:14<00:01, 411.23 examples/s]Tokenizing train dataset:  92%|█████████▏| 6187/6690 [00:14<00:01, 437.59 examples/s]Tokenizing train dataset:  93%|█████████▎| 6215/6690 [00:14<00:01, 427.64 examples/s]Tokenizing train dataset:  94%|█████████▍| 6293/6690 [00:14<00:00, 402.91 examples/s]Tokenizing train dataset:  93%|█████████▎| 6240/6690 [00:14<00:01, 400.02 examples/s]Tokenizing train dataset:  94%|█████████▎| 6268/6690 [00:14<00:01, 395.86 examples/s]Tokenizing train dataset:  94%|█████████▍| 6285/6690 [00:14<00:00, 407.74 examples/s]Tokenizing train dataset:  95%|█████████▍| 6352/6690 [00:14<00:00, 395.90 examples/s]Tokenizing train dataset:  94%|█████████▍| 6313/6690 [00:14<00:00, 401.92 examples/s]Tokenizing train dataset:  95%|█████████▍| 6327/6690 [00:14<00:00, 404.02 examples/s]Tokenizing train dataset:  96%|█████████▌| 6417/6690 [00:15<00:00, 403.79 examples/s]Tokenizing train dataset:  95%|█████████▌| 6372/6690 [00:15<00:00, 392.62 examples/s]Tokenizing train dataset:  95%|█████████▌| 6384/6690 [00:15<00:00, 388.94 examples/s]Tokenizing train dataset:  97%|█████████▋| 6458/6690 [00:15<00:00, 404.62 examples/s]Tokenizing train dataset:  96%|█████████▌| 6417/6690 [00:15<00:00, 403.48 examples/s]Tokenizing train dataset:  96%|█████████▌| 6432/6690 [00:15<00:00, 408.26 examples/s]Tokenizing train dataset:  97%|█████████▋| 6500/6690 [00:15<00:00, 404.99 examples/s]Tokenizing train dataset:  97%|█████████▋| 6460/6690 [00:15<00:00, 402.94 examples/s]Tokenizing train dataset:  97%|█████████▋| 6474/6690 [00:15<00:00, 408.68 examples/s]Tokenizing train dataset:  98%|█████████▊| 6545/6690 [00:15<00:00, 411.88 examples/s]Tokenizing train dataset:  97%|█████████▋| 6504/6690 [00:15<00:00, 408.27 examples/s]Tokenizing train dataset:  97%|█████████▋| 6518/6690 [00:15<00:00, 411.39 examples/s]Tokenizing train dataset:  98%|█████████▊| 6589/6690 [00:15<00:00, 410.77 examples/s]Tokenizing train dataset:  98%|█████████▊| 6547/6690 [00:15<00:00, 412.78 examples/s]Tokenizing train dataset:  98%|█████████▊| 6589/6690 [00:15<00:00, 409.50 examples/s]Tokenizing train dataset:  98%|█████████▊| 6579/6690 [00:15<00:00, 405.05 examples/s]Tokenizing train dataset:  99%|█████████▉| 6646/6690 [00:15<00:00, 397.16 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 397.48 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 422.57 examples/s]
Tokenizing train dataset:  99%|█████████▉| 6646/6690 [00:15<00:00, 395.53 examples/s]Tokenizing train dataset:  99%|█████████▉| 6637/6690 [00:15<00:00, 396.13 examples/s]Tokenizing train dataset: 100%|█████████▉| 6678/6690 [00:15<00:00, 395.83 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 396.08 examples/s]Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 422.54 examples/s]
Tokenizing train dataset: 100%|██████████| 6690/6690 [00:15<00:00, 421.91 examples/s]
Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Extracting prompt in eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Set up DPO trainer
Extracting prompt in eval dataset:  58%|█████▊    | 556/953 [00:00<00:00, 5512.39 examples/s]Extracting prompt in eval dataset:  59%|█████▉    | 560/953 [00:00<00:00, 5521.01 examples/s]Extracting prompt in eval dataset:  60%|█████▉    | 570/953 [00:00<00:00, 5606.67 examples/s]Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5181.12 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5077.82 examples/s]
Extracting prompt in eval dataset: 100%|██████████| 953/953 [00:00<00:00, 5059.47 examples/s]
Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Applying chat template to eval dataset:  34%|███▎      | 320/953 [00:00<00:00, 3150.33 examples/s]Applying chat template to eval dataset:  31%|███▏      | 299/953 [00:00<00:00, 2958.33 examples/s]Applying chat template to eval dataset:  32%|███▏      | 309/953 [00:00<00:00, 3041.32 examples/s]Applying chat template to eval dataset:  69%|██████▊   | 653/953 [00:00<00:00, 3252.06 examples/s]Applying chat template to eval dataset:  64%|██████▍   | 614/953 [00:00<00:00, 3064.29 examples/s]Applying chat template to eval dataset:  66%|██████▋   | 632/953 [00:00<00:00, 3128.62 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3244.32 examples/s]
Applying chat template to eval dataset:  98%|█████████▊| 932/953 [00:00<00:00, 3115.26 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3161.07 examples/s]Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3122.50 examples/s]
Applying chat template to eval dataset: 100%|██████████| 953/953 [00:00<00:00, 3073.04 examples/s]
Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   0%|          | 0/953 [00:00<?, ? examples/s]Tokenizing eval dataset:   3%|▎         | 32/953 [00:00<00:03, 292.88 examples/s]Tokenizing eval dataset:   3%|▎         | 29/953 [00:00<00:03, 284.27 examples/s]Tokenizing eval dataset:   3%|▎         | 30/953 [00:00<00:03, 289.36 examples/s]Tokenizing eval dataset:   7%|▋         | 70/953 [00:00<00:03, 262.10 examples/s]Tokenizing eval dataset:   7%|▋         | 67/953 [00:00<00:03, 254.98 examples/s]Tokenizing eval dataset:   7%|▋         | 68/953 [00:00<00:03, 252.92 examples/s]Tokenizing eval dataset:  10%|▉         | 93/953 [00:00<00:03, 253.90 examples/s]Tokenizing eval dataset:  10%|▉         | 94/953 [00:00<00:03, 250.01 examples/s]Tokenizing eval dataset:  11%|█▏        | 108/953 [00:00<00:03, 251.25 examples/s]Tokenizing eval dataset:  13%|█▎        | 125/953 [00:00<00:03, 232.59 examples/s]Tokenizing eval dataset:  13%|█▎        | 126/953 [00:00<00:03, 231.31 examples/s]Tokenizing eval dataset:  15%|█▌        | 144/953 [00:00<00:03, 236.40 examples/s]Tokenizing eval dataset:  17%|█▋        | 159/953 [00:00<00:03, 224.98 examples/s]Tokenizing eval dataset:  17%|█▋        | 159/953 [00:00<00:03, 222.35 examples/s]Tokenizing eval dataset:  18%|█▊        | 174/953 [00:00<00:03, 218.62 examples/s]Tokenizing eval dataset:  20%|█▉        | 186/953 [00:00<00:03, 206.38 examples/s]Tokenizing eval dataset:  20%|█▉        | 186/953 [00:00<00:03, 203.81 examples/s]Tokenizing eval dataset:  22%|██▏       | 206/953 [00:00<00:03, 214.49 examples/s]Tokenizing eval dataset:  22%|██▏       | 209/953 [00:00<00:03, 208.78 examples/s]Tokenizing eval dataset:  22%|██▏       | 209/953 [00:00<00:03, 205.94 examples/s]Tokenizing eval dataset:  25%|██▌       | 240/953 [00:01<00:02, 242.27 examples/s]Tokenizing eval dataset:  26%|██▌       | 250/953 [00:01<00:02, 258.15 examples/s]Tokenizing eval dataset:  26%|██▌       | 249/953 [00:01<00:02, 254.95 examples/s]Tokenizing eval dataset:  31%|███▏      | 300/953 [00:01<00:01, 329.01 examples/s]Tokenizing eval dataset:  32%|███▏      | 304/953 [00:01<00:01, 329.54 examples/s]Tokenizing eval dataset:  32%|███▏      | 302/953 [00:01<00:01, 327.19 examples/s]Tokenizing eval dataset:  37%|███▋      | 353/953 [00:01<00:01, 379.73 examples/s]Tokenizing eval dataset:  38%|███▊      | 359/953 [00:01<00:01, 387.59 examples/s]Tokenizing eval dataset:  37%|███▋      | 353/953 [00:01<00:01, 375.68 examples/s]Tokenizing eval dataset:  42%|████▏     | 400/953 [00:01<00:01, 401.53 examples/s]Tokenizing eval dataset:  43%|████▎     | 408/953 [00:01<00:01, 408.05 examples/s]Tokenizing eval dataset:  42%|████▏     | 397/953 [00:01<00:01, 390.57 examples/s]Tokenizing eval dataset:  49%|████▊     | 464/953 [00:01<00:01, 466.25 examples/s]Tokenizing eval dataset:  49%|████▉     | 469/953 [00:01<00:01, 457.76 examples/s]Tokenizing eval dataset:  48%|████▊     | 461/953 [00:01<00:01, 455.88 examples/s]Tokenizing eval dataset:  56%|█████▌    | 536/953 [00:01<00:00, 467.99 examples/s]Tokenizing eval dataset:  55%|█████▍    | 521/953 [00:01<00:00, 468.70 examples/s]Tokenizing eval dataset:  53%|█████▎    | 508/953 [00:01<00:00, 454.05 examples/s]Tokenizing eval dataset:  62%|██████▏   | 593/953 [00:01<00:00, 491.13 examples/s]Tokenizing eval dataset:  60%|██████    | 575/953 [00:01<00:00, 485.76 examples/s]Tokenizing eval dataset:  59%|█████▉    | 562/953 [00:01<00:00, 470.67 examples/s]Tokenizing eval dataset:  68%|██████▊   | 645/953 [00:01<00:00, 498.17 examples/s]Tokenizing eval dataset:  65%|██████▍   | 615/953 [00:01<00:00, 484.82 examples/s]Tokenizing eval dataset:  66%|██████▋   | 633/953 [00:01<00:00, 501.33 examples/s]Tokenizing eval dataset:  73%|███████▎  | 696/953 [00:01<00:00, 494.99 examples/s]Tokenizing eval dataset:  72%|███████▏  | 684/953 [00:01<00:00, 502.82 examples/s]Tokenizing eval dataset:  70%|██████▉   | 666/953 [00:01<00:00, 489.24 examples/s]Tokenizing eval dataset:  80%|███████▉  | 762/953 [00:02<00:00, 473.54 examples/s]Tokenizing eval dataset:  78%|███████▊  | 744/953 [00:02<00:00, 460.72 examples/s]Tokenizing eval dataset:  77%|███████▋  | 733/953 [00:02<00:00, 469.50 examples/s]Tokenizing eval dataset:  87%|████████▋ | 826/953 [00:02<00:00, 441.97 examples/s]Tokenizing eval dataset:  85%|████████▍ | 807/953 [00:02<00:00, 443.22 examples/s]Tokenizing eval dataset:  83%|████████▎ | 795/953 [00:02<00:00, 444.98 examples/s]Tokenizing eval dataset:  93%|█████████▎| 886/953 [00:02<00:00, 427.35 examples/s]Tokenizing eval dataset:  89%|████████▉ | 852/953 [00:02<00:00, 421.65 examples/s]Tokenizing eval dataset:  91%|█████████ | 869/953 [00:02<00:00, 427.41 examples/s]Tokenizing eval dataset:  99%|█████████▉| 945/953 [00:02<00:00, 415.14 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 378.48 examples/s]
Tokenizing eval dataset:  95%|█████████▌| 910/953 [00:02<00:00, 406.73 examples/s]Tokenizing eval dataset:  98%|█████████▊| 931/953 [00:02<00:00, 419.01 examples/s]No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 372.78 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 401.07 examples/s]Tokenizing eval dataset: 100%|██████████| 953/953 [00:02<00:00, 365.96 examples/s]
No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.495809555053711 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.34212327003479 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.3314852714538574 seconds
Installed CUDA version 12.1 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination
Using /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...
Emitting ninja build file /ceph/hpc/home/dv70648/.cache/torch_extensions/py310_cu124/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module cpu_adam...
Time to load cpu_adam op: 2.321575403213501 seconds
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training complete
Saving model
[rank12]:[W611 01:23:25.705193129 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
--- Script finished on Node Rank: 3 ---
